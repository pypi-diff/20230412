# Comparing `tmp/praxis-0.4.0-py3-none-any.whl.zip` & `tmp/praxis-1.0.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,150 +1,150 @@
-Zip file size: 595577 bytes, number of entries: 148
--rw-r--r--  2.0 unx      596 b- defN 23-Mar-31 06:07 praxis/__init__.py
--rw-r--r--  2.0 unx    22146 b- defN 23-Mar-31 06:07 praxis/asserts.py
--rw-r--r--  2.0 unx    11203 b- defN 23-Mar-31 06:07 praxis/asserts_test.py
--rw-r--r--  2.0 unx    43875 b- defN 23-Mar-31 06:07 praxis/base_hyperparams.py
--rw-r--r--  2.0 unx    26956 b- defN 23-Mar-31 06:07 praxis/base_hyperparams_test.py
--rw-r--r--  2.0 unx    39227 b- defN 23-Mar-31 06:07 praxis/base_input.py
--rw-r--r--  2.0 unx    27138 b- defN 23-Mar-31 06:07 praxis/base_input_test.py
--rw-r--r--  2.0 unx    92077 b- defN 23-Mar-31 06:07 praxis/base_layer.py
--rw-r--r--  2.0 unx    25465 b- defN 23-Mar-31 06:07 praxis/base_layer_test.py
--rw-r--r--  2.0 unx     6216 b- defN 23-Mar-31 06:07 praxis/base_model.py
--rw-r--r--  2.0 unx    14137 b- defN 23-Mar-31 06:07 praxis/beam_search.py
--rw-r--r--  2.0 unx     7747 b- defN 23-Mar-31 06:07 praxis/beam_search_test.py
--rw-r--r--  2.0 unx     4225 b- defN 23-Mar-31 06:07 praxis/decoder_hparams.py
--rw-r--r--  2.0 unx    16851 b- defN 23-Mar-31 06:07 praxis/decoder_utils.py
--rw-r--r--  2.0 unx     6952 b- defN 23-Mar-31 06:07 praxis/decoder_utils_test.py
--rw-r--r--  2.0 unx     1475 b- defN 23-Mar-31 06:07 praxis/fiddle_tags.py
--rw-r--r--  2.0 unx    11843 b- defN 23-Mar-31 06:07 praxis/flat_beam_search.py
--rw-r--r--  2.0 unx     4451 b- defN 23-Mar-31 06:07 praxis/flat_beam_search_test.py
--rw-r--r--  2.0 unx     5383 b- defN 23-Mar-31 06:07 praxis/flax_utils.py
--rw-r--r--  2.0 unx    36078 b- defN 23-Mar-31 06:07 praxis/gshard_utils.py
--rw-r--r--  2.0 unx     1430 b- defN 23-Mar-31 06:07 praxis/lingvo_lib.py
--rw-r--r--  2.0 unx     2714 b- defN 23-Mar-31 06:07 praxis/metric_utils.py
--rw-r--r--  2.0 unx    13717 b- defN 23-Mar-31 06:07 praxis/optimizer_prefix_vectorization.py
--rw-r--r--  2.0 unx   109219 b- defN 23-Mar-31 06:07 praxis/optimizers.py
--rw-r--r--  2.0 unx     2965 b- defN 23-Mar-31 06:07 praxis/optimizers_test.py
--rw-r--r--  2.0 unx    17859 b- defN 23-Mar-31 06:07 praxis/pax_fiddle.py
--rw-r--r--  2.0 unx    23579 b- defN 23-Mar-31 06:07 praxis/pax_fiddle_test.py
--rw-r--r--  2.0 unx    33768 b- defN 23-Mar-31 06:07 praxis/py_utils.py
--rw-r--r--  2.0 unx    16188 b- defN 23-Mar-31 06:07 praxis/py_utils_test.py
--rw-r--r--  2.0 unx     2626 b- defN 23-Mar-31 06:07 praxis/pytypes.py
--rw-r--r--  2.0 unx    50131 b- defN 23-Mar-31 06:07 praxis/sample_decode.py
--rw-r--r--  2.0 unx    15878 b- defN 23-Mar-31 06:07 praxis/sample_decode_test.py
--rw-r--r--  2.0 unx    21985 b- defN 23-Mar-31 06:07 praxis/schedules.py
--rw-r--r--  2.0 unx    21444 b- defN 23-Mar-31 06:07 praxis/schedules_test.py
--rw-r--r--  2.0 unx    18096 b- defN 23-Mar-31 06:07 praxis/test_utils.py
--rw-r--r--  2.0 unx     6472 b- defN 23-Mar-31 06:07 praxis/layers/__init__.py
--rw-r--r--  2.0 unx     4918 b- defN 23-Mar-31 06:07 praxis/layers/activations.py
--rw-r--r--  2.0 unx     1856 b- defN 23-Mar-31 06:07 praxis/layers/activations_test.py
--rw-r--r--  2.0 unx     8430 b- defN 23-Mar-31 06:07 praxis/layers/adapters.py
--rw-r--r--  2.0 unx     6737 b- defN 23-Mar-31 06:07 praxis/layers/adapters_test.py
--rw-r--r--  2.0 unx   120079 b- defN 23-Mar-31 06:07 praxis/layers/attentions.py
--rw-r--r--  2.0 unx    64759 b- defN 23-Mar-31 06:07 praxis/layers/attentions_test.py
--rw-r--r--  2.0 unx     7235 b- defN 23-Mar-31 06:07 praxis/layers/augmentations.py
--rw-r--r--  2.0 unx     6401 b- defN 23-Mar-31 06:07 praxis/layers/augmentations_test.py
--rw-r--r--  2.0 unx    12718 b- defN 23-Mar-31 06:07 praxis/layers/bregman.py
--rw-r--r--  2.0 unx     5218 b- defN 23-Mar-31 06:07 praxis/layers/bregman_test.py
--rw-r--r--  2.0 unx     3489 b- defN 23-Mar-31 06:07 praxis/layers/checkpoint_policy.py
--rw-r--r--  2.0 unx    17684 b- defN 23-Mar-31 06:07 praxis/layers/conformers.py
--rw-r--r--  2.0 unx    11236 b- defN 23-Mar-31 06:07 praxis/layers/conformers_test.py
--rw-r--r--  2.0 unx    33899 b- defN 23-Mar-31 06:07 praxis/layers/convolutions.py
--rw-r--r--  2.0 unx    15531 b- defN 23-Mar-31 06:07 praxis/layers/convolutions_test.py
--rw-r--r--  2.0 unx    10522 b- defN 23-Mar-31 06:07 praxis/layers/ctc_objectives.py
--rw-r--r--  2.0 unx     6728 b- defN 23-Mar-31 06:07 praxis/layers/ctc_objectives_test.py
--rw-r--r--  2.0 unx    41550 b- defN 23-Mar-31 06:07 praxis/layers/embedding_softmax.py
--rw-r--r--  2.0 unx    34914 b- defN 23-Mar-31 06:07 praxis/layers/embedding_softmax_test.py
--rw-r--r--  2.0 unx     5920 b- defN 23-Mar-31 06:07 praxis/layers/flax_adapter.py
--rw-r--r--  2.0 unx    10856 b- defN 23-Mar-31 06:07 praxis/layers/flax_adapter_test.py
--rw-r--r--  2.0 unx    11266 b- defN 23-Mar-31 06:07 praxis/layers/frnn.py
--rw-r--r--  2.0 unx     7873 b- defN 23-Mar-31 06:07 praxis/layers/frnn_test.py
--rw-r--r--  2.0 unx    13773 b- defN 23-Mar-31 06:07 praxis/layers/glam.py
--rw-r--r--  2.0 unx     6718 b- defN 23-Mar-31 06:07 praxis/layers/gpu_fast_attention.py
--rw-r--r--  2.0 unx    17664 b- defN 23-Mar-31 06:07 praxis/layers/linears.py
--rw-r--r--  2.0 unx    18668 b- defN 23-Mar-31 06:07 praxis/layers/linears_test.py
--rw-r--r--  2.0 unx     3826 b- defN 23-Mar-31 06:07 praxis/layers/losses.py
--rw-r--r--  2.0 unx    53946 b- defN 23-Mar-31 06:07 praxis/layers/models.py
--rw-r--r--  2.0 unx    38885 b- defN 23-Mar-31 06:07 praxis/layers/models_test.py
--rw-r--r--  2.0 unx    54126 b- defN 23-Mar-31 06:07 praxis/layers/multi_query_attention.py
--rw-r--r--  2.0 unx    13686 b- defN 23-Mar-31 06:07 praxis/layers/multi_query_attention_test.py
--rw-r--r--  2.0 unx    41473 b- defN 23-Mar-31 06:07 praxis/layers/ngrammer.py
--rw-r--r--  2.0 unx    26471 b- defN 23-Mar-31 06:07 praxis/layers/ngrammer_test.py
--rw-r--r--  2.0 unx    21164 b- defN 23-Mar-31 06:07 praxis/layers/normalizations.py
--rw-r--r--  2.0 unx    14358 b- defN 23-Mar-31 06:07 praxis/layers/normalizations_test.py
--rw-r--r--  2.0 unx    48295 b- defN 23-Mar-31 06:07 praxis/layers/pipeline.py
--rw-r--r--  2.0 unx     9724 b- defN 23-Mar-31 06:07 praxis/layers/poolings.py
--rw-r--r--  2.0 unx     7555 b- defN 23-Mar-31 06:07 praxis/layers/poolings_test.py
--rw-r--r--  2.0 unx    16380 b- defN 23-Mar-31 06:07 praxis/layers/quantizer.py
--rw-r--r--  2.0 unx     4134 b- defN 23-Mar-31 06:07 praxis/layers/quantizer_objectives.py
--rw-r--r--  2.0 unx     1525 b- defN 23-Mar-31 06:07 praxis/layers/quantizer_objectives_test.py
--rw-r--r--  2.0 unx     4351 b- defN 23-Mar-31 06:07 praxis/layers/quantizer_test.py
--rw-r--r--  2.0 unx    21330 b- defN 23-Mar-31 06:07 praxis/layers/repeats.py
--rw-r--r--  2.0 unx    15886 b- defN 23-Mar-31 06:07 praxis/layers/repeats_test.py
--rw-r--r--  2.0 unx    15684 b- defN 23-Mar-31 06:07 praxis/layers/resnets.py
--rw-r--r--  2.0 unx    18926 b- defN 23-Mar-31 06:07 praxis/layers/rnn_cell.py
--rw-r--r--  2.0 unx     5094 b- defN 23-Mar-31 06:07 praxis/layers/rnn_cell_test.py
--rw-r--r--  2.0 unx     1289 b- defN 23-Mar-31 06:07 praxis/layers/sequential.py
--rw-r--r--  2.0 unx     3167 b- defN 23-Mar-31 06:07 praxis/layers/sequential_test.py
--rw-r--r--  2.0 unx    10129 b- defN 23-Mar-31 06:07 praxis/layers/shared_layers_test.py
--rw-r--r--  2.0 unx    10152 b- defN 23-Mar-31 06:07 praxis/layers/spectrum_augmenter.py
--rw-r--r--  2.0 unx     6714 b- defN 23-Mar-31 06:07 praxis/layers/spectrum_augmenter_test.py
--rw-r--r--  2.0 unx     8409 b- defN 23-Mar-31 06:07 praxis/layers/ssm.py
--rw-r--r--  2.0 unx     2963 b- defN 23-Mar-31 06:07 praxis/layers/ssm_test.py
--rw-r--r--  2.0 unx    27036 b- defN 23-Mar-31 06:07 praxis/layers/ssm_transformers.py
--rw-r--r--  2.0 unx     5729 b- defN 23-Mar-31 06:07 praxis/layers/ssm_transformers_test.py
--rw-r--r--  2.0 unx     1587 b- defN 23-Mar-31 06:07 praxis/layers/stats.py
--rw-r--r--  2.0 unx     1258 b- defN 23-Mar-31 06:07 praxis/layers/stats_test.py
--rw-r--r--  2.0 unx     5184 b- defN 23-Mar-31 06:07 praxis/layers/stochastics.py
--rw-r--r--  2.0 unx     4517 b- defN 23-Mar-31 06:07 praxis/layers/stochastics_test.py
--rw-r--r--  2.0 unx     7874 b- defN 23-Mar-31 06:07 praxis/layers/test_layers.py
--rw-r--r--  2.0 unx    71267 b- defN 23-Mar-31 06:07 praxis/layers/transformer_models.py
--rw-r--r--  2.0 unx     9529 b- defN 23-Mar-31 06:07 praxis/layers/transformer_models_encoder_decoder_test.py
--rw-r--r--  2.0 unx    49372 b- defN 23-Mar-31 06:07 praxis/layers/transformer_models_test.py
--rw-r--r--  2.0 unx    90052 b- defN 23-Mar-31 06:07 praxis/layers/transformers.py
--rw-r--r--  2.0 unx    60292 b- defN 23-Mar-31 06:07 praxis/layers/transformers_test.py
--rw-r--r--  2.0 unx    10190 b- defN 23-Mar-31 06:07 praxis/layers/vanillanets.py
--rw-r--r--  2.0 unx     2836 b- defN 23-Mar-31 06:07 praxis/layers/vanillanets_test.py
--rw-r--r--  2.0 unx    18506 b- defN 23-Mar-31 06:07 praxis/layers/vits.py
--rw-r--r--  2.0 unx     9666 b- defN 23-Mar-31 06:07 praxis/layers/vits_test.py
--rw-r--r--  2.0 unx     1294 b- defN 23-Mar-31 06:07 praxis/layers/chain/__init__.py
--rw-r--r--  2.0 unx     5228 b- defN 23-Mar-31 06:07 praxis/layers/chain/chain.py
--rw-r--r--  2.0 unx     8768 b- defN 23-Mar-31 06:07 praxis/layers/chain/chain_extensions.py
--rw-r--r--  2.0 unx     6919 b- defN 23-Mar-31 06:07 praxis/layers/chain/chain_test.py
--rw-r--r--  2.0 unx     1549 b- defN 23-Mar-31 06:07 praxis/layers/quantization/__init__.py
--rw-r--r--  2.0 unx    32523 b- defN 23-Mar-31 06:07 praxis/layers/quantization/attentions.py
--rw-r--r--  2.0 unx    19487 b- defN 23-Mar-31 06:07 praxis/layers/quantization/attentions_test.py
--rw-r--r--  2.0 unx     2897 b- defN 23-Mar-31 06:07 praxis/layers/quantization/automl_select.py
--rw-r--r--  2.0 unx     1834 b- defN 23-Mar-31 06:07 praxis/layers/quantization/automl_select_test.py
--rw-r--r--  2.0 unx     2748 b- defN 23-Mar-31 06:07 praxis/layers/quantization/conformers.py
--rw-r--r--  2.0 unx     3177 b- defN 23-Mar-31 06:07 praxis/layers/quantization/conformers_test.py
--rw-r--r--  2.0 unx    12681 b- defN 23-Mar-31 06:07 praxis/layers/quantization/embedding_softmax.py
--rw-r--r--  2.0 unx    11609 b- defN 23-Mar-31 06:07 praxis/layers/quantization/embedding_softmax_test.py
--rw-r--r--  2.0 unx     9521 b- defN 23-Mar-31 06:07 praxis/layers/quantization/linears.py
--rw-r--r--  2.0 unx    11317 b- defN 23-Mar-31 06:07 praxis/layers/quantization/linears_test.py
--rw-r--r--  2.0 unx     8728 b- defN 23-Mar-31 06:07 praxis/layers/quantization/multi_query_attention.py
--rw-r--r--  2.0 unx     4824 b- defN 23-Mar-31 06:07 praxis/layers/quantization/multi_query_attention_test.py
--rw-r--r--  2.0 unx    15988 b- defN 23-Mar-31 06:07 praxis/layers/quantization/operations.py
--rw-r--r--  2.0 unx    10568 b- defN 23-Mar-31 06:07 praxis/layers/quantization/operations_test.py
--rw-r--r--  2.0 unx     2036 b- defN 23-Mar-31 06:07 praxis/layers/quantization/optimization.py
--rw-r--r--  2.0 unx     1512 b- defN 23-Mar-31 06:07 praxis/layers/quantization/optimization_test.py
--rw-r--r--  2.0 unx     4916 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantization_hparams.py
--rw-r--r--  2.0 unx     5706 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantization_test.py
--rw-r--r--  2.0 unx    13868 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantize.py
--rw-r--r--  2.0 unx     6608 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantize_test.py
--rw-r--r--  2.0 unx    14530 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantizer.py
--rw-r--r--  2.0 unx    22299 b- defN 23-Mar-31 06:07 praxis/layers/quantization/quantizer_test.py
--rw-r--r--  2.0 unx     3711 b- defN 23-Mar-31 06:07 praxis/layers/quantization/searchable.py
--rw-r--r--  2.0 unx     3189 b- defN 23-Mar-31 06:07 praxis/layers/quantization/searchable_test.py
--rw-r--r--  2.0 unx     7843 b- defN 23-Mar-31 06:07 praxis/layers/quantization/utils.py
--rw-r--r--  2.0 unx     3886 b- defN 23-Mar-31 06:07 praxis/layers/quantization/utils_test.py
--rw-r--r--  2.0 unx      754 b- defN 23-Mar-31 06:07 praxis/layers/sparsity/__init__.py
--rw-r--r--  2.0 unx     3141 b- defN 23-Mar-31 06:07 praxis/layers/sparsity/linears.py
--rw-r--r--  2.0 unx     5667 b- defN 23-Mar-31 06:07 praxis/layers/sparsity/linears_test.py
--rw-r--r--  2.0 unx    11311 b- defN 23-Mar-31 06:07 praxis/layers/sparsity/sparsity.py
--rw-r--r--  2.0 unx     2978 b- defN 23-Mar-31 06:07 praxis/layers/sparsity/sparsity_hparams.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Mar-31 06:38 praxis-0.4.0.dist-info/LICENSE
--rw-r--r--  2.0 unx      776 b- defN 23-Mar-31 06:38 praxis-0.4.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Mar-31 06:38 praxis-0.4.0.dist-info/WHEEL
--rw-r--r--  2.0 unx        7 b- defN 23-Mar-31 06:38 praxis-0.4.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    13088 b- defN 23-Mar-31 06:38 praxis-0.4.0.dist-info/RECORD
-148 files, 2434226 bytes uncompressed, 575029 bytes compressed:  76.4%
+Zip file size: 595241 bytes, number of entries: 148
+-rw-r--r--  2.0 unx      596 b- defN 23-Apr-12 18:20 praxis/__init__.py
+-rw-r--r--  2.0 unx    22146 b- defN 23-Apr-12 18:20 praxis/asserts.py
+-rw-r--r--  2.0 unx    11203 b- defN 23-Apr-12 18:20 praxis/asserts_test.py
+-rw-r--r--  2.0 unx    43875 b- defN 23-Apr-12 18:20 praxis/base_hyperparams.py
+-rw-r--r--  2.0 unx    26956 b- defN 23-Apr-12 18:20 praxis/base_hyperparams_test.py
+-rw-r--r--  2.0 unx    39232 b- defN 23-Apr-12 18:20 praxis/base_input.py
+-rw-r--r--  2.0 unx    27138 b- defN 23-Apr-12 18:20 praxis/base_input_test.py
+-rw-r--r--  2.0 unx    92077 b- defN 23-Apr-12 18:20 praxis/base_layer.py
+-rw-r--r--  2.0 unx    25465 b- defN 23-Apr-12 18:20 praxis/base_layer_test.py
+-rw-r--r--  2.0 unx     6216 b- defN 23-Apr-12 18:20 praxis/base_model.py
+-rw-r--r--  2.0 unx    14137 b- defN 23-Apr-12 18:20 praxis/beam_search.py
+-rw-r--r--  2.0 unx     7747 b- defN 23-Apr-12 18:20 praxis/beam_search_test.py
+-rw-r--r--  2.0 unx     4225 b- defN 23-Apr-12 18:20 praxis/decoder_hparams.py
+-rw-r--r--  2.0 unx    16851 b- defN 23-Apr-12 18:20 praxis/decoder_utils.py
+-rw-r--r--  2.0 unx     6952 b- defN 23-Apr-12 18:20 praxis/decoder_utils_test.py
+-rw-r--r--  2.0 unx     1475 b- defN 23-Apr-12 18:20 praxis/fiddle_tags.py
+-rw-r--r--  2.0 unx    11843 b- defN 23-Apr-12 18:20 praxis/flat_beam_search.py
+-rw-r--r--  2.0 unx     4451 b- defN 23-Apr-12 18:20 praxis/flat_beam_search_test.py
+-rw-r--r--  2.0 unx     5383 b- defN 23-Apr-12 18:20 praxis/flax_utils.py
+-rw-r--r--  2.0 unx    36078 b- defN 23-Apr-12 18:20 praxis/gshard_utils.py
+-rw-r--r--  2.0 unx     1430 b- defN 23-Apr-12 18:20 praxis/lingvo_lib.py
+-rw-r--r--  2.0 unx     2714 b- defN 23-Apr-12 18:20 praxis/metric_utils.py
+-rw-r--r--  2.0 unx    13649 b- defN 23-Apr-12 18:20 praxis/optimizer_prefix_vectorization.py
+-rw-r--r--  2.0 unx   109219 b- defN 23-Apr-12 18:20 praxis/optimizers.py
+-rw-r--r--  2.0 unx     2965 b- defN 23-Apr-12 18:20 praxis/optimizers_test.py
+-rw-r--r--  2.0 unx    18053 b- defN 23-Apr-12 18:20 praxis/pax_fiddle.py
+-rw-r--r--  2.0 unx    23156 b- defN 23-Apr-12 18:20 praxis/pax_fiddle_test.py
+-rw-r--r--  2.0 unx    33768 b- defN 23-Apr-12 18:20 praxis/py_utils.py
+-rw-r--r--  2.0 unx    16188 b- defN 23-Apr-12 18:20 praxis/py_utils_test.py
+-rw-r--r--  2.0 unx     2604 b- defN 23-Apr-12 18:20 praxis/pytypes.py
+-rw-r--r--  2.0 unx    49538 b- defN 23-Apr-12 18:20 praxis/sample_decode.py
+-rw-r--r--  2.0 unx    15878 b- defN 23-Apr-12 18:20 praxis/sample_decode_test.py
+-rw-r--r--  2.0 unx    22347 b- defN 23-Apr-12 18:20 praxis/schedules.py
+-rw-r--r--  2.0 unx    21444 b- defN 23-Apr-12 18:20 praxis/schedules_test.py
+-rw-r--r--  2.0 unx    18096 b- defN 23-Apr-12 18:20 praxis/test_utils.py
+-rw-r--r--  2.0 unx     6472 b- defN 23-Apr-12 18:20 praxis/layers/__init__.py
+-rw-r--r--  2.0 unx     4918 b- defN 23-Apr-12 18:20 praxis/layers/activations.py
+-rw-r--r--  2.0 unx     1856 b- defN 23-Apr-12 18:20 praxis/layers/activations_test.py
+-rw-r--r--  2.0 unx     8430 b- defN 23-Apr-12 18:20 praxis/layers/adapters.py
+-rw-r--r--  2.0 unx     6737 b- defN 23-Apr-12 18:20 praxis/layers/adapters_test.py
+-rw-r--r--  2.0 unx   120079 b- defN 23-Apr-12 18:20 praxis/layers/attentions.py
+-rw-r--r--  2.0 unx    64759 b- defN 23-Apr-12 18:20 praxis/layers/attentions_test.py
+-rw-r--r--  2.0 unx     7235 b- defN 23-Apr-12 18:20 praxis/layers/augmentations.py
+-rw-r--r--  2.0 unx     6401 b- defN 23-Apr-12 18:20 praxis/layers/augmentations_test.py
+-rw-r--r--  2.0 unx    12718 b- defN 23-Apr-12 18:20 praxis/layers/bregman.py
+-rw-r--r--  2.0 unx     5218 b- defN 23-Apr-12 18:20 praxis/layers/bregman_test.py
+-rw-r--r--  2.0 unx     3489 b- defN 23-Apr-12 18:20 praxis/layers/checkpoint_policy.py
+-rw-r--r--  2.0 unx    17684 b- defN 23-Apr-12 18:20 praxis/layers/conformers.py
+-rw-r--r--  2.0 unx    11236 b- defN 23-Apr-12 18:20 praxis/layers/conformers_test.py
+-rw-r--r--  2.0 unx    33820 b- defN 23-Apr-12 18:20 praxis/layers/convolutions.py
+-rw-r--r--  2.0 unx    15010 b- defN 23-Apr-12 18:20 praxis/layers/convolutions_test.py
+-rw-r--r--  2.0 unx    10522 b- defN 23-Apr-12 18:20 praxis/layers/ctc_objectives.py
+-rw-r--r--  2.0 unx     6728 b- defN 23-Apr-12 18:20 praxis/layers/ctc_objectives_test.py
+-rw-r--r--  2.0 unx    41550 b- defN 23-Apr-12 18:20 praxis/layers/embedding_softmax.py
+-rw-r--r--  2.0 unx    34914 b- defN 23-Apr-12 18:20 praxis/layers/embedding_softmax_test.py
+-rw-r--r--  2.0 unx     5920 b- defN 23-Apr-12 18:20 praxis/layers/flax_adapter.py
+-rw-r--r--  2.0 unx    10856 b- defN 23-Apr-12 18:20 praxis/layers/flax_adapter_test.py
+-rw-r--r--  2.0 unx    11266 b- defN 23-Apr-12 18:20 praxis/layers/frnn.py
+-rw-r--r--  2.0 unx     7873 b- defN 23-Apr-12 18:20 praxis/layers/frnn_test.py
+-rw-r--r--  2.0 unx    13773 b- defN 23-Apr-12 18:20 praxis/layers/glam.py
+-rw-r--r--  2.0 unx     6718 b- defN 23-Apr-12 18:20 praxis/layers/gpu_fast_attention.py
+-rw-r--r--  2.0 unx    17664 b- defN 23-Apr-12 18:20 praxis/layers/linears.py
+-rw-r--r--  2.0 unx    18668 b- defN 23-Apr-12 18:20 praxis/layers/linears_test.py
+-rw-r--r--  2.0 unx     3826 b- defN 23-Apr-12 18:20 praxis/layers/losses.py
+-rw-r--r--  2.0 unx    54187 b- defN 23-Apr-12 18:20 praxis/layers/models.py
+-rw-r--r--  2.0 unx    38613 b- defN 23-Apr-12 18:20 praxis/layers/models_test.py
+-rw-r--r--  2.0 unx    54126 b- defN 23-Apr-12 18:20 praxis/layers/multi_query_attention.py
+-rw-r--r--  2.0 unx    13686 b- defN 23-Apr-12 18:20 praxis/layers/multi_query_attention_test.py
+-rw-r--r--  2.0 unx    41473 b- defN 23-Apr-12 18:20 praxis/layers/ngrammer.py
+-rw-r--r--  2.0 unx    26471 b- defN 23-Apr-12 18:20 praxis/layers/ngrammer_test.py
+-rw-r--r--  2.0 unx    21164 b- defN 23-Apr-12 18:20 praxis/layers/normalizations.py
+-rw-r--r--  2.0 unx    14358 b- defN 23-Apr-12 18:20 praxis/layers/normalizations_test.py
+-rw-r--r--  2.0 unx    48295 b- defN 23-Apr-12 18:20 praxis/layers/pipeline.py
+-rw-r--r--  2.0 unx     9724 b- defN 23-Apr-12 18:20 praxis/layers/poolings.py
+-rw-r--r--  2.0 unx     7555 b- defN 23-Apr-12 18:20 praxis/layers/poolings_test.py
+-rw-r--r--  2.0 unx    16380 b- defN 23-Apr-12 18:20 praxis/layers/quantizer.py
+-rw-r--r--  2.0 unx     4134 b- defN 23-Apr-12 18:20 praxis/layers/quantizer_objectives.py
+-rw-r--r--  2.0 unx     1525 b- defN 23-Apr-12 18:20 praxis/layers/quantizer_objectives_test.py
+-rw-r--r--  2.0 unx     4351 b- defN 23-Apr-12 18:20 praxis/layers/quantizer_test.py
+-rw-r--r--  2.0 unx    21330 b- defN 23-Apr-12 18:20 praxis/layers/repeats.py
+-rw-r--r--  2.0 unx    15886 b- defN 23-Apr-12 18:20 praxis/layers/repeats_test.py
+-rw-r--r--  2.0 unx    15684 b- defN 23-Apr-12 18:20 praxis/layers/resnets.py
+-rw-r--r--  2.0 unx    18926 b- defN 23-Apr-12 18:20 praxis/layers/rnn_cell.py
+-rw-r--r--  2.0 unx     5094 b- defN 23-Apr-12 18:20 praxis/layers/rnn_cell_test.py
+-rw-r--r--  2.0 unx     1289 b- defN 23-Apr-12 18:20 praxis/layers/sequential.py
+-rw-r--r--  2.0 unx     3167 b- defN 23-Apr-12 18:20 praxis/layers/sequential_test.py
+-rw-r--r--  2.0 unx    10129 b- defN 23-Apr-12 18:20 praxis/layers/shared_layers_test.py
+-rw-r--r--  2.0 unx    10152 b- defN 23-Apr-12 18:20 praxis/layers/spectrum_augmenter.py
+-rw-r--r--  2.0 unx     6714 b- defN 23-Apr-12 18:20 praxis/layers/spectrum_augmenter_test.py
+-rw-r--r--  2.0 unx     8409 b- defN 23-Apr-12 18:20 praxis/layers/ssm.py
+-rw-r--r--  2.0 unx     2963 b- defN 23-Apr-12 18:20 praxis/layers/ssm_test.py
+-rw-r--r--  2.0 unx    27036 b- defN 23-Apr-12 18:20 praxis/layers/ssm_transformers.py
+-rw-r--r--  2.0 unx     5729 b- defN 23-Apr-12 18:20 praxis/layers/ssm_transformers_test.py
+-rw-r--r--  2.0 unx     1587 b- defN 23-Apr-12 18:20 praxis/layers/stats.py
+-rw-r--r--  2.0 unx     1258 b- defN 23-Apr-12 18:20 praxis/layers/stats_test.py
+-rw-r--r--  2.0 unx     5184 b- defN 23-Apr-12 18:20 praxis/layers/stochastics.py
+-rw-r--r--  2.0 unx     4563 b- defN 23-Apr-12 18:20 praxis/layers/stochastics_test.py
+-rw-r--r--  2.0 unx     7874 b- defN 23-Apr-12 18:20 praxis/layers/test_layers.py
+-rw-r--r--  2.0 unx    71033 b- defN 23-Apr-12 18:20 praxis/layers/transformer_models.py
+-rw-r--r--  2.0 unx     9529 b- defN 23-Apr-12 18:20 praxis/layers/transformer_models_encoder_decoder_test.py
+-rw-r--r--  2.0 unx    46465 b- defN 23-Apr-12 18:20 praxis/layers/transformer_models_test.py
+-rw-r--r--  2.0 unx    90052 b- defN 23-Apr-12 18:20 praxis/layers/transformers.py
+-rw-r--r--  2.0 unx    60292 b- defN 23-Apr-12 18:20 praxis/layers/transformers_test.py
+-rw-r--r--  2.0 unx    10190 b- defN 23-Apr-12 18:20 praxis/layers/vanillanets.py
+-rw-r--r--  2.0 unx     2836 b- defN 23-Apr-12 18:20 praxis/layers/vanillanets_test.py
+-rw-r--r--  2.0 unx    18506 b- defN 23-Apr-12 18:20 praxis/layers/vits.py
+-rw-r--r--  2.0 unx     9666 b- defN 23-Apr-12 18:20 praxis/layers/vits_test.py
+-rw-r--r--  2.0 unx     1294 b- defN 23-Apr-12 18:20 praxis/layers/chain/__init__.py
+-rw-r--r--  2.0 unx     5131 b- defN 23-Apr-12 18:20 praxis/layers/chain/chain.py
+-rw-r--r--  2.0 unx     8700 b- defN 23-Apr-12 18:20 praxis/layers/chain/chain_extensions.py
+-rw-r--r--  2.0 unx     6469 b- defN 23-Apr-12 18:20 praxis/layers/chain/chain_test.py
+-rw-r--r--  2.0 unx     1549 b- defN 23-Apr-12 18:20 praxis/layers/quantization/__init__.py
+-rw-r--r--  2.0 unx    32523 b- defN 23-Apr-12 18:20 praxis/layers/quantization/attentions.py
+-rw-r--r--  2.0 unx    19487 b- defN 23-Apr-12 18:20 praxis/layers/quantization/attentions_test.py
+-rw-r--r--  2.0 unx     2897 b- defN 23-Apr-12 18:20 praxis/layers/quantization/automl_select.py
+-rw-r--r--  2.0 unx     1834 b- defN 23-Apr-12 18:20 praxis/layers/quantization/automl_select_test.py
+-rw-r--r--  2.0 unx     2748 b- defN 23-Apr-12 18:20 praxis/layers/quantization/conformers.py
+-rw-r--r--  2.0 unx     3177 b- defN 23-Apr-12 18:20 praxis/layers/quantization/conformers_test.py
+-rw-r--r--  2.0 unx    12657 b- defN 23-Apr-12 18:20 praxis/layers/quantization/embedding_softmax.py
+-rw-r--r--  2.0 unx    11758 b- defN 23-Apr-12 18:20 praxis/layers/quantization/embedding_softmax_test.py
+-rw-r--r--  2.0 unx     9521 b- defN 23-Apr-12 18:20 praxis/layers/quantization/linears.py
+-rw-r--r--  2.0 unx    11317 b- defN 23-Apr-12 18:20 praxis/layers/quantization/linears_test.py
+-rw-r--r--  2.0 unx     8728 b- defN 23-Apr-12 18:20 praxis/layers/quantization/multi_query_attention.py
+-rw-r--r--  2.0 unx     4824 b- defN 23-Apr-12 18:20 praxis/layers/quantization/multi_query_attention_test.py
+-rw-r--r--  2.0 unx    15988 b- defN 23-Apr-12 18:20 praxis/layers/quantization/operations.py
+-rw-r--r--  2.0 unx    10568 b- defN 23-Apr-12 18:20 praxis/layers/quantization/operations_test.py
+-rw-r--r--  2.0 unx     2036 b- defN 23-Apr-12 18:20 praxis/layers/quantization/optimization.py
+-rw-r--r--  2.0 unx     1512 b- defN 23-Apr-12 18:20 praxis/layers/quantization/optimization_test.py
+-rw-r--r--  2.0 unx     4916 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantization_hparams.py
+-rw-r--r--  2.0 unx     5706 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantization_test.py
+-rw-r--r--  2.0 unx    14813 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantize.py
+-rw-r--r--  2.0 unx     6608 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantize_test.py
+-rw-r--r--  2.0 unx    14530 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantizer.py
+-rw-r--r--  2.0 unx    22299 b- defN 23-Apr-12 18:20 praxis/layers/quantization/quantizer_test.py
+-rw-r--r--  2.0 unx     3711 b- defN 23-Apr-12 18:20 praxis/layers/quantization/searchable.py
+-rw-r--r--  2.0 unx     3189 b- defN 23-Apr-12 18:20 praxis/layers/quantization/searchable_test.py
+-rw-r--r--  2.0 unx     7843 b- defN 23-Apr-12 18:20 praxis/layers/quantization/utils.py
+-rw-r--r--  2.0 unx     3886 b- defN 23-Apr-12 18:20 praxis/layers/quantization/utils_test.py
+-rw-r--r--  2.0 unx      754 b- defN 23-Apr-12 18:20 praxis/layers/sparsity/__init__.py
+-rw-r--r--  2.0 unx     3141 b- defN 23-Apr-12 18:20 praxis/layers/sparsity/linears.py
+-rw-r--r--  2.0 unx     5667 b- defN 23-Apr-12 18:20 praxis/layers/sparsity/linears_test.py
+-rw-r--r--  2.0 unx    11311 b- defN 23-Apr-12 18:20 praxis/layers/sparsity/sparsity.py
+-rw-r--r--  2.0 unx     2978 b- defN 23-Apr-12 18:20 praxis/layers/sparsity/sparsity_hparams.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Apr-12 18:54 praxis-1.0.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx      888 b- defN 23-Apr-12 18:54 praxis-1.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-12 18:54 praxis-1.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx        7 b- defN 23-Apr-12 18:54 praxis-1.0.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    13088 b- defN 23-Apr-12 18:54 praxis-1.0.0.dist-info/RECORD
+148 files, 2430522 bytes uncompressed, 574693 bytes compressed:  76.4%
```

## zipnote {}

```diff
@@ -423,23 +423,23 @@
 
 Filename: praxis/layers/sparsity/sparsity.py
 Comment: 
 
 Filename: praxis/layers/sparsity/sparsity_hparams.py
 Comment: 
 
-Filename: praxis-0.4.0.dist-info/LICENSE
+Filename: praxis-1.0.0.dist-info/LICENSE
 Comment: 
 
-Filename: praxis-0.4.0.dist-info/METADATA
+Filename: praxis-1.0.0.dist-info/METADATA
 Comment: 
 
-Filename: praxis-0.4.0.dist-info/WHEEL
+Filename: praxis-1.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: praxis-0.4.0.dist-info/top_level.txt
+Filename: praxis-1.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: praxis-0.4.0.dist-info/RECORD
+Filename: praxis-1.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## praxis/base_input.py

```diff
@@ -265,15 +265,15 @@
       op_sharding.type = xc.OpSharding.Type.OTHER
       # Fully sharded on the batch dim.
       op_sharding.tile_assignment_dimensions = [len(device_order)] + [1] * (
           len(global_shape.shape) - 1)
       # Custom device order.
       op_sharding.tile_assignment_devices = device_order
       dbs = py_utils.put_to_devices(x, global_mesh.local_devices)
-      sharding = jax.sharding.GSPMDSharding(
+      sharding = jax.sharding.OpShardingSharding(
           list(global_mesh.devices.flat), op_sharding)
       return jax.make_array_from_single_device_arrays(global_shape.shape,
                                                       sharding, dbs)
 
     return jax.tree_util.tree_map(_make_array, arrays, global_shapes)
```

## praxis/optimizer_prefix_vectorization.py

```diff
@@ -125,15 +125,15 @@
   if d.startswith('s'):
     return d[1:]
 
   assert d.startswith('t')
   if len(d) == 1:
     return ()
   tuple_elements = [_decode_sharding_dim(e) for e in d[1:].split(',')]
-  return tuple(tuple_elements)  # pytype: disable=bad-return-type  # always-use-return-annotations
+  return tuple(tuple_elements)
 
 
 def _get_var_param_repeat_prefix_key(var_param: base_layer.WeightHParams,
                                      repeat_prefix_sep: str) -> str:
   """Returns string keys that uniquely identify shape and sharding prefixes."""
   if not var_param.repeat_prefix:
     return NO_PREFIX_KEY
```

## praxis/pax_fiddle.py

```diff
@@ -17,21 +17,21 @@
 
 from __future__ import annotations
 
 import contextlib
 import copy
 import dataclasses
 import functools
+import typing
 from typing import Any, Callable, Collection, Container, Generic, Optional, TypeVar, Union, overload, Mapping, Sequence, List, Tuple, Dict
 
 import fiddle as fdl
 from fiddle import building
 from fiddle import daglish
 from fiddle import history
-from fiddle import signatures
 from fiddle.experimental import auto_config as fdl_auto_config
 from fiddle.experimental import dataclasses as fdl_dataclasses
 from fiddle.experimental.dataclasses import field as fdl_field
 import fiddle.extensions.jax
 from flax.linen import module as flax_module
 import typing_extensions
 
@@ -359,15 +359,18 @@
 
   Returns:
     The built version of `buildable`.
   """
   def _build(value, state):
     if isinstance(value, fdl.Buildable):
       arguments = {}
-      annotations = signatures.get_type_hints(value.__fn_or_cls__)
+      try:
+        annotations = _get_type_hints(value.__fn_or_cls__)
+      except TypeError:  # e.g., if fn_or_cls is a functor object.
+        annotations = {}
       for key, sub_value in value.__arguments__.items():
         context = f'{value.__fn_or_cls__}.{key}'
         annotation = annotations.get(key, None)
         if _contains_buildable_type(annotation, context):
           if not _is_supported_buildable_type(annotation):
             raise ValueError(
                 f'Unsupported type {annotation!r} in BaseLayer field {context}:'
@@ -392,14 +395,18 @@
           value, arguments, current_path=state.current_path)
     else:
       return state.map_children(value)
 
   return daglish.MemoizedTraversal.run(_build, buildable)
 
 
+# get_type_hints can be slow, so cache its results.
+_get_type_hints = functools.lru_cache(typing.get_type_hints)
+
+
 def _is_buildable_type(typ):
   """Returns true if `typ` is a subclass of `Buildable` or `Buildable[...]`."""
   origin = typing_extensions.get_origin(typ)
   if origin is not None:
     typ = origin
   return isinstance(typ, type) and issubclass(typ, fdl.Buildable)
```

## praxis/pax_fiddle_test.py

```diff
@@ -13,15 +13,15 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 """Tests for pax_fiddle."""
 
 import copy
 import dataclasses
-from typing import Any, Dict, List, NamedTuple, Optional, Sequence, Union
+from typing import Any, Dict, List, Optional, Sequence, Union
 
 from absl.testing import absltest
 from absl.testing import parameterized
 import fiddle as fdl
 from fiddle import daglish
 from fiddle import testing
 from flax import core as flax_core
@@ -106,24 +106,14 @@
 
 @dataclasses.dataclass
 class WheelFactory:
   wheel_tpl: List[pax_fiddle.Config[Wheel]] = dataclasses.field(
       default_factory=list)
 
 
-class NonDataclassWheelFactory:
-
-  def __init__(self, wheel_tpl: List[pax_fiddle.Config]):
-    self.wheel_tpl = wheel_tpl
-
-
-class NamedTupleWheelFactory(NamedTuple):
-  wheel_tpl: List[pax_fiddle.Config[Wheel]]
-
-
 class SubFieldAndTemplateFieldTest(testing.TestCase):
 
   def test_default_fleet_config(self):
     config = pax_fiddle.Config(Fleet)
     with self.subTest("expected_config"):
       self.assertDagEqual(
           config,
@@ -593,15 +583,16 @@
       return x
 
     for fn in [f1, f2, f3, f4]:
       cfg = pax_fiddle.Config(fn, x=pax_fiddle.Config(Wheel))
       result = pax_fiddle.build(cfg)
       self.assertDagEqual(result, pax_fiddle.Config(Wheel))
 
-  def test_do_not_build_function_args_if_arg_is_pax_config_container(self):
+  def test_do_not_build_if_type_is_pax_config_container(self):
+
     def f1(x: List[pax_fiddle.Config]):
       return x
 
     def f2(x: List[pax_fiddle.Config[Wheel]]):
       return x
 
     def f3(x: Optional[List[pax_fiddle.Config[Wheel]]]):
@@ -611,24 +602,21 @@
       return x
 
     for fn in [f1, f2, f3, f4]:
       cfg = pax_fiddle.Config(fn, x=[pax_fiddle.Config(Wheel)])
       result = pax_fiddle.build(cfg)
       self.assertDagEqual(result, [pax_fiddle.Config(Wheel)])
 
-  @parameterized.named_parameters([
-      ("_dataclass", WheelFactory),
-      ("_regular_class", NonDataclassWheelFactory),
-      ("_named_tuple", NamedTupleWheelFactory),
-  ])
-  def test_do_not_build_type_args_if_arg_is_pax_config_container(self, cls):
-    wheel_tpl = [pax_fiddle.Config(Wheel), pax_fiddle.Config(Wheel, radius=8)]
-    cfg = pax_fiddle.Config(cls, wheel_tpl=wheel_tpl)
-    instance = pax_fiddle.build(cfg)
-    self.assertDagEqual(instance.wheel_tpl, wheel_tpl)
+    with self.subTest("Dataclass"):
+      wheel_tpl=[
+          pax_fiddle.Config(Wheel), pax_fiddle.Config(Wheel, radius=8)
+      ]
+      cfg = pax_fiddle.Config(WheelFactory, wheel_tpl=wheel_tpl)
+      factory = pax_fiddle.build(cfg)
+      self.assertDagEqual(factory.wheel_tpl, wheel_tpl)
 
   def test_do_build_default_factory_list(self):
     cfg = pax_fiddle.Config(WheelFactory)
     factory = pax_fiddle.build(cfg)
     self.assertEqual(factory.wheel_tpl, [])
```

## praxis/pytypes.py

```diff
@@ -14,15 +14,14 @@
 # limitations under the License.
 
 """Common pytype definitions."""
 from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, TypeVar, Union
 
 import clu.metrics as clu_metrics
 import jax
-from jax import core
 from jax import numpy as jnp
 import numpy as np
 from praxis import py_utils
 
 HParamsT = py_utils.HParams
 JTensor = jnp.ndarray
 PRNGKey = JTensor
@@ -39,15 +38,15 @@
 NestedNpTensor = Nested[NpTensor]
 NestedBool = Nested[bool]
 NestedInt = Nested[int]
 NestedHParams = Nested[HParamsT]
 NestedPartitionSpec = Nested[jax.sharding.PartitionSpec]
 NestedJTensorOrPartitionSpec = Nested[JTensorOrPartitionSpec]
 NestedShapeDtypeStruct = Nested[jax.ShapeDtypeStruct]
-NestedShapedArray = Nested[core.ShapedArray]
+NestedShapedArray = Nested[jax.ShapedArray]
 NestedShapeDtypeLike = Union[NestedJTensor, NestedNpTensor,
                              NestedShapeDtypeStruct, NestedShapedArray]
 
 # Sharding annotation for a dim can be a single int, or a str, or a sequence of
 # (int, str), or None. For example "1", "-1", "None", "data", "(data, replia)"
 # are all valid sharding annoations for a particular tensor axis.
 DimShardingAnnotation = Optional[Union[Sequence[Union[int, str]], int, str]]
```

## praxis/sample_decode.py

```diff
@@ -472,27 +472,14 @@
         logits: JTensor of shape [B, V] containing the (possibly modified)
           logits at the current step. This return value is used to ensure that
           the recorded logprobs per sequence takes into account the
           modifications made to the logits as part of the next token sampling
           logic.
     """
 
-  def init_decode_loop_state(
-      self,
-      decode_loop_state: NestedMap,
-      batch_size: Optional[int] = None,
-      eos_id: Optional[Union[int, Sequence[int], JTensor]] = None) -> NestedMap:
-    """Initialize any addition decode loop state."""
-    return decode_loop_state
-
-  def post_process_decode_loop_state(
-      self, decode_loop_state: NestedMap) -> NestedMap:
-    """Delete unused decode loop state."""
-    return decode_loop_state
-
 
 class DefaultNextTokenSampler(BaseNextTokenSampler):
   """The default sampling logic implementing top-K and top-P sampling.
 
   If all the values in gumbel_prng_key is set to DUMMY_PRNG_KEY, gumbel_prng_key
   will be ignored and model.next_prng_key() is used to generate random noise
   for sampling decode.
@@ -853,15 +840,14 @@
   val.output_ids = output_ids
   # Shape [batch_size], whether each row has terminated and should stop.
   val.done = jnp.zeros(shape=batch_size, dtype=jnp.bool_)
   val.has_eos = jnp.zeros(shape=batch_size, dtype=jnp.bool_)
   val.decode_lengths = jnp.ones_like(prefix_lengths) * seq_len
   # We use a positive value of 1.0 to indicate blank or padded positions.
   val.logprobs = jnp.ones_like(output_ids, dtype=jnp.float32)
-  val = next_token_sampler.init_decode_loop_state(val, batch_size, eos_id)
 
   if result_callback is not None and result_callback.init_fn is not None:
     result_callback.init_fn((original_batch_size, num_samples))
 
   def get_cond_func(stop_at_decode_steps):
     """Gets conditional function for different decode steps."""
 
@@ -1197,15 +1183,14 @@
                                                         prefix_lengths,
                                                         max_prefix_len)
     result.logprobs = decoder_utils.left_align_tensor(result.logprobs,
                                                       prefix_lengths,
                                                       max_prefix_len)
 
   del result.start_step, result.step, result.done, result.has_eos
-  result = next_token_sampler.post_process_decode_loop_state(result)
 
   if cf_guidance_scale is not None:
     # Split cond / uncond branches and only return conditioned branch.
     result = jax.tree_map(
         lambda x: split_batch_dim(x, 0, 2 * num_samples)[:, :num_samples],
         result)
   else:
```

## praxis/schedules.py

```diff
@@ -22,14 +22,15 @@
 import math
 from typing import Any, Optional, Sequence, Tuple
 
 import jax
 from jax import numpy as jnp
 import optax
 from praxis import base_hyperparams
+from praxis import pax_fiddle
 from praxis import pytypes
 
 JTensor = pytypes.JTensor
 InstantiableHyperParams = base_hyperparams.InstantiableHyperParams
 instantiate = base_hyperparams.instantiate
 
 
@@ -575,65 +576,71 @@
     boundaries: Boundaries between subschedules.
     schedules: A list of sub-schedules. The length must be len(boundaries) + 1.
       schedules[i] starts at boundaries[i-1] (inclusive) and ends at
       boundaries[i] (exclusive). The *relative* step in each interval will be
       passed to the sub-schedule for Value.
   """
   boundaries: Optional[Sequence[int]] = None
-  schedules: Optional[Sequence[BaseSchedule]] = None
+  schedules: Optional[Sequence[pax_fiddle.Config[BaseSchedule]]] = None
+  _schedules_inst: Any = dataclasses.field(init=False, repr=False)
 
   def __post_init__(self):
     super().__post_init__()
+    p = self.hparams
     prev_boundary = 0
-    for boundary in self.boundaries:
+    for boundary in p.boundaries:
       if boundary < prev_boundary:
         raise ValueError('Invalid boundary %s < %s' % (boundary, prev_boundary))
       prev_boundary = boundary
-    if len(self.schedules) != len(self.boundaries) + 1:
+    if len(p.schedules) != len(p.boundaries) + 1:
       raise ValueError('len(schedules) != len(boundaries) + 1: %s vs %s' %
-                       (len(self.schedules), len(self.boundaries)))
+                       (len(p.schedules), len(p.boundaries)))
+    self._schedules_inst = [instantiate(s) for s in p.schedules]
 
   def value_at(self, step: JTensor) -> JTensor:
     p = self.hparams
     return jnp.array(
         optax.join_schedules(
-            [s.value_at for s in self.schedules], p.boundaries
+            [s.value_at for s in self._schedules_inst], p.boundaries
         )(step),
         jnp.float32,
     )
 
 
 class CycleSchedule(BaseSchedule):
   """Piecewise schedule composed of sub-schedules in a cycle.
 
   Attributes:
     schedules: A list of sub-schedules. Unlike PiecewiseSchedule, the absolute
       step is passed to the sub-schedule.
     steps: The number of steps to run each sub-schedule.
   """
-  schedules: Optional[Sequence[BaseSchedule]] = None
+  schedules: Optional[Sequence[pax_fiddle.Config[BaseSchedule]]] = None
   steps: Optional[Sequence[int]] = None
+  _schedules_inst: Any = dataclasses.field(init=False, repr=False)
   _period: Any = dataclasses.field(init=False, repr=False)
   _boundaries: Any = dataclasses.field(init=False, repr=False)
 
   def __post_init__(self):
     super().__post_init__()
-    if len(self.schedules) != len(self.steps):
+    p = self.hparams
+    if len(p.schedules) != len(p.steps):
       raise ValueError('len(schedules) != len(steps): %s vs %s' %
-                       (len(self.schedules), len(self.steps)))
+                       (len(p.schedules), len(p.steps)))
+    self._schedules_inst = [instantiate(s) for s in p.schedules]
     boundaries = [0]
-    for step in self.steps:
+    for step in p.steps:
       boundaries.append(boundaries[-1] + step)
     self._period = boundaries[-1]
     self._boundaries = boundaries[1:-1]
 
   def value_at(self, step: JTensor) -> JTensor:
     relative_step = jnp.mod(step, self._period)
-    output = self.schedules[0].value_at(step)
-    for boundary, schedule in zip(self._boundaries, self.schedules[1:]):
+    output = self._schedules_inst[0].value_at(step)
+    for boundary, schedule in zip(self._boundaries, self._schedules_inst[1:]):
       output = jnp.where(
           relative_step < boundary, output, schedule.value_at(step)
       )
     return output
 
 
 class ContinuousSchedule(BaseSchedule):
```

## praxis/layers/convolutions.py

```diff
@@ -69,16 +69,15 @@
     dilations: An optional list of ints. Defaults to (1, 1). 1-D tensor of
       length 2. The dilation factor for each dimension of input. If set to k >
       1, there will be k-1 skipped cells between each filter element on that
       dimension.
     bias: Whether or not to apply a bias before activation.
     bias_init: Bias initializer to use if bias is to be applied.
     kernel_init: Optional kernel initializer to use.
-    padding: The type of padding to use. It can be 'SAME' or 'VALID'.
-      Note that only 'SAME' padding can be combined with is_causal=True.
+    padding: The type of padding to use.
     tf_equivalent_padding: Whether to make it equivalent to tf. By default we
       apply extra padding that is different than tf conv when stride > 1. This
       is mainly used for multimodal which leads to better accuracy.
     is_causal: Whether this is a causal convolution. This assumes the first
       dimension of filter is time and if is_causal=True, each position would not
       observe any positions in the right. This is achieved by adding extra
       padding in the left to shift the whole convolution.
@@ -163,23 +162,21 @@
       )
 
     wn = self.weight_norm_tpl.clone().set(dim=self.filter_shape[-1])
     self.weight_norm: normalizations.BaseNormalization
     self.create_child('weight_norm', wn)
 
   def _compute_padding(self, inputs_shape, pad_height_zero=False):
-    filter_height = (self.filter_shape[0] - 1) * self.dilations[0] + 1
-    filter_width = (self.filter_shape[1] - 1) * self.dilations[1] + 1
     if not self.tf_equivalent_padding:
       if self.padding == 'SAME':
         pad_height_beg, pad_height_end = _extract_pad_beg_end(
-            filter_height
+            self.filter_shape[0]
         )
         pad_width_beg, pad_width_end = _extract_pad_beg_end(
-            filter_width
+            self.filter_shape[1]
         )
       else:
         assert self.padding == 'VALID', self.padding
         pad_height_beg = 0
         pad_height_end = 0
         pad_width_beg = 0
         pad_width_end = 0
@@ -188,14 +185,16 @@
     else:
       if not self.is_causal:
         padding = self.padding
       else:
         # Compute padding for causal convolution
         # Reference:
         # https://www.tensorflow.org/api_docs/python/tf/nn#notes_on_padding_2
+        filter_height = (self.filter_shape[0] - 1) * self.dilations[0] + 1
+        filter_width = (self.filter_shape[1] - 1) * self.dilations[1] + 1
         pad_height_total = _causal_padding(
             inputs_shape[1], filter_height, self.filter_stride[0]
         )
         pad_width_total = _causal_padding(
             inputs_shape[2], filter_width, self.filter_stride[1]
         )
```

## praxis/layers/convolutions_test.py

```diff
@@ -42,39 +42,26 @@
 class ConvolutionsTest(test_utils.TestCase):
 
   def setUp(self):
     super().setUp()
     np.random.seed(123456)
 
   @parameterized.parameters(
-      ((5, 4, 24, 36), (1, 1), (1, 1), False, [2, 16, 36, 72]),
-      ((2, 4, 16, 8), (2, 2), (1, 1), False, [2, 16, 32, 128]),
-      ((4, 8, 16, 32), (1, 1), (1, 1), False, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (1, 1), (2, 2), False, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (2, 2), (2, 2), False, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (1, 1), (2, 1), False, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (2, 2), (2, 1), False, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (1, 1), (2, 2), True, [2, 16, 32, 64]),
-      ((2, 8, 16, 32), (2, 2), (2, 2), True, [2, 16, 32, 64]),
+      ((5, 4, 24, 36), (1, 1), [2, 16, 36, 72]),
+      ((2, 4, 16, 8), (2, 2), [2, 16, 32, 128]),
+      ((4, 8, 16, 32), (1, 1), [2, 16, 32, 64]),
   )
-  def test_conv2d_layer_same_padding(
-      self,
-      filter_shape,
-      filter_stride,
-      dilations,
-      tf_equivalent_padding,
-      input_shape,
-  ):
+  def test_conv2d_layer_same_padding(self, filter_shape, filter_stride,
+                                     input_shape):
     p = pax_fiddle.Config(
         convolutions.Conv2D,
         name='jax_conv2d',
         filter_shape=filter_shape,
         filter_stride=filter_stride,
-        dilations=dilations,
-        tf_equivalent_padding=tf_equivalent_padding,
+        dilations=(1, 1),
         padding='SAME',
     )
     conv_layer = instantiate(p)
     npy_inputs = np.random.normal(1.0, 0.5, input_shape).astype('float32')
     inputs = jnp.asarray(npy_inputs)
 
     prng_key = jax.random.PRNGKey(seed=123)
```

## praxis/layers/models.py

```diff
@@ -294,15 +294,18 @@
       prefix_lengths = jnp.sum(
           input_batch.inputs_indicator.astype(jnp.int32), axis=1)
     else:
       # The max lengths of the prefix, which are the number of unpadded tokens.
       # Note that computing the sum with bf16 is not precise enough, so convert
       # paddings to integers first.
       maxval = jnp.sum(1 - input_batch.paddings.astype(jnp.int32), axis=1)
-      prefix_lengths = maxval
+      minval = jnp.minimum(maxval, decoder_params.min_prefix_len)
+      prefix_lengths = jax.random.randint(self.next_prng_key(), [batch_size],
+                                          minval, maxval + 1,
+                                          input_batch.ids.dtype)
 
     if self.model_type == LanguageModelType.BIDIRECTIONAL:
       raise NotImplementedError(type(self))
     elif self.model_type == LanguageModelType.PREFIX:
       if 'inputs_indicator' in input_batch:
         causal_attention_mask = 1 - input_batch.inputs_indicator
       else:
```

## praxis/layers/models_test.py

```diff
@@ -501,19 +501,18 @@
             [0, 0, 0, 0, 1],
             [0, 0, 0, 1, 0],  # argmax=[4, 4, 3]
         ],
     ]
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(3, 3), dtype=jnp.float32),
     )
     results = self._run_decode(p, logits, input_batch)
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([[2], [1], [0]], dtype=np.int32))
     # Row 0 copies 2 ids from the input as prefix, and continues without
     # ever hitting EOS. Row 1 and 2 only copies the first id from the input,
     # and continues until EOS is found.
     self.assertArraysEqual(
         results.output_ids,
@@ -550,20 +549,19 @@
             [0, 0, 0, 0, 1],
             [0, 0, 0, 1, 0],  # argmax=[4, 4, 3]
         ],
     ]
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [0, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [0, 1, 1]], dtype=jnp.int32),
         prefix_lengths=jnp.array([2, 1, 1], dtype=jnp.int32),
     )
     results = self._run_decode(p, logits, input_batch)
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([2, 1, 1], dtype=np.int32))
     # Row 0 copies 2 ids from the input as prefix, and continues without
     # ever hitting EOS. Row 1 and 2 only copies the first id from the input,
     # and continues until EOS is found.
     # The prefix is right aligned to the generated sequence.
     self.assertArraysEqual(
@@ -591,18 +589,18 @@
         [
             [0, 0, 0, 1, 0],
             [0, 0, 1, 0, 0],  # argmax=[3, 2]
         ],
     ]
     input_batch = NestedMap(
         ids=jnp.array([[2, 2, 2], [2, 2, 2]], dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 0], [0, 1, 1]], dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(2, 3), dtype=jnp.float32),
     )
     results = self._run_decode(p, logits, input_batch)
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([[3], [1]], dtype=np.int32))
     # Row 0 copies the first 3 ids, and does not terminate even though these
     # ids are EOS. Row 1 copies the first EOS from ids, and uses argmax for the
     # remaining 3.
     self.assertArraysEqual(
         results.output_ids,
@@ -632,19 +630,18 @@
             [0, 0, 0, 0, 1],
             [0, 0, 0, 1, 0],  # argmax=[3, 4, 3]
         ],
     ]
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(3, 3), dtype=jnp.float32),
     )
     results = self._run_decode(p, logits, input_batch)
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([[2], [1], [0]], dtype=np.int32))
     # Row 0 has prefix length 2, and hit EOS after decode for one step, so it
     # stops. Row 1 has prefix length 1, and hit max decode steps of 2, so it
     # stops at 3 decoded ids. Row 2 has prefix length 0, and stops after
     # hitting the max decode step of 2, ending with 2 decoded ids.
     # Note that logically prefix length 1 and 0 are equivalent, because
@@ -699,20 +696,19 @@
             [0, 0, 0, 1, 0],  # argmax=[4, 4, 3]
         ],
     ]
     sample_logits = jnp.repeat(jnp.array(logits), axis=1, repeats=2)
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(3, 3), dtype=jnp.float32),
     )
     results = self._run_decode(p, sample_logits, input_batch)
 
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([[2, 2], [1, 1], [0, 0]], dtype=np.int32))
     # Row 0 copies 2 ids from the input as prefix, and continues without
     # ever hitting EOS. Row 1 and 2 only copies the first id from the input,
     # and continues until EOS is found.
     if k == 1:
       self.assertArraysEqual(
@@ -829,16 +825,15 @@
             [0, 0, 0, 1, 0],  # argmax=[4, 4, 3]
         ],
     ]
     sample_logits = jnp.repeat(jnp.array(logits), axis=1, repeats=2)
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [0, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(3, 3), dtype=jnp.float32),
         prefix_lengths=jnp.array([2, 1, 1], dtype=jnp.int32),
     )
 
     if is_dynamic_input:
       # Test if JTensor type temperature could work.
       input_batch['temperature'] = jnp.array([0.5, 0.5, 0.5], dtype=jnp.float32)
       input_batch['stop_decode_steps'] = jnp.array([4, 4, 3], dtype=jnp.int32)
@@ -848,15 +843,15 @@
       input_batch['per_example_top_k'] = jnp.array([2, 2, 2], dtype=jnp.int32)
       input_batch['eos_id'] = jnp.array(
           [[0, 2], [0, 2], [0, 2]], dtype=jnp.int32
       )
 
     results = self._run_decode(p, sample_logits, input_batch)
 
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([2, 1, 1], dtype=np.int32))
     # Row 0 copies 2 ids from the input as prefix, and continues without
     # ever hitting EOS. Row 1 and 2 only copies the first id from the input,
     # and continues until EOS is found.
     if k == 1:
       self.assertArraysEqual(
@@ -911,20 +906,19 @@
             [0, 0, 0, 0, 1],
             [0, 0, 0, 1, 0],  # argmax=[4, 4, 3]
         ],
     ]
     input_batch = NestedMap(
         ids=jnp.array([[11, 13, 15], [12, 14, 16], [20, 30, 40]],
                       dtype=jnp.int32),
-        paddings=jnp.array([[0, 0, 1], [0, 1, 1], [1, 1, 1]],
-                           dtype=jnp.float32),
+        paddings=jnp.zeros(shape=(3, 3), dtype=jnp.float32),
     )
     results = self._run_decode(p, logits, input_batch)
 
-    # This is fixed by the paddings provided.
+    # This is fixed by the prng seed provided.
     self.assertArraysEqual(results.prefix_lengths,
                            np.array([[2], [1], [0]], dtype=np.int32))
     # Row 0 copies 2 ids from the input as prefix, and continues without
     # ever hitting EOS. Row 1 and 2 only copies the first id from the input,
     # and continues until EOS is found.
     self.assertArraysEqual(
         results.output_ids,
```

## praxis/layers/stochastics_test.py

```diff
@@ -58,15 +58,15 @@
     logging.info('out2_sum: %s', out2_sum)
     logging.info('out1_nonzero: %s', out1_nonzero)
     logging.info('out2_nonzero: %s', out2_nonzero)
 
     finfo = jnp.finfo(inputs.dtype)
     nmant = finfo.nmant
     self.assertEqual(9984.0, out1_sum)
-    if nmant < 8:
+    if jax._src.lib.xla_extension_version >= 140 and nmant < 8:
       self.assertEqual(9984.0, out2_sum)
       self.assertEqual(7983.0, out1_nonzero)
       self.assertEqual(7964.0, out2_nonzero)
     else:
       self.assertEqual(9920.0, out2_sum)
       self.assertEqual(8000.0, out1_nonzero)
       self.assertEqual(7952.0, out2_nonzero)
```

## praxis/layers/transformer_models.py

```diff
@@ -1142,27 +1142,18 @@
 
     return model_p
 
   def _clone_layer_params(self, layer_tpl: LayerTpl) -> LayerTpl:
     """Useful to let sublasses switch the class (e.g. Streaming version)."""
     return layer_tpl.clone()
 
-  def _validate_encoder_mask_self_attention(
-      self, mask_self_attention: bool
-  ) -> None:
-    """This method can be overridden to remove the check."""
-    if mask_self_attention:
-      raise ValueError(
-          'Encoder attention should be un-masked in TransformerEncoderDecoder.'
-      )
-
   def _validate_decoder_mask_self_attention(
       self, mask_self_attention: bool
   ) -> None:
-    """This method can be overridden to remove the check."""
+    """This method can be overriden to remove the check."""
     if not mask_self_attention:
       raise ValueError(
           'Decoder attention should be masked in TransformerEncoderDecoder.'
       )
 
   def setup(self) -> None:
     """Constructor."""
@@ -1274,15 +1265,17 @@
       layer_tpl = stacked_encoder_block_params.transformer_layer_params_tpl
       if isinstance(layer_tpl, (list, tuple)):
         for tpl in layer_tpl:
           tpl.tr_atten_tpl.decode_cache = False
       else:
         layer_tpl.tr_atten_tpl.decode_cache = False
 
-    self._validate_encoder_mask_self_attention(mask_self_attention)
+    if mask_self_attention:
+      raise ValueError(
+          'Encoder attention should be un-masked in TransformerEncoderDecoder.')
     self.create_child('encoder', encoder_params)
 
     # Optional separate embedding layer for source ids.
     if self.encoder_embedding_tpl is not None:
       encoder_embedding_params = self.encoder_embedding_tpl.clone()
       assert (
           encoder_embedding_params.input_dims == 0
```

## praxis/layers/transformer_models_test.py

```diff
@@ -104,91 +104,14 @@
           input_ids,
           input_paddings,
           labels=labels,
           segment_ids=input_segment_ids,
           segment_pos=input_segment_pos)
       logging.info('outputs: %s', outputs)
 
-  @parameterized.parameters(*list(itertools.product([True, False], repeat=2)))
-  def test_lm_causal(
-      self, use_rotary_position_emb, share_embedding_and_softmax
-  ):
-    vocab_size = 8
-    num_layers = 2
-    num_heads = 2
-    dim_per_head = 4
-    p = pax_fiddle.Config(
-        transformer_models.TransformerLm,
-        name='jax_lm_layer',
-        model_dims=num_heads * dim_per_head,
-        model_type=transformer_models.LanguageModelType.CAUSAL,
-        packed_input=False,
-        vocab_size=vocab_size,
-    )
-    stacked_transformer_tpl = p.stacked_transformer_tpl
-    stacked_transformer_tpl.model_dims = num_heads * dim_per_head
-    stacked_transformer_tpl.hidden_dims = 2 * num_heads * dim_per_head
-    stacked_transformer_tpl.num_heads = num_heads
-    stacked_transformer_tpl.num_layers = num_layers
-    if not share_embedding_and_softmax:
-      p.separate_embedding_tpl = pax_fiddle.Config(embedding_softmax.Embedding)
-      p.softmax_tpl = pax_fiddle.Config(embedding_softmax.FullSoftmax)
-    seq_len = 12
-    batch_size = 3
-    # Rotary position embedding.
-    params = p.stacked_transformer_tpl.transformer_layer_params_tpl
-    params.tr_atten_tpl = pax_fiddle.Config(
-        attentions.DotProductAttentionWithLPB,
-        input_dim=num_heads * dim_per_head,
-        hidden_dim=2 * num_heads * dim_per_head,
-        num_heads=num_heads,
-        dim_per_head=dim_per_head if use_rotary_position_emb else None,
-        atten_logit_cap=20.0,
-        combine_qkv=True,
-        use_rotary_position_emb=use_rotary_position_emb,
-    )
-    transformer_lm = instantiate(p)
-    npy_inputs = np.random.randint(
-        vocab_size, size=(batch_size, seq_len)
-    ).astype('int32')
-    inputs = jnp.asarray(npy_inputs)
-    context_params = base_layer.JaxContext.HParams(do_eval=True)
-    with base_layer.JaxContext.new_context(hparams=context_params):
-      prng_key = jax.random.PRNGKey(seed=123)
-      initial_vars = transformer_lm.init(
-          prng_key,
-          inputs,
-          jnp.zeros_like(inputs),
-      )
-      fprop_outputs = transformer_lm.apply(
-          initial_vars,
-          inputs,
-          jnp.zeros_like(inputs),
-          method=transformer_lm.__call__,
-      )
-      logits = fprop_outputs.logits
-
-      for i in range(1, seq_len):
-        new_npy_inputs = npy_inputs.copy()
-        new_npy_inputs[:, i] = (new_npy_inputs[:, i] + 1) % vocab_size
-        new_inputs = jnp.asarray(new_npy_inputs)
-
-        new_fprop_outputs = transformer_lm.apply(
-            initial_vars,
-            new_inputs,
-            jnp.zeros_like(inputs),
-            method=transformer_lm.__call__,
-        )
-        new_logits = new_fprop_outputs.logits
-
-        # check that logits at position < i are unchanged
-        self.assertAllClose(logits[:, :i], new_logits[:, :i])
-        # check that logits at position i are changed
-        self.assertNotAllClose(logits[:, i], new_logits[:, i])
-
   @parameterized.parameters(*list(itertools.product([True, False], repeat=5)))
   def test_ngrammer_lm_extendstep(self, use_vq_ngrams, use_rotary_position_emb,
                                   use_post_attention_ngrammer,
                                   ngram_using_attention_scores,
                                   share_embedding_and_softmax):
     vocab_size = 8
     num_layers = 2
```

## praxis/layers/chain/chain.py

```diff
@@ -75,28 +75,25 @@
     **kwargs: The optional kwargs.
 
   Returns:
     A list with the output from each layer.
   """
   outputs = []
   args_stack = args
-  for i, l in enumerate([l for l in layers if l is not None]):
+  for i, l in enumerate(layers):
     try:
       layer_outs = call_chained_layer(l, *args_stack, **kwargs)
     except Exception as e:
       raise type(e)(
           str(e) + f' Layer index={i} name={_name_attr(l)} args: {args_stack}'
       ) from e
 
     outputs.append(layer_outs)
     args_stack = ensure_tuple(layer_outs)
 
-  if not outputs:
-    return [args if len(args) > 1 else args[0]]
-
   return outputs
 
 
 def call_chained_layer(
     layer: Callable[..., Any], *args: Any, **kwargs: Any
 ) -> Any:
   """Passes required arguments and matching kwargs to `layer`.
```

## praxis/layers/chain/chain_extensions.py

```diff
@@ -119,15 +119,14 @@
       positional_args_as_scan_carry=True,
       **kwargs_with_name('chain_repeat', **kwargs),
   )
 
 
 def _name_layers_uniquely(layers_tpl: Sequence[LayerTpl]) -> List[LayerTpl]:
   """Returns layers with unique names assigned."""
-  layers_tpl = [l for l in layers_tpl if l]
   if not layers_tpl:
     return []
   name_or_default = lambda layer: layer.name or 'layers'
   count = collections.defaultdict(int)
   for layer in layers_tpl:
     name = name_or_default(layer)
     count[name] += 1
@@ -205,15 +204,15 @@
       outputs.append(input_batch[k])
     if len(outputs) > 1:
       return tuple(outputs)
     return outputs[0]  # pytype: disable=bad-return-type  # jax-ndarray
 
 
 def dict_to_args(*keys: str, **kwargs: Any) -> Config[DictToArgs]:
-  """`Config(DictToArgs)`; looks up tensors by keys in a dictionary."""
+  """`Config(DictToArgs); looks up tensors by keys in a dictionary."""
   return Config(
       DictToArgs,
       keys=keys,
       **kwargs_with_name('dict_to_args', **kwargs),
   )
 
 
@@ -247,20 +246,20 @@
       return (
           f'{arg.shape} values=\n{arg}' if self.log_values else f'{arg.shape}'
       )
     return f'{arg}'
 
 
 def log_args(
-    message: Optional[str] = None, log_values: bool = False, **kwargs: Any
+    message: str, log_values: bool = False, **kwargs: Any
 ) -> Config[LogArgs]:
   """`Config(LogArgs)`; logs the arguments (for easy debugging)."""
   return Config(
       LogArgs,
-      message=message or '',
+      message=message,
       log_values=log_values,
       **kwargs_with_name('log_args', **kwargs),
   )
 
 
 class FullLike(BaseLayer):
   """Simple wrapper around jnp.full_like().
```

## praxis/layers/chain/chain_test.py

```diff
@@ -75,30 +75,14 @@
     p = chain.chain(chain.full_like(2.0), chain.apply_padding(), _scale(3.0))
     outputs, paddings = self._run(
         p, jnp.ones((2, 2)), jnp.array([[0.0, 1], [0, 0]])
     )
     self.assertArraysEqual([[6.0, 0], [6, 6]], outputs)
     self.assertArraysEqual([[0.0, 1], [0, 0]], paddings)
 
-  def test_chain_with_none_layer(self):
-    p = chain.chain(
-        chain.chain(),
-        chain.full_like(2.0),
-        chain.chain(None),
-        chain.apply_padding(),
-        None,
-        _scale(3.0),
-        chain.chain(),
-    )
-    outputs, paddings = self._run(
-        p, jnp.ones((2, 2)), jnp.array([[0.0, 1], [0, 0]])
-    )
-    self.assertArraysEqual([[6.0, 0], [6, 6]], outputs)
-    self.assertArraysEqual([[0.0, 1], [0, 0]], paddings)
-
   def test_chain_with_special_kwarg(self):
     class SpecialKWarg(BaseLayer):
 
       def __call__(self, inputs, input_batch):
         return input_batch.override
 
     p = chain.chain(
```

## praxis/layers/quantization/embedding_softmax.py

```diff
@@ -71,16 +71,15 @@
           output_dims=self.num_classes,
           activation_tpl=pax_fiddle.Config(activations.Identity),
           bias_init=self.bias_init,
           weight_split_dims_mapping=wp.clone(),
           activation_split_dims_mapping=ap.clone(),
       )
       new_linear_tpl = pax_fiddle.Config(
-          quantized_linears.Linear,
-          quantization=copy.deepcopy(self.quantization),
+          quantized_linears.Linear, copy.deepcopy(self.quantization)
       )
       new_linear_tpl.copy_fields_from(ff_p.linear_tpl)
       ff_p.linear_tpl = new_linear_tpl
       self.create_child('logits_ffn', ff_p)
     if self.bi_tempered_loss_tpl:
       self.create_child('bi_tempered_loss', self.bi_tempered_loss_tpl)
```

## praxis/layers/quantization/embedding_softmax_test.py

```diff
@@ -78,14 +78,18 @@
         weight_split_dims_mapping=base_layer.BaseLayer.WeightSharding(
             wt=['mdl', 'data']
         ),
         quantization=quantization_option,
         input_dims=self.INPUT_DIMS,
         num_classes=self.NUM_CLASSES,
     )
+    q_p.feed_forward_tpl.linear_tpl = pax_fiddle.Config(
+        quantization.Linear,
+        quantization=copy.deepcopy(quantization_option),
+    )
     q_layer = instantiate(q_p)
     f_layer = instantiate(f_p)
 
     inputs = np.random.normal(1.5, 2.0, [2, q_p.input_dims]).astype(np.float32)
     class_weights = np.random.normal(1.5, 2.0, [2, 1])
     class_ids = np.random.randint(1, q_p.num_classes, [2, 1])
```

## praxis/layers/quantization/quantize.py

```diff
@@ -368,30 +368,54 @@
           weight_quantization_params,
           transposed_embedding_softmax,
       )  # pytype: disable=wrong-arg-types  # py310-upgrade
 
 
 def set_inference_mode(
     config: LayerTpl,
+    target: Type[base_layer.BaseLayer] = layers.transformers.Transformer,
 ):
   """Sets quantization mode to be INFERENCE while keeping other quantization configs unchanged.
 
   Args:
     config: The config to apply quantization on.
+    target: The target component to be replaced.
   """
   def set_quantization_mode_inference(tpl):
     if hasattr(tpl, 'quantization'):
       tpl.quantization.mode = QuantizationMode.INFERENCE
 
-  to_process = [config]
-  while to_process:
-    param = to_process.pop()
-    set_quantization_mode_inference(param)
-    if isinstance(param, fdl.Config):
-      to_process.extend(fdl.ordered_arguments(param).values())
+  target_tpls = find_target_tpl(config, target)
+  for target_tpl in target_tpls:
+    set_quantization_mode_inference(target_tpl.tr_atten_tpl.proj_tpl)
+    if (
+        issubclass(
+            target_tpl.tr_atten_tpl.cls, layers.attentions.DotProductAttention
+        )
+        and target_tpl.tr_atten_tpl.combine_qkv
+    ):
+      set_quantization_mode_inference(
+          target_tpl.tr_atten_tpl.combined_qkv_proj_tpl
+      )
+    if issubclass(
+        target_tpl.tr_atten_tpl.cls,
+        layers.multi_query_attention.MultiQueryDotProductAttention,
+    ):
+      set_quantization_mode_inference(target_tpl.tr_atten_tpl.headless_proj_tpl)
+    set_quantization_mode_inference(
+        target_tpl.tr_fflayer_tpl.fflayer_tpl.linear_tpl
+    )
+
+  lm_tpls = find_target_tpl(config, layers.TransformerLm)
+  for lm_tpl in lm_tpls:
+    set_quantization_mode_inference(lm_tpl.softmax_tpl)
+    if hasattr(lm_tpl.softmax_tpl, 'feed_forward_tpl'):
+      set_quantization_mode_inference(
+          lm_tpl.softmax_tpl.feed_forward_tpl.linear_tpl
+      )
 
 
 # Traverse entire config HParam and find the tpl of the target type.
 def find_target_tpl(config: LayerTpl, target: Type[base_layer.BaseLayer]):
   """Find and return target tpl from the config."""
   to_process = [config]
   target_tpl = []
```

## Comparing `praxis-0.4.0.dist-info/LICENSE` & `praxis-1.0.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `praxis-0.4.0.dist-info/METADATA` & `praxis-1.0.0.dist-info/METADATA`

 * *Files 19% similar despite different names*

```diff
@@ -1,30 +1,30 @@
 Metadata-Version: 2.1
 Name: praxis
-Version: 0.4.0
+Version: 1.0.0
 Summary: Functionalities such as a layers for building neural networks in Jax.
 Home-page: https://github.com/google/praxis
 Author: PAX team
 Author-email: pax-dev@google.com
 License: Apache-2.0
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
 Requires-Python: >=3.8
-Requires-Dist: absl-py
-Requires-Dist: clu
-Requires-Dist: einops
-Requires-Dist: fiddle
-Requires-Dist: flax
-Requires-Dist: jax
-Requires-Dist: jax-bitempered-loss
-Requires-Dist: lingvo
-Requires-Dist: numpy
-Requires-Dist: optax
-Requires-Dist: optax-shampoo
-Requires-Dist: tensorflow-text (~=2.9.0)
-Requires-Dist: tensorflow (~=2.9.2)
+Requires-Dist: absl-py (==1.4.0)
+Requires-Dist: clu (==0.0.8)
+Requires-Dist: einops (==0.6.0)
+Requires-Dist: fiddle (==0.2.6)
+Requires-Dist: flax (==0.6.8)
+Requires-Dist: jax-bitempered-loss (==0.0.2)
+Requires-Dist: jax (==0.4.7)
+Requires-Dist: lingvo (==0.12.6)
+Requires-Dist: numpy (~=1.24.2)
+Requires-Dist: optax-shampoo (==0.0.6)
+Requires-Dist: optax (==0.1.4)
+Requires-Dist: tensorflow-text (==2.9.0)
+Requires-Dist: tensorflow (==2.9.3)
 
 UNKNOWN
```

## Comparing `praxis-0.4.0.dist-info/RECORD` & `praxis-1.0.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 praxis/__init__.py,sha256=yRyT7qjhz2-cckhBdD_4WvjWbUZOBSr7gEo_VbuRdLE,596
 praxis/asserts.py,sha256=4MNbrjD0XTFOkgUvE8m1F6nMangxbqQn3x0ev2xUUEY,22146
 praxis/asserts_test.py,sha256=bSRyR3eDyyActjReayIr2OGuJul5RCYa6VlfseXSvnA,11203
 praxis/base_hyperparams.py,sha256=JhyA-3crE7GkVbek6lS3U5UTJtzXWIWhtQM9vdut9Kc,43875
 praxis/base_hyperparams_test.py,sha256=vOeKTXpM0U6odmJ7tnzgvpw6UGDQEoLhD453t9vzgs0,26956
-praxis/base_input.py,sha256=S2kSomzNMMsW1LX8FLCOyZpcgdplQg8gA8hbTOb7nao,39227
+praxis/base_input.py,sha256=hVkK7IHPqcbalvfUdOgudYBcR648Ysp58VD6GG2-L0k,39232
 praxis/base_input_test.py,sha256=eAdW_FSL34vv2avsf_gUkzVTfheXdnF-yUiz3K0GXv8,27138
 praxis/base_layer.py,sha256=oPiAZGOFSh8gaoUVkEwLqvYivu4BtBmcmk6zZc0etMA,92077
 praxis/base_layer_test.py,sha256=joPvZOlmaBBAf6dRHdorz3p4nBs4tD54Ki58wD02GLM,25465
 praxis/base_model.py,sha256=BpgjYCjubbrNHBMDFaNnD1GX57SUu5z2NPewh6tqIQo,6216
 praxis/beam_search.py,sha256=nW7mWUQBGsHrut92rNb5-Ni7xKAFWIuNJ7thd10bGXI,14137
 praxis/beam_search_test.py,sha256=-SuWlAb5kLq0IUx0WrOuywmU9iS-tcneZizb5Bx43FA,7747
 praxis/decoder_hparams.py,sha256=8Qdojt1ci8Nj76lp5d_hjliwVjksx_ke7MGmH-2thB8,4225
@@ -16,25 +16,25 @@
 praxis/fiddle_tags.py,sha256=9HTB1o8TUQDrnCt_STgirwRtOIMFQu4cdqcWL9iyggw,1475
 praxis/flat_beam_search.py,sha256=WcqP6pwslTCePNoKibyTU3iVZUcZEea1u4NHDJNoGcA,11843
 praxis/flat_beam_search_test.py,sha256=8Pj7oA1OX4pKbINKGTh5q755HV_wmeZ1Q0v_Q3c7FM4,4451
 praxis/flax_utils.py,sha256=YLpExnLuxfOTBIunjBCCfv6_DVKj6qT-xr1a0N5rr0I,5383
 praxis/gshard_utils.py,sha256=ziyHbI_gJq_yu-V0f0ZzwLP7V8aYlE9Dkbk_7A15Q8E,36078
 praxis/lingvo_lib.py,sha256=sYWCv0iVnW0Cuy_ZFKvIDN4YNwuYfQIw1Wl7OS8abic,1430
 praxis/metric_utils.py,sha256=mPLS5FerIJFhIXORaa98DUMy1aIxc09_cXdjFTkcSR0,2714
-praxis/optimizer_prefix_vectorization.py,sha256=gfcGtTJhMreUEA2ktsQbiK6NVaFABwOVjbl0yLq_JgE,13717
+praxis/optimizer_prefix_vectorization.py,sha256=puCStVR_P_8xZQKOkm3OcpFyZLpvWjTyXiUmAqqA3Cc,13649
 praxis/optimizers.py,sha256=Om3sM0CGyskY4bdv6TB6W7-qjPoO6YMpX8ZrA6yMrbo,109219
 praxis/optimizers_test.py,sha256=ySABhOx0OJC7OZ0GpHr2dHjIwtAb5JzxLgifMXG2DBI,2965
-praxis/pax_fiddle.py,sha256=NKn681Zj6r5UYG7PzaO1i8x3DQ7QiVMv7k-3Bl7C5-E,17859
-praxis/pax_fiddle_test.py,sha256=PBCLOki067x3pL5cbJZ7POrifmF4aLM4trDGhvVnwVI,23579
+praxis/pax_fiddle.py,sha256=SEXyNgzovSDGiPdO1kxApIs_LDITne5MRXQ8ovUmq7I,18053
+praxis/pax_fiddle_test.py,sha256=SQanw4P2KU1dUkauBEjlknuqzpCR13Ys0QYpWc7Tsb8,23156
 praxis/py_utils.py,sha256=SixYiv7nM-rM8jDQpwtEcpQ0VjT_OzeETwpuDw5KNT8,33768
 praxis/py_utils_test.py,sha256=dFzyjikNehBUNfMRRS-tqS2EYnwvnXHAaGCjMGnLIZ4,16188
-praxis/pytypes.py,sha256=vczg3uvWN1fCVv_8dDO-79Efg6Rhd3dpZ8cQHhas9hY,2626
-praxis/sample_decode.py,sha256=aXmcZFOiVflMXDOMQIrCK4yoJqDdgS3_c0An1UOmZvk,50131
+praxis/pytypes.py,sha256=eow3eoNcbfemFsPHJj7TrmsILcoUjJiYCLogDg6cnlE,2604
+praxis/sample_decode.py,sha256=S2CS4c6qZaicwu18_RWS7jswMDmDUj1VE1KeV7SSV5s,49538
 praxis/sample_decode_test.py,sha256=JUoYRalfFIUizGn3tNfdXPtrYHi8Qp_i796JegTK3mU,15878
-praxis/schedules.py,sha256=7nC2pvR3g_w7RwGe0fvcL0dzdvsPqhrGovJpbSGmZho,21985
+praxis/schedules.py,sha256=6PkhdNJ2KcNEMBp31XD_JK8NWQId3BhxBfoahBQW_7s,22347
 praxis/schedules_test.py,sha256=spIuewVdLtALJbIeBfVZmzMNALVFuVZt8d-Hx4BQJYk,21444
 praxis/test_utils.py,sha256=ZeCQ0rVX8pIaDPkNDjBX_rT2oFm56p6qxTrCCOiLlPo,18096
 praxis/layers/__init__.py,sha256=5qCofe3KktPp4QLjZTFoY8KKNZ0OhtIJORFWdlFzk9o,6472
 praxis/layers/activations.py,sha256=YfhnuSXSRMB_3GWOQhvOFcMvJgxVVN1QD7BrokuZKIk,4918
 praxis/layers/activations_test.py,sha256=l_k1O3gJz_tnyZY-aKGEuRMUOfDcQeRm2ZOC3k28CRQ,1856
 praxis/layers/adapters.py,sha256=3HCqbH4jOI45ROXWwV4Bzbf_lbE21a5_Rj2BXCz7P1Y,8430
 praxis/layers/adapters_test.py,sha256=NGgZsR39kwGapPkdpX96ufRtfIeoOwp_TX1WQ5lDvp8,6737
@@ -43,31 +43,31 @@
 praxis/layers/augmentations.py,sha256=qCgpNMuuqWml8L4ppCxHchrPHNX5xkp6ZAyKqiXCYfo,7235
 praxis/layers/augmentations_test.py,sha256=M4Rzu061gkpOTgc5wlgHi2_V7veXS3_xj0JdvcVJvqk,6401
 praxis/layers/bregman.py,sha256=TTaI8_Jm8D69CScRapL0fXjGhl9kSCFD-pRXNF4vcA0,12718
 praxis/layers/bregman_test.py,sha256=pWBWYZ-rg7S84hU_o_z3F3d1NA7VflEZq2cY4KrfIyQ,5218
 praxis/layers/checkpoint_policy.py,sha256=1EZExRG5iqaMvcZfVW79zSfLZwNYN0HSbz8TefUGco0,3489
 praxis/layers/conformers.py,sha256=CDxvUbOor2DZ2hUFIbLcKNW8VtclLHpTNZc-DuIsh0Q,17684
 praxis/layers/conformers_test.py,sha256=MzyduVNylPBsdZhNb0Eft7e9PBVSS61Rrvb_5kuTWXM,11236
-praxis/layers/convolutions.py,sha256=zpelzbSBkfEIM4wROcxS3KY7m7Xc91-uwYed4Z48oB8,33899
-praxis/layers/convolutions_test.py,sha256=PA7Nwy5M0GuOa2w2bpOAnFwmqd6Qc-oqKmD6PCzMT44,15531
+praxis/layers/convolutions.py,sha256=Hio38d1trdLQGKh_kkdj97dL3CAoDqEnhvser0Ea1Zg,33820
+praxis/layers/convolutions_test.py,sha256=GRfu1DbI1Ph7Mp8w3wKA8sx8KFiGPr6JR-AvNDiCBsQ,15010
 praxis/layers/ctc_objectives.py,sha256=FsA7G1A7-EqJbM5Ogcr53IZ-U9z-FeyZwl18FitALWg,10522
 praxis/layers/ctc_objectives_test.py,sha256=XWJwBc2h07QJV-bQEQ5ZcYSafkjqipWzvLQPzdq3Myw,6728
 praxis/layers/embedding_softmax.py,sha256=YLVfHzbbmvXiOcIUjS0cNij04nrFo-msKyCyqOKnYpE,41550
 praxis/layers/embedding_softmax_test.py,sha256=KWM0YjXoi22KQ3O2sKbBiiOob7bHQIoclxZmtji0nJA,34914
 praxis/layers/flax_adapter.py,sha256=0KtSWbxyTuOZF4qFHVRqnIBJBdT_ezg8f_ELrJICFbU,5920
 praxis/layers/flax_adapter_test.py,sha256=vDXmj764XrLJ4qu_xEHP22FsoV_2KqzqtBdrqm-oH98,10856
 praxis/layers/frnn.py,sha256=QqKyL_IvcP9Ij9_9TZGCLvhUZ2ojeIEldNQDphAQ3KM,11266
 praxis/layers/frnn_test.py,sha256=oQWgDy34JKVxSAecDh-4LhLigkZnWcNxYFXVnIs-3nk,7873
 praxis/layers/glam.py,sha256=sUQvB3Y5wIkbHYxKk-8kgNmF4X88qUCIOoMiacne-M4,13773
 praxis/layers/gpu_fast_attention.py,sha256=keTT2LOhmBBNDDaBBbaamUIg3AWJH1Er_d7OLsnYPR8,6718
 praxis/layers/linears.py,sha256=N1_VayvBtOX9_aGJyvHrJJdUBMoR3wLZisaxzcocVZY,17664
 praxis/layers/linears_test.py,sha256=mExyvYNwtq8WBXDHKHuQ12zv_dZ3i3X6sjZ_r87BcJw,18668
 praxis/layers/losses.py,sha256=ktTuvTywNtK7trNGOC13f3QO-Mvroctwux0vhxj39xM,3826
-praxis/layers/models.py,sha256=jcpQ7PuK5Rso5ujO3IF6A97Nyw9QYSD9BnckiyPG_jE,53946
-praxis/layers/models_test.py,sha256=zs3GoV1vzgrea09TSJbfv7h6-srV-ebOtLD9bZRKn4A,38885
+praxis/layers/models.py,sha256=hRs5zGwVXDCV8HmnlpYP4VxDkynUxjusq6i_qj4ZnYU,54187
+praxis/layers/models_test.py,sha256=OINYSd5Byfmni4CLOnLfOA_vQ52NF7A7q7YcZfGi1ik,38613
 praxis/layers/multi_query_attention.py,sha256=RE7JB4XgZCIHgaE16xVK0CM9UTdh0i_pvV6DwDdn1Sk,54126
 praxis/layers/multi_query_attention_test.py,sha256=t_W8rzh7hPLK4mnPprYdrhK_D4k9UcBtXLZvTgKHpac,13686
 praxis/layers/ngrammer.py,sha256=pdPCIBayzgXSg1asUXayCuT8UlemZo0sWp_8MtBxn0A,41473
 praxis/layers/ngrammer_test.py,sha256=YikWMZfOL6mXSCEWjB764WAOthbaToeyVVlqyuf6msw,26471
 praxis/layers/normalizations.py,sha256=ye61AKBuGXToLpHGAY0krOFU-tsUaZ-3H3jjn8gQm6Q,21164
 praxis/layers/normalizations_test.py,sha256=3mNTKVdznza95mRdb3quxZODLRu69sL2CVK9peXa8d0,14358
 praxis/layers/pipeline.py,sha256=ZSlaNXxdArXMEhN2iYbCr7VlRvfLDNn0GW7-iIyMMeU,48295
@@ -90,59 +90,59 @@
 praxis/layers/ssm.py,sha256=SjW_eIqj5kn33cGCJxnzFgC58v-qJACBNRYa1RdXqXU,8409
 praxis/layers/ssm_test.py,sha256=8BV06jd8vArnTwbv3ec7Pvtj5fQLzC2hCdOlj6T-aj4,2963
 praxis/layers/ssm_transformers.py,sha256=zjfpwwkyDblYhdDm7mcliEj89XpwuF4eVzSES4zrc6Q,27036
 praxis/layers/ssm_transformers_test.py,sha256=AZBsGGo21Zv-nUYUYRRnCx3di0swWCuZyqxqJJoCm1Y,5729
 praxis/layers/stats.py,sha256=kyUFYw2aIPOnBzv-xQW38vAdO4NkV7hhcKLD6edbE_Q,1587
 praxis/layers/stats_test.py,sha256=aAhFRqh-YGqEUWseyRfRDpPBBzaOpW_-a263kI5JI3I,1258
 praxis/layers/stochastics.py,sha256=3NMhRm15NZ_5XE5i00VEoY8zSgJwAwjabEMWbtcowsQ,5184
-praxis/layers/stochastics_test.py,sha256=5T4keo-a2J4KG6leOn5XynYgma1GerHjLefpIpfRT_g,4517
+praxis/layers/stochastics_test.py,sha256=6MurxcPZQLoXYGiPp6EiaoM7FIPMUjbxcYI9rPqmoIA,4563
 praxis/layers/test_layers.py,sha256=Pw4YMczjNoJO9Gvqv6WNAG6WoCMJxKMuLYTExurTZKI,7874
-praxis/layers/transformer_models.py,sha256=I5rNGwb1eVYf8GeAAKeo7QHauckIoKuy4vDx7Ed4XH0,71267
+praxis/layers/transformer_models.py,sha256=PP3n7nUlPcOSzfdYR3Z9ZzkAEPZjo0sc_qlJWrF1Ay8,71033
 praxis/layers/transformer_models_encoder_decoder_test.py,sha256=MuoOoJgLsEMetdNAUE_cvk5mZ5qmt3cDtTpHSPgXaUU,9529
-praxis/layers/transformer_models_test.py,sha256=SHiQpiYATAjq4Txk9WdZmGkCHA4l9zEv0UeUTeHqmbM,49372
+praxis/layers/transformer_models_test.py,sha256=aR4DVcVrOUT7sYisqZ192nVTY8gP93qyFTruI418Qp0,46465
 praxis/layers/transformers.py,sha256=mVSjTPecLQKl0mE32z-VSwXqwhnmZ_di067qgO0kk9Q,90052
 praxis/layers/transformers_test.py,sha256=Opo9otgNWMBgfmfhGu5q20iW9NOLE1j-oKq15Ug00eo,60292
 praxis/layers/vanillanets.py,sha256=Vr0lQeBx44NcEX9PODI3NJ_NUpB80nxSpGWDp5pIg00,10190
 praxis/layers/vanillanets_test.py,sha256=2mT68jfnoTsOeRkLrT-2E3O9oNklucxnSlshl6AhVLc,2836
 praxis/layers/vits.py,sha256=RrSny40bLlZaxr67qo_5NwgA7_HKJiJR-iZCmxbsuks,18506
 praxis/layers/vits_test.py,sha256=PZZTVkbhE2gTDgYu8cIH4SDZEQbMlbpFTfLMwHStE3E,9666
 praxis/layers/chain/__init__.py,sha256=mPmuAJ5W-VkPKxKRygxSD79UsN7y5zJquNJBAGlvGN4,1294
-praxis/layers/chain/chain.py,sha256=hcKZaqouA2lYC188OaCKJo-o4j8LxFAfzKdUEZKsZCI,5228
-praxis/layers/chain/chain_extensions.py,sha256=xTcGANTu4kGA4irgC7TQ8G218uP9_7HtnCca_DNlBd0,8768
-praxis/layers/chain/chain_test.py,sha256=jucCGotqB7W0ilTfxzP5MO4YsEfH44di0EFCLjf_GSA,6919
+praxis/layers/chain/chain.py,sha256=tQM4LSt2xW8jScjO7tC7iY_YscZ_j1zksobsxigB6oE,5131
+praxis/layers/chain/chain_extensions.py,sha256=w7d5xuN3HX0j1vULZolB9NbRa34sVPSJjjCymt_4VgE,8700
+praxis/layers/chain/chain_test.py,sha256=eUra0JjADW_zkP-acX5kpZQSZmnjGnFodFPUx6pOuZs,6469
 praxis/layers/quantization/__init__.py,sha256=a0Y-An9v1dpLuo4GglCEUTky3r6Ho0Pzxiluhv0BQYI,1549
 praxis/layers/quantization/attentions.py,sha256=Z6y2tERXj_X-eMTAM8D7tzN5d3EP-OFhB1MQzz5I0BI,32523
 praxis/layers/quantization/attentions_test.py,sha256=HXY9O8drnl3W0C29Z90XP9Sz3_oBdPU3yJEhq7l8ey8,19487
 praxis/layers/quantization/automl_select.py,sha256=Anm4h0GpA_kJHGr0mT7Aehd_b99vFNYV0jt1adLvIPI,2897
 praxis/layers/quantization/automl_select_test.py,sha256=hZnM57f7sgdFzdOv9vRfc8oy2W1TZQ8Pwfy_zOHx2g4,1834
 praxis/layers/quantization/conformers.py,sha256=sx0VwMGGbtifqsCqvjBirdV8-3V02__T9TKWzGAiiro,2748
 praxis/layers/quantization/conformers_test.py,sha256=w3RigI2VxeLBAVDRLsxIkt7928SqMWBPx0r3NqYrCEY,3177
-praxis/layers/quantization/embedding_softmax.py,sha256=yWIBAS4PxekFOmMCVbu1aj56sS3jo-74RTzTsP_nC2A,12681
-praxis/layers/quantization/embedding_softmax_test.py,sha256=pyXd52ZWOqFudHQWM32KcTkC-TdIt5L7ok8N6JlBjZU,11609
+praxis/layers/quantization/embedding_softmax.py,sha256=bzq64vc1527IB9I3NmdgQpVhed4rVhhrfP_tm6nFh44,12657
+praxis/layers/quantization/embedding_softmax_test.py,sha256=n3eRS-sinXGqrxDJDktfCQPY2folYkXrRwijlp9vvN0,11758
 praxis/layers/quantization/linears.py,sha256=HOX-1Hk8cmx6LtTNwSldP3LKliTQ78yLnGo-Drtuy7M,9521
 praxis/layers/quantization/linears_test.py,sha256=Ka3cIwqHsJ4N1-4TWNJsMoOxcwPnARXQPhWHbC0LmJE,11317
 praxis/layers/quantization/multi_query_attention.py,sha256=IM6YhUSViWmOIBI0qUuJF_ycwMFicXLt7yHIavYYOk4,8728
 praxis/layers/quantization/multi_query_attention_test.py,sha256=ep7YDWNo4RA6Yu6aHerKIm0quqqjKTpQ7c0a7pBuRfI,4824
 praxis/layers/quantization/operations.py,sha256=fejjZnN68LUw6fxQjYOS-Oe77aXJBZsOvxhfR7KwvJs,15988
 praxis/layers/quantization/operations_test.py,sha256=zGno7shZEaq4hH_Re3WN_xSxppQM_oday73vDkr69b8,10568
 praxis/layers/quantization/optimization.py,sha256=Aq_HrDn_stxWkOAazSmlz4dShKlkN3jf6BmjPYJhHAk,2036
 praxis/layers/quantization/optimization_test.py,sha256=3UWi16JBqveJIcUGWDxEE3ZUuparCOPzXIuDCSjvzLY,1512
 praxis/layers/quantization/quantization_hparams.py,sha256=br0QHCpsvo_OHMGQjNgwEpju000T9WospP_VGXC2-XM,4916
 praxis/layers/quantization/quantization_test.py,sha256=BqUxX0FTj7KLhs2sBoKYZGoQi-NbRYZsh2U0EG-8k18,5706
-praxis/layers/quantization/quantize.py,sha256=q7n55bV3GT-HWGjGi-dF55tGBPaYSt2aXn-jZrI8X9M,13868
+praxis/layers/quantization/quantize.py,sha256=gySQqMYtoorNA7gtZpYoZ5TSFAXq6erV1OmDid2y9z4,14813
 praxis/layers/quantization/quantize_test.py,sha256=-hHl7hxk3124YKK-lmW7ahc-PH50WG_9MFwtAGK6K_w,6608
 praxis/layers/quantization/quantizer.py,sha256=RkCn4sn2SseVFsW5NU2y-rcVzyIJQukOz8c-qyFZpLI,14530
 praxis/layers/quantization/quantizer_test.py,sha256=ydIgvkMs-MCgaLX8e1rvAqblo09jtzTEZBlnOWgugVI,22299
 praxis/layers/quantization/searchable.py,sha256=NM4JXrqSAbuuM_lBDV5MXYF3E-IQlddQJMhNZpDV-Vs,3711
 praxis/layers/quantization/searchable_test.py,sha256=d1GRz0DcnA8d8aNy3_Begechg-X_584bV096azLrofI,3189
 praxis/layers/quantization/utils.py,sha256=rfbj21ns29LhtBNaL-LpvgMg1OwlGUQ9QD43qiHj__c,7843
 praxis/layers/quantization/utils_test.py,sha256=3wbs2qE1Zw9_w5ThgYRUFYlNs-vpfJCJECGK9xb9F0U,3886
 praxis/layers/sparsity/__init__.py,sha256=_Tguzfo47ba5uRIE74f-BmeZl6LIqricH1s2gs6ycP8,754
 praxis/layers/sparsity/linears.py,sha256=6HHpkS0TX-sWbvJxaFABhLqmBAN3Hz4Ot_PtVVKAtbM,3141
 praxis/layers/sparsity/linears_test.py,sha256=FeQ-Ok2EXTQ7o095pvls3c6A4npyNiIGNW7d44BalcY,5667
 praxis/layers/sparsity/sparsity.py,sha256=2KQzqzVYyp3H_XvzrJDPVspW7NdEyY71hqpogBrJ2yQ,11311
 praxis/layers/sparsity/sparsity_hparams.py,sha256=j5RyIKER9kxqkczkThIsq97nLveWBjaJLCz7o_VoyyU,2978
-praxis-0.4.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
-praxis-0.4.0.dist-info/METADATA,sha256=PSCvXmIpxFDYBZQ_jXqc-oAZFUXN1nzEG6L6SJiNWYc,776
-praxis-0.4.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
-praxis-0.4.0.dist-info/top_level.txt,sha256=jQcnqGC96gVr2kN5zV78HwY1gNcsZ-8JnoLEWogxu1g,7
-praxis-0.4.0.dist-info/RECORD,,
+praxis-1.0.0.dist-info/LICENSE,sha256=WNHhf_5RCaeuKWyq_K39vmp9F28LxKsB4SpomwSZ2L0,11357
+praxis-1.0.0.dist-info/METADATA,sha256=SlD1JfFPdNtkMdhC0uFxcFesoaBWr6GBw3-3HvP172U,888
+praxis-1.0.0.dist-info/WHEEL,sha256=g4nMs7d-Xl9-xC9XovUrsDHGXt-FT0E17Yqo92DEfvY,92
+praxis-1.0.0.dist-info/top_level.txt,sha256=jQcnqGC96gVr2kN5zV78HwY1gNcsZ-8JnoLEWogxu1g,7
+praxis-1.0.0.dist-info/RECORD,,
```

