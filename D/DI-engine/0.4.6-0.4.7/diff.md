# Comparing `tmp/DI-engine-0.4.6.tar.gz` & `tmp/DI-engine-0.4.7.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "/Users/niuyazhe/code/opendilab/DI-engine/dist/.tmp-_igp0gbs/DI-engine-0.4.6.tar", last modified: Sat Feb 18 14:04:11 2023, max compression
+gzip compressed data, was "/Users/niuyazhe/code/opendilab/DI-engine/dist/.tmp-12iumh9g/DI-engine-0.4.7.tar", last modified: Wed Apr 12 15:08:02 2023, max compression
```

## Comparing `DI-engine-0.4.6.tar` & `DI-engine-0.4.7.tar`

### file list

```diff
@@ -1,1335 +1,1361 @@
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.187535 DI-engine-0.4.6/
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:09.998222 DI-engine-0.4.6/DI_engine.egg-info/
--rw-r--r--   0 niuyazhe (1370690143) 453037844    51288 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/PKG-INFO
--rw-r--r--   0 niuyazhe (1370690143) 453037844    48176 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/SOURCES.txt
--rw-r--r--   0 niuyazhe (1370690143) 453037844        1 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/dependency_links.txt
--rw-r--r--   0 niuyazhe (1370690143) 453037844       86 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/entry_points.txt
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1140 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/requires.txt
--rw-r--r--   0 niuyazhe (1370690143) 453037844       11 2023-02-18 14:04:09.000000 DI-engine-0.4.6/DI_engine.egg-info/top_level.txt
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11342 2021-07-08 05:53:26.000000 DI-engine-0.4.6/LICENSE
--rw-r--r--   0 niuyazhe (1370690143) 453037844    51288 2023-02-18 14:04:11.187002 DI-engine-0.4.6/PKG-INFO
--rw-r--r--   0 niuyazhe (1370690143) 453037844    50022 2023-02-17 09:50:07.000000 DI-engine-0.4.6/README.md
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.000213 DI-engine-0.4.6/ding/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      380 2023-02-18 14:00:24.000000 DI-engine-0.4.6/ding/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.004118 DI-engine-0.4.6/ding/bonus/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       23 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/bonus/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5807 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/bonus/config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      901 2023-01-06 07:07:56.000000 DI-engine-0.4.6/ding/bonus/demo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9348 2023-01-29 08:17:58.000000 DI-engine-0.4.6/ding/bonus/model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7796 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/bonus/ppof.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      211 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/compatibility.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.006524 DI-engine-0.4.6/ding/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      227 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    24376 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/config/config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    28087 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/config/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.010357 DI-engine-0.4.6/ding/data/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      352 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.014380 DI-engine-0.4.6/ding/data/buffer/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      150 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7617 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/data/buffer/buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14785 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/data/buffer/deque_buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4750 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/data/buffer/deque_buffer_wrapper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.020647 DI-engine-0.4.6/ding/data/buffer/middleware/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      291 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1064 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/clone_object.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1648 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/group_sample.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1496 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/padding.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6962 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/priority.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      979 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/data/buffer/middleware/sample_range_view.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1830 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/staleness_check.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1887 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/data/buffer/middleware/use_time_check.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.021364 DI-engine-0.4.6/ding/data/level_replay/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/data/level_replay/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13812 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/data/level_replay/level_sampler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5501 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/model_loader.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5008 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/shm_buffer.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.023716 DI-engine-0.4.6/ding/data/storage/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/storage/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      605 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/storage/file.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      326 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/storage/storage.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11978 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/data/storage_loader.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.044198 DI-engine-0.4.6/ding/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1684 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11820 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/application_entry.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9797 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/application_entry_drex_collect_data.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6652 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/application_entry_trex_collect_data.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13003 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/cli.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5475 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/cli_ditask.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.046178 DI-engine-0.4.6/ding/entry/cli_parsers/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      136 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/entry/cli_parsers/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5855 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/cli_parsers/k8s_parser.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5656 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/cli_parsers/slurm_parser.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11742 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/entry/dist_entry.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5631 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/entry/parallel_entry.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1156 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/entry/predefined_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6184 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/serial_entry.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4005 2022-10-31 06:23:24.000000 DI-engine-0.4.6/ding/entry/serial_entry_bc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8933 2023-01-04 14:21:42.000000 DI-engine-0.4.6/ding/entry/serial_entry_bco.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3405 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_decision_transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12388 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_dqfd.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8037 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/serial_entry_gail.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8452 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_guided_cost.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10336 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_mbrl.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7979 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_ngu.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4645 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_offline.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4963 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/entry/serial_entry_onpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4460 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_onpolicy_ppg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5614 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_plr.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6520 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/entry/serial_entry_preference_based_irl.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4811 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/entry/serial_entry_preference_based_irl_onpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12314 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_r2d3.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6279 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_reward_model_offpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6221 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_reward_model_onpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8837 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_sqil.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10374 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/entry/serial_entry_td3_vae.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2342 2022-09-16 15:18:07.000000 DI-engine-0.4.6/ding/entry/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.046840 DI-engine-0.4.6/ding/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       74 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.049440 DI-engine-0.4.6/ding/envs/common/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      354 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/envs/common/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10829 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/envs/common/common_function.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1625 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/common/env_element.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      986 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/common/env_element_runner.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.052491 DI-engine-0.4.6/ding/envs/env/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      386 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/env/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6902 2023-01-02 16:43:42.000000 DI-engine-0.4.6/ding/envs/env/base_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1638 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env/default_wrapper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8210 2023-02-15 08:06:41.000000 DI-engine-0.4.6/ding/envs/env/ding_env_wrapper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7078 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env/env_implementation_check.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.054796 DI-engine-0.4.6/ding/envs/env/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       30 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/envs/env/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2216 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env/tests/demo_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7618 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env/tests/test_ding_env_wrapper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1974 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/envs/env/tests/test_env_implementation_check.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.059773 DI-engine-0.4.6/ding/envs/env_manager/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      394 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/env_manager/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21000 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env_manager/base_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    24099 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env_manager/env_supervisor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4222 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env_manager/envpool_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5793 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/envs/env_manager/gym_vector_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    36957 2023-02-13 06:53:11.000000 DI-engine-0.4.6/ding/envs/env_manager/subprocess_env_manager.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.067265 DI-engine-0.4.6/ding/envs/env_manager/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8258 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/conftest.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9270 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_base_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    18474 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_env_supervisor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1456 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_envpool_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2306 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_gym_vector_env_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      783 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_shm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9590 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/envs/env_manager/tests/test_subprocess_env_manager.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.068887 DI-engine-0.4.6/ding/envs/env_wrappers/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       28 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/envs/env_wrappers/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    40996 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/envs/env_wrappers/env_wrappers.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.086385 DI-engine-0.4.6/ding/example/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-04 06:30:14.000000 DI-engine-0.4.6/ding/example/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1997 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/c51_nstep.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1509 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/collect_demo_data.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1747 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/cql.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2098 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/d4pg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1962 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/ddpg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4394 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1928 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn_her.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1949 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn_new_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2056 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn_nstep.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2112 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn_per.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2028 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/dqn_rnd.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/iqn_nstep.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1838 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/pdqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2539 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/ppg_offpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1807 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/ppo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1895 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/ppo_offpolicy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2003 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/qrdqn_nstep.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1934 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/r2d2.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/sac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3213 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/sqil.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3377 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/sqil_continuous.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1842 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/sql.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1866 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/td3.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2447 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/example/trex.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.090119 DI-engine-0.4.6/ding/framework/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      361 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/framework/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3095 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/framework/context.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4120 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/framework/event_loop.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.093085 DI-engine-0.4.6/ding/framework/message_queue/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       69 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/framework/message_queue/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1635 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/framework/message_queue/mq.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2524 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/framework/message_queue/nng.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/framework/message_queue/redis.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.096144 DI-engine-0.4.6/ding/framework/middleware/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      244 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3017 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/framework/middleware/ckpt_handler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8143 2023-02-17 05:18:00.000000 DI-engine-0.4.6/ding/framework/middleware/collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11569 2022-12-13 15:12:49.000000 DI-engine-0.4.6/ding/framework/middleware/distributer.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.102136 DI-engine-0.4.6/ding/framework/middleware/functional/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      792 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/functional/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3256 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/functional/advantage_estimator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5456 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/functional/collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      772 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/framework/middleware/functional/ctx_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11619 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/functional/data_processor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3968 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/functional/enhancer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17372 2023-02-13 07:02:35.000000 DI-engine-0.4.6/ding/framework/middleware/functional/evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1541 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/functional/explorer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16332 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/functional/logger.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2054 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/functional/termination_checker.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1021 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/functional/timer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3611 2023-02-16 11:31:32.000000 DI-engine-0.4.6/ding/framework/middleware/functional/trainer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3765 2023-02-17 06:37:12.000000 DI-engine-0.4.6/ding/framework/middleware/learner.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.108502 DI-engine-0.4.6/ding/framework/middleware/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2023-01-29 05:51:22.000000 DI-engine-0.4.6/ding/framework/middleware/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3256 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/tests/mock_for_test.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3198 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_advantage_estimator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2074 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_ckpt_handler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3049 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9202 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_data_processor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7070 2022-12-28 05:45:32.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_distributer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2590 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_enhancer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1200 2022-10-19 12:18:13.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      731 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_explorer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8865 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_logger.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3379 2023-01-29 05:51:23.000000 DI-engine-0.4.6/ding/framework/middleware/tests/test_trainer.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.111813 DI-engine-0.4.6/ding/framework/middleware_v3/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      167 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2553 2022-09-04 07:49:02.000000 DI-engine-0.4.6/ding/framework/middleware_v3/ckpt_handler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5011 2022-09-01 10:45:00.000000 DI-engine-0.4.6/ding/framework/middleware_v3/collector.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.118922 DI-engine-0.4.6/ding/framework/middleware_v3/functional/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      578 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2742 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/advantage_estimator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5391 2022-09-05 08:12:51.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      746 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/ctx_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10145 2022-09-01 11:15:00.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/data_processor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3672 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/enhancer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7765 2022-09-01 11:07:51.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1408 2022-09-01 10:26:49.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/explorer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      742 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/termination_checker.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2951 2022-09-01 11:16:03.000000 DI-engine-0.4.6/ding/framework/middleware_v3/functional/trainer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3507 2022-09-01 11:09:28.000000 DI-engine-0.4.6/ding/framework/middleware_v3/learner.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.126404 DI-engine-0.4.6/ding/framework/middleware_v3/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2023-01-29 05:51:23.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3252 2022-09-01 10:26:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/mock_for_test.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3198 2022-09-01 10:26:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_advantage_estimator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1987 2022-09-04 07:49:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_ckpt_handler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3122 2022-09-01 10:26:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8230 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_data_processor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2642 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_enhancer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1200 2022-09-01 10:26:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      731 2022-09-01 10:26:15.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_explorer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3286 2022-09-01 10:26:16.000000 DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_trainer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15133 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/parallel.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14388 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/supervisor.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19497 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/framework/task.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.127717 DI-engine-0.4.6/ding/framework/wrapper/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/framework/wrapper/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1922 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/framework/wrapper/step_timer.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.129408 DI-engine-0.4.6/ding/hpc_rl/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       33 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/hpc_rl/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5953 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/hpc_rl/wrapper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.130345 DI-engine-0.4.6/ding/interaction/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       43 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.134775 DI-engine-0.4.6/ding/interaction/base/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      382 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/base/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2806 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/base/app.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5341 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/base/common.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4846 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/base/network.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2163 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/base/threading.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.135825 DI-engine-0.4.6/ding/interaction/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      297 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      432 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/config/base.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.138439 DI-engine-0.4.6/ding/interaction/exception/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      848 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/exception/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2466 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/exception/base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3512 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/exception/master.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3914 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/exception/slave.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.143468 DI-engine-0.4.6/ding/interaction/master/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       27 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/master/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      293 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/master/base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17041 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/master/connection.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    27918 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/master/master.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9524 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/master/task.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.145578 DI-engine-0.4.6/ding/interaction/slave/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      105 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/slave/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3753 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/slave/action.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19857 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/slave/slave.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.146514 DI-engine-0.4.6/ding/interaction/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.149333 DI-engine-0.4.6/ding/interaction/tests/base/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      318 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/base/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6161 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/base/test_app.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2036 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/base/test_common.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6046 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/base/test_network.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3324 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/base/test_threading.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.150799 DI-engine-0.4.6/ding/interaction/tests/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       45 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      226 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/config/test_base.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.153298 DI-engine-0.4.6/ding/interaction/tests/exception/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      110 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/exception/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1562 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/exception/test_base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2877 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/exception/test_master.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3293 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/exception/test_slave.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.157035 DI-engine-0.4.6/ding/interaction/tests/interaction/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/interaction/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1401 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/interaction/bases.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1439 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/interaction/test_errors.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4795 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/interaction/test_simple.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.158667 DI-engine-0.4.6/ding/interaction/tests/test_utils/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/test_utils/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      417 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/test_utils/random.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1099 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/interaction/tests/test_utils/stream.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.163123 DI-engine-0.4.6/ding/league/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      345 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1749 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/algorithm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13037 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/base_league.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4590 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/metric.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5355 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/one_vs_one_league.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13828 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/player.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10938 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/shared_payoff.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9522 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/league/starcraft_player.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.163725 DI-engine-0.4.6/ding/model/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       69 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.166488 DI-engine-0.4.6/ding/model/common/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      342 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/model/common/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12097 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/model/common/encoder.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    55770 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/model/common/head.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      628 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/common/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.188250 DI-engine-0.4.6/ding/model/template/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      736 2023-02-13 06:42:54.000000 DI-engine-0.4.6/ding/model/template/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8851 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/acer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    18749 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/atoc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8292 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/model/template/bc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19316 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/model/template/collaq.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7751 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/coma.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4713 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/model/template/decision_transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17708 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/model/template/ebm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1790 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/model/template/madqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    20794 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/maqac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12386 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/model/template/mavac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10574 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/ngu.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9574 2023-02-17 05:20:57.000000 DI-engine-0.4.6/ding/model/template/pdqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2793 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/model/template/pg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2389 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/model/template/ppg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5046 2023-02-13 06:42:54.000000 DI-engine-0.4.6/ding/model/template/procedure_cloning.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    51425 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/model/template/q_learning.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    25229 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/qac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12226 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/qac_dist.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9551 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/model/template/qmix.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7140 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/qtran.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      589 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/sqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15891 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/vac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12328 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/vae.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12586 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/template/wqmix.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.190333 DI-engine-0.4.6/ding/model/wrapper/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       90 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/wrapper/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    37923 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/model/wrapper/model_wrappers.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21725 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/model/wrapper/test_model_wrappers.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.235429 DI-engine-0.4.6/ding/policy/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1471 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/policy/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11277 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/a2c.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    24412 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/acer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16276 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/atoc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10700 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/base_policy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13128 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/bc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19884 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/policy/bdq.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12198 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/c51.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21108 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/collaq.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    18506 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/coma.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12039 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/policy/command_mode_policy_instance.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2023-02-17 05:18:00.000000 DI-engine-0.4.6/ding/policy/common_utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    37338 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/cql.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16025 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/d4pg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    22534 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/ddpg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15677 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/decision_transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13829 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/policy/dqfd.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    34922 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/dqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12522 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/fqf.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7023 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/ibc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9060 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/il.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    23191 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/impala.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9314 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/iqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15461 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/policy/madqn.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.237074 DI-engine-0.4.6/ding/policy/mbpolicy/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       31 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/policy/mbpolicy/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16288 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/mbpolicy/mbsac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1318 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/policy/mbpolicy/utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    28616 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/ngu.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13769 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/offppo_collect_traj.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    22530 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/pdqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8533 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/policy/pg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2892 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/policy/policy_factory.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    56120 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/ppg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    56764 2022-12-23 13:01:31.000000 DI-engine-0.4.6/ding/policy/ppo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13722 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/policy/ppof.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    20258 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/qmix.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9087 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/qrdqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21125 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/qtran.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    24697 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/r2d2.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    23817 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/r2d2_collect_traj.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    24209 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/r2d2_gtrxl.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    26591 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/r2d3.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13997 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/rainbow.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    81478 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/sac.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12700 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/sql.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14465 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/sqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9739 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/td3.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17446 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/policy/td3_bc.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    32894 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/td3_vae.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15861 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/policy/wqmix.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.249652 DI-engine-0.4.6/ding/reward_model/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      670 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4083 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/base_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2587 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/reward_model/drex_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12340 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/gail_irl_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5736 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/guided_cost_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6595 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/her_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10782 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/icm_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    26355 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/ngu_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7638 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/pdeil_irl_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9507 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/pwil_irl_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7485 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/red_irl_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8430 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/reward_model/rnd_reward_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19372 2023-01-06 07:05:23.000000 DI-engine-0.4.6/ding/reward_model/trex_reward_model.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.379431 DI-engine-0.4.6/ding/rl_utils/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1829 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/rl_utils/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1700 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/a2c.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4723 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/acer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10256 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/adder.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1127 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/beta_function.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2438 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/coma.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7580 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/exploration.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2435 2023-01-02 12:59:23.000000 DI-engine-0.4.6/ding/rl_utils/gae.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2782 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/rl_utils/isw.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1409 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/ppg.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12781 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/rl_utils/ppo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1729 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/retrace.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1510 2023-02-12 11:34:47.000000 DI-engine-0.4.6/ding/rl_utils/sampler.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    57170 2023-01-04 09:44:45.000000 DI-engine-0.4.6/ding/rl_utils/td.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3752 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/upgo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      664 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/rl_utils/value_rescale.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9477 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/rl_utils/vtrace.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      722 2023-02-17 05:30:28.000000 DI-engine-0.4.6/ding/t.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.388480 DI-engine-0.4.6/ding/torch_utils/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      745 2022-11-16 04:38:22.000000 DI-engine-0.4.6/ding/torch_utils/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12831 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/checkpoint_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13718 2022-12-26 15:42:15.000000 DI-engine-0.4.6/ding/torch_utils/data_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      358 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/dataparallel.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9024 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/distribution.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.392286 DI-engine-0.4.6/ding/torch_utils/loss/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      178 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/loss/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4196 2023-01-28 12:50:23.000000 DI-engine-0.4.6/ding/torch_utils/loss/contrastive_loss.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2514 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/loss/cross_entropy_loss.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4820 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/torch_utils/loss/multi_logits_loss.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1648 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/math_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3278 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/metric.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.403154 DI-engine-0.4.6/ding/torch_utils/network/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      651 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/torch_utils/network/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3186 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/activation.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    35769 2022-01-21 07:34:18.000000 DI-engine-0.4.6/ding/torch_utils/network/coverage.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844    25400 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/torch_utils/network/gtrxl.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1808 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/gumbel_softmax.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    25613 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/torch_utils/network/nn_module.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1264 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/normalization.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5688 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/torch_utils/network/res_block.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21848 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/resnet.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13797 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/rnn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3626 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/scatter_connection.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/soft_argmax.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8667 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/network/transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1836 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/torch_utils/nn_test_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    32328 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/torch_utils/optimizer_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3018 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/torch_utils/reshape_helper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.433047 DI-engine-0.4.6/ding/utils/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2569 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/utils/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.437265 DI-engine-0.4.6/ding/utils/autolog/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      236 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      701 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5499 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/data.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9391 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/model.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.440907 DI-engine-0.4.6/ding/utils/autolog/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2304 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/tests/test_data.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17237 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/tests/test_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3452 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/tests/test_time.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5855 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/time_ctl.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1407 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/autolog/value.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      818 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/collection_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4942 2023-02-13 06:43:00.000000 DI-engine-0.4.6/ding/utils/compression_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844   179593 2021-10-17 09:03:07.000000 DI-engine-0.4.6/ding/utils/coverage.xml
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.444413 DI-engine-0.4.6/ding/utils/data/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      284 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/data/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1052 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/data/base_dataloader.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11063 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/utils/data/collate_fn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17023 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/data/dataloader.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    20438 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/data/dataset.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.446580 DI-engine-0.4.6/ding/utils/data/structure/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       59 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/utils/data/structure/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5297 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/data/structure/cache.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      300 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/utils/data/structure/lifo_deque.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19782 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/default_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      650 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/design_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      344 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/fake_linklink.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1480 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/fast_copy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10416 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/file_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2897 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/import_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7063 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/k8s_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5414 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/linklink_dist_helper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.454787 DI-engine-0.4.6/ding/utils/loader/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      783 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4306 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3043 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/collection.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      871 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/dict.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      369 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/exception.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3093 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/mapping.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5747 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/norm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6391 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/number.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2228 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/string.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.456360 DI-engine-0.4.6/ding/utils/loader/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      111 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.464655 DI-engine-0.4.6/ding/utils/loader/tests/loader/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      426 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5225 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5989 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_collection.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1090 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_dict.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1914 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_mapping.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11175 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_norm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    21822 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_number.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4119 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_string.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3183 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_types.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1478 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/loader/test_utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4334 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/tests/test_cartpole_dqn_serial_config_loader.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1366 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/types.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      409 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/loader/utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2796 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/lock_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5332 2022-11-14 04:31:11.000000 DI-engine-0.4.6/ding/utils/log_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4397 2022-10-19 10:55:38.000000 DI-engine-0.4.6/ding/utils/log_writer_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4657 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/orchestrator_launcher.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1135 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/profiler_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3999 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/utils/pytorch_ddp_dist_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3853 2023-01-02 16:52:37.000000 DI-engine-0.4.6/ding/utils/registry.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1495 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/utils/registry_factory.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1525 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/utils/render_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7572 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/scheduler_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9207 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/segment_tree.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2855 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/slurm_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1743 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/system_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4520 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/time_helper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      956 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/time_helper_base.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1908 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/time_helper_cuda.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      173 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/utils/type_helper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.465522 DI-engine-0.4.6/ding/worker/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      127 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.467877 DI-engine-0.4.6/ding/worker/adapter/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       50 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/adapter/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13378 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/adapter/learner_aggregator.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.482352 DI-engine-0.4.6/ding/worker/collector/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1080 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/worker/collector/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8862 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/base_parallel_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7405 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/worker/collector/base_serial_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8544 2022-12-26 08:59:29.000000 DI-engine-0.4.6/ding/worker/collector/base_serial_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15498 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/battle_episode_serial_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12060 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/battle_interaction_serial_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16301 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/battle_sample_serial_collector.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.486697 DI-engine-0.4.6/ding/worker/collector/comm/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      177 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/comm/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4089 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/comm/base_comm_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8874 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/comm/flask_fs_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2436 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/comm/utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14697 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/episode_serial_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13920 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/worker/collector/interaction_serial_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14421 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/marine_parallel_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9156 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/metric_serial_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17609 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/sample_serial_collector.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.493443 DI-engine-0.4.6/ding/worker/collector/tests/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      698 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/fake_cls_policy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2804 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/fake_cpong_dqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.498030 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2917 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/fake_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3034 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/fake_policy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7399 2023-01-02 10:31:34.000000 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/test_collector_profile.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      107 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/speed_test/utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2277 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/test_episode_serial_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2846 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/tests/test_marine_parallel_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3019 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/collector/tests/test_metric_serial_evaluator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11708 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/collector/zergling_parallel_collector.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.507443 DI-engine-0.4.6/ding/worker/coordinator/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      183 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7099 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/base_parallel_commander.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2070 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/base_serial_commander.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    26271 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/comm_coordinator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    20584 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/coordinator.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16808 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/coordinator/one_vs_one_parallel_commander.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3634 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/operator_server.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2675 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/coordinator/resource_manager.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10871 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/coordinator/solo_parallel_commander.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.510627 DI-engine-0.4.6/ding/worker/learner/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      252 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/learner/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    20149 2022-12-13 15:12:50.000000 DI-engine-0.4.6/ding/worker/learner/base_learner.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.514020 DI-engine-0.4.6/ding/worker/learner/comm/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      165 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/learner/comm/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4937 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/learner/comm/base_comm_learner.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15576 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/learner/comm/flask_fs_learner.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2120 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/learner/comm/utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15500 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/worker/learner/learner_hook.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.521932 DI-engine-0.4.6/ding/worker/replay_buffer/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      206 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/replay_buffer/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    37644 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/worker/replay_buffer/advanced_buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4460 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/worker/replay_buffer/base_buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      800 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/replay_buffer/episode_buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    19279 2022-12-11 15:35:39.000000 DI-engine-0.4.6/ding/worker/replay_buffer/naive_buffer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10920 2022-09-30 04:52:24.000000 DI-engine-0.4.6/ding/worker/replay_buffer/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.530364 DI-engine-0.4.6/ding/world_model/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      139 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/world_model/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    13985 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/world_model/base_world_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    22524 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/world_model/ddppo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6228 2022-10-19 09:16:29.000000 DI-engine-0.4.6/ding/world_model/idm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11421 2022-08-02 16:56:36.000000 DI-engine-0.4.6/ding/world_model/mbpo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      852 2022-07-19 08:28:37.000000 DI-engine-0.4.6/ding/world_model/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.531437 DI-engine-0.4.6/dizoo/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.531925 DI-engine-0.4.6/dizoo/atari/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.532378 DI-engine-0.4.6/dizoo/atari/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/config/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.532863 DI-engine-0.4.6/dizoo/atari/config/serial/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      191 2022-07-19 08:28:37.000000 DI-engine-0.4.6/dizoo/atari/config/serial/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.537915 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2022-07-19 08:28:37.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1731 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2976 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2237 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1758 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1816 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_rainbow_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.562421 DI-engine-0.4.6/dizoo/atari/config/serial/pong/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      233 2022-07-19 08:28:37.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2064 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_a2c_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2480 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_acer_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1742 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_c51_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2123 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2877 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqfd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1673 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1853 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_envpool_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1711 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_multi_gpu_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1814 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_render_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1975 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_stdim_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1784 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_fqf_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3109 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_gail_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2735 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1724 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_iqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4989 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_ngu_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2137 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2615 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2065 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_qrdqn_generation_data_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3300 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3011 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_gtrxl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3017 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_residual_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6446 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d3_offppoexpert_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6795 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d3_r2d2expert_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1729 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_rainbow_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2290 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_sqil_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1571 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_sql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4218 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_trex_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3561 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_trex_sql_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.583269 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      146 2022-07-19 08:28:37.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2058 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_a2c_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2541 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_acer_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1669 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_c51_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2076 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2732 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_dqfd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1844 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_fqf_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2808 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1650 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_iqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2154 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2184 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2459 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1645 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2073 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_qrdqn_generation_data_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3387 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_r2d2_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2760 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_r2d2_gtrxl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1757 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_rainbow_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2415 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_sqil_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1684 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_sql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3001 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_trex_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3442 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_trex_offppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.610802 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      194 2022-07-19 08:28:37.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2266 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_a2c_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2811 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_acer_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1937 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_c51_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2925 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2029 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1895 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_ddp.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1978 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_dp.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1939 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_fqf_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3105 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1909 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_iqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2395 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2310 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2716 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1912 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3492 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3033 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_gtrxl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3167 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_residual_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2031 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_rainbow_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2516 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sqil_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1827 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4055 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4554 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_offppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.623150 DI-engine-0.4.6/dizoo/atari/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3135 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/atari_dqn_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3534 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/atari_ppg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3272 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/entry/phoenix_fqf_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3272 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/entry/phoenix_iqn_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2118 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/pong_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3743 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/atari/entry/pong_dqn_envpool_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3260 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/entry/pong_fqf_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2151 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/qbert_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3264 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/entry/qbert_fqf_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2185 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3251 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_ddp.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_dp.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3296 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_fqf_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.626331 DI-engine-0.4.6/dizoo/atari/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       44 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/atari/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5233 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/envs/atari_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6246 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/atari/envs/atari_wrappers.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2440 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/atari/envs/test_atari_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.627021 DI-engine-0.4.6/dizoo/beergame/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:07.000000 DI-engine-0.4.6/dizoo/beergame/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.632750 DI-engine-0.4.6/dizoo/beergame/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8359 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/beergame/envs/BGAgent.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844       71 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/beergame/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4738 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/beergame/envs/beergame_core.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3046 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/beergame/envs/beergame_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    23724 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/beergame/envs/clBeergame.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2721 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/beergame/envs/plotting.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17343 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/beergame/envs/utils.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.633938 DI-engine-0.4.6/dizoo/bitflip/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-03-14 08:09:40.000000 DI-engine-0.4.6/dizoo/bitflip/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.636037 DI-engine-0.4.6/dizoo/bitflip/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      183 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2616 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/config/bitflip_her_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/config/bitflip_pure_dqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.637450 DI-engine-0.4.6/dizoo/bitflip/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-03-14 08:09:40.000000 DI-engine-0.4.6/dizoo/bitflip/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4350 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/entry/bitflip_dqn_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.639985 DI-engine-0.4.6/dizoo/bitflip/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/bitflip/envs/bitflip_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      756 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/bitflip/envs/test_bitfilp_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.640694 DI-engine-0.4.6/dizoo/box2d/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/box2d/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.641120 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       47 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.649556 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       96 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3050 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_bco_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2504 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_dt_config.py
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844     3504 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2819 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1837 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2644 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppopg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2078 2022-09-30 04:52:24.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2135 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_td3_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.654113 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2181 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/entry/bipedalwalker_ppo_eval.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.658322 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       48 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3868 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1034 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/test_bipedalwalker.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.659969 DI-engine-0.4.6/dizoo/box2d/carracing/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.661599 DI-engine-0.4.6/dizoo/box2d/carracing/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       85 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1767 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/config/carracing_dqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.665043 DI-engine-0.4.6/dizoo/box2d/carracing/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       41 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5913 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/envs/carracing_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1082 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/box2d/carracing/envs/test_carracing_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.667063 DI-engine-0.4.6/dizoo/box2d/lunarlander/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.696984 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      597 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1375 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_a2c_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3181 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_acer_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2827 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_bco_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1698 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_c51_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2418 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1922 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2269 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_vae_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2484 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_decision_transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2086 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_discrete_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2888 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqfd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2636 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2580 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqn_deque_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3962 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_gail_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2272 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_gcl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5345 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_ngu_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1502 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1381 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1530 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3325 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d2_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3056 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d2_gtrxl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6624 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d3_ppoexpert_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6992 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d3_r2d2expert_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2612 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_rnd_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2498 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_sqil_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1623 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_sql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4289 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_trex_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3462 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_trex_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8211 2022-09-28 13:14:17.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/config/t.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.699413 DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2177 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/lunarlander_dqn_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2985 2023-02-13 06:56:00.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/lunarlander_dqn_example.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.702314 DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       44 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5535 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/lunarlander_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1248 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/test_lunarlander_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.703292 DI-engine-0.4.6/dizoo/bsuite/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/bsuite/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.703976 DI-engine-0.4.6/dizoo/bsuite/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        1 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/bsuite/config/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.708207 DI-engine-0.4.6/dizoo/bsuite/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/bsuite/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3752 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/bsuite/envs/bsuite_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1467 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/bsuite/envs/test_bsuite_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.710119 DI-engine-0.4.6/dizoo/classic_control/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.710814 DI-engine-0.4.6/dizoo/classic_control/acrobot/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.712579 DI-engine-0.4.6/dizoo/classic_control/acrobot/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       79 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1659 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/config/acrobot_dqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.715799 DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3591 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/acrobot_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1323 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/test_acrobot_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.717029 DI-engine-0.4.6/dizoo/classic_control/cartpole/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.763964 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1956 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1682 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_a2c_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2489 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_acer_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1330 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2524 2022-10-31 06:23:20.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_bco_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_c51_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1649 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2585 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_decision_transformer.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2849 2022-10-31 06:23:20.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqfd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1776 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2701 2022-10-31 06:23:20.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_gail_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1556 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_rnd_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2055 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_stdim_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3173 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_drex_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1813 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_fqf_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2423 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_gcl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2781 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_impala_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1649 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_iqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4857 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ngu_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1277 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2357 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1684 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1894 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_icm_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1797 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_offpolicy_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2084 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_stdim_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1383 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppopg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1672 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_qrdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1739 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_qrdqn_generation_data_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2909 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2775 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_gtrxl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3219 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_residual_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1664 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_rainbow_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2592 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_rnd_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2258 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sqil_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1554 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1702 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2845 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2948 2023-02-13 06:42:54.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_offppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3044 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_onppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.767443 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      109 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2921 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3286 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config_k8s.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.779786 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1282 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_c51_deploy.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3403 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_c51_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2217 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3391 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_buffer_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2014 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3857 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3959 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_fqf_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3582 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2278 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppo_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2781 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppo_offpolicy_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2470 2021-07-16 08:07:45.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/formatted_total_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.783578 DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3774 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/cartpole_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1327 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/test_cartpole_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1350 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/test_cartpole_env_manager.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.784896 DI-engine-0.4.6/dizoo/classic_control/mountain_car/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/mountain_car/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.788894 DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4679 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/mtcar_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1400 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/test_mtcar_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.790566 DI-engine-0.4.6/dizoo/classic_control/pendulum/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.806104 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      619 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1986 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_bdq_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1866 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1851 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_d4pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1763 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1949 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1353 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ibc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1376 2022-11-16 06:44:04.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1816 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1766 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1347 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sac_data_generation_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2664 2023-01-06 07:05:23.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sqil_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2234 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1832 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2203 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_data_generation_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.814273 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2177 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_cql_ddpg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2266 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3083 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_d4pg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3576 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_ddpg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2178 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_dqn_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2178 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_ppo_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2325 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_td3_bc_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3381 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_td3_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.817103 DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4943 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/pendulum_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2226 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/test_pendulum_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.818015 DI-engine-0.4.6/dizoo/common/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/common/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.820899 DI-engine-0.4.6/dizoo/common/policy/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/common/policy/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4323 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/common/policy/md_dqn.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8472 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/common/policy/md_ppo.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4183 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/common/policy/md_rainbow_dqn.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.821860 DI-engine-0.4.6/dizoo/competitive_rl/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/competitive_rl/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.825388 DI-engine-0.4.6/dizoo/competitive_rl/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       49 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/competitive_rl/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6561 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/competitive_rl/envs/competitive_rl_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8597 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/competitive_rl/envs/competitive_rl_env_wrapper.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2554 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/competitive_rl/envs/test_competitive_rl.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.826655 DI-engine-0.4.6/dizoo/d4rl/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.883576 DI-engine-0.4.6/dizoo/d4rl/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      175 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2450 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2450 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1499 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2471 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1746 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1499 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2471 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1746 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2449 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1449 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1489 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2406 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1458 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_ar_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1342 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1348 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_mcmc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1736 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1489 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2406 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1736 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2384 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1448 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1453 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_ar_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1337 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1343 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_mcmc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1431 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/pen_human_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1501 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_ar_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1324 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1330 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_mcmc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2411 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2411 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1493 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1493 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_td3bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2410 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_dt_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_td3bc_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.887401 DI-engine-0.4.6/dizoo/d4rl/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      685 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1407 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_dt_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1175 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_ibc_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      694 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_td3_bc_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.890542 DI-engine-0.4.6/dizoo/d4rl/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       30 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/d4rl/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3986 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/d4rl/envs/d4rl_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2286 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/d4rl/envs/d4rl_wrappers.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.891525 DI-engine-0.4.6/dizoo/dmc2gym/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/dmc2gym/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.895865 DI-engine-0.4.6/dizoo/dmc2gym/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/dmc2gym/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7921 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/dmc2gym/envs/dmc2gym_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1644 2022-08-02 16:56:36.000000 DI-engine-0.4.6/dizoo/dmc2gym/envs/test_dmc2gym_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.897808 DI-engine-0.4.6/dizoo/evogym/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/evogym/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.901885 DI-engine-0.4.6/dizoo/evogym/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/evogym/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6926 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/evogym/envs/evogym_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.903378 DI-engine-0.4.6/dizoo/gfootball/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.912591 DI-engine-0.4.6/dizoo/gfootball/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2523 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2542 2022-10-31 06:23:24.000000 DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_kaggle5th_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4374 2022-10-31 06:23:24.000000 DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_rule_lt0_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4010 2022-10-31 06:23:24.000000 DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_rule_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1896 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2197 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/entry/show_dataset.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1884 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/entry/test_accuracy.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.919757 DI-engine-0.4.6/dizoo/gfootball/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      174 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2648 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/envs/fake_dataset.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    14135 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gfootball/envs/gfootball_academy_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8689 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gfootball/envs/gfootball_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7430 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gfootball/envs/gfootballsp_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.920702 DI-engine-0.4.6/dizoo/gfootball/model/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/model/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.923932 DI-engine-0.4.6/dizoo/gfootball/model/bots/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      120 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/model/bots/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1780 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/model/bots/kaggle_5th_place_model.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    26957 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gfootball/model/bots/rule_based_bot_model.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.926645 DI-engine-0.4.6/dizoo/gfootball/policy/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       55 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/policy/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15237 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/policy/ppo_lstm.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1268 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gfootball/replay.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.927805 DI-engine-0.4.6/dizoo/gym_anytrading/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.929391 DI-engine-0.4.6/dizoo/gym_anytrading/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2952 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/config/stocks_dqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.934703 DI-engine-0.4.6/dizoo/gym_anytrading/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       90 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6412 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/envs/stocks_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1506 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/envs/test_stocks_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10840 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gym_anytrading/envs/trading_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.936698 DI-engine-0.4.6/dizoo/gym_anytrading/worker/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       51 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_anytrading/worker/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9730 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gym_anytrading/worker/trading_serial_evaluator.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.937786 DI-engine-0.4.6/dizoo/gym_hybrid/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gym_hybrid/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.942563 DI-engine-0.4.6/dizoo/gym_hybrid/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-12-22 08:45:23.000000 DI-engine-0.4.6/dizoo/gym_hybrid/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1985 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_hppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2747 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_mpdqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2675 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_pdqn_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.947226 DI-engine-0.4.6/dizoo/gym_hybrid/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_hybrid/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2316 2022-12-26 09:01:35.000000 DI-engine-0.4.6/dizoo/gym_hybrid/entry/e.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3996 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.950772 DI-engine-0.4.6/dizoo/gym_hybrid/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       41 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gym_hybrid/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5940 2023-01-02 10:31:34.000000 DI-engine-0.4.6/dizoo/gym_hybrid/envs/gym_hybrid_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1149 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/gym_hybrid/envs/test_gym_hybrid_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.952062 DI-engine-0.4.6/dizoo/gym_pybullet_drones/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/gym_pybullet_drones/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.956411 DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       58 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9546 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/gym_pybullet_drones_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1034 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/test_ding_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      830 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/test_ori_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.957832 DI-engine-0.4.6/dizoo/gym_soccer/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gym_soccer/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.960481 DI-engine-0.4.6/dizoo/gym_soccer/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/gym_soccer/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6247 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gym_soccer/envs/gym_soccer_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1248 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/gym_soccer/envs/test_gym_soccer_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.961892 DI-engine-0.4.6/dizoo/image_classification/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.964981 DI-engine-0.4.6/dizoo/image_classification/data/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/data/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4660 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/data/dataset.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2207 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/data/sampler.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.967219 DI-engine-0.4.6/dizoo/image_classification/policy/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       46 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/policy/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2961 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/image_classification/policy/policy.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.981031 DI-engine-0.4.6/dizoo/league_demo/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/league_demo/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1805 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/league_demo/demo_league.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3024 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/league_demo/game_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16428 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/league_demo/league_demo_collector.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2768 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/league_demo/league_demo_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    10209 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/league_demo/league_demo_ppo_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1199 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/league_demo/selfplay_demo_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4662 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/league_demo/selfplay_demo_ppo_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.984553 DI-engine-0.4.6/dizoo/mario/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:08:18.000000 DI-engine-0.4.6/dizoo/mario/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1354 2023-01-17 12:08:18.000000 DI-engine-0.4.6/dizoo/mario/mario_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2787 2023-02-13 06:54:46.000000 DI-engine-0.4.6/dizoo/mario/mario_dqn_example.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4454 2023-01-17 12:08:18.000000 DI-engine-0.4.6/dizoo/mario/mario_dqn_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.985652 DI-engine-0.4.6/dizoo/metadrive/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.987372 DI-engine-0.4.6/dizoo/metadrive/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4258 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/config/metadrive_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3693 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/config/metadrive_onppo_eval_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.991107 DI-engine-0.4.6/dizoo/metadrive/env/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/env/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    15152 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/env/drive_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4020 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/env/drive_utils.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5642 2023-02-15 07:39:03.000000 DI-engine-0.4.6/dizoo/metadrive/env/drive_wrapper.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.992063 DI-engine-0.4.6/dizoo/minigrid/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      662 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/minigrid/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.997176 DI-engine-0.4.6/dizoo/minigrid/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      271 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/minigrid/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7320 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/minigrid/envs/app_key_to_door_treasure.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6650 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/minigrid/envs/minigrid_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3769 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/minigrid/envs/test_minigrid_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:10.998133 DI-engine-0.4.6/dizoo/mujoco/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.050933 DI-engine-0.4.6/dizoo/mujoco/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2105 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3352 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_gail_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2154 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1705 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1976 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2009 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2544 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_trex_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2829 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/ant_trex_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2954 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_bco_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2057 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_bdq_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2074 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_d4pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1915 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3430 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_gail_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2814 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_gcl_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2335 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2750 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_sqil_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2039 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4130 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_trex_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3966 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_trex_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2847 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_bco_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2246 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_bdq_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2093 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_cql_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1966 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_d4pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1857 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3397 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_gail_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2372 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_gcl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2131 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1936 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2610 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_sac_data_generation_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2601 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_sqil_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2455 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_bc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1969 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2668 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_data_generation_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3635 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_trex_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3870 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/hopper_trex_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2000 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_d4pg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1879 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3579 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gail_ddpg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3425 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gail_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gcl_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2346 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1958 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2567 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_sqil_sac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_td3_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3659 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_trex_onppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3894 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/config/walker2d_trex_sac_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.058856 DI-engine-0.4.6/dizoo/mujoco/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      983 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_cql_generation_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      436 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_cql_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2861 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_d4pg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1937 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ddpg_eval.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2697 2022-11-14 04:31:11.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ddpg_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2338 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ppo_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2290 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_td3_bc_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.064180 DI-engine-0.4.6/dizoo/mujoco/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6144 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_disc_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8606 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1992 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_gym_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1415 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_wrappers.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.065165 DI-engine-0.4.6/dizoo/multiagent_mujoco/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.073406 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      185 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.080168 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8773 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/coupled_half_cheetah.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844     8333 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_ant.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5215 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_ant__stage1.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2900 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer__bckp2.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2570 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer_bckp.xml
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1852 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/coupled_half_cheetah.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5661 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/manyagent_ant.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3593 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/manyagent_swimmer.py
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844     9649 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/mujoco_multi.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4066 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/multi_mujoco_env.py
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844     2404 2022-03-14 08:09:40.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/multiagentenv.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    23700 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/obsk.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.081152 DI-engine-0.4.6/dizoo/overcooked/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/overcooked/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.082608 DI-engine-0.4.6/dizoo/overcooked/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       68 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/overcooked/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2357 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/overcooked/config/overcooked_ppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.085823 DI-engine-0.4.6/dizoo/overcooked/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       58 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/overcooked/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    12316 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/overcooked/envs/overcooked_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/overcooked/envs/test_overcooked_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.087981 DI-engine-0.4.6/dizoo/petting_zoo/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/petting_zoo/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.098434 DI-engine-0.4.6/dizoo/petting_zoo/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1022 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2391 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_atoc_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2456 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_collaq_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2512 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_coma_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2581 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_madqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2960 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_mappo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3032 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_masac_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2082 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_qmix_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2490 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_qtran_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_vdn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     2394 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_wqmix_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.101165 DI-engine-0.4.6/dizoo/petting_zoo/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/petting_zoo/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16652 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/petting_zoo/envs/petting_zoo_simple_spread_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5416 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/petting_zoo/envs/test_petting_zoo_simple_spread_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.102048 DI-engine-0.4.6/dizoo/pomdp/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pomdp/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.106983 DI-engine-0.4.6/dizoo/pomdp/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pomdp/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4022 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/pomdp/envs/atari_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6746 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pomdp/envs/atari_wrappers.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1031 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/pomdp/envs/test_atari_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.109512 DI-engine-0.4.6/dizoo/procgen/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:29.000000 DI-engine-0.4.6/dizoo/procgen/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.119934 DI-engine-0.4.6/dizoo/procgen/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      118 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1719 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/bigfish_plr_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1617 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/bigfish_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1362 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/coinrun_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1616 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/coinrun_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1449 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/coinrun_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1369 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/maze_dqn_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1641 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/maze_ppg_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1447 2022-12-11 15:35:39.000000 DI-engine-0.4.6/dizoo/procgen/config/maze_ppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.122863 DI-engine-0.4.6/dizoo/procgen/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-07-19 08:28:38.000000 DI-engine-0.4.6/dizoo/procgen/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4485 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/procgen/envs/procgen_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      810 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/procgen/envs/test_coinrun_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.123900 DI-engine-0.4.6/dizoo/pybullet/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pybullet/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.127090 DI-engine-0.4.6/dizoo/pybullet/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pybullet/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    11254 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/pybullet/envs/pybullet_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1663 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/pybullet/envs/pybullet_wrappers.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.128053 DI-engine-0.4.6/dizoo/rocket/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:32.000000 DI-engine-0.4.6/dizoo/rocket/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.129996 DI-engine-0.4.6/dizoo/rocket/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:45.000000 DI-engine-0.4.6/dizoo/rocket/config/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1818 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/rocket/config/rocket_hover_ppo_config.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1828 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/rocket/config/rocket_landing_ppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.135067 DI-engine-0.4.6/dizoo/rocket/entry/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:50.000000 DI-engine-0.4.6/dizoo/rocket/entry/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4039 2023-02-13 06:56:29.000000 DI-engine-0.4.6/dizoo/rocket/entry/rocket_hover_onppo_main_v2.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2023-02-13 06:57:07.000000 DI-engine-0.4.6/dizoo/rocket/entry/rocket_hover_ppo_main.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4022 2023-02-13 06:57:32.000000 DI-engine-0.4.6/dizoo/rocket/entry/rocket_landing_onppo_main_v2.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     3236 2023-02-13 06:56:46.000000 DI-engine-0.4.6/dizoo/rocket/entry/rocket_landing_ppo_main.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.139391 DI-engine-0.4.6/dizoo/rocket/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/rocket/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4011 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/rocket/envs/rocket_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1391 2023-01-04 09:44:45.000000 DI-engine-0.4.6/dizoo/rocket/envs/test_rocket_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.140320 DI-engine-0.4.6/dizoo/slime_volley/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/slime_volley/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.140851 DI-engine-0.4.6/dizoo/slime_volley/config/
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1685 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/slime_volley/config/slime_volley_ppo_config.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.143646 DI-engine-0.4.6/dizoo/slime_volley/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       46 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/slime_volley/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     7877 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/slime_volley/envs/slime_volley_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1274 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/slime_volley/envs/test_slime_volley_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.144848 DI-engine-0.4.6/dizoo/smac/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.152261 DI-engine-0.4.6/dizoo/smac/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844      194 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1666 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/smac/envs/fake_smac_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.153078 DI-engine-0.4.6/dizoo/smac/envs/maps/
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.179124 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14925 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/10m_vs_11m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16965 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/1c3s5z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14923 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/25m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14929 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/27m_vs_30m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16477 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2c_vs_64zg.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15539 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2m_vs_1z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15692 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2s3z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14202 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2s_vs_1sc.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14920 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15697 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s5z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15704 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s5z_vs_3s6z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    13995 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_3z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14000 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_4z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    13998 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_5z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14930 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/5m_vs_6m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14342 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/6h_vs_8z.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14922 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/8m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14927 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/8m_vs_9m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    17411 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/MMM.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    17419 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/MMM2.SC2Map
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/__init__.py
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16782 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/bane_vs_bane.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    18055 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/corridor.SC2Map
--rw-r--r--   0 niuyazhe (1370690143) 453037844    17080 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/infestor_viper.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    18133 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/so_many_baneling.SC2Map
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.181934 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14835 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/3m.SC2Map
--rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15596 2021-08-01 05:19:48.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/3s5z.SC2Map
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/maps/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    16272 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/smac_action.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844    70855 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/smac/envs/smac_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     5383 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/smac_map.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     9098 2022-09-30 04:52:25.000000 DI-engine-0.4.6/dizoo/smac/envs/smac_reward.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6050 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/smac/envs/test_smac_env.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.182294 DI-engine-0.4.6/dizoo/sokoban/
--rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:19.000000 DI-engine-0.4.6/dizoo/sokoban/__init__.py
-drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-02-18 14:04:11.186032 DI-engine-0.4.6/dizoo/sokoban/envs/
--rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-10-19 09:16:29.000000 DI-engine-0.4.6/dizoo/sokoban/envs/__init__.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     4021 2022-12-13 15:12:50.000000 DI-engine-0.4.6/dizoo/sokoban/envs/sokoban_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844     1658 2022-08-02 16:56:36.000000 DI-engine-0.4.6/dizoo/sokoban/envs/sokoban_wrappers.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844      779 2022-08-02 16:56:36.000000 DI-engine-0.4.6/dizoo/sokoban/envs/test_sokoban_env.py
--rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2023-02-18 14:04:11.187666 DI-engine-0.4.6/setup.cfg
--rw-r--r--   0 niuyazhe (1370690143) 453037844     6110 2023-02-18 12:39:51.000000 DI-engine-0.4.6/setup.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.461826 DI-engine-0.4.7/
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.595677 DI-engine-0.4.7/DI_engine.egg-info/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    56871 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/PKG-INFO
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    49085 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/SOURCES.txt
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        1 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/dependency_links.txt
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       86 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/entry_points.txt
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1149 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/requires.txt
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       11 2023-04-12 15:08:01.000000 DI-engine-0.4.7/DI_engine.egg-info/top_level.txt
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11342 2021-07-08 05:53:26.000000 DI-engine-0.4.7/LICENSE
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    56871 2023-04-12 15:08:02.461529 DI-engine-0.4.7/PKG-INFO
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    55605 2023-04-11 14:03:16.000000 DI-engine-0.4.7/README.md
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.596679 DI-engine-0.4.7/ding/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      380 2023-04-11 14:01:01.000000 DI-engine-0.4.7/ding/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.600853 DI-engine-0.4.7/ding/bonus/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       58 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/bonus/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10868 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/bonus/config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      901 2023-01-06 07:07:56.000000 DI-engine-0.4.7/ding/bonus/demo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9777 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/bonus/model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9287 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/bonus/ppof.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9060 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/bonus/td3.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      211 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/compatibility.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.604101 DI-engine-0.4.7/ding/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      227 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    24354 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/config/config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    28073 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/config/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.606727 DI-engine-0.4.7/ding/data/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      352 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.609015 DI-engine-0.4.7/ding/data/buffer/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      150 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7617 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/data/buffer/buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14785 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/data/buffer/deque_buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4701 2023-02-20 14:45:59.000000 DI-engine-0.4.7/ding/data/buffer/deque_buffer_wrapper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.613970 DI-engine-0.4.7/ding/data/buffer/middleware/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      291 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1064 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/clone_object.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1648 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/group_sample.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1496 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/padding.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6962 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/priority.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      979 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/data/buffer/middleware/sample_range_view.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1830 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/staleness_check.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1887 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/data/buffer/middleware/use_time_check.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.614846 DI-engine-0.4.7/ding/data/level_replay/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/data/level_replay/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13812 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/data/level_replay/level_sampler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5501 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/model_loader.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5008 2023-03-09 12:42:49.000000 DI-engine-0.4.7/ding/data/shm_buffer.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.616635 DI-engine-0.4.7/ding/data/storage/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/storage/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      605 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/storage/file.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      326 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/storage/storage.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11978 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/data/storage_loader.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.638031 DI-engine-0.4.7/ding/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11820 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/application_entry.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9797 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/application_entry_drex_collect_data.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6652 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/application_entry_trex_collect_data.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13003 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/cli.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5475 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/cli_ditask.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.639964 DI-engine-0.4.7/ding/entry/cli_parsers/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      136 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/entry/cli_parsers/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5855 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/cli_parsers/k8s_parser.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5656 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/cli_parsers/slurm_parser.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11742 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/entry/dist_entry.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5631 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/entry/parallel_entry.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1156 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/entry/predefined_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6335 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/entry/serial_entry.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4005 2022-10-31 06:23:24.000000 DI-engine-0.4.7/ding/entry/serial_entry_bc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8933 2023-03-21 10:44:34.000000 DI-engine-0.4.7/ding/entry/serial_entry_bco.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3405 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_decision_transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12388 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_dqfd.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8037 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/serial_entry_gail.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8452 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_guided_cost.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10336 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_mbrl.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7979 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_ngu.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4645 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_offline.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4963 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/entry/serial_entry_onpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4460 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_onpolicy_ppg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4444 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/entry/serial_entry_pc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5614 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_plr.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6520 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/entry/serial_entry_preference_based_irl.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4811 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/entry/serial_entry_preference_based_irl_onpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12314 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_r2d3.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6809 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/entry/serial_entry_reward_model_offpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6557 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/entry/serial_entry_reward_model_onpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8837 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_sqil.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10374 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/entry/serial_entry_td3_vae.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2342 2022-09-16 15:18:07.000000 DI-engine-0.4.7/ding/entry/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.640484 DI-engine-0.4.7/ding/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       74 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.642653 DI-engine-0.4.7/ding/envs/common/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      354 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/envs/common/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10829 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/envs/common/common_function.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1625 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/common/env_element.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      986 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/common/env_element_runner.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.645161 DI-engine-0.4.7/ding/envs/env/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      386 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/env/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6902 2023-01-02 16:43:42.000000 DI-engine-0.4.7/ding/envs/env/base_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1751 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/envs/env/default_wrapper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9454 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/envs/env/ding_env_wrapper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7078 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env/env_implementation_check.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.647533 DI-engine-0.4.7/ding/envs/env/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       30 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/envs/env/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2216 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env/tests/demo_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7618 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env/tests/test_ding_env_wrapper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1974 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/envs/env/tests/test_env_implementation_check.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.650754 DI-engine-0.4.7/ding/envs/env_manager/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      394 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/env_manager/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21523 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/envs/env_manager/base_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    24099 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env_manager/env_supervisor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4222 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env_manager/envpool_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5793 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/envs/env_manager/gym_vector_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    36999 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/envs/env_manager/subprocess_env_manager.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.655304 DI-engine-0.4.7/ding/envs/env_manager/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8258 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/conftest.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9270 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_base_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    18474 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_env_supervisor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1456 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_envpool_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2306 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_gym_vector_env_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      783 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_shm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9590 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/envs/env_manager/tests/test_subprocess_env_manager.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.656448 DI-engine-0.4.7/ding/envs/env_wrappers/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       28 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/envs/env_wrappers/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    43614 2023-03-21 10:15:23.000000 DI-engine-0.4.7/ding/envs/env_wrappers/env_wrappers.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.671253 DI-engine-0.4.7/ding/example/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-04 06:30:14.000000 DI-engine-0.4.7/ding/example/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1997 2023-03-09 07:52:49.000000 DI-engine-0.4.7/ding/example/c51_nstep.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1509 2023-03-06 03:18:03.000000 DI-engine-0.4.7/ding/example/collect_demo_data.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1747 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/cql.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2098 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/d4pg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1962 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/ddpg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4394 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1928 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn_her.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1949 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn_new_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2056 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn_nstep.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2112 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn_per.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2028 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/dqn_rnd.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2131 2023-03-08 06:20:41.000000 DI-engine-0.4.7/ding/example/dt.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/iqn_nstep.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1838 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/pdqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2539 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/ppg_offpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1807 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/ppo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1895 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/ppo_offpolicy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2003 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/qrdqn_nstep.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1934 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/r2d2.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/sac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3213 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/sqil.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3377 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/sqil_continuous.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1842 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/sql.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1866 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/td3.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2447 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/example/trex.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.675069 DI-engine-0.4.7/ding/framework/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      361 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/framework/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3247 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/context.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4120 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/framework/event_loop.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.677778 DI-engine-0.4.7/ding/framework/message_queue/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       69 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/framework/message_queue/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1635 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/framework/message_queue/mq.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2524 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/framework/message_queue/nng.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/framework/message_queue/redis.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.681255 DI-engine-0.4.7/ding/framework/middleware/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      244 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/framework/middleware/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3017 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/framework/middleware/ckpt_handler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8143 2023-02-17 05:18:00.000000 DI-engine-0.4.7/ding/framework/middleware/collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11569 2022-12-13 15:12:49.000000 DI-engine-0.4.7/ding/framework/middleware/distributer.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.688577 DI-engine-0.4.7/ding/framework/middleware/functional/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      792 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/framework/middleware/functional/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3242 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/framework/middleware/functional/advantage_estimator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5426 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/functional/collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      912 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/functional/ctx_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11619 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/framework/middleware/functional/data_processor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3968 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/functional/enhancer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17448 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/functional/evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1541 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/functional/explorer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21299 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/functional/logger.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2054 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/functional/termination_checker.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1021 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/functional/timer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3611 2023-03-10 09:15:22.000000 DI-engine-0.4.7/ding/framework/middleware/functional/trainer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3717 2023-02-22 10:35:54.000000 DI-engine-0.4.7/ding/framework/middleware/learner.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.695539 DI-engine-0.4.7/ding/framework/middleware/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2023-03-21 10:44:32.000000 DI-engine-0.4.7/ding/framework/middleware/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3256 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/tests/mock_for_test.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3213 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_advantage_estimator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2074 2023-01-06 07:05:23.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_ckpt_handler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3049 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9202 2023-01-04 09:44:45.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_data_processor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7060 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_distributer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2590 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_enhancer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1200 2022-10-19 12:18:13.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      731 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_explorer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9113 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_logger.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3379 2023-01-29 05:51:23.000000 DI-engine-0.4.7/ding/framework/middleware/tests/test_trainer.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.697838 DI-engine-0.4.7/ding/framework/middleware_v3/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      167 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2553 2022-09-04 07:49:02.000000 DI-engine-0.4.7/ding/framework/middleware_v3/ckpt_handler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5011 2022-09-01 10:45:00.000000 DI-engine-0.4.7/ding/framework/middleware_v3/collector.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.703189 DI-engine-0.4.7/ding/framework/middleware_v3/functional/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      578 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2742 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/advantage_estimator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5391 2022-09-05 08:12:51.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      746 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/ctx_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10145 2022-09-01 11:15:00.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/data_processor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3672 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/enhancer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7765 2022-09-01 11:07:51.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1408 2022-09-01 10:26:49.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/explorer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      742 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/termination_checker.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2951 2022-09-01 11:16:03.000000 DI-engine-0.4.7/ding/framework/middleware_v3/functional/trainer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3507 2022-09-01 11:09:28.000000 DI-engine-0.4.7/ding/framework/middleware_v3/learner.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.708562 DI-engine-0.4.7/ding/framework/middleware_v3/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2023-03-21 10:44:33.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3252 2022-09-01 10:26:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/mock_for_test.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3198 2022-09-01 10:26:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_advantage_estimator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1987 2022-09-04 07:49:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_ckpt_handler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3122 2022-09-01 10:26:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8230 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_data_processor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2642 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_enhancer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1200 2022-09-01 10:26:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      731 2022-09-01 10:26:15.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_explorer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3286 2022-09-01 10:26:16.000000 DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_trainer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15133 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/parallel.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14388 2023-04-05 10:04:46.000000 DI-engine-0.4.7/ding/framework/supervisor.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19497 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/framework/task.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.709654 DI-engine-0.4.7/ding/framework/wrapper/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/framework/wrapper/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1922 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/framework/wrapper/step_timer.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.710757 DI-engine-0.4.7/ding/hpc_rl/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       33 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/hpc_rl/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5953 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/hpc_rl/wrapper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.711333 DI-engine-0.4.7/ding/interaction/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       43 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.714377 DI-engine-0.4.7/ding/interaction/base/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      382 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/base/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2806 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/base/app.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5341 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/base/common.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4846 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/base/network.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2163 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/base/threading.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.715337 DI-engine-0.4.7/ding/interaction/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      297 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      432 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/config/base.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.717325 DI-engine-0.4.7/ding/interaction/exception/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      848 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/exception/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2466 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/exception/base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3512 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/exception/master.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3914 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/exception/slave.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.720753 DI-engine-0.4.7/ding/interaction/master/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       27 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/master/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      293 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/master/base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17041 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/master/connection.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    27918 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/master/master.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9524 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/master/task.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.722337 DI-engine-0.4.7/ding/interaction/slave/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      105 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/slave/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3753 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/slave/action.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19857 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/slave/slave.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.723242 DI-engine-0.4.7/ding/interaction/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.725797 DI-engine-0.4.7/ding/interaction/tests/base/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      318 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/base/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6161 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/base/test_app.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2036 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/base/test_common.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6046 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/base/test_network.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3324 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/base/test_threading.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.726868 DI-engine-0.4.7/ding/interaction/tests/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       45 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      226 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/config/test_base.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.729176 DI-engine-0.4.7/ding/interaction/tests/exception/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      110 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/exception/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1562 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/exception/test_base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2877 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/exception/test_master.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3293 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/exception/test_slave.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.731175 DI-engine-0.4.7/ding/interaction/tests/interaction/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/interaction/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1401 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/interaction/bases.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1439 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/interaction/test_errors.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4795 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/interaction/test_simple.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.732573 DI-engine-0.4.7/ding/interaction/tests/test_utils/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       94 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/test_utils/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      417 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/test_utils/random.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1099 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/interaction/tests/test_utils/stream.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.737183 DI-engine-0.4.7/ding/league/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      345 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1749 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/algorithm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13037 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/base_league.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4590 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/metric.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5355 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/one_vs_one_league.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13828 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/player.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10938 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/shared_payoff.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9522 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/league/starcraft_player.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.737899 DI-engine-0.4.7/ding/model/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       69 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.741041 DI-engine-0.4.7/ding/model/common/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      355 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/model/common/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12145 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/model/common/encoder.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    59073 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/model/common/head.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      628 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/common/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.758974 DI-engine-0.4.7/ding/model/template/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      761 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/model/template/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8851 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/acer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    18749 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/atoc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8563 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/model/template/bc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19316 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/model/template/collaq.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7751 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/coma.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4713 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/model/template/decision_transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17708 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/model/template/ebm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1790 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/model/template/madqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    20794 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/maqac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12386 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/model/template/mavac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10574 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/ngu.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9646 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/model/template/pdqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2793 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/model/template/pg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2389 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/model/template/ppg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8657 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/model/template/procedure_cloning.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    51526 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/model/template/q_learning.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    27157 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/model/template/qac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12226 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/qac_dist.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9551 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/model/template/qmix.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7140 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/qtran.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      589 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/sqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15891 2023-02-23 09:45:40.000000 DI-engine-0.4.7/ding/model/template/vac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12328 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/vae.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12586 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/template/wqmix.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.760977 DI-engine-0.4.7/ding/model/wrapper/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       90 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/wrapper/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    37923 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/model/wrapper/model_wrappers.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21725 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/model/wrapper/test_model_wrappers.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.797607 DI-engine-0.4.7/ding/policy/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1543 2023-03-22 09:00:17.000000 DI-engine-0.4.7/ding/policy/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11203 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/a2c.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    24338 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/acer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16201 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/atoc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11919 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/base_policy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13093 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/bc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19714 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/bdq.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12526 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/c51.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21034 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/collaq.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    18431 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/coma.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12184 2023-03-22 09:00:17.000000 DI-engine-0.4.7/ding/policy/command_mode_policy_instance.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2023-03-22 09:00:17.000000 DI-engine-0.4.7/ding/policy/common_utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    37331 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/cql.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15951 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/d4pg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    22393 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/ddpg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15603 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/decision_transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13749 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/dqfd.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    36869 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/dqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12436 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/fqf.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6988 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/ibc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9032 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/il.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    23117 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/impala.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9233 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/iqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15386 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/madqn.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.799436 DI-engine-0.4.7/ding/policy/mbpolicy/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       31 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/policy/mbpolicy/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15898 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/mbpolicy/mbsac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1318 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/policy/mbpolicy/utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12521 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/mdqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    28541 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/ngu.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13695 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/offppo_collect_traj.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6878 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/pc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    22366 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/pdqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8459 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/pg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2892 2023-03-22 09:00:17.000000 DI-engine-0.4.7/ding/policy/policy_factory.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    55970 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/ppg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    56458 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/ppo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15076 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/policy/ppof.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    20183 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/qmix.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9007 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/qrdqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21050 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/qtran.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    24622 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/r2d2.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    23742 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/r2d2_collect_traj.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    24134 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/r2d2_gtrxl.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    26516 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/r2d3.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13923 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/rainbow.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    56196 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/sac.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12620 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/sql.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14436 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/sqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9877 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/td3.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17418 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/td3_bc.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    32866 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/td3_vae.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15786 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/policy/wqmix.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.807449 DI-engine-0.4.7/ding/reward_model/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      670 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/reward_model/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4914 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/reward_model/base_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4791 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/drex_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13515 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/gail_irl_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8298 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/guided_cost_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6595 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/reward_model/her_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16854 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/icm_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    26350 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/ngu_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9980 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/pdeil_irl_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12376 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/pwil_irl_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10439 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/red_irl_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12103 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/rnd_reward_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    20419 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/reward_model/trex_reward_model.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.818483 DI-engine-0.4.7/ding/rl_utils/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1894 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/rl_utils/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1700 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/a2c.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4723 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/acer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10256 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/adder.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1127 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/beta_function.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2438 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/coma.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7580 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/exploration.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2435 2023-01-02 12:59:23.000000 DI-engine-0.4.7/ding/rl_utils/gae.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2782 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/rl_utils/isw.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1409 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/ppg.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12781 2023-02-21 09:28:07.000000 DI-engine-0.4.7/ding/rl_utils/ppo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1729 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/retrace.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1510 2023-02-12 11:34:47.000000 DI-engine-0.4.7/ding/rl_utils/sampler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    62641 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/rl_utils/td.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3752 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/rl_utils/upgo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2266 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/rl_utils/value_rescale.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9477 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/rl_utils/vtrace.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.827734 DI-engine-0.4.7/ding/torch_utils/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      754 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/torch_utils/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      181 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/backend_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12831 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/checkpoint_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14581 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/torch_utils/data_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      358 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/dataparallel.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9024 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/distribution.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.830692 DI-engine-0.4.7/ding/torch_utils/loss/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      178 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/loss/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4196 2023-01-28 12:50:23.000000 DI-engine-0.4.7/ding/torch_utils/loss/contrastive_loss.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2514 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/loss/cross_entropy_loss.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4820 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/torch_utils/loss/multi_logits_loss.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1273 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/lr_scheduler.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1648 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/math_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3278 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/metric.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      210 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/model_helper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.840197 DI-engine-0.4.7/ding/torch_utils/network/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      678 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/network/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3769 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/network/activation.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    35769 2022-01-21 07:34:18.000000 DI-engine-0.4.7/ding/torch_utils/network/coverage.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    25400 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/torch_utils/network/gtrxl.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1808 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/gumbel_softmax.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    25613 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/torch_utils/network/nn_module.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1264 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/normalization.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4429 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/network/popart.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5688 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/torch_utils/network/res_block.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21848 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/resnet.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13797 2023-04-11 05:01:48.000000 DI-engine-0.4.7/ding/torch_utils/network/rnn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3626 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/scatter_connection.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/soft_argmax.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8667 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/network/transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1836 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/torch_utils/nn_test_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    34698 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/torch_utils/optimizer_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3018 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/torch_utils/reshape_helper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.858731 DI-engine-0.4.7/ding/utils/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2609 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/utils/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.861996 DI-engine-0.4.7/ding/utils/autolog/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      236 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      701 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5499 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/data.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9391 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/model.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.864211 DI-engine-0.4.7/ding/utils/autolog/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2304 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/tests/test_data.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17237 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/tests/test_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3452 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/tests/test_time.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5855 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/time_ctl.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1407 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/autolog/value.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2264 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/utils/bfs_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      818 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/collection_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4942 2023-02-13 06:43:00.000000 DI-engine-0.4.7/ding/utils/compression_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844   179593 2021-10-17 09:03:07.000000 DI-engine-0.4.7/ding/utils/coverage.xml
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.867339 DI-engine-0.4.7/ding/utils/data/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      284 2023-03-06 03:18:03.000000 DI-engine-0.4.7/ding/utils/data/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1052 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/data/base_dataloader.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11055 2023-04-11 05:01:52.000000 DI-engine-0.4.7/ding/utils/data/collate_fn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17023 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/data/dataloader.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    22901 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/utils/data/dataset.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.869230 DI-engine-0.4.7/ding/utils/data/structure/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       59 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/utils/data/structure/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5297 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/data/structure/cache.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      300 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/utils/data/structure/lifo_deque.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19782 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/utils/default_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      650 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/design_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      344 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/fake_linklink.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1480 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/fast_copy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10416 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/file_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2897 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/utils/import_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7063 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/utils/k8s_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5414 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/linklink_dist_helper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.876961 DI-engine-0.4.7/ding/utils/loader/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      783 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4306 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3043 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/collection.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      871 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/dict.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      369 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/exception.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3093 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/mapping.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5747 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/norm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6391 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/number.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2228 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/string.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.877907 DI-engine-0.4.7/ding/utils/loader/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      111 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.883044 DI-engine-0.4.7/ding/utils/loader/tests/loader/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      426 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5225 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5989 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_collection.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1090 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_dict.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1914 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_mapping.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11175 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_norm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    21822 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_number.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4119 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_string.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3183 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_types.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1478 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/loader/test_utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4334 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/tests/test_cartpole_dqn_serial_config_loader.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1366 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/types.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      409 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/loader/utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2796 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/lock_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5332 2022-11-14 04:31:11.000000 DI-engine-0.4.7/ding/utils/log_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4397 2022-10-19 10:55:38.000000 DI-engine-0.4.7/ding/utils/log_writer_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4657 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/orchestrator_launcher.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1135 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/profiler_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3999 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/utils/pytorch_ddp_dist_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3853 2023-01-02 16:52:37.000000 DI-engine-0.4.7/ding/utils/registry.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1495 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/utils/registry_factory.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1525 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/utils/render_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7572 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/scheduler_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9207 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/segment_tree.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2855 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/slurm_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1743 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/system_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4522 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/utils/time_helper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      956 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/time_helper_base.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1908 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/time_helper_cuda.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      173 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/utils/type_helper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.883542 DI-engine-0.4.7/ding/worker/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      127 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.884681 DI-engine-0.4.7/ding/worker/adapter/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       50 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/adapter/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13378 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/adapter/learner_aggregator.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.892613 DI-engine-0.4.7/ding/worker/collector/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1080 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/worker/collector/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8862 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/base_parallel_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7839 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/base_serial_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8544 2022-12-26 08:59:29.000000 DI-engine-0.4.7/ding/worker/collector/base_serial_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15584 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/battle_episode_serial_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12112 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/worker/collector/battle_interaction_serial_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16387 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/battle_sample_serial_collector.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.895072 DI-engine-0.4.7/ding/worker/collector/comm/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      177 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/comm/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4089 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/comm/base_comm_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8874 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/comm/flask_fs_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2436 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/comm/utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14757 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/episode_serial_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13972 2023-03-09 11:08:49.000000 DI-engine-0.4.7/ding/worker/collector/interaction_serial_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14421 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/collector/marine_parallel_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9156 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/collector/metric_serial_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17601 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/sample_serial_collector.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.898415 DI-engine-0.4.7/ding/worker/collector/tests/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      698 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/fake_cls_policy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2804 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/fake_cpong_dqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.900350 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2917 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/fake_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3034 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/fake_policy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7399 2023-01-02 10:31:34.000000 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/test_collector_profile.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      107 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/speed_test/utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1705 2023-04-11 13:59:56.000000 DI-engine-0.4.7/ding/worker/collector/tests/test_base_serial_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2277 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/test_episode_serial_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2846 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/collector/tests/test_marine_parallel_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3019 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/collector/tests/test_metric_serial_evaluator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11723 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/worker/collector/zergling_parallel_collector.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.904477 DI-engine-0.4.7/ding/worker/coordinator/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      183 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7099 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/base_parallel_commander.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2070 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/base_serial_commander.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    26271 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/comm_coordinator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    20584 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/coordinator.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16808 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/coordinator/one_vs_one_parallel_commander.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3634 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/operator_server.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2675 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/coordinator/resource_manager.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10871 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/coordinator/solo_parallel_commander.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.906364 DI-engine-0.4.7/ding/worker/learner/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      252 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/learner/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    20149 2022-12-13 15:12:50.000000 DI-engine-0.4.7/ding/worker/learner/base_learner.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.908700 DI-engine-0.4.7/ding/worker/learner/comm/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      165 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/learner/comm/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4937 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/learner/comm/base_comm_learner.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15576 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/learner/comm/flask_fs_learner.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2120 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/learner/comm/utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15500 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/worker/learner/learner_hook.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.913742 DI-engine-0.4.7/ding/worker/replay_buffer/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      206 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/replay_buffer/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    37630 2023-04-05 10:04:48.000000 DI-engine-0.4.7/ding/worker/replay_buffer/advanced_buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4460 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/worker/replay_buffer/base_buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      800 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/replay_buffer/episode_buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    19279 2022-12-11 15:35:39.000000 DI-engine-0.4.7/ding/worker/replay_buffer/naive_buffer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10920 2022-09-30 04:52:24.000000 DI-engine-0.4.7/ding/worker/replay_buffer/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.917728 DI-engine-0.4.7/ding/world_model/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      139 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/world_model/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13985 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/world_model/base_world_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    22524 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/world_model/ddppo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6228 2022-10-19 09:16:29.000000 DI-engine-0.4.7/ding/world_model/idm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11421 2022-08-02 16:56:36.000000 DI-engine-0.4.7/ding/world_model/mbpo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      852 2022-07-19 08:28:37.000000 DI-engine-0.4.7/ding/world_model/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.918302 DI-engine-0.4.7/dizoo/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-03-22 09:07:56.000000 DI-engine-0.4.7/dizoo/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.918513 DI-engine-0.4.7/dizoo/atari/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.918712 DI-engine-0.4.7/dizoo/atari/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/config/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.919007 DI-engine-0.4.7/dizoo/atari/config/serial/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      239 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/atari/config/serial/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.920262 DI-engine-0.4.7/dizoo/atari/config/serial/asterix/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       82 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/atari/config/serial/asterix/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1913 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/atari/config/serial/asterix/asterix_mdqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.923986 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2022-07-19 08:28:37.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1731 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2976 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1900 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_mdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2237 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1758 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1816 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_rainbow_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.940359 DI-engine-0.4.7/dizoo/atari/config/serial/pong/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      233 2022-07-19 08:28:37.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2064 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_a2c_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2480 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_acer_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1742 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_c51_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2123 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2877 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqfd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1673 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1853 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_envpool_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1707 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_multi_gpu_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1814 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_render_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1975 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_stdim_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1784 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_fqf_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3109 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_gail_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2735 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1724 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_iqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4989 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_ngu_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2137 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2615 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2065 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_qrdqn_generation_data_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3300 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3011 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_gtrxl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3017 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_residual_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6446 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d3_offppoexpert_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6795 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d3_r2d2expert_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1729 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_rainbow_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2290 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_sqil_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1571 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_sql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4218 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_trex_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3561 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_trex_sql_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.951523 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      146 2022-07-19 08:28:37.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2058 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_a2c_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2541 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_acer_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1669 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_c51_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2076 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2732 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_dqfd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1844 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_fqf_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2808 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1650 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_iqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2154 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2184 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2459 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1645 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2073 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_qrdqn_generation_data_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3387 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_r2d2_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2760 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_r2d2_gtrxl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1757 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_rainbow_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2415 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_sqil_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1684 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_sql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3001 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_trex_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3442 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_trex_offppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.964896 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      194 2022-07-19 08:28:37.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2266 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_a2c_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2811 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_acer_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1937 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_c51_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2925 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2029 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1891 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_ddp.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1978 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_dp.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1939 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_fqf_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3105 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1909 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_iqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1979 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_mdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2395 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2310 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2716 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1912 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3492 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3033 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_gtrxl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3167 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_residual_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2031 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_rainbow_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2516 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sqil_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1827 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4055 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4554 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_offppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.972164 DI-engine-0.4.7/dizoo/atari/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3135 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/atari_dqn_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3534 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/atari_ppg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3272 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/entry/phoenix_fqf_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3272 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/entry/phoenix_iqn_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2118 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/pong_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3743 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/atari/entry/pong_dqn_envpool_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3260 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/entry/pong_fqf_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2151 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/qbert_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3264 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/entry/qbert_fqf_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2185 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3251 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_ddp.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_dp.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3296 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_fqf_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.974573 DI-engine-0.4.7/dizoo/atari/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       44 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/atari/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5233 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/envs/atari_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6246 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/atari/envs/atari_wrappers.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2440 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/atari/envs/test_atari_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.975077 DI-engine-0.4.7/dizoo/beergame/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:07.000000 DI-engine-0.4.7/dizoo/beergame/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.979485 DI-engine-0.4.7/dizoo/beergame/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8359 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/beergame/envs/BGAgent.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       71 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/beergame/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4738 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/beergame/envs/beergame_core.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3046 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/beergame/envs/beergame_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    23724 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/beergame/envs/clBeergame.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2721 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/beergame/envs/plotting.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17343 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/beergame/envs/utils.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.980276 DI-engine-0.4.7/dizoo/bitflip/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-03-14 08:09:40.000000 DI-engine-0.4.7/dizoo/bitflip/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.982118 DI-engine-0.4.7/dizoo/bitflip/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      183 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2616 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/config/bitflip_her_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/config/bitflip_pure_dqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.982927 DI-engine-0.4.7/dizoo/bitflip/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-03-14 08:09:40.000000 DI-engine-0.4.7/dizoo/bitflip/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4350 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/entry/bitflip_dqn_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.986980 DI-engine-0.4.7/dizoo/bitflip/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/bitflip/envs/bitflip_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      756 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/bitflip/envs/test_bitfilp_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.987629 DI-engine-0.4.7/dizoo/box2d/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/box2d/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.987856 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       47 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.993291 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       96 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3050 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_bco_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2504 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_dt_config.py
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844     3504 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2819 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1837 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2644 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppopg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2078 2022-09-30 04:52:24.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2135 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_td3_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.994032 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2181 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/entry/bipedalwalker_ppo_eval.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.995691 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       48 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3868 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1034 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/test_bipedalwalker.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.996237 DI-engine-0.4.7/dizoo/box2d/carracing/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.997046 DI-engine-0.4.7/dizoo/box2d/carracing/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       85 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1767 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/config/carracing_dqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.998957 DI-engine-0.4.7/dizoo/box2d/carracing/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       41 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5913 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/envs/carracing_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1082 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/box2d/carracing/envs/test_carracing_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:01.999446 DI-engine-0.4.7/dizoo/box2d/lunarlander/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.019260 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      597 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1375 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_a2c_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3181 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_acer_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2827 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_bco_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1671 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_c51_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2418 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1677 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1817 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2269 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_vae_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2484 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_decision_transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2086 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_discrete_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2888 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqfd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2636 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2541 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqn_deque_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3962 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_gail_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2272 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_gcl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5345 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_ngu_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1502 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1381 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1530 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3325 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d2_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3056 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d2_gtrxl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6624 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d3_ppoexpert_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6992 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d3_r2d2expert_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2612 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_rnd_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2498 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_sqil_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1623 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_sql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4289 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_trex_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3462 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_trex_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8211 2022-09-28 13:14:17.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/config/t.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.021601 DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2177 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/lunarlander_dqn_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2985 2023-02-13 06:56:00.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/lunarlander_dqn_example.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.024178 DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       44 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5535 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/lunarlander_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1248 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/test_lunarlander_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.025100 DI-engine-0.4.7/dizoo/bsuite/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/bsuite/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.025453 DI-engine-0.4.7/dizoo/bsuite/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        1 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/bsuite/config/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.028695 DI-engine-0.4.7/dizoo/bsuite/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/bsuite/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3752 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/bsuite/envs/bsuite_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1467 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/bsuite/envs/test_bsuite_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.030197 DI-engine-0.4.7/dizoo/classic_control/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.030525 DI-engine-0.4.7/dizoo/classic_control/acrobot/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.031610 DI-engine-0.4.7/dizoo/classic_control/acrobot/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       79 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1659 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/config/acrobot_dqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.033813 DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3591 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/acrobot_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1323 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/test_acrobot_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.034435 DI-engine-0.4.7/dizoo/classic_control/cartpole/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.068754 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2040 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1682 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_a2c_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2489 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_acer_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1330 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2524 2022-10-31 06:23:20.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_bco_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1687 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_c51_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1649 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2585 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_decision_transformer.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2849 2023-02-22 06:18:30.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqfd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1776 2023-02-22 06:18:46.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2701 2022-10-31 06:23:20.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_gail_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1556 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_rnd_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2055 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_stdim_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3173 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_drex_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1813 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_fqf_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2423 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_gcl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2781 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_impala_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1649 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_iqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1721 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_mdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4857 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ngu_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1277 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2357 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1684 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1894 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_icm_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1797 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_offpolicy_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2084 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_stdim_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1383 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppopg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1672 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_qrdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1739 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_qrdqn_generation_data_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2909 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2775 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_gtrxl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3219 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_residual_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1664 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_rainbow_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2245 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_rnd_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1998 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2258 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sqil_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1554 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1702 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2845 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2948 2023-02-13 06:42:54.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_offppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3044 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_onppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.071650 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      109 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2921 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3286 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config_k8s.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.082030 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1282 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_c51_deploy.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3403 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_c51_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2217 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3391 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_buffer_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2014 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3857 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3959 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_fqf_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3582 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2278 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppo_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2781 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppo_offpolicy_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2470 2021-07-16 08:07:45.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/formatted_total_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.085971 DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3774 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/cartpole_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1327 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/test_cartpole_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1350 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/test_cartpole_env_manager.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.086937 DI-engine-0.4.7/dizoo/classic_control/mountain_car/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/mountain_car/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.089474 DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4679 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/mtcar_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1400 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/test_mtcar_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.090246 DI-engine-0.4.7/dizoo/classic_control/pendulum/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.102212 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      619 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1986 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_bdq_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1866 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1851 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_d4pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1763 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1949 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1068 2023-03-03 07:55:14.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1353 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ibc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1376 2022-11-16 06:44:04.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1816 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1766 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1347 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sac_data_generation_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2664 2023-01-06 07:05:23.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sqil_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2234 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1832 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2203 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_data_generation_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.107722 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2177 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_cql_ddpg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2266 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3083 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_d4pg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3576 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_ddpg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2178 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_dqn_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2178 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_ppo_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2325 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_td3_bc_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3381 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_td3_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.109524 DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4943 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/pendulum_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2226 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/test_pendulum_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.110134 DI-engine-0.4.7/dizoo/common/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/common/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.113244 DI-engine-0.4.7/dizoo/common/policy/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/common/policy/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4317 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/common/policy/md_dqn.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8472 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/common/policy/md_ppo.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4183 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/common/policy/md_rainbow_dqn.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.114003 DI-engine-0.4.7/dizoo/competitive_rl/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/competitive_rl/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.116732 DI-engine-0.4.7/dizoo/competitive_rl/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       49 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/competitive_rl/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6561 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/competitive_rl/envs/competitive_rl_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8597 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/competitive_rl/envs/competitive_rl_env_wrapper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2554 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/competitive_rl/envs/test_competitive_rl.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.117501 DI-engine-0.4.7/dizoo/d4rl/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.161731 DI-engine-0.4.7/dizoo/d4rl/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      175 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2450 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2450 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1499 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2471 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1746 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1499 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2471 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1746 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1485 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2449 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2385 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1449 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1489 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2406 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1458 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_ar_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1342 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1348 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_mcmc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1736 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1489 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2406 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1736 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1475 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2384 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1722 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1448 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1453 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_ar_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1337 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1343 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_mcmc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1431 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/pen_human_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1501 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_ar_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1324 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1330 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_mcmc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2411 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1732 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2411 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1493 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1493 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1740 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_td3bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1479 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2410 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_dt_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_td3bc_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.165231 DI-engine-0.4.7/dizoo/d4rl/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      685 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1407 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_dt_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1171 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_ibc_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      694 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_td3_bc_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.167580 DI-engine-0.4.7/dizoo/d4rl/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       30 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/d4rl/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3986 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/d4rl/envs/d4rl_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2286 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/d4rl/envs/d4rl_wrappers.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.168349 DI-engine-0.4.7/dizoo/dmc2gym/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/dmc2gym/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.170371 DI-engine-0.4.7/dizoo/dmc2gym/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/dmc2gym/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8742 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/dmc2gym/envs/dmc2gym_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1645 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/dmc2gym/envs/test_dmc2gym_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.171145 DI-engine-0.4.7/dizoo/evogym/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/evogym/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.173220 DI-engine-0.4.7/dizoo/evogym/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/evogym/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6926 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/evogym/envs/evogym_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.174966 DI-engine-0.4.7/dizoo/gfootball/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.181602 DI-engine-0.4.7/dizoo/gfootball/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2523 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2542 2022-10-31 06:23:24.000000 DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_kaggle5th_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4374 2022-10-31 06:23:24.000000 DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_rule_lt0_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4010 2022-10-31 06:23:24.000000 DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_rule_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1896 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2197 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/entry/show_dataset.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1884 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/entry/test_accuracy.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.193227 DI-engine-0.4.7/dizoo/gfootball/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      174 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2648 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/envs/fake_dataset.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    14135 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gfootball/envs/gfootball_academy_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8689 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gfootball/envs/gfootball_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7430 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gfootball/envs/gfootballsp_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.193903 DI-engine-0.4.7/dizoo/gfootball/model/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/model/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.197176 DI-engine-0.4.7/dizoo/gfootball/model/bots/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      120 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/model/bots/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1780 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/model/bots/kaggle_5th_place_model.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    26957 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gfootball/model/bots/rule_based_bot_model.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.198780 DI-engine-0.4.7/dizoo/gfootball/policy/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       55 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/policy/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15162 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/gfootball/policy/ppo_lstm.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1268 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gfootball/replay.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.199826 DI-engine-0.4.7/dizoo/gym_anytrading/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.201216 DI-engine-0.4.7/dizoo/gym_anytrading/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       76 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2952 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/config/stocks_dqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.205626 DI-engine-0.4.7/dizoo/gym_anytrading/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       90 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6412 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/envs/stocks_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1506 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/envs/test_stocks_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10840 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gym_anytrading/envs/trading_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.207267 DI-engine-0.4.7/dizoo/gym_anytrading/worker/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       51 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_anytrading/worker/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9730 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gym_anytrading/worker/trading_serial_evaluator.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.208214 DI-engine-0.4.7/dizoo/gym_hybrid/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gym_hybrid/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.211084 DI-engine-0.4.7/dizoo/gym_hybrid/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-12-22 08:45:23.000000 DI-engine-0.4.7/dizoo/gym_hybrid/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2023-01-02 10:31:34.000000 DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1985 2023-01-02 10:31:34.000000 DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_hppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2453 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_mpdqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2381 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_pdqn_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.214412 DI-engine-0.4.7/dizoo/gym_hybrid/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_hybrid/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2316 2022-12-26 09:01:35.000000 DI-engine-0.4.7/dizoo/gym_hybrid/entry/e.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2023-01-02 10:31:34.000000 DI-engine-0.4.7/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3996 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.218911 DI-engine-0.4.7/dizoo/gym_hybrid/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       41 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gym_hybrid/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5940 2023-01-02 10:31:34.000000 DI-engine-0.4.7/dizoo/gym_hybrid/envs/gym_hybrid_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1149 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/gym_hybrid/envs/test_gym_hybrid_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.220944 DI-engine-0.4.7/dizoo/gym_pybullet_drones/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/gym_pybullet_drones/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.224183 DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       58 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9559 2023-03-10 09:15:22.000000 DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/gym_pybullet_drones_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1034 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/test_ding_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      830 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/test_ori_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.225089 DI-engine-0.4.7/dizoo/gym_soccer/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gym_soccer/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.226968 DI-engine-0.4.7/dizoo/gym_soccer/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/gym_soccer/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6247 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gym_soccer/envs/gym_soccer_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1248 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/gym_soccer/envs/test_gym_soccer_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.228073 DI-engine-0.4.7/dizoo/image_classification/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/image_classification/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.230975 DI-engine-0.4.7/dizoo/image_classification/data/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/image_classification/data/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4660 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/image_classification/data/dataset.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2207 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/image_classification/data/sampler.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.233079 DI-engine-0.4.7/dizoo/image_classification/policy/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       46 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/image_classification/policy/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2955 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/image_classification/policy/policy.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.240376 DI-engine-0.4.7/dizoo/league_demo/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/league_demo/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1805 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/league_demo/demo_league.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3024 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/league_demo/game_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16428 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/league_demo/league_demo_collector.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2768 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/league_demo/league_demo_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    10209 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/league_demo/league_demo_ppo_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1199 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/league_demo/selfplay_demo_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4662 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/league_demo/selfplay_demo_ppo_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.242679 DI-engine-0.4.7/dizoo/mario/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:08:18.000000 DI-engine-0.4.7/dizoo/mario/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1354 2023-01-17 12:08:18.000000 DI-engine-0.4.7/dizoo/mario/mario_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2787 2023-02-13 06:54:46.000000 DI-engine-0.4.7/dizoo/mario/mario_dqn_example.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4454 2023-01-17 12:08:18.000000 DI-engine-0.4.7/dizoo/mario/mario_dqn_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.243384 DI-engine-0.4.7/dizoo/maze/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      100 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/maze/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.245379 DI-engine-0.4.7/dizoo/maze/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       27 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/maze/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    13099 2023-03-10 04:24:02.000000 DI-engine-0.4.7/dizoo/maze/envs/maze_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      793 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/maze/envs/test_maze_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.246119 DI-engine-0.4.7/dizoo/metadrive/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.7/dizoo/metadrive/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.248040 DI-engine-0.4.7/dizoo/metadrive/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.7/dizoo/metadrive/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4258 2023-02-23 09:38:44.000000 DI-engine-0.4.7/dizoo/metadrive/config/metadrive_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3693 2023-02-15 07:39:03.000000 DI-engine-0.4.7/dizoo/metadrive/config/metadrive_onppo_eval_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.250265 DI-engine-0.4.7/dizoo/metadrive/env/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-02-15 07:39:03.000000 DI-engine-0.4.7/dizoo/metadrive/env/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    15662 2023-03-10 09:15:22.000000 DI-engine-0.4.7/dizoo/metadrive/env/drive_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4010 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/metadrive/env/drive_utils.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5709 2023-04-05 10:04:48.000000 DI-engine-0.4.7/dizoo/metadrive/env/drive_wrapper.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.250958 DI-engine-0.4.7/dizoo/minigrid/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      750 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/minigrid/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.255341 DI-engine-0.4.7/dizoo/minigrid/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      323 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/minigrid/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7470 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/minigrid/envs/app_key_to_door_treasure.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6933 2023-03-10 09:15:22.000000 DI-engine-0.4.7/dizoo/minigrid/envs/minigrid_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1163 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/minigrid/envs/minigrid_wrapper.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7249 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/minigrid/envs/noisy_tv.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3769 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/minigrid/envs/test_minigrid_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.256214 DI-engine-0.4.7/dizoo/mujoco/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.324766 DI-engine-0.4.7/dizoo/mujoco/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2105 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3352 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_gail_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2154 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1705 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1976 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2009 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2544 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_trex_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2829 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/ant_trex_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2954 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_bco_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2057 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_bdq_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2074 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_d4pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1915 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3430 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_gail_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2814 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_gcl_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2335 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2750 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_sqil_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2039 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4130 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_trex_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3966 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_trex_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2847 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_bco_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2246 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_bdq_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2093 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_cql_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1966 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_d4pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1857 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3397 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_gail_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2372 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_gcl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2131 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1936 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2610 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_sac_data_generation_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2601 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_sqil_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2455 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_bc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1969 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2668 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_data_generation_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3635 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_trex_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3870 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/hopper_trex_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2000 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_d4pg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1879 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3579 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gail_ddpg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3425 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gail_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2432 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gcl_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2346 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1958 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2567 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_sqil_sac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1991 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_td3_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3659 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_trex_onppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3894 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/config/walker2d_trex_sac_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.333522 DI-engine-0.4.7/dizoo/mujoco/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      983 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_cql_generation_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      436 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_cql_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2861 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_d4pg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1937 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ddpg_eval.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2697 2022-11-14 04:31:11.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ddpg_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2338 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ppo_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2290 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_td3_bc_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.342247 DI-engine-0.4.7/dizoo/mujoco/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       77 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6144 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_disc_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8606 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1992 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_gym_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1415 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_wrappers.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.342914 DI-engine-0.4.7/dizoo/multiagent_mujoco/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.348802 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      185 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.366099 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8773 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/coupled_half_cheetah.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     8333 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_ant.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5215 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_ant__stage1.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2900 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer__bckp2.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2570 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer_bckp.xml
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1852 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/coupled_half_cheetah.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5661 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/manyagent_ant.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3593 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/manyagent_swimmer.py
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844     9649 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/mujoco_multi.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4066 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/multi_mujoco_env.py
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844     2404 2022-03-14 08:09:40.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/multiagentenv.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    23700 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/obsk.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.367071 DI-engine-0.4.7/dizoo/overcooked/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/overcooked/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.368469 DI-engine-0.4.7/dizoo/overcooked/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       68 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/overcooked/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2357 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/overcooked/config/overcooked_ppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.371417 DI-engine-0.4.7/dizoo/overcooked/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       58 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/overcooked/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    12316 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/overcooked/envs/overcooked_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1726 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/overcooked/envs/test_overcooked_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.372297 DI-engine-0.4.7/dizoo/petting_zoo/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/petting_zoo/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.388217 DI-engine-0.4.7/dizoo/petting_zoo/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1022 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2391 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_atoc_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2456 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_collaq_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2512 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_coma_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2581 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_madqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2960 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_mappo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3032 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_masac_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2082 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_qmix_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2490 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_qtran_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2196 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_vdn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     2394 2023-03-01 15:44:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_wqmix_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.391618 DI-engine-0.4.7/dizoo/petting_zoo/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/petting_zoo/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16736 2023-03-09 11:08:49.000000 DI-engine-0.4.7/dizoo/petting_zoo/envs/petting_zoo_simple_spread_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5416 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/petting_zoo/envs/test_petting_zoo_simple_spread_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.392653 DI-engine-0.4.7/dizoo/pomdp/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pomdp/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.395720 DI-engine-0.4.7/dizoo/pomdp/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       37 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pomdp/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4022 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/pomdp/envs/atari_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6746 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pomdp/envs/atari_wrappers.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1031 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/pomdp/envs/test_atari_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.396487 DI-engine-0.4.7/dizoo/procgen/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:29.000000 DI-engine-0.4.7/dizoo/procgen/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.402975 DI-engine-0.4.7/dizoo/procgen/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      118 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1719 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/bigfish_plr_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1617 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/bigfish_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1362 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/coinrun_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1616 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/coinrun_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1449 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/coinrun_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1369 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/maze_dqn_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1641 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/maze_ppg_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1447 2022-12-11 15:35:39.000000 DI-engine-0.4.7/dizoo/procgen/config/maze_ppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.405351 DI-engine-0.4.7/dizoo/procgen/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-07-19 08:28:38.000000 DI-engine-0.4.7/dizoo/procgen/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4485 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/procgen/envs/procgen_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      810 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/procgen/envs/test_coinrun_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.406083 DI-engine-0.4.7/dizoo/pybullet/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pybullet/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.408387 DI-engine-0.4.7/dizoo/pybullet/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pybullet/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    11254 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/pybullet/envs/pybullet_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1663 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/pybullet/envs/pybullet_wrappers.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.409133 DI-engine-0.4.7/dizoo/rocket/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:32.000000 DI-engine-0.4.7/dizoo/rocket/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.410481 DI-engine-0.4.7/dizoo/rocket/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:45.000000 DI-engine-0.4.7/dizoo/rocket/config/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1818 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/rocket/config/rocket_hover_ppo_config.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1828 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/rocket/config/rocket_landing_ppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.414361 DI-engine-0.4.7/dizoo/rocket/entry/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:05:50.000000 DI-engine-0.4.7/dizoo/rocket/entry/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4039 2023-02-13 06:56:29.000000 DI-engine-0.4.7/dizoo/rocket/entry/rocket_hover_onppo_main_v2.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3261 2023-02-13 06:57:07.000000 DI-engine-0.4.7/dizoo/rocket/entry/rocket_hover_ppo_main.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4022 2023-02-13 06:57:32.000000 DI-engine-0.4.7/dizoo/rocket/entry/rocket_landing_onppo_main_v2.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     3236 2023-02-13 06:56:46.000000 DI-engine-0.4.7/dizoo/rocket/entry/rocket_landing_ppo_main.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.416877 DI-engine-0.4.7/dizoo/rocket/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       34 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/rocket/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4024 2023-03-10 09:15:22.000000 DI-engine-0.4.7/dizoo/rocket/envs/rocket_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1391 2023-01-04 09:44:45.000000 DI-engine-0.4.7/dizoo/rocket/envs/test_rocket_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.417796 DI-engine-0.4.7/dizoo/slime_volley/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/slime_volley/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.418265 DI-engine-0.4.7/dizoo/slime_volley/config/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1685 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/slime_volley/config/slime_volley_ppo_config.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.421559 DI-engine-0.4.7/dizoo/slime_volley/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       46 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/slime_volley/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     7877 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/slime_volley/envs/slime_volley_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1274 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/slime_volley/envs/test_slime_volley_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.422809 DI-engine-0.4.7/dizoo/smac/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.430979 DI-engine-0.4.7/dizoo/smac/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      194 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1666 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/smac/envs/fake_smac_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.431894 DI-engine-0.4.7/dizoo/smac/envs/maps/
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.455798 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14925 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/10m_vs_11m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16965 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/1c3s5z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14923 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/25m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14929 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/27m_vs_30m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16477 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2c_vs_64zg.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15539 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2m_vs_1z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15692 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2s3z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14202 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2s_vs_1sc.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14920 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15697 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s5z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15704 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s5z_vs_3s6z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    13995 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_3z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14000 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_4z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    13998 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_5z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14930 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/5m_vs_6m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14342 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/6h_vs_8z.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14922 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/8m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14927 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/8m_vs_9m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    17411 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/MMM.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    17419 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/MMM2.SC2Map
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/__init__.py
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    16782 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/bane_vs_bane.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    18055 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/corridor.SC2Map
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    17080 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/infestor_viper.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    18133 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/so_many_baneling.SC2Map
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.458210 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    14835 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/3m.SC2Map
+-rwxr-xr-x   0 niuyazhe (1370690143) 453037844    15596 2021-08-01 05:19:48.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/3s5z.SC2Map
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/maps/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    16272 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/smac_action.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844    70855 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/smac/envs/smac_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     5383 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/smac_map.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     9098 2022-09-30 04:52:25.000000 DI-engine-0.4.7/dizoo/smac/envs/smac_reward.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6050 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/smac/envs/test_smac_env.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.458476 DI-engine-0.4.7/dizoo/sokoban/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844        0 2023-01-17 12:06:19.000000 DI-engine-0.4.7/dizoo/sokoban/__init__.py
+drwxr-xr-x   0 niuyazhe (1370690143) 453037844        0 2023-04-12 15:08:02.460779 DI-engine-0.4.7/dizoo/sokoban/envs/
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       36 2022-10-19 09:16:29.000000 DI-engine-0.4.7/dizoo/sokoban/envs/__init__.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     4021 2022-12-13 15:12:50.000000 DI-engine-0.4.7/dizoo/sokoban/envs/sokoban_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     1658 2022-08-02 16:56:36.000000 DI-engine-0.4.7/dizoo/sokoban/envs/sokoban_wrappers.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844      779 2022-08-02 16:56:36.000000 DI-engine-0.4.7/dizoo/sokoban/envs/test_sokoban_env.py
+-rw-r--r--   0 niuyazhe (1370690143) 453037844       38 2023-04-12 15:08:02.461912 DI-engine-0.4.7/setup.cfg
+-rw-r--r--   0 niuyazhe (1370690143) 453037844     6130 2023-03-10 09:15:22.000000 DI-engine-0.4.7/setup.py
```

### Comparing `DI-engine-0.4.6/DI_engine.egg-info/PKG-INFO` & `DI-engine-0.4.7/DI_engine.egg-info/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: DI-engine
-Version: 0.4.6
+Version: 0.4.7
 Summary: Decision AI Engine
 Home-page: https://github.com/opendilab/DI-engine
 Author: OpenDILab Contributors
 Author-email: opendilab@pjlab.org.cn
 License: Apache License, Version 2.0
 Keywords: Decision AI Engine
 Classifier: Development Status :: 5 - Production/Stable
@@ -66,70 +66,73 @@
 [![GitHub forks](https://img.shields.io/github/forks/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/network)
 ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/opendilab/DI-engine)
 [![GitHub issues](https://img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/issues)
 [![GitHub pulls](https://img.shields.io/github/issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/graphs/contributors)
 [![GitHub license](https://img.shields.io/github/license/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/blob/master/LICENSE)
 
-Updated on 2023.02.17 DI-engine-v0.4.6
+Updated on 2023.04.11 DI-engine-v0.4.7
 
 
 ## Introduction to DI-engine
-[DI-engine doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/)
+[Documentation](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) | [Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap](https://github.com/opendilab/DI-engine/issues/548)
 
-**DI-engine** is a generalized decision intelligence engine. It supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** ([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/index.html)):
+**DI-engine** is a generalized decision intelligence engine for PyTorch and JAX. 
+
+It provides **python-first** and **asynchronous-native** task and middleware abstractions, and modularly integrates several of the most important decision-making concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with superior performance, high efficiency, well-organized [documentation](https://di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/opendilab/DI-engine/actions):
 
 - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
 - Multi-agent RL algorithms like QMIX, MAPPO, ACE
-- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit Behavioral Cloning
-- Exploration algorithms like HER, RND, ICM, NGU
+- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit BC
 - Offline RL algorithms: CQL, TD3BC, Decision Transformer
 - Model-based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO
+- Exploration algorithms like HER, RND, ICM, NGU
 
-**DI-engine** aims to **standardize different Decision Intelligence enviroments and applications**. Various training pipelines and customized decision AI applications are also supported.
+**DI-engine** aims to **standardize different Decision Intelligence environments and applications**, supporting both academic research and prototype applications. Various training pipelines and customized decision AI applications are also supported:
 
 <details open>
 <summary>(Click to Collapse)</summary>
 
 - Traditional academic environments
-  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility)
+  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility): various decision intelligence demonstrations and benchmark environments with DI-engine.
 - Tutorial courses
   - [PPOxFamily](https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course
 - Real world decision AI applications
   - [DI-star](https://github.com/opendilab/DI-star): Decision AI in StarCraftII
   - [DI-drive](https://github.com/opendilab/DI-drive): Auto-driving platform
   - [GoBigger](https://github.com/opendilab/GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment
   - [DI-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game
   - [DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light Control
   - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in Biological Sequence Prediction and Searching
   - [DI-1024](https://github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game
 - Research paper
   - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer
   - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency
-- General nested data lib
-  - [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
-  - [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - Docs and Tutorials
-  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs)
+  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best practice and the API reference.
   - [awesome-model-based-RL](https://github.com/opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL resources
   - [awesome-exploration-RL](https://github.com/opendilab/awesome-exploration-rl): A curated list of awesome exploration RL resources
   - [awesome-decision-transformer](https://github.com/opendilab/awesome-decision-transformer): A curated list of Decision Transformer resources
+  - [awesome-RLHF](https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement learning with human feedback resources
   - [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal Reinforcement Learning resources
   - [awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-based-protein-design): a collection of research papers for AI-based protein design
   - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of Diffusion Model in RL resources
   - [awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous Driving resources
   - [awesome-driving-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-prediction): A collection of research papers for Driving Behavior Prediction
 </details>
 
+On the low-level end, DI-engine comes with a set of highly re-usable modules, including [RL optimization functions](https://github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities](https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and [auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
 
-**DI-engine** also has some **system optimization and design** for efficient and robust large-scale RL training:
+BTW, **DI-engine** also has some special **system optimization and design** for efficient and robust large-scale RL training:
 
 <details close>
 <summary>(Click for Details)</summary>
 
+- [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - [DI-orchestrator](https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource and Operator Lib
 - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
 - [DI-store](https://github.com/opendilab/DI-store): RL Object Store
 </details>
 
 Have fun with exploration and exploitation.
 
@@ -138,14 +141,15 @@
 - [Introduction to DI-engine](#introduction-to-di-engine)
 - [Outline](#outline)
 - [Installation](#installation)
 - [Quick Start](#quick-start)
 - [Feature](#feature)
   - [&#8627; Algorithm Versatility](#algorithm-versatility)
   - [&#8627; Environment Versatility](#environment-versatility)
+  - [&#8627; General Data Container: TreeTensor](#general-data-container-treetensor)
 - [Feedback and Contribution](#feedback-and-contribution)
 - [Supporters](#supporters)
   - [&#8627; Stargazers](#-stargazers)
   - [&#8627; Forkers](#-forkers)
 - [Citation](#citation)
 - [License](#license)
 
@@ -165,20 +169,24 @@
 
 And our dockerhub repo can be found [here](https://hub.docker.com/repository/docker/opendilab/ding)，we prepare `base image` and `env image` with common RL environments.
 
 <details close>
 <summary>(Click for Details)</summary>
 
 - base: opendilab/ding:nightly
+- rpc: opendilab/ding:nightly-rpc
 - atari: opendilab/ding:nightly-atari
 - mujoco: opendilab/ding:nightly-mujoco
 - dmc: opendilab/ding:nightly-dmc2gym
 - metaworld: opendilab/ding:nightly-metaworld
 - smac: opendilab/ding:nightly-smac
 - grf: opendilab/ding:nightly-grf
+- cityflow: opendilab/ding:nightly-cityflow
+- evogym: opendilab/ding:nightly-evogym
+- d4rl: opendilab/ding:nightly-d4rl
 </details>
 
 The detailed documentation are hosted on [doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/).
 
 ## Quick Start
 
 [3 Minutes Kickoff](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html)
@@ -195,43 +203,43 @@
 
 ```bash
 ding -m serial -e cartpole -p dqn -s 0
 ```
 
 ## Feature
 ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-18)
 
-![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-18)
+<details open>
+<summary>(Click to Collapse)</summary>
+
+![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-23)
 
-![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-18)
+![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-23)
+
+![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-23)
 
 ![dist](https://img.shields.io/badge/-distributed-blue) &nbsp;[Distributed Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)｜[分布式强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html)
 
 ![MARL](https://img.shields.io/badge/-MARL-yellow) &nbsp;[Multi-Agent Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)｜[多智能体强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html)
 
 ![exp](https://img.shields.io/badge/-exploration-orange) &nbsp;[Exploration Mechanisms in Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)｜[强化学习中的探索机制](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html)
 
 ![IL](https://img.shields.io/badge/-IL-purple) &nbsp;[Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)｜[模仿学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html)
 
 ![offline](https://img.shields.io/badge/-offlineRL-darkblue) &nbsp;[Offiline Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)｜[离线强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
 
 
 ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) &nbsp;[Model-Based Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)｜[基于模型的强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html)
 
-![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithm, usually as plugin-in in the whole pipeline
+![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithms, usually as plugin-in in the whole pipeline
 
 P.S: The `.py` file in `Runnable Demo` can be found in `dizoo`
 
 
 
-
-<details open>
-<summary>(Click to Collapse)</summary>
-
 |  No.  |                          Algorithm                           |                            Label                             |                        Doc and Implementation                        |                        Runnable Demo                         |
 | :--: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
 |  1   |         [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)<br>[DQN中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/dqn_zh.html)<br>[policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -c cartpole_dqn_config.py -s 0 |
 |  2   |         [C51](https://arxiv.org/pdf/1707.06887.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/c51.html)<br>[policy/c51](https://github.com/opendilab/DI-engine/blob/main/ding/policy/c51.py) |        ding -m serial -c cartpole_c51_config.py -s 0         |
 |  3   |         [QRDQN](https://arxiv.org/pdf/1710.10044.pdf)        | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [QRDQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qrdqn.html)<br>[policy/qrdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qrdqn.py) |       ding -m serial -c cartpole_qrdqn_config.py -s 0        |
 |  4   |         [IQN](https://arxiv.org/pdf/1806.06923.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [IQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/iqn.html)<br>[policy/iqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/iqn.py) |        ding -m serial -c cartpole_iqn_config.py -s 0         |
 |  5   |         [FQF](https://arxiv.org/pdf/1911.02140.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [FQF doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/fqf.html)<br>[policy/fqf](https://github.com/opendilab/DI-engine/blob/main/ding/policy/fqf.py) |        ding -m serial -c cartpole_fqf_config.py -s 0         |
@@ -247,44 +255,45 @@
 |  15  |         [DDPG](https://arxiv.org/pdf/1509.02971.pdf)/[PADDPG](https://arxiv.org/pdf/1511.04143.pdf)         | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [DDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |        ding -m serial -c pendulum_ddpg_config.py -s 0        |
 |  16  |         [TD3](https://arxiv.org/pdf/1802.09477.pdf)          | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [TD3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3.html)<br>[policy/td3](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3.py) | python3 -u pendulum_td3_main.py / ding -m serial -c pendulum_td3_config.py -s 0 |
 |  17  | [D4PG](https://arxiv.org/pdf/1804.08617.pdf) | ![continuous](https://img.shields.io/badge/-continous-green) | [D4PG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/d4pg.html)<br>[policy/d4pg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/d4pg.py) | python3 -u pendulum_d4pg_config.py |
 |  18  |           [SAC](https://arxiv.org/abs/1801.01290)/[MASAC]            | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![continuous](https://img.shields.io/badge/-continous-green)![MARL](https://img.shields.io/badge/-MARL-yellow) | [SAC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sac.html)<br>[policy/sac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/sac.py) |        ding -m serial -c pendulum_sac_config.py -s 0         |
 |  19  | [PDQN](https://arxiv.org/pdf/1810.06394.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_pdqn_config.py -s 0 |
 |  20  | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_mpdqn_config.py -s 0 |
 |  21  | [HPPO](https://arxiv.org/pdf/1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 |
-|  22  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
-|  23  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
-|  24  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
-|  25  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
-|  26  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
-|  27  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
-|  28  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
-|  29  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
-|  30  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
-|  31  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
-|  32  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
-|  33  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
-|  34  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
-|  35  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
-|  36  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
-|  37  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
-|  38  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
-|  39  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
-|  40  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
-|  41  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
-|  42  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
-|  43  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
-|  44  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
-|  45  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
-|  46  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
-|  47  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
-|  48  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
-|  49  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
-|  50  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
-|  51  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  22  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  23  |         [MDQN](https://arxiv.org/abs/2007.14430)          |   ![discrete](https://img.shields.io/badge/-discrete-brightgreen)    | [policy/mdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mdqn.py) |        python3 -u asterix_mdqn_config.py       |
+|  24  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
+|  25  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
+|  26  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
+|  27  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
+|  28  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
+|  29  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
+|  30  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
+|  31  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
+|  32  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
+|  33  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
+|  34  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
+|  35  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
+|  36  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
+|  37  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
+|  38  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
+|  39  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
+|  40  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
+|  41  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
+|  42  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
+|  43  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
+|  44  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
+|  45  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
+|  46  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
+|  47  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
+|  48  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
+|  49  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
+|  50  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
+|  51  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
+|  52  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
 </details>
 
 
 ### Environment Versatility
 <details open>
 <summary>(Click to Collapse)</summary>
 
@@ -303,32 +312,32 @@
 |  11  |       [overcooked](https://github.com/HumanCompatibleAI/overcooked-demo)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)  | ![original](./dizoo/overcooked/overcooked.gif)       |   [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/overcooded/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/overcooked.html)   |
 |  12  |       [procgen](https://github.com/openai/procgen)                          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   | ![original](./dizoo/procgen/coinrun.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/procgen)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/procgen.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/procgen_zh.html) |
 |  13  |       [pybullet](https://github.com/benelot/pybullet-gym)    | ![continuous](https://img.shields.io/badge/-continous-green)  | ![original](./dizoo/pybullet/pybullet.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pybullet/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/pybullet_zh.html) |
 |  14  |       [smac](https://github.com/oxwhirl/smac)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue)![sparse](https://img.shields.io/badge/-sparse%20reward-orange) | ![original](./dizoo/smac/smac.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/smac/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/smac.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/smac_zh.html) |
 | 15 | [d4rl](https://github.com/rail-berkeley/d4rl) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | ![ori](dizoo/d4rl/d4rl.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/d4rl)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/d4rl_zh.html) |
 |  16  |       league_demo                      | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![original](./dizoo/league_demo/league_demo.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/league_demo/envs)                |
 |  17  |       pomdp atari                    | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   |  | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs) |
-|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) |
+|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bsuite_zh.html) |
 |  19  | [ImageNet](https://www.image-net.org/) | ![IL](https://img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/image_classification)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/image_cls_zh.html) |
 |  20  | [slime_volleyball](https://github.com/hardmaru/slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/slime_volley)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/slime_volleyball.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/slime_volleyball_zh.html) |
 |  21  | [gym_hybrid](https://github.com/thomashirtz/gym-hybrid) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_hybrid/moving_v0.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_hybrid)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_hybrid.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_hybrid_zh.html) |
 |  22  | [GoBigger](https://github.com/opendilab/GoBigger) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](./dizoo/gobigger_overview.gif) | [dizoo link](https://github.com/opendilab/GoBigger-Challenge-2021/tree/main/di_baseline)<br>[env tutorial](https://gobigger.readthedocs.io/en/latest/index.html)<br>[环境指南](https://gobigger.readthedocs.io/zh_CN/latest/) |
 |  23  | [gym_soccer](https://github.com/openai/gym-soccer) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_soccer/half_offensive.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_soccer)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_soccer_zh.html) |
 |  24  |[multiagent_mujoco](https://github.com/schroederdewitt/multiagent_mujoco)       |  ![continuous](https://img.shields.io/badge/-continous-green) ![marl](https://img.shields.io/badge/-MARL-yellow) | ![original](./dizoo/mujoco/mujoco.gif)                    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/multiagent_mujoco/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/mujoco_zh.html) |
 |  25  |bitflip                                | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![sparse](https://img.shields.io/badge/-sparse%20reward-orange)  | ![original](./dizoo/bitflip/bitflip.gif)    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bitflip/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bitflip_zh.html) |
 |  26  |[sokoban](https://github.com/mpSchrader/gym-sokoban) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![Game 2](https://github.com/mpSchrader/gym-sokoban/raw/default/docs/Animations/solved_4.gif?raw=true) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/sokoban/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/sokoban.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/sokoban_zh.html) |
-|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br>环境指南 |
+|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br> [env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/gym_anytrading/envs/README.md) |
 |  28  |[mario](https://github.com/Kautenja/gym-super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/mario) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_super_mario_bros.html) <br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_super_mario_bros_zh.html) |
 |  29  |[dmc2gym](https://github.com/denisyarats/dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/dmc2gym.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/dmc2gym_zh.html) |
-|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br>环境指南 |
+|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/Evogym_zh.html) |
 |  31  |[gym-pybullet-drones](https://github.com/utiasDSL/gym-pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_pybullet_drones/envs)<br>环境指南 |
 |  32  |[beergame](https://github.com/OptMLGroup/DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)<br>环境指南 |
-|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br>环境指南 |
+|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/acrobot_zh.html) |
 |  34  |[box2d/car_racing](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) <br> ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)<br>环境指南 |
-|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br>环境指南 |
+|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/metadrive_zh.html) |
 
 ![discrete](https://img.shields.io/badge/-discrete-brightgreen) means discrete action space
 
 ![continuous](https://img.shields.io/badge/-continous-green) means continuous action space
 
 ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous) action space
 
@@ -342,14 +351,119 @@
 
 ![selfplay](https://img.shields.io/badge/-selfplay-blue) means environment that allows agent VS agent battle
 
 P.S. some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward type
 </details>
 
 
+### General Data Container: TreeTensor
+
+DI-engine utilizes [TreeTensor](https://github.com/opendilab/DI-treetensor) as the basic data container in various components, which is ease of use and consistent across different code modules such as environment definition, data processing and DRL optimization. Here are some concrete code examples:
+
+- TreeTensor can easily extend all the operations of `torch.Tensor` to nested data:
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```python
+    import treetensor.torch as ttorch
+
+
+    # create random tensor
+    data = ttorch.randn({'a': (3, 2), 'b': {'c': (3, )}})
+    # clone+detach tensor
+    data_clone = data.clone().detach()
+    # access tree structure like attribute
+    a = data.a
+    c = data.b.c
+    # stack/cat/split
+    stacked_data = ttorch.stack([data, data_clone], 0)
+    cat_data = ttorch.cat([data, data_clone], 0)
+    data, data_clone = ttorch.split(stacked_data, 1)
+    # reshape
+    data = data.unsqueeze(-1)
+    data = data.squeeze(-1)
+    flatten_data = data.view(-1)
+    # indexing
+    data_0 = data[0]
+    data_1to2 = data[1:2]
+    # execute math calculations
+    data = data.sin()
+    data.b.c.cos_().clamp_(-1, 1)
+    data += data ** 2
+    # backward
+    data.requires_grad_(True)
+    loss = data.arctan().mean()
+    loss.backward()
+    # print shape
+    print(data.shape)
+    # result
+    # <Size 0x7fbd3346ddc0>
+    # ├── 'a' --> torch.Size([1, 3, 2])
+    # └── 'b' --> <Size 0x7fbd3346dd00>
+    #     └── 'c' --> torch.Size([1, 3])
+    ```
+
+  </details>
+
+- TreeTensor can make it simple yet effective to implement classic deep reinforcement learning pipeline
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```diff
+    import torch
+    import treetensor.torch as ttorch
+
+    B = 4
+
+
+    def get_item():
+        return {
+            'obs': {
+                'scalar': torch.randn(12),
+                'image': torch.randn(3, 32, 32),
+            },
+            'action': torch.randint(0, 10, size=(1,)),
+            'reward': torch.rand(1),
+            'done': False,
+        }
+
+
+    data = [get_item() for _ in range(B)]
+
+
+    # execute `stack` op
+    - def stack(data, dim):
+    -     elem = data[0]
+    -     if isinstance(elem, torch.Tensor):
+    -         return torch.stack(data, dim)
+    -     elif isinstance(elem, dict):
+    -         return {k: stack([item[k] for item in data], dim) for k in elem.keys()}
+    -     elif isinstance(elem, bool):
+    -         return torch.BoolTensor(data)
+    -     else:
+    -         raise TypeError("not support elem type: {}".format(type(elem)))
+    - stacked_data = stack(data, dim=0)
+    + data = [ttorch.tensor(d) for d in data]
+    + stacked_data = ttorch.stack(data, dim=0)
+
+    # validate
+    - assert stacked_data['obs']['image'].shape == (B, 3, 32, 32)
+    - assert stacked_data['action'].shape == (B, 1)
+    - assert stacked_data['reward'].shape == (B, 1)
+    - assert stacked_data['done'].shape == (B,)
+    - assert stacked_data['done'].dtype == torch.bool
+    + assert stacked_data.obs.image.shape == (B, 3, 32, 32)
+    + assert stacked_data.action.shape == (B, 1)
+    + assert stacked_data.reward.shape == (B, 1)
+    + assert stacked_data.done.shape == (B,)
+    + assert stacked_data.done.dtype == torch.bool
+    ```
+
+  </details>
+
 ## Feedback and Contribution
 
 - [File an issue](https://github.com/opendilab/DI-engine/issues/new/choose) on Github
 - Open or participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
 - Discuss on DI-engine [slack communication channel](https://join.slack.com/t/opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ)
 - Discuss on DI-engine's QQ group (700157520) or add us on WeChat
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: DI-engine Version: 0.4.6 Summary: Decision AI
+Metadata-Version: 2.1 Name: DI-engine Version: 0.4.7 Summary: Decision AI
 Engine Home-page: https://github.com/opendilab/DI-engine Author: OpenDILab
 Contributors Author-email: opendilab@pjlab.org.cn License: Apache License,
 Version 2.0 Keywords: Decision AI Engine Classifier: Development Status :: 5 -
 Production/Stable Classifier: Intended Audience :: Science/Research Classifier:
 License :: OSI Approved :: Apache Software License Classifier: Operating System
 :: POSIX :: Linux Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS :: MacOS X Classifier: Programming
@@ -48,145 +48,166 @@
 commit-activity/m/opendilab/DI-engine) [![GitHub issues](https://
 img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/
 opendilab/DI-engine/issues) [![GitHub pulls](https://img.shields.io/github/
 issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-
 engine)](https://github.com/opendilab/DI-engine/graphs/contributors) [![GitHub
 license](https://img.shields.io/github/license/opendilab/DI-engine)](https://
-github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.02.17 DI-
-engine-v0.4.6 ## Introduction to DI-engine [DI-engine doc](https://di-engine-
+github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.04.11 DI-
+engine-v0.4.7 ## Introduction to DI-engine [Documentation](https://di-engine-
 docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/) **DI-engine** is a generalized decision
-intelligence engine. It supports **various [deep reinforcement learning](https:
-//di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms**
-([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
-index.html)): - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
-- Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation learning
-algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit
-Behavioral Cloning - Exploration algorithms like HER, RND, ICM, NGU - Offline
-RL algorithms: CQL, TD3BC, Decision Transformer - Model-based RL algorithms:
-SVG, MVE, STEVE / MBPO, DDPPO **DI-engine** aims to **standardize different
-Decision Intelligence enviroments and applications**. Various training
-pipelines and customized decision AI applications are also supported.  (Click
-to Collapse) - Traditional academic environments - [DI-zoo](https://github.com/
-opendilab/DI-engine#environment-versatility) - Tutorial courses - [PPOxFamily]
-(https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course -
-Real world decision AI applications - [DI-star](https://github.com/opendilab/
-DI-star): Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/
-DI-drive): Auto-driving platform - [GoBigger](https://github.com/opendilab/
-GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-
-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game -
-[DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in
-Traffic Light Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq):
-Decision AI in Biological Sequence Prediction and Searching - [DI-1024](https:/
-/github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game -
-Research paper - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL
-2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion
-Transformer - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE:
-Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency -
-General nested data lib - [treevalue](https://github.com/opendilab/treevalue):
-Tree-nested data structure - [DI-treetensor](https://github.com/opendilab/DI-
-treetensor): Tree-nested PyTorch tensor Lib - Docs and Tutorials - [DI-engine-
-docs](https://github.com/opendilab/DI-engine-docs) - [awesome-model-based-RL]
-(https://github.com/opendilab/awesome-model-based-RL): A curated list of
-awesome Model-Based RL resources - [awesome-exploration-RL](https://github.com/
-opendilab/awesome-exploration-rl): A curated list of awesome exploration RL
-resources - [awesome-decision-transformer](https://github.com/opendilab/
-awesome-decision-transformer): A curated list of Decision Transformer resources
-- [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/
-awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal
-Reinforcement Learning resources - [awesome-AI-based-protein-design](https://
-github.com/opendilab/awesome-AI-based-protein-design): a collection of research
-papers for AI-based protein design - [awesome-diffusion-model-in-rl](https://
-github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of
-Diffusion Model in RL resources - [awesome-end-to-end-autonomous-driving]
-(https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated
-list of awesome End-to-End Autonomous Driving resources - [awesome-driving-
-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-
-prediction): A collection of research papers for Driving Behavior Prediction
-**DI-engine** also has some **system optimization and design** for efficient
-and robust large-scale RL training:  (Click for Details) - [DI-orchestrator]
-(https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource
-and Operator Lib - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
-- [DI-store](https://github.com/opendilab/DI-store): RL Object Store  Have fun
-with exploration and exploitation. ## Outline - [Introduction to DI-engine]
-(#introduction-to-di-engine) - [Outline](#outline) - [Installation]
-(#installation) - [Quick Start](#quick-start) - [Feature](#feature) - [↳
-Algorithm Versatility](#algorithm-versatility) - [↳ Environment Versatility]
-(#environment-versatility) - [Feedback and Contribution](#feedback-and-
-contribution) - [Supporters](#supporters) - [↳ Stargazers](#-stargazers) - [↳
-Forkers](#-forkers) - [Citation](#citation) - [License](#license) ##
-Installation You can simply install DI-engine from PyPI with the following
-command: ```bash pip install DI-engine ``` If you use Anaconda or Miniconda,
-you can install DI-engine from conda-forge through the following command:
-```bash conda install -c opendilab di-engine ``` For more information about
-installation, you can refer to [installation](https://di-engine-
-docs.readthedocs.io/en/latest/01_quickstart/installation.html). And our
-dockerhub repo can be found [here](https://hub.docker.com/repository/docker/
-opendilab/ding)ï¼we prepare `base image` and `env image` with common RL
-environments.  (Click for Details) - base: opendilab/ding:nightly - atari:
-opendilab/ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc:
-opendilab/ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld -
-smac: opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf  The
-detailed documentation are hosted on [doc](https://di-engine-
-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff](https://
-di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html) [3
-Minutes Kickoff (colab)](https://colab.research.google.com/drive/
-1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing) [How to migrate a new **RL
-Env**](https://di-engine-docs.readthedocs.io/en/latest/11_dizoo/index.html) |
-[å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html) [How to customize the
-neural network model](https://di-engine-docs.readthedocs.io/en/latest/
-04_best_practice/custom_model.html) |
+docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-
+docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) |
+[Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/
+index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap]
+(https://github.com/opendilab/DI-engine/issues/548) **DI-engine** is a
+generalized decision intelligence engine for PyTorch and JAX. It provides
+**python-first** and **asynchronous-native** task and middleware abstractions,
+and modularly integrates several of the most important decision-making
+concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine
+supports **various [deep reinforcement learning](https://di-engine-
+docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with
+superior performance, high efficiency, well-organized [documentation](https://
+di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/
+opendilab/DI-engine/actions): - Most basic DRL algorithms, such as DQN, PPO,
+SAC, R2D2, IMPALA - Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation
+learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning,
+Implicit BC - Offline RL algorithms: CQL, TD3BC, Decision Transformer - Model-
+based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO - Exploration algorithms
+like HER, RND, ICM, NGU **DI-engine** aims to **standardize different Decision
+Intelligence environments and applications**, supporting both academic research
+and prototype applications. Various training pipelines and customized decision
+AI applications are also supported:  (Click to Collapse) - Traditional academic
+environments - [DI-zoo](https://github.com/opendilab/DI-engine#environment-
+versatility): various decision intelligence demonstrations and benchmark
+environments with DI-engine. - Tutorial courses - [PPOxFamily](https://
+github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course - Real world
+decision AI applications - [DI-star](https://github.com/opendilab/DI-star):
+Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/DI-drive):
+Auto-driving platform - [GoBigger](https://github.com/opendilab/GoBigger):
+[ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-sheep](https://
+github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game - [DI-smartcross]
+(https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light
+Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in
+Biological Sequence Prediction and Searching - [DI-1024](https://github.com/
+opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game - Research paper -
+[InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-
+Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer -
+[ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-
+agent Q-learning with Bidirectional Action-Dependency - Docs and Tutorials -
+[DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best
+practice and the API reference. - [awesome-model-based-RL](https://github.com/
+opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL
+resources - [awesome-exploration-RL](https://github.com/opendilab/awesome-
+exploration-rl): A curated list of awesome exploration RL resources - [awesome-
+decision-transformer](https://github.com/opendilab/awesome-decision-
+transformer): A curated list of Decision Transformer resources - [awesome-RLHF]
+(https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement
+learning with human feedback resources - [awesome-multi-modal-reinforcement-
+learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-
+learning): A curated list of Multi-Modal Reinforcement Learning resources -
+[awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-
+based-protein-design): a collection of research papers for AI-based protein
+design - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-
+diffusion-model-in-rl): A curated list of Diffusion Model in RL resources -
+[awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-
+end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous
+Driving resources - [awesome-driving-behavior-prediction](https://github.com/
+opendilab/awesome-driving-behavior-prediction): A collection of research papers
+for Driving Behavior Prediction  On the low-level end, DI-engine comes with a
+set of highly re-usable modules, including [RL optimization functions](https://
+github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities]
+(https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and
+[auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
+BTW, **DI-engine** also has some special **system optimization and design** for
+efficient and robust large-scale RL training:  (Click for Details) -
+[treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested
+PyTorch tensor Lib - [DI-orchestrator](https://github.com/opendilab/DI-
+orchestrator): RL Kubernetes Custom Resource and Operator Lib - [DI-hpc](https:
+//github.com/opendilab/DI-hpc): RL HPC OP Lib - [DI-store](https://github.com/
+opendilab/DI-store): RL Object Store  Have fun with exploration and
+exploitation. ## Outline - [Introduction to DI-engine](#introduction-to-di-
+engine) - [Outline](#outline) - [Installation](#installation) - [Quick Start]
+(#quick-start) - [Feature](#feature) - [↳ Algorithm Versatility](#algorithm-
+versatility) - [↳ Environment Versatility](#environment-versatility) - [↳
+General Data Container: TreeTensor](#general-data-container-treetensor) -
+[Feedback and Contribution](#feedback-and-contribution) - [Supporters]
+(#supporters) - [↳ Stargazers](#-stargazers) - [↳ Forkers](#-forkers) -
+[Citation](#citation) - [License](#license) ## Installation You can simply
+install DI-engine from PyPI with the following command: ```bash pip install DI-
+engine ``` If you use Anaconda or Miniconda, you can install DI-engine from
+conda-forge through the following command: ```bash conda install -c opendilab
+di-engine ``` For more information about installation, you can refer to
+[installation](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+installation.html). And our dockerhub repo can be found [here](https://
+hub.docker.com/repository/docker/opendilab/ding)ï¼we prepare `base image` and
+`env image` with common RL environments.  (Click for Details) - base:
+opendilab/ding:nightly - rpc: opendilab/ding:nightly-rpc - atari: opendilab/
+ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc: opendilab/
+ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld - smac:
+opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf - cityflow:
+opendilab/ding:nightly-cityflow - evogym: opendilab/ding:nightly-evogym - d4rl:
+opendilab/ding:nightly-d4rl  The detailed documentation are hosted on [doc]
+(https://di-engine-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff]
+(https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+first_rl_program.html) [3 Minutes Kickoff (colab)](https://
+colab.research.google.com/drive/1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing)
+[How to migrate a new **RL Env**](https://di-engine-docs.readthedocs.io/en/
+latest/11_dizoo/index.html) | [å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**]
+(https://di-engine-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html)
+[How to customize the neural network model](https://di-engine-
+docs.readthedocs.io/en/latest/04_best_practice/custom_model.html) |
 [å¦ä½å®å¶ç­ç¥ä½¿ç¨ç**ç¥ç»ç½ç»æ¨¡å**](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/04_best_practice/custom_model_zh.html)
 [æµè¯/é¨ç½² **å¼ºåå­¦ä¹ ç­ç¥** çæ ·ä¾](https://github.com/opendilab/
 DI-engine/blob/main/dizoo/classic_control/cartpole/entry/
 cartpole_c51_deploy.py) **Bonus: Train RL agent in one line code:** ```bash
 ding -m serial -e cartpole -p dqn -s 0 ``` ## Feature ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen)  discrete means
-discrete action space, which is only label in normal DRL algorithms (1-18) !
-[continuous](https://img.shields.io/badge/-continous-green)  means continuous
-action space, which is only label in normal DRL algorithms (1-18) ![hybrid]
-(https://img.shields.io/badge/-hybrid-darkgreen)  means hybrid (discrete +
-continuous) action space (1-18) ![dist](https://img.shields.io/badge/-
-distributed-blue)  [Distributed Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)ï½
-[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/distributed_rl_zh.html) ![MARL](https://img.shields.io/badge/-MARL-
-yellow)  [Multi-Agent Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)ï½
-[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/
-badge/-exploration-orange)  [Exploration Mechanisms in Reinforcement Learning]
+(Click to Collapse) ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen)  discrete means discrete action space, which is only label in
+normal DRL algorithms (1-23) ![continuous](https://img.shields.io/badge/-
+continous-green)  means continuous action space, which is only label in normal
+DRL algorithms (1-23) ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)
+ means hybrid (discrete + continuous) action space (1-23) ![dist](https://
+img.shields.io/badge/-distributed-blue)  [Distributed Reinforcement Learning]
+(https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+distributed_rl.html)ï½[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html) ![MARL](https:
+//img.shields.io/badge/-MARL-yellow)  [Multi-Agent Reinforcement Learning]
 (https://di-engine-docs.readthedocs.io/en/latest/02_algo/
-exploration_rl.html)ï½[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html) ![IL](https://
-img.shields.io/badge/-IL-purple)  [Imitation Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/
-imitation_learning_zh.html) ![offline](https://img.shields.io/badge/-offlineRL-
-darkblue)  [Offiline Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
-![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue)  [Model-Based
-Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/
-02_algo/model_based_rl.html)ï½[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html) ![other]
-(https://img.shields.io/badge/-other-lightgrey)  means other sub-direction
-algorithm, usually as plugin-in in the whole pipeline P.S: The `.py` file in
-`Runnable Demo` can be found in `dizoo`  (Click to Collapse) | No. | Algorithm
-| Label | Doc and Implementation | Runnable Demo | | :--: | :------------------
-----------------------------------------: | :----------------------------------
-------------------------: | :--------------------------------------------------
---------: | :----------------------------------------------------------: | | 1
-| [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) |
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc]
-(https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)
+multi_agent_cooperation_rl.html)ï½[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/02_algo/
+multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/badge/-
+exploration-orange)  [Exploration Mechanisms in Reinforcement Learning](https:/
+/di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)ï½
+[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/exploration_rl_zh.html) ![IL](https://img.shields.io/badge/-IL-
+purple)  [Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/
+02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html) ![offline]
+(https://img.shields.io/badge/-offlineRL-darkblue)  [Offiline Reinforcement
+Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/
+zh_CN/latest/02_algo/offline_rl_zh.html) ![mbrl](https://img.shields.io/badge/-
+ModelBasedRL-lightblue)  [Model-Based Reinforcement Learning](https://di-
+engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)ï½
+[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/model_based_rl_zh.html) ![other](https://img.shields.io/badge/-
+other-lightgrey)  means other sub-direction algorithms, usually as plugin-in in
+the whole pipeline P.S: The `.py` file in `Runnable Demo` can be found in
+`dizoo` | No. | Algorithm | Label | Doc and Implementation | Runnable Demo | |
+:--: | :----------------------------------------------------------: | :--------
+--------------------------------------------------: | :------------------------
+----------------------------------: | :----------------------------------------
+------------------: | | 1 | [DQN](https://storage.googleapis.com/deepmind-
+media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/
+latest/12_policies/dqn.html)
 [DQNä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/dqn_zh.html)
 [policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -
 c cartpole_dqn_config.py -s 0 | | 2 | [C51](https://arxiv.org/pdf/
 1707.06887.pdf) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/
@@ -289,163 +310,166 @@
 | 20 | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -
 c gym_hybrid_mpdqn_config.py -s 0 | | 21 | [HPPO](https://arxiv.org/pdf/
 1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) |
 [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 | | 22 |
+[BDQ](https://arxiv.org/pdf/1711.08946.pdf) | ![hybrid](https://img.shields.io/
+badge/-hybrid-darkgreen) | [policy/bdq](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/dqn.py) | python3 -u hopper_bdq_config.py | | 23 | [MDQN]
+(https://arxiv.org/abs/2007.14430) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [policy/mdqn](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/mdqn.py) | python3 -u asterix_mdqn_config.py | | 24 |
 [QMIX](https://arxiv.org/pdf/1803.11485.pdf) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [QMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/qmix.html)
 [policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 23 | [COMA]
+qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 25 | [COMA]
 (https://arxiv.org/pdf/1705.08926.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/coma.html)
 [policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 24 | [QTran]
+coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 26 | [QTran]
 (https://arxiv.org/abs/1905.05408) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/
-ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 25
+ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 27
 | [WQMIX](https://arxiv.org/abs/2006.10800) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/wqmix.html)
 [policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 26 | [CollaQ]
+wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 28 | [CollaQ]
 (https://arxiv.org/pdf/2010.08531.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/collaq.html)
 [policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 27 |
+collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 29 |
 [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) | ![MARL](https://
 img.shields.io/badge/-MARL-yellow) | [MADDPG doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/ddpg.html)
 [policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 28 | [GAIL](https://
+ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 30 | [GAIL](https://
 arxiv.org/pdf/1606.03476.pdf) | ![IL](https://img.shields.io/badge/-IL-purple)
 | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 gail.html)
 [reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/gail_irl_model.py) | ding -m serial_gail -
-c cartpole_dqn_gail_config.py -s 0 | | 29 | [SQIL](https://arxiv.org/pdf/
+c cartpole_dqn_gail_config.py -s 0 | | 31 | [SQIL](https://arxiv.org/pdf/
 1905.11108.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [SQIL doc]
 (https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)
 [entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/
 serial_entry_sqil.py) | ding -m serial_sqil -c cartpole_sqil_config.py -s 0 | |
-30 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
+32 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
 img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/dqfd.html)
 [policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 31 | [R2D3]
+dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 33 | [R2D3]
 (https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-
 IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/r2d3.html)
 [R2D3ä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/r2d3_zh.html)
 [policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/
-r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 32 | [Guided Cost
+r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 34 | [Guided Cost
 Learning](https://arxiv.org/pdf/1603.00448.pdf) | ![IL](https://img.shields.io/
 badge/-IL-purple) | [Guided Cost Learningä¸­æææ¡£](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)
 [reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/
 ding/reward_model/guided_cost_reward_model.py) | python3
-lunarlander_gcl_config.py | | 33 | [TREX](https://arxiv.org/abs/1904.06387) | !
+lunarlander_gcl_config.py | | 35 | [TREX](https://arxiv.org/abs/1904.06387) | !
 [IL](https://img.shields.io/badge/-IL-purple) | [TREX doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/trex.html)
 [reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 34 |
+reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 36 |
 [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC) | ![IL]
 (https://img.shields.io/badge/-IL-purple) | [policy/ibc](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/ibc.py)
 [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/
 model/template/ebm.py) | python3 d4rl_ibc_main.py -s 0 -
-c pen_human_ibc_mcmc_config.py | | 35 | [BCO](https://arxiv.org/pdf/
+c pen_human_ibc_mcmc_config.py | | 37 | [BCO](https://arxiv.org/pdf/
 1805.01954.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco]
 (https://github.com/opendilab/DI-engine/blob/main/ding/entry/
-serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 36 | [HER](https:/
+serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 38 | [HER](https:/
 /arxiv.org/pdf/1707.01495.pdf) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [HER doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/her.html)
 [reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 37 |
+reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 39 |
 [RND](https://arxiv.org/abs/1810.12894) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [RND doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/rnd.html)
 [reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/rnd_reward_model.py) | python3 -u cartpole_rnd_onppo_config.py | |
-38 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
+40 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
 img.shields.io/badge/-exploration-orange) | [ICM doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/icm.html)
 [ICMä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/icm_zh.html)
 [reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/icm_reward_model.py) | python3 -u cartpole_ppo_icm_config.py | |
-39 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
+41 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
 img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/cql.html)
 [policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-cql.py) | python3 -u d4rl_cql_main.py | | 40 | [TD3BC](https://arxiv.org/pdf/
+cql.py) | python3 -u d4rl_cql_main.py | | 42 | [TD3BC](https://arxiv.org/pdf/
 2106.06860.pdf) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue)
 | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 td3_bc.html)
 [policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 41 | [Decision Transformer]
+td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 43 | [Decision Transformer]
 (https://arxiv.org/pdf/2106.01345.pdf) | ![offline](https://img.shields.io/
 badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-
 engine/blob/main/ding/policy/decision_transformer.py) | python3 -
-u d4rl_dt_main.py | | 42 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
+u d4rl_dt_main.py | | 44 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
 (https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142)) | !
 [continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://
 img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https:/
 /github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |
 python3 -u pendulum_mbsac_mbpo_config.py \ python3 -
-u pendulum_mbsac_ddppo_config.py | | 43 | STEVESAC([SAC](https://arxiv.org/abs/
+u pendulum_mbsac_ddppo_config.py | | 45 | STEVESAC([SAC](https://arxiv.org/abs/
 1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/
 abs/1510.09142)) | ![continuous](https://img.shields.io/badge/-continous-
 green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/
 mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 44 |
+mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 46 |
 [MBPO](https://arxiv.org/pdf/1906.08253.pdf) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/mbpo.html)
 [world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/
-world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 45 | [DDPPO]
+world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 47 | [DDPPO]
 (https://openreview.net/forum?id=rzvOQrnclO0) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/
 opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) | python3 -
-u pendulum_mbsac_ddppo_config.py | | 46 | [PER](https://arxiv.org/pdf/
+u pendulum_mbsac_ddppo_config.py | | 48 | [PER](https://arxiv.org/pdf/
 1511.05952.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/
-worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 47 | [GAE](https:
+worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 49 | [GAE](https:
 //arxiv.org/pdf/1506.02438.pdf) | ![other](https://img.shields.io/badge/-other-
 lightgrey) | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/
-ding/rl_utils/gae.py) | `ppo demo` | | 48 | [ST-DIM](https://arxiv.org/pdf/
+ding/rl_utils/gae.py) | `ppo demo` | | 50 | [ST-DIM](https://arxiv.org/pdf/
 1906.08226.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/
 blob/main/ding/torch_utils/loss/contrastive_loss.py) | ding -m serial -
-c cartpole_dqn_stdim_config.py -s 0 | | 49 | [PLR](https://arxiv.org/pdf/
+c cartpole_dqn_stdim_config.py -s 0 | | 51 | [PLR](https://arxiv.org/pdf/
 2010.03934.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)
 [data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/
 main/ding/data/level_replay/level_sampler.py) | python3 -
-u bigfish_plr_config.py -s 0 | | 50 | [PCGrad](https://arxiv.org/pdf/
+u bigfish_plr_config.py -s 0 | | 52 | [PCGrad](https://arxiv.org/pdf/
 2001.06782.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/
 blob/main/ding/data/torch_utils/optimizer_helper.py) | python3 -
-u multi_mnist_pcgrad_main.py -s 0 | | 51 | [BDQ](https://arxiv.org/pdf/
-1711.08946.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
-[policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqn.py) | python3 -u hopper_bdq_config.py |  ### Environment Versatility
-(Click to Collapse) | No | Environment | Label | Visualization | Code and Doc
-Links | | :--: | :--------------------------------------: | :------------------
----------------: | :--------------------------------:|:------------------------
----------------------------------: | | 1 | [Atari](https://github.com/openai/
-gym/tree/master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-
-discrete-brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link]
-(https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
+u multi_mnist_pcgrad_main.py -s 0 |  ### Environment Versatility  (Click to
+Collapse) | No | Environment | Label | Visualization | Code and Doc Links | | :
+--: | :--------------------------------------: | :-----------------------------
+----: | :--------------------------------:|:-----------------------------------
+----------------------: | | 1 | [Atari](https://github.com/openai/gym/tree/
+master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 atari.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 atari_zh.html) | | 2 | [box2d/bipedalwalker](https://github.com/openai/gym/
 tree/master/gym/envs/box2d) | ![continuous](https://img.shields.io/badge/-
 continous-green) | ![original](./dizoo/box2d/bipedalwalker/original.gif) |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/
@@ -555,18 +579,20 @@
 atari | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs)
 | | 18 | [bsuite](https://github.com/deepmind/bsuite) | ![discrete](https://
 img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/
 bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/bsuite/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//
-bsuite.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https://
-img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/
-imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
-dizoo/image_classification)
+bsuite.html)
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+bsuite_zh.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https:/
+/img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/
+image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/
+DI-engine/tree/main/dizoo/image_classification)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 image_cls_zh.html) | | 20 | [slime_volleyball](https://github.com/hardmaru/
 slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori]
 (dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/slime_volley)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
@@ -610,18 +636,19 @@
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 sokoban.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 sokoban_zh.html) | | 27 |[gym_anytrading](https://github.com/AminHP/gym-
 anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) |
 ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading)
-ç¯å¢æå | | 28 |[mario](https://github.com/Kautenja/gym-super-mario-bros)
-| ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original]
-(./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-
-engine/tree/main/dizoo/mario)
+[env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/
+gym_anytrading/envs/README.md) | | 28 |[mario](https://github.com/Kautenja/gym-
+super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/mario)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 gym_super_mario_bros.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 gym_super_mario_bros_zh.html) | | 29 |[dmc2gym](https://github.com/denisyarats/
 dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | !
 [original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)
@@ -630,57 +657,96 @@
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 dmc2gym_zh.html) | | 30 |[evogym](https://github.com/EvolutionGym/evogym) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/
 tree/main/dizoo/evogym/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 evogym.html)
-ç¯å¢æå | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+Evogym_zh.html) | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
 pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green)
 | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo
 link](https://github.com/opendilab/DI-engine/tree/main/dizoo/
 gym_pybullet_drones/envs)
 ç¯å¢æå | | 32 |[beergame](https://github.com/OptMLGroup/
 DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https:
 //github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)
 ç¯å¢æå | | 33 |[classic_control/acrobot](https://github.com/openai/gym/
 tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/
 acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/classic_control/acrobot/envs)
-ç¯å¢æå | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+acrobot_zh.html) | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
 master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen)
 ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)
 ç¯å¢æå | | 35 |[metadrive](https://github.com/metadriverse/metadrive) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/
 DI-engine/tree/main/dizoo/metadrive/env)
-ç¯å¢æå | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)
-means discrete action space ![continuous](https://img.shields.io/badge/-
-continous-green) means continuous action space ![hybrid](https://
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+metadrive_zh.html) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) means discrete action space ![continuous](https://img.shields.io/
+badge/-continous-green) means continuous action space ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous)
 action space ![MARL](https://img.shields.io/badge/-MARL-yellow) means multi-
 agent RL environment ![sparse](https://img.shields.io/badge/-sparse%20reward-
 orange) means environment which is related to exploration and sparse reward !
 [offline](https://img.shields.io/badge/-offlineRL-darkblue) means offline RL
 environment ![IL](https://img.shields.io/badge/-IL/SL-purple) means Imitation
 Learning or Supervised Learning Dataset ![selfplay](https://img.shields.io/
 badge/-selfplay-blue) means environment that allows agent VS agent battle P.S.
 some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward
-type  ## Feedback and Contribution - [File an issue](https://github.com/
-opendilab/DI-engine/issues/new/choose) on Github - Open or participate in our
-[forum](https://github.com/opendilab/DI-engine/discussions) - Discuss on DI-
-engine [slack communication channel](https://join.slack.com/t/opendilab/
-shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-engine's QQ
-group (700157520) or add us on WeChat ![WeChat](https://github.com/opendilab/
-DI-engine/blob/main/assets/wechat.png) - Contact our email
+type  ### General Data Container: TreeTensor DI-engine utilizes [TreeTensor]
+(https://github.com/opendilab/DI-treetensor) as the basic data container in
+various components, which is ease of use and consistent across different code
+modules such as environment definition, data processing and DRL optimization.
+Here are some concrete code examples: - TreeTensor can easily extend all the
+operations of `torch.Tensor` to nested data:  (Click for Details) ```python
+import treetensor.torch as ttorch # create random tensor data = ttorch.randn(
+{'a': (3, 2), 'b': {'c': (3, )}}) # clone+detach tensor data_clone = data.clone
+().detach() # access tree structure like attribute a = data.a c = data.b.c #
+stack/cat/split stacked_data = ttorch.stack([data, data_clone], 0) cat_data =
+ttorch.cat([data, data_clone], 0) data, data_clone = ttorch.split(stacked_data,
+1) # reshape data = data.unsqueeze(-1) data = data.squeeze(-1) flatten_data =
+data.view(-1) # indexing data_0 = data[0] data_1to2 = data[1:2] # execute math
+calculations data = data.sin() data.b.c.cos_().clamp_(-1, 1) data += data ** 2
+# backward data.requires_grad_(True) loss = data.arctan().mean() loss.backward
+() # print shape print(data.shape) # result #
+x7fbd3346ddc0> # âââ 'a' --> torch.Size([1, 3, 2]) # âââ 'b' -->
+x7fbd3346dd00> # âââ 'c' --> torch.Size([1, 3]) ```  - TreeTensor can
+make it simple yet effective to implement classic deep reinforcement learning
+pipeline  (Click for Details) ```diff import torch import treetensor.torch as
+ttorch B = 4 def get_item(): return { 'obs': { 'scalar': torch.randn(12),
+'image': torch.randn(3, 32, 32), }, 'action': torch.randint(0, 10, size=(1,)),
+'reward': torch.rand(1), 'done': False, } data = [get_item() for _ in range(B)]
+# execute `stack` op - def stack(data, dim): - elem = data[0] - if isinstance
+(elem, torch.Tensor): - return torch.stack(data, dim) - elif isinstance(elem,
+dict): - return {k: stack([item[k] for item in data], dim) for k in elem.keys
+()} - elif isinstance(elem, bool): - return torch.BoolTensor(data) - else: -
+raise TypeError("not support elem type: {}".format(type(elem))) - stacked_data
+= stack(data, dim=0) + data = [ttorch.tensor(d) for d in data] + stacked_data =
+ttorch.stack(data, dim=0) # validate - assert stacked_data['obs']
+['image'].shape == (B, 3, 32, 32) - assert stacked_data['action'].shape == (B,
+1) - assert stacked_data['reward'].shape == (B, 1) - assert stacked_data
+['done'].shape == (B,) - assert stacked_data['done'].dtype == torch.bool +
+assert stacked_data.obs.image.shape == (B, 3, 32, 32) + assert
+stacked_data.action.shape == (B, 1) + assert stacked_data.reward.shape == (B,
+1) + assert stacked_data.done.shape == (B,) + assert stacked_data.done.dtype ==
+torch.bool ```  ## Feedback and Contribution - [File an issue](https://
+github.com/opendilab/DI-engine/issues/new/choose) on Github - Open or
+participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
+- Discuss on DI-engine [slack communication channel](https://join.slack.com/t/
+opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-
+engine's QQ group (700157520) or add us on WeChat ![WeChat](https://github.com/
+opendilab/DI-engine/blob/main/assets/wechat.png) - Contact our email
 (opendilab@pjlab.org.cn) - Contributes to our future plan [Roadmap](https://
 github.com/opendilab/DI-engine/issues/548) We appreciate all the feedbacks and
 contributions to improve DI-engine, both algorithms and system designs. And
 `CONTRIBUTING.md` offers some necessary information. ## Supporters ### ↳
 Stargazers [![Stargazers repo roster for @opendilab/DI-engine](https://
 reporoster.com/stars/opendilab/DI-engine)](https://github.com/opendilab/DI-
 engine/stargazers) ### ↳ Forkers [![Forkers repo roster for @opendilab/DI-
```

### Comparing `DI-engine-0.4.6/DI_engine.egg-info/SOURCES.txt` & `DI-engine-0.4.7/DI_engine.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -5,20 +5,20 @@
 DI_engine.egg-info/SOURCES.txt
 DI_engine.egg-info/dependency_links.txt
 DI_engine.egg-info/entry_points.txt
 DI_engine.egg-info/requires.txt
 DI_engine.egg-info/top_level.txt
 ding/__init__.py
 ding/compatibility.py
-ding/t.py
 ding/bonus/__init__.py
 ding/bonus/config.py
 ding/bonus/demo.py
 ding/bonus/model.py
 ding/bonus/ppof.py
+ding/bonus/td3.py
 ding/config/__init__.py
 ding/config/config.py
 ding/config/utils.py
 ding/data/__init__.py
 ding/data/model_loader.py
 ding/data/shm_buffer.py
 ding/data/storage_loader.py
@@ -56,14 +56,15 @@
 ding/entry/serial_entry_gail.py
 ding/entry/serial_entry_guided_cost.py
 ding/entry/serial_entry_mbrl.py
 ding/entry/serial_entry_ngu.py
 ding/entry/serial_entry_offline.py
 ding/entry/serial_entry_onpolicy.py
 ding/entry/serial_entry_onpolicy_ppg.py
+ding/entry/serial_entry_pc.py
 ding/entry/serial_entry_plr.py
 ding/entry/serial_entry_preference_based_irl.py
 ding/entry/serial_entry_preference_based_irl_onpolicy.py
 ding/entry/serial_entry_r2d3.py
 ding/entry/serial_entry_reward_model_offpolicy.py
 ding/entry/serial_entry_reward_model_onpolicy.py
 ding/entry/serial_entry_sqil.py
@@ -110,14 +111,15 @@
 ding/example/ddpg.py
 ding/example/dqn.py
 ding/example/dqn_her.py
 ding/example/dqn_new_env.py
 ding/example/dqn_nstep.py
 ding/example/dqn_per.py
 ding/example/dqn_rnd.py
+ding/example/dt.py
 ding/example/iqn_nstep.py
 ding/example/pdqn.py
 ding/example/ppg_offpolicy.py
 ding/example/ppo.py
 ding/example/ppo_offpolicy.py
 ding/example/qrdqn_nstep.py
 ding/example/r2d2.py
@@ -294,16 +296,18 @@
 ding/policy/dqn.py
 ding/policy/fqf.py
 ding/policy/ibc.py
 ding/policy/il.py
 ding/policy/impala.py
 ding/policy/iqn.py
 ding/policy/madqn.py
+ding/policy/mdqn.py
 ding/policy/ngu.py
 ding/policy/offppo_collect_traj.py
+ding/policy/pc.py
 ding/policy/pdqn.py
 ding/policy/pg.py
 ding/policy/policy_factory.py
 ding/policy/ppg.py
 ding/policy/ppo.py
 ding/policy/ppof.py
 ding/policy/qmix.py
@@ -351,41 +355,46 @@
 ding/rl_utils/retrace.py
 ding/rl_utils/sampler.py
 ding/rl_utils/td.py
 ding/rl_utils/upgo.py
 ding/rl_utils/value_rescale.py
 ding/rl_utils/vtrace.py
 ding/torch_utils/__init__.py
+ding/torch_utils/backend_helper.py
 ding/torch_utils/checkpoint_helper.py
 ding/torch_utils/data_helper.py
 ding/torch_utils/dataparallel.py
 ding/torch_utils/distribution.py
+ding/torch_utils/lr_scheduler.py
 ding/torch_utils/math_helper.py
 ding/torch_utils/metric.py
+ding/torch_utils/model_helper.py
 ding/torch_utils/nn_test_helper.py
 ding/torch_utils/optimizer_helper.py
 ding/torch_utils/reshape_helper.py
 ding/torch_utils/loss/__init__.py
 ding/torch_utils/loss/contrastive_loss.py
 ding/torch_utils/loss/cross_entropy_loss.py
 ding/torch_utils/loss/multi_logits_loss.py
 ding/torch_utils/network/__init__.py
 ding/torch_utils/network/activation.py
 ding/torch_utils/network/coverage.xml
 ding/torch_utils/network/gtrxl.py
 ding/torch_utils/network/gumbel_softmax.py
 ding/torch_utils/network/nn_module.py
 ding/torch_utils/network/normalization.py
+ding/torch_utils/network/popart.py
 ding/torch_utils/network/res_block.py
 ding/torch_utils/network/resnet.py
 ding/torch_utils/network/rnn.py
 ding/torch_utils/network/scatter_connection.py
 ding/torch_utils/network/soft_argmax.py
 ding/torch_utils/network/transformer.py
 ding/utils/__init__.py
+ding/utils/bfs_helper.py
 ding/utils/collection_helper.py
 ding/utils/compression_helper.py
 ding/utils/coverage.xml
 ding/utils/default_helper.py
 ding/utils/design_helper.py
 ding/utils/fake_linklink.py
 ding/utils/fast_copy.py
@@ -470,14 +479,15 @@
 ding/worker/collector/comm/__init__.py
 ding/worker/collector/comm/base_comm_collector.py
 ding/worker/collector/comm/flask_fs_collector.py
 ding/worker/collector/comm/utils.py
 ding/worker/collector/tests/__init__.py
 ding/worker/collector/tests/fake_cls_policy.py
 ding/worker/collector/tests/fake_cpong_dqn_config.py
+ding/worker/collector/tests/test_base_serial_collector.py
 ding/worker/collector/tests/test_episode_serial_collector.py
 ding/worker/collector/tests/test_marine_parallel_collector.py
 ding/worker/collector/tests/test_metric_serial_evaluator.py
 ding/worker/collector/tests/speed_test/__init__.py
 ding/worker/collector/tests/speed_test/fake_env.py
 ding/worker/collector/tests/speed_test/fake_policy.py
 ding/worker/collector/tests/speed_test/test_collector_profile.py
@@ -510,17 +520,20 @@
 ding/world_model/idm.py
 ding/world_model/mbpo.py
 ding/world_model/utils.py
 dizoo/__init__.py
 dizoo/atari/__init__.py
 dizoo/atari/config/__init__.py
 dizoo/atari/config/serial/__init__.py
+dizoo/atari/config/serial/asterix/__init__.py
+dizoo/atari/config/serial/asterix/asterix_mdqn_config.py
 dizoo/atari/config/serial/enduro/__init__.py
 dizoo/atari/config/serial/enduro/enduro_dqn_config.py
 dizoo/atari/config/serial/enduro/enduro_impala_config.py
+dizoo/atari/config/serial/enduro/enduro_mdqn_config.py
 dizoo/atari/config/serial/enduro/enduro_onppo_config.py
 dizoo/atari/config/serial/enduro/enduro_qrdqn_config.py
 dizoo/atari/config/serial/enduro/enduro_rainbow_config.py
 dizoo/atari/config/serial/pong/__init__.py
 dizoo/atari/config/serial/pong/pong_a2c_config.py
 dizoo/atari/config/serial/pong/pong_acer_config.py
 dizoo/atari/config/serial/pong/pong_c51_config.py
@@ -579,14 +592,15 @@
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_ddp.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_dp.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_fqf_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_impala_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_iqn_config.py
+dizoo/atari/config/serial/spaceinvaders/spaceinvaders_mdqn_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_ppg_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_qrdqn_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_gtrxl_config.py
 dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_residual_config.py
@@ -655,14 +669,15 @@
 dizoo/box2d/lunarlander/__init__.py
 dizoo/box2d/lunarlander/config/__init__.py
 dizoo/box2d/lunarlander/config/lunarlander_a2c_config.py
 dizoo/box2d/lunarlander/config/lunarlander_acer_config.py
 dizoo/box2d/lunarlander/config/lunarlander_bco_config.py
 dizoo/box2d/lunarlander/config/lunarlander_c51_config.py
 dizoo/box2d/lunarlander/config/lunarlander_cont_ddpg_config.py
+dizoo/box2d/lunarlander/config/lunarlander_cont_sac_config.py
 dizoo/box2d/lunarlander/config/lunarlander_cont_td3_config.py
 dizoo/box2d/lunarlander/config/lunarlander_cont_td3_vae_config.py
 dizoo/box2d/lunarlander/config/lunarlander_decision_transformer.py
 dizoo/box2d/lunarlander/config/lunarlander_discrete_sac_config.py
 dizoo/box2d/lunarlander/config/lunarlander_dqfd_config.py
 dizoo/box2d/lunarlander/config/lunarlander_dqn_config.py
 dizoo/box2d/lunarlander/config/lunarlander_dqn_deque_config.py
@@ -715,14 +730,15 @@
 dizoo/classic_control/cartpole/config/cartpole_dqn_rnd_config.py
 dizoo/classic_control/cartpole/config/cartpole_dqn_stdim_config.py
 dizoo/classic_control/cartpole/config/cartpole_drex_dqn_config.py
 dizoo/classic_control/cartpole/config/cartpole_fqf_config.py
 dizoo/classic_control/cartpole/config/cartpole_gcl_config.py
 dizoo/classic_control/cartpole/config/cartpole_impala_config.py
 dizoo/classic_control/cartpole/config/cartpole_iqn_config.py
+dizoo/classic_control/cartpole/config/cartpole_mdqn_config.py
 dizoo/classic_control/cartpole/config/cartpole_ngu_config.py
 dizoo/classic_control/cartpole/config/cartpole_pg_config.py
 dizoo/classic_control/cartpole/config/cartpole_ppg_config.py
 dizoo/classic_control/cartpole/config/cartpole_ppo_config.py
 dizoo/classic_control/cartpole/config/cartpole_ppo_icm_config.py
 dizoo/classic_control/cartpole/config/cartpole_ppo_offpolicy_config.py
 dizoo/classic_control/cartpole/config/cartpole_ppo_stdim_config.py
@@ -767,14 +783,15 @@
 dizoo/classic_control/pendulum/__init__.py
 dizoo/classic_control/pendulum/config/__init__.py
 dizoo/classic_control/pendulum/config/pendulum_bdq_config.py
 dizoo/classic_control/pendulum/config/pendulum_cql_config.py
 dizoo/classic_control/pendulum/config/pendulum_d4pg_config.py
 dizoo/classic_control/pendulum/config/pendulum_ddpg_config.py
 dizoo/classic_control/pendulum/config/pendulum_dqn_config.py
+dizoo/classic_control/pendulum/config/pendulum_dt_config.py
 dizoo/classic_control/pendulum/config/pendulum_ibc_config.py
 dizoo/classic_control/pendulum/config/pendulum_pg_config.py
 dizoo/classic_control/pendulum/config/pendulum_ppo_config.py
 dizoo/classic_control/pendulum/config/pendulum_sac_config.py
 dizoo/classic_control/pendulum/config/pendulum_sac_data_generation_config.py
 dizoo/classic_control/pendulum/config/pendulum_sqil_sac_config.py
 dizoo/classic_control/pendulum/config/pendulum_td3_bc_config.py
@@ -942,26 +959,32 @@
 dizoo/league_demo/league_demo_ppo_main.py
 dizoo/league_demo/selfplay_demo_ppo_config.py
 dizoo/league_demo/selfplay_demo_ppo_main.py
 dizoo/mario/__init__.py
 dizoo/mario/mario_dqn_config.py
 dizoo/mario/mario_dqn_example.py
 dizoo/mario/mario_dqn_main.py
+dizoo/maze/__init__.py
+dizoo/maze/envs/__init__.py
+dizoo/maze/envs/maze_env.py
+dizoo/maze/envs/test_maze_env.py
 dizoo/metadrive/__init__.py
 dizoo/metadrive/config/__init__.py
 dizoo/metadrive/config/metadrive_onppo_config.py
 dizoo/metadrive/config/metadrive_onppo_eval_config.py
 dizoo/metadrive/env/__init__.py
 dizoo/metadrive/env/drive_env.py
 dizoo/metadrive/env/drive_utils.py
 dizoo/metadrive/env/drive_wrapper.py
 dizoo/minigrid/__init__.py
 dizoo/minigrid/envs/__init__.py
 dizoo/minigrid/envs/app_key_to_door_treasure.py
 dizoo/minigrid/envs/minigrid_env.py
+dizoo/minigrid/envs/minigrid_wrapper.py
+dizoo/minigrid/envs/noisy_tv.py
 dizoo/minigrid/envs/test_minigrid_env.py
 dizoo/mujoco/__init__.py
 dizoo/mujoco/config/__init__.py
 dizoo/mujoco/config/ant_ddpg_config.py
 dizoo/mujoco/config/ant_gail_sac_config.py
 dizoo/mujoco/config/ant_onppo_config.py
 dizoo/mujoco/config/ant_ppo_config.py
```

### Comparing `DI-engine-0.4.6/DI_engine.egg-info/requires.txt` & `DI-engine-0.4.7/DI_engine.egg-info/requires.txt`

 * *Files 15% similar despite different names*

```diff
@@ -22,24 +22,25 @@
 readerwriterlock
 enum_tools
 trueskill
 h5py
 mpire>=2.3.5
 pynng
 redis
-pettingzoo==1.12.0
+pettingzoo
 DI-treetensor>=0.3.0
 DI-toolkit>=0.0.2
 hbutils>=0.5.0
 wandb
 matplotlib
 MarkupSafe==2.0.1
 h5py
 scikit-learn
 hickle
+gymnasium
 
 [bsuite_env]
 bsuite
 
 [common_env]
 ale-py
 autorom
@@ -65,15 +66,15 @@
 [k8s]
 kubernetes
 
 [mario]
 gym-super-mario-bros>=7.3.0
 
 [minigrid_env]
-minigrid
+minigrid>=2.0.0
 
 [procgen_env]
 procgen
 
 [slimevolleygym_env]
 slimevolleygym
```

### Comparing `DI-engine-0.4.6/LICENSE` & `DI-engine-0.4.7/LICENSE`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/PKG-INFO` & `DI-engine-0.4.7/PKG-INFO`

 * *Files 4% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: DI-engine
-Version: 0.4.6
+Version: 0.4.7
 Summary: Decision AI Engine
 Home-page: https://github.com/opendilab/DI-engine
 Author: OpenDILab Contributors
 Author-email: opendilab@pjlab.org.cn
 License: Apache License, Version 2.0
 Keywords: Decision AI Engine
 Classifier: Development Status :: 5 - Production/Stable
@@ -66,70 +66,73 @@
 [![GitHub forks](https://img.shields.io/github/forks/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/network)
 ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/opendilab/DI-engine)
 [![GitHub issues](https://img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/issues)
 [![GitHub pulls](https://img.shields.io/github/issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/graphs/contributors)
 [![GitHub license](https://img.shields.io/github/license/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/blob/master/LICENSE)
 
-Updated on 2023.02.17 DI-engine-v0.4.6
+Updated on 2023.04.11 DI-engine-v0.4.7
 
 
 ## Introduction to DI-engine
-[DI-engine doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/)
+[Documentation](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) | [Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap](https://github.com/opendilab/DI-engine/issues/548)
 
-**DI-engine** is a generalized decision intelligence engine. It supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** ([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/index.html)):
+**DI-engine** is a generalized decision intelligence engine for PyTorch and JAX. 
+
+It provides **python-first** and **asynchronous-native** task and middleware abstractions, and modularly integrates several of the most important decision-making concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with superior performance, high efficiency, well-organized [documentation](https://di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/opendilab/DI-engine/actions):
 
 - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
 - Multi-agent RL algorithms like QMIX, MAPPO, ACE
-- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit Behavioral Cloning
-- Exploration algorithms like HER, RND, ICM, NGU
+- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit BC
 - Offline RL algorithms: CQL, TD3BC, Decision Transformer
 - Model-based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO
+- Exploration algorithms like HER, RND, ICM, NGU
 
-**DI-engine** aims to **standardize different Decision Intelligence enviroments and applications**. Various training pipelines and customized decision AI applications are also supported.
+**DI-engine** aims to **standardize different Decision Intelligence environments and applications**, supporting both academic research and prototype applications. Various training pipelines and customized decision AI applications are also supported:
 
 <details open>
 <summary>(Click to Collapse)</summary>
 
 - Traditional academic environments
-  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility)
+  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility): various decision intelligence demonstrations and benchmark environments with DI-engine.
 - Tutorial courses
   - [PPOxFamily](https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course
 - Real world decision AI applications
   - [DI-star](https://github.com/opendilab/DI-star): Decision AI in StarCraftII
   - [DI-drive](https://github.com/opendilab/DI-drive): Auto-driving platform
   - [GoBigger](https://github.com/opendilab/GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment
   - [DI-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game
   - [DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light Control
   - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in Biological Sequence Prediction and Searching
   - [DI-1024](https://github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game
 - Research paper
   - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer
   - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency
-- General nested data lib
-  - [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
-  - [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - Docs and Tutorials
-  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs)
+  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best practice and the API reference.
   - [awesome-model-based-RL](https://github.com/opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL resources
   - [awesome-exploration-RL](https://github.com/opendilab/awesome-exploration-rl): A curated list of awesome exploration RL resources
   - [awesome-decision-transformer](https://github.com/opendilab/awesome-decision-transformer): A curated list of Decision Transformer resources
+  - [awesome-RLHF](https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement learning with human feedback resources
   - [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal Reinforcement Learning resources
   - [awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-based-protein-design): a collection of research papers for AI-based protein design
   - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of Diffusion Model in RL resources
   - [awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous Driving resources
   - [awesome-driving-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-prediction): A collection of research papers for Driving Behavior Prediction
 </details>
 
+On the low-level end, DI-engine comes with a set of highly re-usable modules, including [RL optimization functions](https://github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities](https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and [auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
 
-**DI-engine** also has some **system optimization and design** for efficient and robust large-scale RL training:
+BTW, **DI-engine** also has some special **system optimization and design** for efficient and robust large-scale RL training:
 
 <details close>
 <summary>(Click for Details)</summary>
 
+- [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - [DI-orchestrator](https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource and Operator Lib
 - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
 - [DI-store](https://github.com/opendilab/DI-store): RL Object Store
 </details>
 
 Have fun with exploration and exploitation.
 
@@ -138,14 +141,15 @@
 - [Introduction to DI-engine](#introduction-to-di-engine)
 - [Outline](#outline)
 - [Installation](#installation)
 - [Quick Start](#quick-start)
 - [Feature](#feature)
   - [&#8627; Algorithm Versatility](#algorithm-versatility)
   - [&#8627; Environment Versatility](#environment-versatility)
+  - [&#8627; General Data Container: TreeTensor](#general-data-container-treetensor)
 - [Feedback and Contribution](#feedback-and-contribution)
 - [Supporters](#supporters)
   - [&#8627; Stargazers](#-stargazers)
   - [&#8627; Forkers](#-forkers)
 - [Citation](#citation)
 - [License](#license)
 
@@ -165,20 +169,24 @@
 
 And our dockerhub repo can be found [here](https://hub.docker.com/repository/docker/opendilab/ding)，we prepare `base image` and `env image` with common RL environments.
 
 <details close>
 <summary>(Click for Details)</summary>
 
 - base: opendilab/ding:nightly
+- rpc: opendilab/ding:nightly-rpc
 - atari: opendilab/ding:nightly-atari
 - mujoco: opendilab/ding:nightly-mujoco
 - dmc: opendilab/ding:nightly-dmc2gym
 - metaworld: opendilab/ding:nightly-metaworld
 - smac: opendilab/ding:nightly-smac
 - grf: opendilab/ding:nightly-grf
+- cityflow: opendilab/ding:nightly-cityflow
+- evogym: opendilab/ding:nightly-evogym
+- d4rl: opendilab/ding:nightly-d4rl
 </details>
 
 The detailed documentation are hosted on [doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/).
 
 ## Quick Start
 
 [3 Minutes Kickoff](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html)
@@ -195,43 +203,43 @@
 
 ```bash
 ding -m serial -e cartpole -p dqn -s 0
 ```
 
 ## Feature
 ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-18)
 
-![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-18)
+<details open>
+<summary>(Click to Collapse)</summary>
+
+![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-23)
 
-![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-18)
+![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-23)
+
+![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-23)
 
 ![dist](https://img.shields.io/badge/-distributed-blue) &nbsp;[Distributed Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)｜[分布式强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html)
 
 ![MARL](https://img.shields.io/badge/-MARL-yellow) &nbsp;[Multi-Agent Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)｜[多智能体强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html)
 
 ![exp](https://img.shields.io/badge/-exploration-orange) &nbsp;[Exploration Mechanisms in Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)｜[强化学习中的探索机制](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html)
 
 ![IL](https://img.shields.io/badge/-IL-purple) &nbsp;[Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)｜[模仿学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html)
 
 ![offline](https://img.shields.io/badge/-offlineRL-darkblue) &nbsp;[Offiline Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)｜[离线强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
 
 
 ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) &nbsp;[Model-Based Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)｜[基于模型的强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html)
 
-![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithm, usually as plugin-in in the whole pipeline
+![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithms, usually as plugin-in in the whole pipeline
 
 P.S: The `.py` file in `Runnable Demo` can be found in `dizoo`
 
 
 
-
-<details open>
-<summary>(Click to Collapse)</summary>
-
 |  No.  |                          Algorithm                           |                            Label                             |                        Doc and Implementation                        |                        Runnable Demo                         |
 | :--: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
 |  1   |         [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)<br>[DQN中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/dqn_zh.html)<br>[policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -c cartpole_dqn_config.py -s 0 |
 |  2   |         [C51](https://arxiv.org/pdf/1707.06887.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/c51.html)<br>[policy/c51](https://github.com/opendilab/DI-engine/blob/main/ding/policy/c51.py) |        ding -m serial -c cartpole_c51_config.py -s 0         |
 |  3   |         [QRDQN](https://arxiv.org/pdf/1710.10044.pdf)        | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [QRDQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qrdqn.html)<br>[policy/qrdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qrdqn.py) |       ding -m serial -c cartpole_qrdqn_config.py -s 0        |
 |  4   |         [IQN](https://arxiv.org/pdf/1806.06923.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [IQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/iqn.html)<br>[policy/iqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/iqn.py) |        ding -m serial -c cartpole_iqn_config.py -s 0         |
 |  5   |         [FQF](https://arxiv.org/pdf/1911.02140.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [FQF doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/fqf.html)<br>[policy/fqf](https://github.com/opendilab/DI-engine/blob/main/ding/policy/fqf.py) |        ding -m serial -c cartpole_fqf_config.py -s 0         |
@@ -247,44 +255,45 @@
 |  15  |         [DDPG](https://arxiv.org/pdf/1509.02971.pdf)/[PADDPG](https://arxiv.org/pdf/1511.04143.pdf)         | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [DDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |        ding -m serial -c pendulum_ddpg_config.py -s 0        |
 |  16  |         [TD3](https://arxiv.org/pdf/1802.09477.pdf)          | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [TD3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3.html)<br>[policy/td3](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3.py) | python3 -u pendulum_td3_main.py / ding -m serial -c pendulum_td3_config.py -s 0 |
 |  17  | [D4PG](https://arxiv.org/pdf/1804.08617.pdf) | ![continuous](https://img.shields.io/badge/-continous-green) | [D4PG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/d4pg.html)<br>[policy/d4pg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/d4pg.py) | python3 -u pendulum_d4pg_config.py |
 |  18  |           [SAC](https://arxiv.org/abs/1801.01290)/[MASAC]            | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![continuous](https://img.shields.io/badge/-continous-green)![MARL](https://img.shields.io/badge/-MARL-yellow) | [SAC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sac.html)<br>[policy/sac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/sac.py) |        ding -m serial -c pendulum_sac_config.py -s 0         |
 |  19  | [PDQN](https://arxiv.org/pdf/1810.06394.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_pdqn_config.py -s 0 |
 |  20  | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_mpdqn_config.py -s 0 |
 |  21  | [HPPO](https://arxiv.org/pdf/1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 |
-|  22  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
-|  23  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
-|  24  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
-|  25  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
-|  26  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
-|  27  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
-|  28  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
-|  29  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
-|  30  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
-|  31  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
-|  32  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
-|  33  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
-|  34  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
-|  35  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
-|  36  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
-|  37  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
-|  38  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
-|  39  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
-|  40  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
-|  41  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
-|  42  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
-|  43  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
-|  44  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
-|  45  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
-|  46  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
-|  47  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
-|  48  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
-|  49  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
-|  50  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
-|  51  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  22  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  23  |         [MDQN](https://arxiv.org/abs/2007.14430)          |   ![discrete](https://img.shields.io/badge/-discrete-brightgreen)    | [policy/mdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mdqn.py) |        python3 -u asterix_mdqn_config.py       |
+|  24  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
+|  25  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
+|  26  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
+|  27  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
+|  28  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
+|  29  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
+|  30  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
+|  31  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
+|  32  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
+|  33  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
+|  34  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
+|  35  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
+|  36  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
+|  37  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
+|  38  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
+|  39  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
+|  40  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
+|  41  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
+|  42  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
+|  43  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
+|  44  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
+|  45  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
+|  46  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
+|  47  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
+|  48  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
+|  49  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
+|  50  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
+|  51  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
+|  52  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
 </details>
 
 
 ### Environment Versatility
 <details open>
 <summary>(Click to Collapse)</summary>
 
@@ -303,32 +312,32 @@
 |  11  |       [overcooked](https://github.com/HumanCompatibleAI/overcooked-demo)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)  | ![original](./dizoo/overcooked/overcooked.gif)       |   [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/overcooded/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/overcooked.html)   |
 |  12  |       [procgen](https://github.com/openai/procgen)                          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   | ![original](./dizoo/procgen/coinrun.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/procgen)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/procgen.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/procgen_zh.html) |
 |  13  |       [pybullet](https://github.com/benelot/pybullet-gym)    | ![continuous](https://img.shields.io/badge/-continous-green)  | ![original](./dizoo/pybullet/pybullet.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pybullet/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/pybullet_zh.html) |
 |  14  |       [smac](https://github.com/oxwhirl/smac)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue)![sparse](https://img.shields.io/badge/-sparse%20reward-orange) | ![original](./dizoo/smac/smac.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/smac/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/smac.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/smac_zh.html) |
 | 15 | [d4rl](https://github.com/rail-berkeley/d4rl) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | ![ori](dizoo/d4rl/d4rl.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/d4rl)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/d4rl_zh.html) |
 |  16  |       league_demo                      | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![original](./dizoo/league_demo/league_demo.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/league_demo/envs)                |
 |  17  |       pomdp atari                    | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   |  | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs) |
-|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) |
+|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bsuite_zh.html) |
 |  19  | [ImageNet](https://www.image-net.org/) | ![IL](https://img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/image_classification)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/image_cls_zh.html) |
 |  20  | [slime_volleyball](https://github.com/hardmaru/slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/slime_volley)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/slime_volleyball.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/slime_volleyball_zh.html) |
 |  21  | [gym_hybrid](https://github.com/thomashirtz/gym-hybrid) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_hybrid/moving_v0.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_hybrid)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_hybrid.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_hybrid_zh.html) |
 |  22  | [GoBigger](https://github.com/opendilab/GoBigger) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](./dizoo/gobigger_overview.gif) | [dizoo link](https://github.com/opendilab/GoBigger-Challenge-2021/tree/main/di_baseline)<br>[env tutorial](https://gobigger.readthedocs.io/en/latest/index.html)<br>[环境指南](https://gobigger.readthedocs.io/zh_CN/latest/) |
 |  23  | [gym_soccer](https://github.com/openai/gym-soccer) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_soccer/half_offensive.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_soccer)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_soccer_zh.html) |
 |  24  |[multiagent_mujoco](https://github.com/schroederdewitt/multiagent_mujoco)       |  ![continuous](https://img.shields.io/badge/-continous-green) ![marl](https://img.shields.io/badge/-MARL-yellow) | ![original](./dizoo/mujoco/mujoco.gif)                    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/multiagent_mujoco/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/mujoco_zh.html) |
 |  25  |bitflip                                | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![sparse](https://img.shields.io/badge/-sparse%20reward-orange)  | ![original](./dizoo/bitflip/bitflip.gif)    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bitflip/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bitflip_zh.html) |
 |  26  |[sokoban](https://github.com/mpSchrader/gym-sokoban) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![Game 2](https://github.com/mpSchrader/gym-sokoban/raw/default/docs/Animations/solved_4.gif?raw=true) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/sokoban/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/sokoban.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/sokoban_zh.html) |
-|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br>环境指南 |
+|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br> [env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/gym_anytrading/envs/README.md) |
 |  28  |[mario](https://github.com/Kautenja/gym-super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/mario) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_super_mario_bros.html) <br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_super_mario_bros_zh.html) |
 |  29  |[dmc2gym](https://github.com/denisyarats/dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/dmc2gym.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/dmc2gym_zh.html) |
-|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br>环境指南 |
+|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/Evogym_zh.html) |
 |  31  |[gym-pybullet-drones](https://github.com/utiasDSL/gym-pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_pybullet_drones/envs)<br>环境指南 |
 |  32  |[beergame](https://github.com/OptMLGroup/DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)<br>环境指南 |
-|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br>环境指南 |
+|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/acrobot_zh.html) |
 |  34  |[box2d/car_racing](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) <br> ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)<br>环境指南 |
-|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br>环境指南 |
+|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/metadrive_zh.html) |
 
 ![discrete](https://img.shields.io/badge/-discrete-brightgreen) means discrete action space
 
 ![continuous](https://img.shields.io/badge/-continous-green) means continuous action space
 
 ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous) action space
 
@@ -342,14 +351,119 @@
 
 ![selfplay](https://img.shields.io/badge/-selfplay-blue) means environment that allows agent VS agent battle
 
 P.S. some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward type
 </details>
 
 
+### General Data Container: TreeTensor
+
+DI-engine utilizes [TreeTensor](https://github.com/opendilab/DI-treetensor) as the basic data container in various components, which is ease of use and consistent across different code modules such as environment definition, data processing and DRL optimization. Here are some concrete code examples:
+
+- TreeTensor can easily extend all the operations of `torch.Tensor` to nested data:
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```python
+    import treetensor.torch as ttorch
+
+
+    # create random tensor
+    data = ttorch.randn({'a': (3, 2), 'b': {'c': (3, )}})
+    # clone+detach tensor
+    data_clone = data.clone().detach()
+    # access tree structure like attribute
+    a = data.a
+    c = data.b.c
+    # stack/cat/split
+    stacked_data = ttorch.stack([data, data_clone], 0)
+    cat_data = ttorch.cat([data, data_clone], 0)
+    data, data_clone = ttorch.split(stacked_data, 1)
+    # reshape
+    data = data.unsqueeze(-1)
+    data = data.squeeze(-1)
+    flatten_data = data.view(-1)
+    # indexing
+    data_0 = data[0]
+    data_1to2 = data[1:2]
+    # execute math calculations
+    data = data.sin()
+    data.b.c.cos_().clamp_(-1, 1)
+    data += data ** 2
+    # backward
+    data.requires_grad_(True)
+    loss = data.arctan().mean()
+    loss.backward()
+    # print shape
+    print(data.shape)
+    # result
+    # <Size 0x7fbd3346ddc0>
+    # ├── 'a' --> torch.Size([1, 3, 2])
+    # └── 'b' --> <Size 0x7fbd3346dd00>
+    #     └── 'c' --> torch.Size([1, 3])
+    ```
+
+  </details>
+
+- TreeTensor can make it simple yet effective to implement classic deep reinforcement learning pipeline
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```diff
+    import torch
+    import treetensor.torch as ttorch
+
+    B = 4
+
+
+    def get_item():
+        return {
+            'obs': {
+                'scalar': torch.randn(12),
+                'image': torch.randn(3, 32, 32),
+            },
+            'action': torch.randint(0, 10, size=(1,)),
+            'reward': torch.rand(1),
+            'done': False,
+        }
+
+
+    data = [get_item() for _ in range(B)]
+
+
+    # execute `stack` op
+    - def stack(data, dim):
+    -     elem = data[0]
+    -     if isinstance(elem, torch.Tensor):
+    -         return torch.stack(data, dim)
+    -     elif isinstance(elem, dict):
+    -         return {k: stack([item[k] for item in data], dim) for k in elem.keys()}
+    -     elif isinstance(elem, bool):
+    -         return torch.BoolTensor(data)
+    -     else:
+    -         raise TypeError("not support elem type: {}".format(type(elem)))
+    - stacked_data = stack(data, dim=0)
+    + data = [ttorch.tensor(d) for d in data]
+    + stacked_data = ttorch.stack(data, dim=0)
+
+    # validate
+    - assert stacked_data['obs']['image'].shape == (B, 3, 32, 32)
+    - assert stacked_data['action'].shape == (B, 1)
+    - assert stacked_data['reward'].shape == (B, 1)
+    - assert stacked_data['done'].shape == (B,)
+    - assert stacked_data['done'].dtype == torch.bool
+    + assert stacked_data.obs.image.shape == (B, 3, 32, 32)
+    + assert stacked_data.action.shape == (B, 1)
+    + assert stacked_data.reward.shape == (B, 1)
+    + assert stacked_data.done.shape == (B,)
+    + assert stacked_data.done.dtype == torch.bool
+    ```
+
+  </details>
+
 ## Feedback and Contribution
 
 - [File an issue](https://github.com/opendilab/DI-engine/issues/new/choose) on Github
 - Open or participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
 - Discuss on DI-engine [slack communication channel](https://join.slack.com/t/opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ)
 - Discuss on DI-engine's QQ group (700157520) or add us on WeChat
```

#### html2text {}

```diff
@@ -1,8 +1,8 @@
-Metadata-Version: 2.1 Name: DI-engine Version: 0.4.6 Summary: Decision AI
+Metadata-Version: 2.1 Name: DI-engine Version: 0.4.7 Summary: Decision AI
 Engine Home-page: https://github.com/opendilab/DI-engine Author: OpenDILab
 Contributors Author-email: opendilab@pjlab.org.cn License: Apache License,
 Version 2.0 Keywords: Decision AI Engine Classifier: Development Status :: 5 -
 Production/Stable Classifier: Intended Audience :: Science/Research Classifier:
 License :: OSI Approved :: Apache Software License Classifier: Operating System
 :: POSIX :: Linux Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS :: MacOS X Classifier: Programming
@@ -48,145 +48,166 @@
 commit-activity/m/opendilab/DI-engine) [![GitHub issues](https://
 img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/
 opendilab/DI-engine/issues) [![GitHub pulls](https://img.shields.io/github/
 issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-
 engine)](https://github.com/opendilab/DI-engine/graphs/contributors) [![GitHub
 license](https://img.shields.io/github/license/opendilab/DI-engine)](https://
-github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.02.17 DI-
-engine-v0.4.6 ## Introduction to DI-engine [DI-engine doc](https://di-engine-
+github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.04.11 DI-
+engine-v0.4.7 ## Introduction to DI-engine [Documentation](https://di-engine-
 docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/) **DI-engine** is a generalized decision
-intelligence engine. It supports **various [deep reinforcement learning](https:
-//di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms**
-([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
-index.html)): - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
-- Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation learning
-algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit
-Behavioral Cloning - Exploration algorithms like HER, RND, ICM, NGU - Offline
-RL algorithms: CQL, TD3BC, Decision Transformer - Model-based RL algorithms:
-SVG, MVE, STEVE / MBPO, DDPPO **DI-engine** aims to **standardize different
-Decision Intelligence enviroments and applications**. Various training
-pipelines and customized decision AI applications are also supported.  (Click
-to Collapse) - Traditional academic environments - [DI-zoo](https://github.com/
-opendilab/DI-engine#environment-versatility) - Tutorial courses - [PPOxFamily]
-(https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course -
-Real world decision AI applications - [DI-star](https://github.com/opendilab/
-DI-star): Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/
-DI-drive): Auto-driving platform - [GoBigger](https://github.com/opendilab/
-GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-
-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game -
-[DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in
-Traffic Light Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq):
-Decision AI in Biological Sequence Prediction and Searching - [DI-1024](https:/
-/github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game -
-Research paper - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL
-2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion
-Transformer - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE:
-Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency -
-General nested data lib - [treevalue](https://github.com/opendilab/treevalue):
-Tree-nested data structure - [DI-treetensor](https://github.com/opendilab/DI-
-treetensor): Tree-nested PyTorch tensor Lib - Docs and Tutorials - [DI-engine-
-docs](https://github.com/opendilab/DI-engine-docs) - [awesome-model-based-RL]
-(https://github.com/opendilab/awesome-model-based-RL): A curated list of
-awesome Model-Based RL resources - [awesome-exploration-RL](https://github.com/
-opendilab/awesome-exploration-rl): A curated list of awesome exploration RL
-resources - [awesome-decision-transformer](https://github.com/opendilab/
-awesome-decision-transformer): A curated list of Decision Transformer resources
-- [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/
-awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal
-Reinforcement Learning resources - [awesome-AI-based-protein-design](https://
-github.com/opendilab/awesome-AI-based-protein-design): a collection of research
-papers for AI-based protein design - [awesome-diffusion-model-in-rl](https://
-github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of
-Diffusion Model in RL resources - [awesome-end-to-end-autonomous-driving]
-(https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated
-list of awesome End-to-End Autonomous Driving resources - [awesome-driving-
-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-
-prediction): A collection of research papers for Driving Behavior Prediction
-**DI-engine** also has some **system optimization and design** for efficient
-and robust large-scale RL training:  (Click for Details) - [DI-orchestrator]
-(https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource
-and Operator Lib - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
-- [DI-store](https://github.com/opendilab/DI-store): RL Object Store  Have fun
-with exploration and exploitation. ## Outline - [Introduction to DI-engine]
-(#introduction-to-di-engine) - [Outline](#outline) - [Installation]
-(#installation) - [Quick Start](#quick-start) - [Feature](#feature) - [↳
-Algorithm Versatility](#algorithm-versatility) - [↳ Environment Versatility]
-(#environment-versatility) - [Feedback and Contribution](#feedback-and-
-contribution) - [Supporters](#supporters) - [↳ Stargazers](#-stargazers) - [↳
-Forkers](#-forkers) - [Citation](#citation) - [License](#license) ##
-Installation You can simply install DI-engine from PyPI with the following
-command: ```bash pip install DI-engine ``` If you use Anaconda or Miniconda,
-you can install DI-engine from conda-forge through the following command:
-```bash conda install -c opendilab di-engine ``` For more information about
-installation, you can refer to [installation](https://di-engine-
-docs.readthedocs.io/en/latest/01_quickstart/installation.html). And our
-dockerhub repo can be found [here](https://hub.docker.com/repository/docker/
-opendilab/ding)ï¼we prepare `base image` and `env image` with common RL
-environments.  (Click for Details) - base: opendilab/ding:nightly - atari:
-opendilab/ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc:
-opendilab/ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld -
-smac: opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf  The
-detailed documentation are hosted on [doc](https://di-engine-
-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff](https://
-di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html) [3
-Minutes Kickoff (colab)](https://colab.research.google.com/drive/
-1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing) [How to migrate a new **RL
-Env**](https://di-engine-docs.readthedocs.io/en/latest/11_dizoo/index.html) |
-[å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html) [How to customize the
-neural network model](https://di-engine-docs.readthedocs.io/en/latest/
-04_best_practice/custom_model.html) |
+docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-
+docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) |
+[Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/
+index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap]
+(https://github.com/opendilab/DI-engine/issues/548) **DI-engine** is a
+generalized decision intelligence engine for PyTorch and JAX. It provides
+**python-first** and **asynchronous-native** task and middleware abstractions,
+and modularly integrates several of the most important decision-making
+concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine
+supports **various [deep reinforcement learning](https://di-engine-
+docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with
+superior performance, high efficiency, well-organized [documentation](https://
+di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/
+opendilab/DI-engine/actions): - Most basic DRL algorithms, such as DQN, PPO,
+SAC, R2D2, IMPALA - Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation
+learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning,
+Implicit BC - Offline RL algorithms: CQL, TD3BC, Decision Transformer - Model-
+based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO - Exploration algorithms
+like HER, RND, ICM, NGU **DI-engine** aims to **standardize different Decision
+Intelligence environments and applications**, supporting both academic research
+and prototype applications. Various training pipelines and customized decision
+AI applications are also supported:  (Click to Collapse) - Traditional academic
+environments - [DI-zoo](https://github.com/opendilab/DI-engine#environment-
+versatility): various decision intelligence demonstrations and benchmark
+environments with DI-engine. - Tutorial courses - [PPOxFamily](https://
+github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course - Real world
+decision AI applications - [DI-star](https://github.com/opendilab/DI-star):
+Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/DI-drive):
+Auto-driving platform - [GoBigger](https://github.com/opendilab/GoBigger):
+[ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-sheep](https://
+github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game - [DI-smartcross]
+(https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light
+Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in
+Biological Sequence Prediction and Searching - [DI-1024](https://github.com/
+opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game - Research paper -
+[InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-
+Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer -
+[ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-
+agent Q-learning with Bidirectional Action-Dependency - Docs and Tutorials -
+[DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best
+practice and the API reference. - [awesome-model-based-RL](https://github.com/
+opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL
+resources - [awesome-exploration-RL](https://github.com/opendilab/awesome-
+exploration-rl): A curated list of awesome exploration RL resources - [awesome-
+decision-transformer](https://github.com/opendilab/awesome-decision-
+transformer): A curated list of Decision Transformer resources - [awesome-RLHF]
+(https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement
+learning with human feedback resources - [awesome-multi-modal-reinforcement-
+learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-
+learning): A curated list of Multi-Modal Reinforcement Learning resources -
+[awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-
+based-protein-design): a collection of research papers for AI-based protein
+design - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-
+diffusion-model-in-rl): A curated list of Diffusion Model in RL resources -
+[awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-
+end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous
+Driving resources - [awesome-driving-behavior-prediction](https://github.com/
+opendilab/awesome-driving-behavior-prediction): A collection of research papers
+for Driving Behavior Prediction  On the low-level end, DI-engine comes with a
+set of highly re-usable modules, including [RL optimization functions](https://
+github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities]
+(https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and
+[auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
+BTW, **DI-engine** also has some special **system optimization and design** for
+efficient and robust large-scale RL training:  (Click for Details) -
+[treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested
+PyTorch tensor Lib - [DI-orchestrator](https://github.com/opendilab/DI-
+orchestrator): RL Kubernetes Custom Resource and Operator Lib - [DI-hpc](https:
+//github.com/opendilab/DI-hpc): RL HPC OP Lib - [DI-store](https://github.com/
+opendilab/DI-store): RL Object Store  Have fun with exploration and
+exploitation. ## Outline - [Introduction to DI-engine](#introduction-to-di-
+engine) - [Outline](#outline) - [Installation](#installation) - [Quick Start]
+(#quick-start) - [Feature](#feature) - [↳ Algorithm Versatility](#algorithm-
+versatility) - [↳ Environment Versatility](#environment-versatility) - [↳
+General Data Container: TreeTensor](#general-data-container-treetensor) -
+[Feedback and Contribution](#feedback-and-contribution) - [Supporters]
+(#supporters) - [↳ Stargazers](#-stargazers) - [↳ Forkers](#-forkers) -
+[Citation](#citation) - [License](#license) ## Installation You can simply
+install DI-engine from PyPI with the following command: ```bash pip install DI-
+engine ``` If you use Anaconda or Miniconda, you can install DI-engine from
+conda-forge through the following command: ```bash conda install -c opendilab
+di-engine ``` For more information about installation, you can refer to
+[installation](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+installation.html). And our dockerhub repo can be found [here](https://
+hub.docker.com/repository/docker/opendilab/ding)ï¼we prepare `base image` and
+`env image` with common RL environments.  (Click for Details) - base:
+opendilab/ding:nightly - rpc: opendilab/ding:nightly-rpc - atari: opendilab/
+ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc: opendilab/
+ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld - smac:
+opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf - cityflow:
+opendilab/ding:nightly-cityflow - evogym: opendilab/ding:nightly-evogym - d4rl:
+opendilab/ding:nightly-d4rl  The detailed documentation are hosted on [doc]
+(https://di-engine-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff]
+(https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+first_rl_program.html) [3 Minutes Kickoff (colab)](https://
+colab.research.google.com/drive/1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing)
+[How to migrate a new **RL Env**](https://di-engine-docs.readthedocs.io/en/
+latest/11_dizoo/index.html) | [å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**]
+(https://di-engine-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html)
+[How to customize the neural network model](https://di-engine-
+docs.readthedocs.io/en/latest/04_best_practice/custom_model.html) |
 [å¦ä½å®å¶ç­ç¥ä½¿ç¨ç**ç¥ç»ç½ç»æ¨¡å**](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/04_best_practice/custom_model_zh.html)
 [æµè¯/é¨ç½² **å¼ºåå­¦ä¹ ç­ç¥** çæ ·ä¾](https://github.com/opendilab/
 DI-engine/blob/main/dizoo/classic_control/cartpole/entry/
 cartpole_c51_deploy.py) **Bonus: Train RL agent in one line code:** ```bash
 ding -m serial -e cartpole -p dqn -s 0 ``` ## Feature ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen)  discrete means
-discrete action space, which is only label in normal DRL algorithms (1-18) !
-[continuous](https://img.shields.io/badge/-continous-green)  means continuous
-action space, which is only label in normal DRL algorithms (1-18) ![hybrid]
-(https://img.shields.io/badge/-hybrid-darkgreen)  means hybrid (discrete +
-continuous) action space (1-18) ![dist](https://img.shields.io/badge/-
-distributed-blue)  [Distributed Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)ï½
-[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/distributed_rl_zh.html) ![MARL](https://img.shields.io/badge/-MARL-
-yellow)  [Multi-Agent Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)ï½
-[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/
-badge/-exploration-orange)  [Exploration Mechanisms in Reinforcement Learning]
+(Click to Collapse) ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen)  discrete means discrete action space, which is only label in
+normal DRL algorithms (1-23) ![continuous](https://img.shields.io/badge/-
+continous-green)  means continuous action space, which is only label in normal
+DRL algorithms (1-23) ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)
+ means hybrid (discrete + continuous) action space (1-23) ![dist](https://
+img.shields.io/badge/-distributed-blue)  [Distributed Reinforcement Learning]
+(https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+distributed_rl.html)ï½[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html) ![MARL](https:
+//img.shields.io/badge/-MARL-yellow)  [Multi-Agent Reinforcement Learning]
 (https://di-engine-docs.readthedocs.io/en/latest/02_algo/
-exploration_rl.html)ï½[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html) ![IL](https://
-img.shields.io/badge/-IL-purple)  [Imitation Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/
-imitation_learning_zh.html) ![offline](https://img.shields.io/badge/-offlineRL-
-darkblue)  [Offiline Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
-![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue)  [Model-Based
-Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/
-02_algo/model_based_rl.html)ï½[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html) ![other]
-(https://img.shields.io/badge/-other-lightgrey)  means other sub-direction
-algorithm, usually as plugin-in in the whole pipeline P.S: The `.py` file in
-`Runnable Demo` can be found in `dizoo`  (Click to Collapse) | No. | Algorithm
-| Label | Doc and Implementation | Runnable Demo | | :--: | :------------------
-----------------------------------------: | :----------------------------------
-------------------------: | :--------------------------------------------------
---------: | :----------------------------------------------------------: | | 1
-| [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) |
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc]
-(https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)
+multi_agent_cooperation_rl.html)ï½[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/02_algo/
+multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/badge/-
+exploration-orange)  [Exploration Mechanisms in Reinforcement Learning](https:/
+/di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)ï½
+[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/exploration_rl_zh.html) ![IL](https://img.shields.io/badge/-IL-
+purple)  [Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/
+02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html) ![offline]
+(https://img.shields.io/badge/-offlineRL-darkblue)  [Offiline Reinforcement
+Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/
+zh_CN/latest/02_algo/offline_rl_zh.html) ![mbrl](https://img.shields.io/badge/-
+ModelBasedRL-lightblue)  [Model-Based Reinforcement Learning](https://di-
+engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)ï½
+[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/model_based_rl_zh.html) ![other](https://img.shields.io/badge/-
+other-lightgrey)  means other sub-direction algorithms, usually as plugin-in in
+the whole pipeline P.S: The `.py` file in `Runnable Demo` can be found in
+`dizoo` | No. | Algorithm | Label | Doc and Implementation | Runnable Demo | |
+:--: | :----------------------------------------------------------: | :--------
+--------------------------------------------------: | :------------------------
+----------------------------------: | :----------------------------------------
+------------------: | | 1 | [DQN](https://storage.googleapis.com/deepmind-
+media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/
+latest/12_policies/dqn.html)
 [DQNä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/dqn_zh.html)
 [policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -
 c cartpole_dqn_config.py -s 0 | | 2 | [C51](https://arxiv.org/pdf/
 1707.06887.pdf) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/
@@ -289,163 +310,166 @@
 | 20 | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -
 c gym_hybrid_mpdqn_config.py -s 0 | | 21 | [HPPO](https://arxiv.org/pdf/
 1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) |
 [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 | | 22 |
+[BDQ](https://arxiv.org/pdf/1711.08946.pdf) | ![hybrid](https://img.shields.io/
+badge/-hybrid-darkgreen) | [policy/bdq](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/dqn.py) | python3 -u hopper_bdq_config.py | | 23 | [MDQN]
+(https://arxiv.org/abs/2007.14430) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [policy/mdqn](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/mdqn.py) | python3 -u asterix_mdqn_config.py | | 24 |
 [QMIX](https://arxiv.org/pdf/1803.11485.pdf) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [QMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/qmix.html)
 [policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 23 | [COMA]
+qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 25 | [COMA]
 (https://arxiv.org/pdf/1705.08926.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/coma.html)
 [policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 24 | [QTran]
+coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 26 | [QTran]
 (https://arxiv.org/abs/1905.05408) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/
-ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 25
+ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 27
 | [WQMIX](https://arxiv.org/abs/2006.10800) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/wqmix.html)
 [policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 26 | [CollaQ]
+wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 28 | [CollaQ]
 (https://arxiv.org/pdf/2010.08531.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/collaq.html)
 [policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 27 |
+collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 29 |
 [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) | ![MARL](https://
 img.shields.io/badge/-MARL-yellow) | [MADDPG doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/ddpg.html)
 [policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 28 | [GAIL](https://
+ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 30 | [GAIL](https://
 arxiv.org/pdf/1606.03476.pdf) | ![IL](https://img.shields.io/badge/-IL-purple)
 | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 gail.html)
 [reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/gail_irl_model.py) | ding -m serial_gail -
-c cartpole_dqn_gail_config.py -s 0 | | 29 | [SQIL](https://arxiv.org/pdf/
+c cartpole_dqn_gail_config.py -s 0 | | 31 | [SQIL](https://arxiv.org/pdf/
 1905.11108.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [SQIL doc]
 (https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)
 [entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/
 serial_entry_sqil.py) | ding -m serial_sqil -c cartpole_sqil_config.py -s 0 | |
-30 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
+32 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
 img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/dqfd.html)
 [policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 31 | [R2D3]
+dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 33 | [R2D3]
 (https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-
 IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/r2d3.html)
 [R2D3ä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/r2d3_zh.html)
 [policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/
-r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 32 | [Guided Cost
+r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 34 | [Guided Cost
 Learning](https://arxiv.org/pdf/1603.00448.pdf) | ![IL](https://img.shields.io/
 badge/-IL-purple) | [Guided Cost Learningä¸­æææ¡£](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)
 [reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/
 ding/reward_model/guided_cost_reward_model.py) | python3
-lunarlander_gcl_config.py | | 33 | [TREX](https://arxiv.org/abs/1904.06387) | !
+lunarlander_gcl_config.py | | 35 | [TREX](https://arxiv.org/abs/1904.06387) | !
 [IL](https://img.shields.io/badge/-IL-purple) | [TREX doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/trex.html)
 [reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 34 |
+reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 36 |
 [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC) | ![IL]
 (https://img.shields.io/badge/-IL-purple) | [policy/ibc](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/ibc.py)
 [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/
 model/template/ebm.py) | python3 d4rl_ibc_main.py -s 0 -
-c pen_human_ibc_mcmc_config.py | | 35 | [BCO](https://arxiv.org/pdf/
+c pen_human_ibc_mcmc_config.py | | 37 | [BCO](https://arxiv.org/pdf/
 1805.01954.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco]
 (https://github.com/opendilab/DI-engine/blob/main/ding/entry/
-serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 36 | [HER](https:/
+serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 38 | [HER](https:/
 /arxiv.org/pdf/1707.01495.pdf) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [HER doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/her.html)
 [reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 37 |
+reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 39 |
 [RND](https://arxiv.org/abs/1810.12894) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [RND doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/rnd.html)
 [reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/rnd_reward_model.py) | python3 -u cartpole_rnd_onppo_config.py | |
-38 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
+40 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
 img.shields.io/badge/-exploration-orange) | [ICM doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/icm.html)
 [ICMä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/icm_zh.html)
 [reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/icm_reward_model.py) | python3 -u cartpole_ppo_icm_config.py | |
-39 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
+41 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
 img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/cql.html)
 [policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-cql.py) | python3 -u d4rl_cql_main.py | | 40 | [TD3BC](https://arxiv.org/pdf/
+cql.py) | python3 -u d4rl_cql_main.py | | 42 | [TD3BC](https://arxiv.org/pdf/
 2106.06860.pdf) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue)
 | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 td3_bc.html)
 [policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 41 | [Decision Transformer]
+td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 43 | [Decision Transformer]
 (https://arxiv.org/pdf/2106.01345.pdf) | ![offline](https://img.shields.io/
 badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-
 engine/blob/main/ding/policy/decision_transformer.py) | python3 -
-u d4rl_dt_main.py | | 42 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
+u d4rl_dt_main.py | | 44 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
 (https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142)) | !
 [continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://
 img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https:/
 /github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |
 python3 -u pendulum_mbsac_mbpo_config.py \ python3 -
-u pendulum_mbsac_ddppo_config.py | | 43 | STEVESAC([SAC](https://arxiv.org/abs/
+u pendulum_mbsac_ddppo_config.py | | 45 | STEVESAC([SAC](https://arxiv.org/abs/
 1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/
 abs/1510.09142)) | ![continuous](https://img.shields.io/badge/-continous-
 green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/
 mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 44 |
+mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 46 |
 [MBPO](https://arxiv.org/pdf/1906.08253.pdf) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/mbpo.html)
 [world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/
-world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 45 | [DDPPO]
+world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 47 | [DDPPO]
 (https://openreview.net/forum?id=rzvOQrnclO0) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/
 opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) | python3 -
-u pendulum_mbsac_ddppo_config.py | | 46 | [PER](https://arxiv.org/pdf/
+u pendulum_mbsac_ddppo_config.py | | 48 | [PER](https://arxiv.org/pdf/
 1511.05952.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/
-worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 47 | [GAE](https:
+worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 49 | [GAE](https:
 //arxiv.org/pdf/1506.02438.pdf) | ![other](https://img.shields.io/badge/-other-
 lightgrey) | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/
-ding/rl_utils/gae.py) | `ppo demo` | | 48 | [ST-DIM](https://arxiv.org/pdf/
+ding/rl_utils/gae.py) | `ppo demo` | | 50 | [ST-DIM](https://arxiv.org/pdf/
 1906.08226.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/
 blob/main/ding/torch_utils/loss/contrastive_loss.py) | ding -m serial -
-c cartpole_dqn_stdim_config.py -s 0 | | 49 | [PLR](https://arxiv.org/pdf/
+c cartpole_dqn_stdim_config.py -s 0 | | 51 | [PLR](https://arxiv.org/pdf/
 2010.03934.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)
 [data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/
 main/ding/data/level_replay/level_sampler.py) | python3 -
-u bigfish_plr_config.py -s 0 | | 50 | [PCGrad](https://arxiv.org/pdf/
+u bigfish_plr_config.py -s 0 | | 52 | [PCGrad](https://arxiv.org/pdf/
 2001.06782.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/
 blob/main/ding/data/torch_utils/optimizer_helper.py) | python3 -
-u multi_mnist_pcgrad_main.py -s 0 | | 51 | [BDQ](https://arxiv.org/pdf/
-1711.08946.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
-[policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqn.py) | python3 -u hopper_bdq_config.py |  ### Environment Versatility
-(Click to Collapse) | No | Environment | Label | Visualization | Code and Doc
-Links | | :--: | :--------------------------------------: | :------------------
----------------: | :--------------------------------:|:------------------------
----------------------------------: | | 1 | [Atari](https://github.com/openai/
-gym/tree/master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-
-discrete-brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link]
-(https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
+u multi_mnist_pcgrad_main.py -s 0 |  ### Environment Versatility  (Click to
+Collapse) | No | Environment | Label | Visualization | Code and Doc Links | | :
+--: | :--------------------------------------: | :-----------------------------
+----: | :--------------------------------:|:-----------------------------------
+----------------------: | | 1 | [Atari](https://github.com/openai/gym/tree/
+master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 atari.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 atari_zh.html) | | 2 | [box2d/bipedalwalker](https://github.com/openai/gym/
 tree/master/gym/envs/box2d) | ![continuous](https://img.shields.io/badge/-
 continous-green) | ![original](./dizoo/box2d/bipedalwalker/original.gif) |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/
@@ -555,18 +579,20 @@
 atari | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs)
 | | 18 | [bsuite](https://github.com/deepmind/bsuite) | ![discrete](https://
 img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/
 bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/bsuite/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//
-bsuite.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https://
-img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/
-imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
-dizoo/image_classification)
+bsuite.html)
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+bsuite_zh.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https:/
+/img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/
+image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/
+DI-engine/tree/main/dizoo/image_classification)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 image_cls_zh.html) | | 20 | [slime_volleyball](https://github.com/hardmaru/
 slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori]
 (dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/slime_volley)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
@@ -610,18 +636,19 @@
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 sokoban.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 sokoban_zh.html) | | 27 |[gym_anytrading](https://github.com/AminHP/gym-
 anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) |
 ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading)
-ç¯å¢æå | | 28 |[mario](https://github.com/Kautenja/gym-super-mario-bros)
-| ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original]
-(./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-
-engine/tree/main/dizoo/mario)
+[env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/
+gym_anytrading/envs/README.md) | | 28 |[mario](https://github.com/Kautenja/gym-
+super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/mario)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 gym_super_mario_bros.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 gym_super_mario_bros_zh.html) | | 29 |[dmc2gym](https://github.com/denisyarats/
 dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | !
 [original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)
@@ -630,57 +657,96 @@
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 dmc2gym_zh.html) | | 30 |[evogym](https://github.com/EvolutionGym/evogym) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/
 tree/main/dizoo/evogym/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 evogym.html)
-ç¯å¢æå | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+Evogym_zh.html) | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
 pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green)
 | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo
 link](https://github.com/opendilab/DI-engine/tree/main/dizoo/
 gym_pybullet_drones/envs)
 ç¯å¢æå | | 32 |[beergame](https://github.com/OptMLGroup/
 DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https:
 //github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)
 ç¯å¢æå | | 33 |[classic_control/acrobot](https://github.com/openai/gym/
 tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/
 acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/classic_control/acrobot/envs)
-ç¯å¢æå | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+acrobot_zh.html) | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
 master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen)
 ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)
 ç¯å¢æå | | 35 |[metadrive](https://github.com/metadriverse/metadrive) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/
 DI-engine/tree/main/dizoo/metadrive/env)
-ç¯å¢æå | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)
-means discrete action space ![continuous](https://img.shields.io/badge/-
-continous-green) means continuous action space ![hybrid](https://
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+metadrive_zh.html) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) means discrete action space ![continuous](https://img.shields.io/
+badge/-continous-green) means continuous action space ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous)
 action space ![MARL](https://img.shields.io/badge/-MARL-yellow) means multi-
 agent RL environment ![sparse](https://img.shields.io/badge/-sparse%20reward-
 orange) means environment which is related to exploration and sparse reward !
 [offline](https://img.shields.io/badge/-offlineRL-darkblue) means offline RL
 environment ![IL](https://img.shields.io/badge/-IL/SL-purple) means Imitation
 Learning or Supervised Learning Dataset ![selfplay](https://img.shields.io/
 badge/-selfplay-blue) means environment that allows agent VS agent battle P.S.
 some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward
-type  ## Feedback and Contribution - [File an issue](https://github.com/
-opendilab/DI-engine/issues/new/choose) on Github - Open or participate in our
-[forum](https://github.com/opendilab/DI-engine/discussions) - Discuss on DI-
-engine [slack communication channel](https://join.slack.com/t/opendilab/
-shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-engine's QQ
-group (700157520) or add us on WeChat ![WeChat](https://github.com/opendilab/
-DI-engine/blob/main/assets/wechat.png) - Contact our email
+type  ### General Data Container: TreeTensor DI-engine utilizes [TreeTensor]
+(https://github.com/opendilab/DI-treetensor) as the basic data container in
+various components, which is ease of use and consistent across different code
+modules such as environment definition, data processing and DRL optimization.
+Here are some concrete code examples: - TreeTensor can easily extend all the
+operations of `torch.Tensor` to nested data:  (Click for Details) ```python
+import treetensor.torch as ttorch # create random tensor data = ttorch.randn(
+{'a': (3, 2), 'b': {'c': (3, )}}) # clone+detach tensor data_clone = data.clone
+().detach() # access tree structure like attribute a = data.a c = data.b.c #
+stack/cat/split stacked_data = ttorch.stack([data, data_clone], 0) cat_data =
+ttorch.cat([data, data_clone], 0) data, data_clone = ttorch.split(stacked_data,
+1) # reshape data = data.unsqueeze(-1) data = data.squeeze(-1) flatten_data =
+data.view(-1) # indexing data_0 = data[0] data_1to2 = data[1:2] # execute math
+calculations data = data.sin() data.b.c.cos_().clamp_(-1, 1) data += data ** 2
+# backward data.requires_grad_(True) loss = data.arctan().mean() loss.backward
+() # print shape print(data.shape) # result #
+x7fbd3346ddc0> # âââ 'a' --> torch.Size([1, 3, 2]) # âââ 'b' -->
+x7fbd3346dd00> # âââ 'c' --> torch.Size([1, 3]) ```  - TreeTensor can
+make it simple yet effective to implement classic deep reinforcement learning
+pipeline  (Click for Details) ```diff import torch import treetensor.torch as
+ttorch B = 4 def get_item(): return { 'obs': { 'scalar': torch.randn(12),
+'image': torch.randn(3, 32, 32), }, 'action': torch.randint(0, 10, size=(1,)),
+'reward': torch.rand(1), 'done': False, } data = [get_item() for _ in range(B)]
+# execute `stack` op - def stack(data, dim): - elem = data[0] - if isinstance
+(elem, torch.Tensor): - return torch.stack(data, dim) - elif isinstance(elem,
+dict): - return {k: stack([item[k] for item in data], dim) for k in elem.keys
+()} - elif isinstance(elem, bool): - return torch.BoolTensor(data) - else: -
+raise TypeError("not support elem type: {}".format(type(elem))) - stacked_data
+= stack(data, dim=0) + data = [ttorch.tensor(d) for d in data] + stacked_data =
+ttorch.stack(data, dim=0) # validate - assert stacked_data['obs']
+['image'].shape == (B, 3, 32, 32) - assert stacked_data['action'].shape == (B,
+1) - assert stacked_data['reward'].shape == (B, 1) - assert stacked_data
+['done'].shape == (B,) - assert stacked_data['done'].dtype == torch.bool +
+assert stacked_data.obs.image.shape == (B, 3, 32, 32) + assert
+stacked_data.action.shape == (B, 1) + assert stacked_data.reward.shape == (B,
+1) + assert stacked_data.done.shape == (B,) + assert stacked_data.done.dtype ==
+torch.bool ```  ## Feedback and Contribution - [File an issue](https://
+github.com/opendilab/DI-engine/issues/new/choose) on Github - Open or
+participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
+- Discuss on DI-engine [slack communication channel](https://join.slack.com/t/
+opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-
+engine's QQ group (700157520) or add us on WeChat ![WeChat](https://github.com/
+opendilab/DI-engine/blob/main/assets/wechat.png) - Contact our email
 (opendilab@pjlab.org.cn) - Contributes to our future plan [Roadmap](https://
 github.com/opendilab/DI-engine/issues/548) We appreciate all the feedbacks and
 contributions to improve DI-engine, both algorithms and system designs. And
 `CONTRIBUTING.md` offers some necessary information. ## Supporters ### ↳
 Stargazers [![Stargazers repo roster for @opendilab/DI-engine](https://
 reporoster.com/stars/opendilab/DI-engine)](https://github.com/opendilab/DI-
 engine/stargazers) ### ↳ Forkers [![Forkers repo roster for @opendilab/DI-
```

### Comparing `DI-engine-0.4.6/README.md` & `DI-engine-0.4.7/README.md`

 * *Files 9% similar despite different names*

```diff
@@ -28,70 +28,73 @@
 [![GitHub forks](https://img.shields.io/github/forks/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/network)
 ![GitHub commit activity](https://img.shields.io/github/commit-activity/m/opendilab/DI-engine)
 [![GitHub issues](https://img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/issues)
 [![GitHub pulls](https://img.shields.io/github/issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/graphs/contributors)
 [![GitHub license](https://img.shields.io/github/license/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/blob/master/LICENSE)
 
-Updated on 2023.02.17 DI-engine-v0.4.6
+Updated on 2023.04.11 DI-engine-v0.4.7
 
 
 ## Introduction to DI-engine
-[DI-engine doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/)
+[Documentation](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) | [Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap](https://github.com/opendilab/DI-engine/issues/548)
 
-**DI-engine** is a generalized decision intelligence engine. It supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** ([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/index.html)):
+**DI-engine** is a generalized decision intelligence engine for PyTorch and JAX. 
+
+It provides **python-first** and **asynchronous-native** task and middleware abstractions, and modularly integrates several of the most important decision-making concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine supports **various [deep reinforcement learning](https://di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with superior performance, high efficiency, well-organized [documentation](https://di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/opendilab/DI-engine/actions):
 
 - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
 - Multi-agent RL algorithms like QMIX, MAPPO, ACE
-- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit Behavioral Cloning
-- Exploration algorithms like HER, RND, ICM, NGU
+- Imitation learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit BC
 - Offline RL algorithms: CQL, TD3BC, Decision Transformer
 - Model-based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO
+- Exploration algorithms like HER, RND, ICM, NGU
 
-**DI-engine** aims to **standardize different Decision Intelligence enviroments and applications**. Various training pipelines and customized decision AI applications are also supported.
+**DI-engine** aims to **standardize different Decision Intelligence environments and applications**, supporting both academic research and prototype applications. Various training pipelines and customized decision AI applications are also supported:
 
 <details open>
 <summary>(Click to Collapse)</summary>
 
 - Traditional academic environments
-  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility)
+  - [DI-zoo](https://github.com/opendilab/DI-engine#environment-versatility): various decision intelligence demonstrations and benchmark environments with DI-engine.
 - Tutorial courses
   - [PPOxFamily](https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course
 - Real world decision AI applications
   - [DI-star](https://github.com/opendilab/DI-star): Decision AI in StarCraftII
   - [DI-drive](https://github.com/opendilab/DI-drive): Auto-driving platform
   - [GoBigger](https://github.com/opendilab/GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment
   - [DI-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game
   - [DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light Control
   - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in Biological Sequence Prediction and Searching
   - [DI-1024](https://github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game
 - Research paper
   - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer
   - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency
-- General nested data lib
-  - [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
-  - [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - Docs and Tutorials
-  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs)
+  - [DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best practice and the API reference.
   - [awesome-model-based-RL](https://github.com/opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL resources
   - [awesome-exploration-RL](https://github.com/opendilab/awesome-exploration-rl): A curated list of awesome exploration RL resources
   - [awesome-decision-transformer](https://github.com/opendilab/awesome-decision-transformer): A curated list of Decision Transformer resources
+  - [awesome-RLHF](https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement learning with human feedback resources
   - [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal Reinforcement Learning resources
   - [awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-based-protein-design): a collection of research papers for AI-based protein design
   - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of Diffusion Model in RL resources
   - [awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous Driving resources
   - [awesome-driving-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-prediction): A collection of research papers for Driving Behavior Prediction
 </details>
 
+On the low-level end, DI-engine comes with a set of highly re-usable modules, including [RL optimization functions](https://github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities](https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and [auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
 
-**DI-engine** also has some **system optimization and design** for efficient and robust large-scale RL training:
+BTW, **DI-engine** also has some special **system optimization and design** for efficient and robust large-scale RL training:
 
 <details close>
 <summary>(Click for Details)</summary>
 
+- [treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested PyTorch tensor Lib
 - [DI-orchestrator](https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource and Operator Lib
 - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
 - [DI-store](https://github.com/opendilab/DI-store): RL Object Store
 </details>
 
 Have fun with exploration and exploitation.
 
@@ -100,14 +103,15 @@
 - [Introduction to DI-engine](#introduction-to-di-engine)
 - [Outline](#outline)
 - [Installation](#installation)
 - [Quick Start](#quick-start)
 - [Feature](#feature)
   - [&#8627; Algorithm Versatility](#algorithm-versatility)
   - [&#8627; Environment Versatility](#environment-versatility)
+  - [&#8627; General Data Container: TreeTensor](#general-data-container-treetensor)
 - [Feedback and Contribution](#feedback-and-contribution)
 - [Supporters](#supporters)
   - [&#8627; Stargazers](#-stargazers)
   - [&#8627; Forkers](#-forkers)
 - [Citation](#citation)
 - [License](#license)
 
@@ -127,20 +131,24 @@
 
 And our dockerhub repo can be found [here](https://hub.docker.com/repository/docker/opendilab/ding)，we prepare `base image` and `env image` with common RL environments.
 
 <details close>
 <summary>(Click for Details)</summary>
 
 - base: opendilab/ding:nightly
+- rpc: opendilab/ding:nightly-rpc
 - atari: opendilab/ding:nightly-atari
 - mujoco: opendilab/ding:nightly-mujoco
 - dmc: opendilab/ding:nightly-dmc2gym
 - metaworld: opendilab/ding:nightly-metaworld
 - smac: opendilab/ding:nightly-smac
 - grf: opendilab/ding:nightly-grf
+- cityflow: opendilab/ding:nightly-cityflow
+- evogym: opendilab/ding:nightly-evogym
+- d4rl: opendilab/ding:nightly-d4rl
 </details>
 
 The detailed documentation are hosted on [doc](https://di-engine-docs.readthedocs.io/en/latest/) | [中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/).
 
 ## Quick Start
 
 [3 Minutes Kickoff](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html)
@@ -157,43 +165,43 @@
 
 ```bash
 ding -m serial -e cartpole -p dqn -s 0
 ```
 
 ## Feature
 ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-18)
 
-![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-18)
+<details open>
+<summary>(Click to Collapse)</summary>
+
+![discrete](https://img.shields.io/badge/-discrete-brightgreen) &nbsp;discrete means discrete action space, which is only label in normal DRL algorithms (1-23)
 
-![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-18)
+![continuous](https://img.shields.io/badge/-continous-green) &nbsp;means continuous action space, which is only label in normal DRL algorithms (1-23)
+
+![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) &nbsp;means hybrid (discrete + continuous) action space (1-23)
 
 ![dist](https://img.shields.io/badge/-distributed-blue) &nbsp;[Distributed Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)｜[分布式强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html)
 
 ![MARL](https://img.shields.io/badge/-MARL-yellow) &nbsp;[Multi-Agent Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)｜[多智能体强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/multi_agent_cooperation_rl_zh.html)
 
 ![exp](https://img.shields.io/badge/-exploration-orange) &nbsp;[Exploration Mechanisms in Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)｜[强化学习中的探索机制](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html)
 
 ![IL](https://img.shields.io/badge/-IL-purple) &nbsp;[Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)｜[模仿学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html)
 
 ![offline](https://img.shields.io/badge/-offlineRL-darkblue) &nbsp;[Offiline Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)｜[离线强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
 
 
 ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) &nbsp;[Model-Based Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)｜[基于模型的强化学习](https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html)
 
-![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithm, usually as plugin-in in the whole pipeline
+![other](https://img.shields.io/badge/-other-lightgrey) &nbsp;means other sub-direction algorithms, usually as plugin-in in the whole pipeline
 
 P.S: The `.py` file in `Runnable Demo` can be found in `dizoo`
 
 
 
-
-<details open>
-<summary>(Click to Collapse)</summary>
-
 |  No.  |                          Algorithm                           |                            Label                             |                        Doc and Implementation                        |                        Runnable Demo                         |
 | :--: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
 |  1   |         [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)<br>[DQN中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/dqn_zh.html)<br>[policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -c cartpole_dqn_config.py -s 0 |
 |  2   |         [C51](https://arxiv.org/pdf/1707.06887.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/c51.html)<br>[policy/c51](https://github.com/opendilab/DI-engine/blob/main/ding/policy/c51.py) |        ding -m serial -c cartpole_c51_config.py -s 0         |
 |  3   |         [QRDQN](https://arxiv.org/pdf/1710.10044.pdf)        | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [QRDQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qrdqn.html)<br>[policy/qrdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qrdqn.py) |       ding -m serial -c cartpole_qrdqn_config.py -s 0        |
 |  4   |         [IQN](https://arxiv.org/pdf/1806.06923.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [IQN doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/iqn.html)<br>[policy/iqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/iqn.py) |        ding -m serial -c cartpole_iqn_config.py -s 0         |
 |  5   |         [FQF](https://arxiv.org/pdf/1911.02140.pdf)          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [FQF doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/fqf.html)<br>[policy/fqf](https://github.com/opendilab/DI-engine/blob/main/ding/policy/fqf.py) |        ding -m serial -c cartpole_fqf_config.py -s 0         |
@@ -209,44 +217,45 @@
 |  15  |         [DDPG](https://arxiv.org/pdf/1509.02971.pdf)/[PADDPG](https://arxiv.org/pdf/1511.04143.pdf)         | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [DDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |        ding -m serial -c pendulum_ddpg_config.py -s 0        |
 |  16  |         [TD3](https://arxiv.org/pdf/1802.09477.pdf)          | ![continuous](https://img.shields.io/badge/-continous-green)![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [TD3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3.html)<br>[policy/td3](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3.py) | python3 -u pendulum_td3_main.py / ding -m serial -c pendulum_td3_config.py -s 0 |
 |  17  | [D4PG](https://arxiv.org/pdf/1804.08617.pdf) | ![continuous](https://img.shields.io/badge/-continous-green) | [D4PG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/d4pg.html)<br>[policy/d4pg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/d4pg.py) | python3 -u pendulum_d4pg_config.py |
 |  18  |           [SAC](https://arxiv.org/abs/1801.01290)/[MASAC]            | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![continuous](https://img.shields.io/badge/-continous-green)![MARL](https://img.shields.io/badge/-MARL-yellow) | [SAC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sac.html)<br>[policy/sac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/sac.py) |        ding -m serial -c pendulum_sac_config.py -s 0         |
 |  19  | [PDQN](https://arxiv.org/pdf/1810.06394.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_pdqn_config.py -s 0 |
 |  20  | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -c gym_hybrid_mpdqn_config.py -s 0 |
 |  21  | [HPPO](https://arxiv.org/pdf/1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 |
-|  22  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
-|  23  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
-|  24  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
-|  25  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
-|  26  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
-|  27  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
-|  28  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
-|  29  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
-|  30  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
-|  31  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
-|  32  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
-|  33  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
-|  34  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
-|  35  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
-|  36  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
-|  37  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
-|  38  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
-|  39  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
-|  40  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
-|  41  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
-|  42  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
-|  43  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
-|  44  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
-|  45  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
-|  46  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
-|  47  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
-|  48  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
-|  49  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
-|  50  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
-|  51  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  22  |         [BDQ](https://arxiv.org/pdf/1711.08946.pdf)          |   ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)    | [policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqn.py) |        python3 -u hopper_bdq_config.py       |
+|  23  |         [MDQN](https://arxiv.org/abs/2007.14430)          |   ![discrete](https://img.shields.io/badge/-discrete-brightgreen)    | [policy/mdqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mdqn.py) |        python3 -u asterix_mdqn_config.py       |
+|  24  |           [QMIX](https://arxiv.org/pdf/1803.11485.pdf)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [QMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/qmix.html)<br>[policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qmix.py) |       ding -m serial -c smac_3s5z_qmix_config.py -s 0        |
+|  25  |         [COMA](https://arxiv.org/pdf/1705.08926.pdf)         |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/coma.html)<br>[policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/coma.py) |       ding -m serial -c smac_3s5z_coma_config.py -s 0        |
+|  26  |          [QTran](https://arxiv.org/abs/1905.05408)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/ding/policy/qtran.py) |       ding -m serial -c smac_3s5z_qtran_config.py -s 0       |
+|  27  |          [WQMIX](https://arxiv.org/abs/2006.10800)           |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/wqmix.html)<br>[policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/wqmix.py) |       ding -m serial -c smac_3s5z_wqmix_config.py -s 0       |
+|  28  |        [CollaQ](https://arxiv.org/pdf/2010.08531.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/collaq.html)<br>[policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/collaq.py) |      ding -m serial -c smac_3s5z_collaq_config.py -s 0       |
+|  29  |        [MADDPG](https://arxiv.org/pdf/1706.02275.pdf)        |      ![MARL](https://img.shields.io/badge/-MARL-yellow)      | [MADDPG doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/ddpg.html)<br>[policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ddpg.py) |      ding -m serial -c ant_maddpg_config.py -s 0       |
+|  30  |           [GAIL](https://arxiv.org/pdf/1606.03476.pdf)           |        ![IL](https://img.shields.io/badge/-IL-purple)        | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/gail.html)<br>[reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/gail_irl_model.py) |  ding -m serial_gail -c cartpole_dqn_gail_config.py -s 0  |
+|  31  |         [SQIL](https://arxiv.org/pdf/1905.11108.pdf)         |        ![IL](https://img.shields.io/badge/-IL-purple)        | [SQIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)<br>[entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_sqil.py) |     ding -m serial_sqil -c cartpole_sqil_config.py -s 0      |
+|  32  | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqfd.html)<br>[policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 |
+|  33  | [R2D3](https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/r2d3.html)<br>[R2D3中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html)<br>[policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py |
+|  34  |     [Guided Cost Learning](https://arxiv.org/pdf/1603.00448.pdf)     |   ![IL](https://img.shields.io/badge/-IL-purple)             | [Guided Cost Learning中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)<br>[reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/guided_cost_reward_model.py) |                          python3 lunarlander_gcl_config.py   |
+|  35  |         [TREX](https://arxiv.org/abs/1904.06387)          |   ![IL](https://img.shields.io/badge/-IL-purple)             | [TREX doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/trex.html)<br>[reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/trex_reward_model.py) |                          python3 mujoco_trex_main.py   |
+|  36  |         [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC)          |   ![IL](https://img.shields.io/badge/-IL-purple)    | [policy/ibc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/ibc.py) <br> [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/model/template/ebm.py) |        python3 d4rl_ibc_main.py -s 0 -c pen_human_ibc_mcmc_config.py  |
+|  37  |         [BCO](https://arxiv.org/pdf/1805.01954.pdf)          | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco](https://github.com/opendilab/DI-engine/blob/main/ding/entry/serial_entry_bco.py) |                python3 -u cartpole_bco_config.py                 |
+|  38  |           [HER](https://arxiv.org/pdf/1707.01495.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [HER doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/her.html)<br>[reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/her_reward_model.py) |                python3 -u bitflip_her_dqn.py                 |
+|  39  |           [RND](https://arxiv.org/abs/1810.12894)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [RND doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/rnd.html)<br>[reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/rnd_reward_model.py) |             python3 -u cartpole_rnd_onppo_config.py           |
+|  40  |           [ICM](https://arxiv.org/pdf/1705.05363.pdf)            |   ![exp](https://img.shields.io/badge/-exploration-orange)   | [ICM doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/icm.html)<br>[ICM中文文档](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/icm_zh.html)<br>[reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/reward_model/icm_reward_model.py) |             python3 -u cartpole_ppo_icm_config.py              |
+|  41  |         [CQL](https://arxiv.org/pdf/2006.04779.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/cql.html)<br>[policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/cql.py) |                 python3 -u d4rl_cql_main.py                  |
+|  42  |         [TD3BC](https://arxiv.org/pdf/2106.06860.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/td3_bc.html)<br>[policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/td3_bc.py) |                 python3 -u d4rl_td3_bc_main.py                  |
+|  43  |         [Decision Transformer](https://arxiv.org/pdf/2106.01345.pdf)          | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-engine/blob/main/ding/policy/decision_transformer.py) |                 python3 -u d4rl_dt_main.py                  |
+|  44  |         MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE](https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_mbsac_mbpo_config.py \ python3 -u pendulum_mbsac_ddppo_config.py    |
+|  45  |         STEVESAC([SAC](https://arxiv.org/abs/1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/abs/1510.09142))         | ![continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |        python3 -u pendulum_stevesac_mbpo_config.py    |
+|  46  |         [MBPO](https://arxiv.org/pdf/1906.08253.pdf)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/mbpo.html)<br>[world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/mbpo.py) |        python3 -u pendulum_sac_mbpo_config.py    |
+|  47  |         [DDPPO](https://openreview.net/forum?id=rzvOQrnclO0)         | ![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) |        python3 -u pendulum_mbsac_ddppo_config.py    |
+|  48  |         [PER](https://arxiv.org/pdf/1511.05952.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/worker/replay_buffer/advanced_buffer.py) |                        `rainbow demo`                        |
+|  49  |         [GAE](https://arxiv.org/pdf/1506.02438.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/ding/rl_utils/gae.py) |                          `ppo demo`                          |
+|  50  |         [ST-DIM](https://arxiv.org/pdf/1906.08226.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/blob/main/ding/torch_utils/loss/contrastive_loss.py) |        ding -m serial -c cartpole_dqn_stdim_config.py -s 0       |
+|  51  |         [PLR](https://arxiv.org/pdf/2010.03934.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)<br>[data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/main/ding/data/level_replay/level_sampler.py) |        python3 -u bigfish_plr_config.py -s 0       |
+|  52  |         [PCGrad](https://arxiv.org/pdf/2001.06782.pdf)          |   ![other](https://img.shields.io/badge/-other-lightgrey)    | [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/blob/main/ding/data/torch_utils/optimizer_helper.py) |        python3 -u multi_mnist_pcgrad_main.py -s 0       |
 </details>
 
 
 ### Environment Versatility
 <details open>
 <summary>(Click to Collapse)</summary>
 
@@ -265,32 +274,32 @@
 |  11  |       [overcooked](https://github.com/HumanCompatibleAI/overcooked-demo)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)  | ![original](./dizoo/overcooked/overcooked.gif)       |   [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/overcooded/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/overcooked.html)   |
 |  12  |       [procgen](https://github.com/openai/procgen)                          | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   | ![original](./dizoo/procgen/coinrun.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/procgen)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/procgen.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/procgen_zh.html) |
 |  13  |       [pybullet](https://github.com/benelot/pybullet-gym)    | ![continuous](https://img.shields.io/badge/-continous-green)  | ![original](./dizoo/pybullet/pybullet.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pybullet/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/pybullet_zh.html) |
 |  14  |       [smac](https://github.com/oxwhirl/smac)     | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue)![sparse](https://img.shields.io/badge/-sparse%20reward-orange) | ![original](./dizoo/smac/smac.gif)       | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/smac/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/smac.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/smac_zh.html) |
 | 15 | [d4rl](https://github.com/rail-berkeley/d4rl) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue) | ![ori](dizoo/d4rl/d4rl.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/d4rl)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/d4rl_zh.html) |
 |  16  |       league_demo                      | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![original](./dizoo/league_demo/league_demo.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/league_demo/envs)                |
 |  17  |       pomdp atari                    | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)   |  | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs) |
-|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) |
+|  18  |       [bsuite](https://github.com/deepmind/bsuite)                         | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bsuite/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//bsuite.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bsuite_zh.html) |
 |  19  | [ImageNet](https://www.image-net.org/) | ![IL](https://img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/image_classification)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/image_cls_zh.html) |
 |  20  | [slime_volleyball](https://github.com/hardmaru/slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/slime_volley)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/slime_volleyball.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/slime_volleyball_zh.html) |
 |  21  | [gym_hybrid](https://github.com/thomashirtz/gym-hybrid) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_hybrid/moving_v0.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_hybrid)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_hybrid.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_hybrid_zh.html) |
 |  22  | [GoBigger](https://github.com/opendilab/GoBigger) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)![marl](https://img.shields.io/badge/-MARL-yellow)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori](./dizoo/gobigger_overview.gif) | [dizoo link](https://github.com/opendilab/GoBigger-Challenge-2021/tree/main/di_baseline)<br>[env tutorial](https://gobigger.readthedocs.io/en/latest/index.html)<br>[环境指南](https://gobigger.readthedocs.io/zh_CN/latest/) |
 |  23  | [gym_soccer](https://github.com/openai/gym-soccer) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) | ![ori](dizoo/gym_soccer/half_offensive.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_soccer)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_soccer_zh.html) |
 |  24  |[multiagent_mujoco](https://github.com/schroederdewitt/multiagent_mujoco)       |  ![continuous](https://img.shields.io/badge/-continous-green) ![marl](https://img.shields.io/badge/-MARL-yellow) | ![original](./dizoo/mujoco/mujoco.gif)                    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/multiagent_mujoco/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/mujoco_zh.html) |
 |  25  |bitflip                                | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) ![sparse](https://img.shields.io/badge/-sparse%20reward-orange)  | ![original](./dizoo/bitflip/bitflip.gif)    | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/bitflip/envs)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/bitflip_zh.html) |
 |  26  |[sokoban](https://github.com/mpSchrader/gym-sokoban) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![Game 2](https://github.com/mpSchrader/gym-sokoban/raw/default/docs/Animations/solved_4.gif?raw=true) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/sokoban/envs)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/sokoban.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/sokoban_zh.html) |
-|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br>环境指南 |
+|  27  |[gym_anytrading](https://github.com/AminHP/gym-anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading) <br> [env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/gym_anytrading/envs/README.md) |
 |  28  |[mario](https://github.com/Kautenja/gym-super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/mario) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/gym_super_mario_bros.html) <br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/gym_super_mario_bros_zh.html) |
 |  29  |[dmc2gym](https://github.com/denisyarats/dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)<br>[env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/dmc2gym.html)<br>[环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/dmc2gym_zh.html) |
-|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br>环境指南 |
+|  30  |[evogym](https://github.com/EvolutionGym/evogym) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/evogym/envs) <br> [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/evogym.html) <br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/Evogym_zh.html) |
 |  31  |[gym-pybullet-drones](https://github.com/utiasDSL/gym-pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/gym_pybullet_drones/envs)<br>环境指南 |
 |  32  |[beergame](https://github.com/OptMLGroup/DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)<br>环境指南 |
-|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br>环境指南 |
+|  33  |[classic_control/acrobot](https://github.com/openai/gym/tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/classic_control/acrobot/envs)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/acrobot_zh.html) |
 |  34  |[box2d/car_racing](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) <br> ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)<br>环境指南 |
-|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br>环境指南 |
+|  35  |[metadrive](https://github.com/metadriverse/metadrive) | ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/metadrive/env)<br> [环境指南](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/metadrive_zh.html) |
 
 ![discrete](https://img.shields.io/badge/-discrete-brightgreen) means discrete action space
 
 ![continuous](https://img.shields.io/badge/-continous-green) means continuous action space
 
 ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous) action space
 
@@ -304,14 +313,119 @@
 
 ![selfplay](https://img.shields.io/badge/-selfplay-blue) means environment that allows agent VS agent battle
 
 P.S. some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward type
 </details>
 
 
+### General Data Container: TreeTensor
+
+DI-engine utilizes [TreeTensor](https://github.com/opendilab/DI-treetensor) as the basic data container in various components, which is ease of use and consistent across different code modules such as environment definition, data processing and DRL optimization. Here are some concrete code examples:
+
+- TreeTensor can easily extend all the operations of `torch.Tensor` to nested data:
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```python
+    import treetensor.torch as ttorch
+
+
+    # create random tensor
+    data = ttorch.randn({'a': (3, 2), 'b': {'c': (3, )}})
+    # clone+detach tensor
+    data_clone = data.clone().detach()
+    # access tree structure like attribute
+    a = data.a
+    c = data.b.c
+    # stack/cat/split
+    stacked_data = ttorch.stack([data, data_clone], 0)
+    cat_data = ttorch.cat([data, data_clone], 0)
+    data, data_clone = ttorch.split(stacked_data, 1)
+    # reshape
+    data = data.unsqueeze(-1)
+    data = data.squeeze(-1)
+    flatten_data = data.view(-1)
+    # indexing
+    data_0 = data[0]
+    data_1to2 = data[1:2]
+    # execute math calculations
+    data = data.sin()
+    data.b.c.cos_().clamp_(-1, 1)
+    data += data ** 2
+    # backward
+    data.requires_grad_(True)
+    loss = data.arctan().mean()
+    loss.backward()
+    # print shape
+    print(data.shape)
+    # result
+    # <Size 0x7fbd3346ddc0>
+    # ├── 'a' --> torch.Size([1, 3, 2])
+    # └── 'b' --> <Size 0x7fbd3346dd00>
+    #     └── 'c' --> torch.Size([1, 3])
+    ```
+
+  </details>
+
+- TreeTensor can make it simple yet effective to implement classic deep reinforcement learning pipeline
+  <details close>
+  <summary>(Click for Details)</summary>
+
+    ```diff
+    import torch
+    import treetensor.torch as ttorch
+
+    B = 4
+
+
+    def get_item():
+        return {
+            'obs': {
+                'scalar': torch.randn(12),
+                'image': torch.randn(3, 32, 32),
+            },
+            'action': torch.randint(0, 10, size=(1,)),
+            'reward': torch.rand(1),
+            'done': False,
+        }
+
+
+    data = [get_item() for _ in range(B)]
+
+
+    # execute `stack` op
+    - def stack(data, dim):
+    -     elem = data[0]
+    -     if isinstance(elem, torch.Tensor):
+    -         return torch.stack(data, dim)
+    -     elif isinstance(elem, dict):
+    -         return {k: stack([item[k] for item in data], dim) for k in elem.keys()}
+    -     elif isinstance(elem, bool):
+    -         return torch.BoolTensor(data)
+    -     else:
+    -         raise TypeError("not support elem type: {}".format(type(elem)))
+    - stacked_data = stack(data, dim=0)
+    + data = [ttorch.tensor(d) for d in data]
+    + stacked_data = ttorch.stack(data, dim=0)
+
+    # validate
+    - assert stacked_data['obs']['image'].shape == (B, 3, 32, 32)
+    - assert stacked_data['action'].shape == (B, 1)
+    - assert stacked_data['reward'].shape == (B, 1)
+    - assert stacked_data['done'].shape == (B,)
+    - assert stacked_data['done'].dtype == torch.bool
+    + assert stacked_data.obs.image.shape == (B, 3, 32, 32)
+    + assert stacked_data.action.shape == (B, 1)
+    + assert stacked_data.reward.shape == (B, 1)
+    + assert stacked_data.done.shape == (B,)
+    + assert stacked_data.done.dtype == torch.bool
+    ```
+
+  </details>
+
 ## Feedback and Contribution
 
 - [File an issue](https://github.com/opendilab/DI-engine/issues/new/choose) on Github
 - Open or participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
 - Discuss on DI-engine [slack communication channel](https://join.slack.com/t/opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ)
 - Discuss on DI-engine's QQ group (700157520) or add us on WeChat
```

#### html2text {}

```diff
@@ -31,145 +31,166 @@
 commit-activity/m/opendilab/DI-engine) [![GitHub issues](https://
 img.shields.io/github/issues/opendilab/DI-engine)](https://github.com/
 opendilab/DI-engine/issues) [![GitHub pulls](https://img.shields.io/github/
 issues-pr/opendilab/DI-engine)](https://github.com/opendilab/DI-engine/pulls)
 [![Contributors](https://img.shields.io/github/contributors/opendilab/DI-
 engine)](https://github.com/opendilab/DI-engine/graphs/contributors) [![GitHub
 license](https://img.shields.io/github/license/opendilab/DI-engine)](https://
-github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.02.17 DI-
-engine-v0.4.6 ## Introduction to DI-engine [DI-engine doc](https://di-engine-
+github.com/opendilab/DI-engine/blob/master/LICENSE) Updated on 2023.04.11 DI-
+engine-v0.4.7 ## Introduction to DI-engine [Documentation](https://di-engine-
 docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/) **DI-engine** is a generalized decision
-intelligence engine. It supports **various [deep reinforcement learning](https:
-//di-engine-docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms**
-([link](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
-index.html)): - Most basic DRL algorithms, such as DQN, PPO, SAC, R2D2, IMPALA
-- Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation learning
-algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning, Implicit
-Behavioral Cloning - Exploration algorithms like HER, RND, ICM, NGU - Offline
-RL algorithms: CQL, TD3BC, Decision Transformer - Model-based RL algorithms:
-SVG, MVE, STEVE / MBPO, DDPPO **DI-engine** aims to **standardize different
-Decision Intelligence enviroments and applications**. Various training
-pipelines and customized decision AI applications are also supported.  (Click
-to Collapse) - Traditional academic environments - [DI-zoo](https://github.com/
-opendilab/DI-engine#environment-versatility) - Tutorial courses - [PPOxFamily]
-(https://github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course -
-Real world decision AI applications - [DI-star](https://github.com/opendilab/
-DI-star): Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/
-DI-drive): Auto-driving platform - [GoBigger](https://github.com/opendilab/
-GoBigger): [ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-
-sheep](https://github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game -
-[DI-smartcross](https://github.com/opendilab/DI-smartcross): Decision AI in
-Traffic Light Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq):
-Decision AI in Biological Sequence Prediction and Searching - [DI-1024](https:/
-/github.com/opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game -
-Research paper - [InterFuser](https://github.com/opendilab/InterFuser): [CoRL
-2022] Safety-Enhanced Autonomous Driving Using Interpretable Sensor Fusion
-Transformer - [ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE:
-Cooperative Multi-agent Q-learning with Bidirectional Action-Dependency -
-General nested data lib - [treevalue](https://github.com/opendilab/treevalue):
-Tree-nested data structure - [DI-treetensor](https://github.com/opendilab/DI-
-treetensor): Tree-nested PyTorch tensor Lib - Docs and Tutorials - [DI-engine-
-docs](https://github.com/opendilab/DI-engine-docs) - [awesome-model-based-RL]
-(https://github.com/opendilab/awesome-model-based-RL): A curated list of
-awesome Model-Based RL resources - [awesome-exploration-RL](https://github.com/
-opendilab/awesome-exploration-rl): A curated list of awesome exploration RL
-resources - [awesome-decision-transformer](https://github.com/opendilab/
-awesome-decision-transformer): A curated list of Decision Transformer resources
-- [awesome-multi-modal-reinforcement-learning](https://github.com/opendilab/
-awesome-multi-modal-reinforcement-learning): A curated list of Multi-Modal
-Reinforcement Learning resources - [awesome-AI-based-protein-design](https://
-github.com/opendilab/awesome-AI-based-protein-design): a collection of research
-papers for AI-based protein design - [awesome-diffusion-model-in-rl](https://
-github.com/opendilab/awesome-diffusion-model-in-rl): A curated list of
-Diffusion Model in RL resources - [awesome-end-to-end-autonomous-driving]
-(https://github.com/opendilab/awesome-end-to-end-autonomous-driving): A curated
-list of awesome End-to-End Autonomous Driving resources - [awesome-driving-
-behavior-prediction](https://github.com/opendilab/awesome-driving-behavior-
-prediction): A collection of research papers for Driving Behavior Prediction
-**DI-engine** also has some **system optimization and design** for efficient
-and robust large-scale RL training:  (Click for Details) - [DI-orchestrator]
-(https://github.com/opendilab/DI-orchestrator): RL Kubernetes Custom Resource
-and Operator Lib - [DI-hpc](https://github.com/opendilab/DI-hpc): RL HPC OP Lib
-- [DI-store](https://github.com/opendilab/DI-store): RL Object Store  Have fun
-with exploration and exploitation. ## Outline - [Introduction to DI-engine]
-(#introduction-to-di-engine) - [Outline](#outline) - [Installation]
-(#installation) - [Quick Start](#quick-start) - [Feature](#feature) - [↳
-Algorithm Versatility](#algorithm-versatility) - [↳ Environment Versatility]
-(#environment-versatility) - [Feedback and Contribution](#feedback-and-
-contribution) - [Supporters](#supporters) - [↳ Stargazers](#-stargazers) - [↳
-Forkers](#-forkers) - [Citation](#citation) - [License](#license) ##
-Installation You can simply install DI-engine from PyPI with the following
-command: ```bash pip install DI-engine ``` If you use Anaconda or Miniconda,
-you can install DI-engine from conda-forge through the following command:
-```bash conda install -c opendilab di-engine ``` For more information about
-installation, you can refer to [installation](https://di-engine-
-docs.readthedocs.io/en/latest/01_quickstart/installation.html). And our
-dockerhub repo can be found [here](https://hub.docker.com/repository/docker/
-opendilab/ding)ï¼we prepare `base image` and `env image` with common RL
-environments.  (Click for Details) - base: opendilab/ding:nightly - atari:
-opendilab/ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc:
-opendilab/ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld -
-smac: opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf  The
-detailed documentation are hosted on [doc](https://di-engine-
-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff](https://
-di-engine-docs.readthedocs.io/en/latest/01_quickstart/first_rl_program.html) [3
-Minutes Kickoff (colab)](https://colab.research.google.com/drive/
-1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing) [How to migrate a new **RL
-Env**](https://di-engine-docs.readthedocs.io/en/latest/11_dizoo/index.html) |
-[å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html) [How to customize the
-neural network model](https://di-engine-docs.readthedocs.io/en/latest/
-04_best_practice/custom_model.html) |
+docs.readthedocs.io/zh_CN/latest/) | [Tutorials](https://di-engine-
+docs.readthedocs.io/en/latest/01_quickstart/index.html) | [Feature](#feature) |
+[Task & Middleware](https://di-engine-docs.readthedocs.io/en/latest/03_system/
+index.html) | [TreeTensor](#general-data-container-treetensor) | [Roadmap]
+(https://github.com/opendilab/DI-engine/issues/548) **DI-engine** is a
+generalized decision intelligence engine for PyTorch and JAX. It provides
+**python-first** and **asynchronous-native** task and middleware abstractions,
+and modularly integrates several of the most important decision-making
+concepts: Env, Policy and Model. Based on the above mechanisms, DI-engine
+supports **various [deep reinforcement learning](https://di-engine-
+docs.readthedocs.io/en/latest/10_concepts/index.html) algorithms** with
+superior performance, high efficiency, well-organized [documentation](https://
+di-engine-docs.readthedocs.io/en/latest/) and [unittest](https://github.com/
+opendilab/DI-engine/actions): - Most basic DRL algorithms, such as DQN, PPO,
+SAC, R2D2, IMPALA - Multi-agent RL algorithms like QMIX, MAPPO, ACE - Imitation
+learning algorithms (BC/IRL/GAIL) , such as GAIL, SQIL, Guided Cost Learning,
+Implicit BC - Offline RL algorithms: CQL, TD3BC, Decision Transformer - Model-
+based RL algorithms: SVG, MVE, STEVE / MBPO, DDPPO - Exploration algorithms
+like HER, RND, ICM, NGU **DI-engine** aims to **standardize different Decision
+Intelligence environments and applications**, supporting both academic research
+and prototype applications. Various training pipelines and customized decision
+AI applications are also supported:  (Click to Collapse) - Traditional academic
+environments - [DI-zoo](https://github.com/opendilab/DI-engine#environment-
+versatility): various decision intelligence demonstrations and benchmark
+environments with DI-engine. - Tutorial courses - [PPOxFamily](https://
+github.com/opendilab/PPOxFamily): PPO x Family DRL Tutorial Course - Real world
+decision AI applications - [DI-star](https://github.com/opendilab/DI-star):
+Decision AI in StarCraftII - [DI-drive](https://github.com/opendilab/DI-drive):
+Auto-driving platform - [GoBigger](https://github.com/opendilab/GoBigger):
+[ICLR 2023] Multi-Agent Decision Intelligence Environment - [DI-sheep](https://
+github.com/opendilab/DI-sheep): Decision AI in 3 Tiles Game - [DI-smartcross]
+(https://github.com/opendilab/DI-smartcross): Decision AI in Traffic Light
+Control - [DI-bioseq](https://github.com/opendilab/DI-bioseq): Decision AI in
+Biological Sequence Prediction and Searching - [DI-1024](https://github.com/
+opendilab/DI-1024): Deep Reinforcement Learning + 1024 Game - Research paper -
+[InterFuser](https://github.com/opendilab/InterFuser): [CoRL 2022] Safety-
+Enhanced Autonomous Driving Using Interpretable Sensor Fusion Transformer -
+[ACE](https://github.com/opendilab/ACE): [AAAI 2023] ACE: Cooperative Multi-
+agent Q-learning with Bidirectional Action-Dependency - Docs and Tutorials -
+[DI-engine-docs](https://github.com/opendilab/DI-engine-docs): Tutorials, best
+practice and the API reference. - [awesome-model-based-RL](https://github.com/
+opendilab/awesome-model-based-RL): A curated list of awesome Model-Based RL
+resources - [awesome-exploration-RL](https://github.com/opendilab/awesome-
+exploration-rl): A curated list of awesome exploration RL resources - [awesome-
+decision-transformer](https://github.com/opendilab/awesome-decision-
+transformer): A curated list of Decision Transformer resources - [awesome-RLHF]
+(https://github.com/opendilab/awesome-RLHF): A curated list of reinforcement
+learning with human feedback resources - [awesome-multi-modal-reinforcement-
+learning](https://github.com/opendilab/awesome-multi-modal-reinforcement-
+learning): A curated list of Multi-Modal Reinforcement Learning resources -
+[awesome-AI-based-protein-design](https://github.com/opendilab/awesome-AI-
+based-protein-design): a collection of research papers for AI-based protein
+design - [awesome-diffusion-model-in-rl](https://github.com/opendilab/awesome-
+diffusion-model-in-rl): A curated list of Diffusion Model in RL resources -
+[awesome-end-to-end-autonomous-driving](https://github.com/opendilab/awesome-
+end-to-end-autonomous-driving): A curated list of awesome End-to-End Autonomous
+Driving resources - [awesome-driving-behavior-prediction](https://github.com/
+opendilab/awesome-driving-behavior-prediction): A collection of research papers
+for Driving Behavior Prediction  On the low-level end, DI-engine comes with a
+set of highly re-usable modules, including [RL optimization functions](https://
+github.com/opendilab/DI-engine/tree/main/ding/rl_utils), [PyTorch utilities]
+(https://github.com/opendilab/DI-engine/tree/main/ding/torch_utils) and
+[auxiliary tools](https://github.com/opendilab/DI-engine/tree/main/ding/utils).
+BTW, **DI-engine** also has some special **system optimization and design** for
+efficient and robust large-scale RL training:  (Click for Details) -
+[treevalue](https://github.com/opendilab/treevalue): Tree-nested data structure
+- [DI-treetensor](https://github.com/opendilab/DI-treetensor): Tree-nested
+PyTorch tensor Lib - [DI-orchestrator](https://github.com/opendilab/DI-
+orchestrator): RL Kubernetes Custom Resource and Operator Lib - [DI-hpc](https:
+//github.com/opendilab/DI-hpc): RL HPC OP Lib - [DI-store](https://github.com/
+opendilab/DI-store): RL Object Store  Have fun with exploration and
+exploitation. ## Outline - [Introduction to DI-engine](#introduction-to-di-
+engine) - [Outline](#outline) - [Installation](#installation) - [Quick Start]
+(#quick-start) - [Feature](#feature) - [↳ Algorithm Versatility](#algorithm-
+versatility) - [↳ Environment Versatility](#environment-versatility) - [↳
+General Data Container: TreeTensor](#general-data-container-treetensor) -
+[Feedback and Contribution](#feedback-and-contribution) - [Supporters]
+(#supporters) - [↳ Stargazers](#-stargazers) - [↳ Forkers](#-forkers) -
+[Citation](#citation) - [License](#license) ## Installation You can simply
+install DI-engine from PyPI with the following command: ```bash pip install DI-
+engine ``` If you use Anaconda or Miniconda, you can install DI-engine from
+conda-forge through the following command: ```bash conda install -c opendilab
+di-engine ``` For more information about installation, you can refer to
+[installation](https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+installation.html). And our dockerhub repo can be found [here](https://
+hub.docker.com/repository/docker/opendilab/ding)ï¼we prepare `base image` and
+`env image` with common RL environments.  (Click for Details) - base:
+opendilab/ding:nightly - rpc: opendilab/ding:nightly-rpc - atari: opendilab/
+ding:nightly-atari - mujoco: opendilab/ding:nightly-mujoco - dmc: opendilab/
+ding:nightly-dmc2gym - metaworld: opendilab/ding:nightly-metaworld - smac:
+opendilab/ding:nightly-smac - grf: opendilab/ding:nightly-grf - cityflow:
+opendilab/ding:nightly-cityflow - evogym: opendilab/ding:nightly-evogym - d4rl:
+opendilab/ding:nightly-d4rl  The detailed documentation are hosted on [doc]
+(https://di-engine-docs.readthedocs.io/en/latest/) | [ä¸­æææ¡£](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/). ## Quick Start [3 Minutes Kickoff]
+(https://di-engine-docs.readthedocs.io/en/latest/01_quickstart/
+first_rl_program.html) [3 Minutes Kickoff (colab)](https://
+colab.research.google.com/drive/1K3DGi3dOT9fhFqa6bBtinwCDdWkOM3zE?usp=sharing)
+[How to migrate a new **RL Env**](https://di-engine-docs.readthedocs.io/en/
+latest/11_dizoo/index.html) | [å¦ä½è¿ç§»ä¸ä¸ªæ°ç**å¼ºåå­¦ä¹ ç¯å¢**]
+(https://di-engine-docs.readthedocs.io/zh_CN/latest/11_dizoo/index_zh.html)
+[How to customize the neural network model](https://di-engine-
+docs.readthedocs.io/en/latest/04_best_practice/custom_model.html) |
 [å¦ä½å®å¶ç­ç¥ä½¿ç¨ç**ç¥ç»ç½ç»æ¨¡å**](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/04_best_practice/custom_model_zh.html)
 [æµè¯/é¨ç½² **å¼ºåå­¦ä¹ ç­ç¥** çæ ·ä¾](https://github.com/opendilab/
 DI-engine/blob/main/dizoo/classic_control/cartpole/entry/
 cartpole_c51_deploy.py) **Bonus: Train RL agent in one line code:** ```bash
 ding -m serial -e cartpole -p dqn -s 0 ``` ## Feature ### Algorithm Versatility
-![discrete](https://img.shields.io/badge/-discrete-brightgreen)  discrete means
-discrete action space, which is only label in normal DRL algorithms (1-18) !
-[continuous](https://img.shields.io/badge/-continous-green)  means continuous
-action space, which is only label in normal DRL algorithms (1-18) ![hybrid]
-(https://img.shields.io/badge/-hybrid-darkgreen)  means hybrid (discrete +
-continuous) action space (1-18) ![dist](https://img.shields.io/badge/-
-distributed-blue)  [Distributed Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/distributed_rl.html)ï½
-[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/distributed_rl_zh.html) ![MARL](https://img.shields.io/badge/-MARL-
-yellow)  [Multi-Agent Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/multi_agent_cooperation_rl.html)ï½
-[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/latest/
-02_algo/multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/
-badge/-exploration-orange)  [Exploration Mechanisms in Reinforcement Learning]
+(Click to Collapse) ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen)  discrete means discrete action space, which is only label in
+normal DRL algorithms (1-23) ![continuous](https://img.shields.io/badge/-
+continous-green)  means continuous action space, which is only label in normal
+DRL algorithms (1-23) ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen)
+ means hybrid (discrete + continuous) action space (1-23) ![dist](https://
+img.shields.io/badge/-distributed-blue)  [Distributed Reinforcement Learning]
+(https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+distributed_rl.html)ï½[åå¸å¼å¼ºåå­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/distributed_rl_zh.html) ![MARL](https:
+//img.shields.io/badge/-MARL-yellow)  [Multi-Agent Reinforcement Learning]
 (https://di-engine-docs.readthedocs.io/en/latest/02_algo/
-exploration_rl.html)ï½[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/exploration_rl_zh.html) ![IL](https://
-img.shields.io/badge/-IL-purple)  [Imitation Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/
-imitation_learning_zh.html) ![offline](https://img.shields.io/badge/-offlineRL-
-darkblue)  [Offiline Reinforcement Learning](https://di-engine-
-docs.readthedocs.io/en/latest/02_algo/offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ]
-(https://di-engine-docs.readthedocs.io/zh_CN/latest/02_algo/offline_rl_zh.html)
-![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue)  [Model-Based
-Reinforcement Learning](https://di-engine-docs.readthedocs.io/en/latest/
-02_algo/model_based_rl.html)ï½[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-
-docs.readthedocs.io/zh_CN/latest/02_algo/model_based_rl_zh.html) ![other]
-(https://img.shields.io/badge/-other-lightgrey)  means other sub-direction
-algorithm, usually as plugin-in in the whole pipeline P.S: The `.py` file in
-`Runnable Demo` can be found in `dizoo`  (Click to Collapse) | No. | Algorithm
-| Label | Doc and Implementation | Runnable Demo | | :--: | :------------------
-----------------------------------------: | :----------------------------------
-------------------------: | :--------------------------------------------------
---------: | :----------------------------------------------------------: | | 1
-| [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) |
-![discrete](https://img.shields.io/badge/-discrete-brightgreen) | [DQN doc]
-(https://di-engine-docs.readthedocs.io/en/latest/12_policies/dqn.html)
+multi_agent_cooperation_rl.html)ï½[å¤æºè½ä½å¼ºåå­¦ä¹ ](https://di-
+engine-docs.readthedocs.io/zh_CN/latest/02_algo/
+multi_agent_cooperation_rl_zh.html) ![exp](https://img.shields.io/badge/-
+exploration-orange)  [Exploration Mechanisms in Reinforcement Learning](https:/
+/di-engine-docs.readthedocs.io/en/latest/02_algo/exploration_rl.html)ï½
+[å¼ºåå­¦ä¹ ä¸­çæ¢ç´¢æºå¶](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/exploration_rl_zh.html) ![IL](https://img.shields.io/badge/-IL-
+purple)  [Imitation Learning](https://di-engine-docs.readthedocs.io/en/latest/
+02_algo/imitation_learning.html)ï½[æ¨¡ä»¿å­¦ä¹ ](https://di-engine-
+docs.readthedocs.io/zh_CN/latest/02_algo/imitation_learning_zh.html) ![offline]
+(https://img.shields.io/badge/-offlineRL-darkblue)  [Offiline Reinforcement
+Learning](https://di-engine-docs.readthedocs.io/en/latest/02_algo/
+offline_rl.html)ï½[ç¦»çº¿å¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/
+zh_CN/latest/02_algo/offline_rl_zh.html) ![mbrl](https://img.shields.io/badge/-
+ModelBasedRL-lightblue)  [Model-Based Reinforcement Learning](https://di-
+engine-docs.readthedocs.io/en/latest/02_algo/model_based_rl.html)ï½
+[åºäºæ¨¡åçå¼ºåå­¦ä¹ ](https://di-engine-docs.readthedocs.io/zh_CN/
+latest/02_algo/model_based_rl_zh.html) ![other](https://img.shields.io/badge/-
+other-lightgrey)  means other sub-direction algorithms, usually as plugin-in in
+the whole pipeline P.S: The `.py` file in `Runnable Demo` can be found in
+`dizoo` | No. | Algorithm | Label | Doc and Implementation | Runnable Demo | |
+:--: | :----------------------------------------------------------: | :--------
+--------------------------------------------------: | :------------------------
+----------------------------------: | :----------------------------------------
+------------------: | | 1 | [DQN](https://storage.googleapis.com/deepmind-
+media/dqn/DQNNaturePaper.pdf) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [DQN doc](https://di-engine-docs.readthedocs.io/en/
+latest/12_policies/dqn.html)
 [DQNä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/dqn_zh.html)
 [policy/dqn](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 dqn.py) | python3 -u cartpole_dqn_main.py / ding -m serial -
 c cartpole_dqn_config.py -s 0 | | 2 | [C51](https://arxiv.org/pdf/
 1707.06887.pdf) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | [C51 doc](https://di-engine-docs.readthedocs.io/en/latest/
@@ -272,163 +293,166 @@
 | 20 | [MPDQN](https://arxiv.org/pdf/1905.04388.pdf) | ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) | [policy/pdqn](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/pdqn.py) | ding -m serial -
 c gym_hybrid_mpdqn_config.py -s 0 | | 21 | [HPPO](https://arxiv.org/pdf/
 1903.01344.pdf) | ![hybrid](https://img.shields.io/badge/-hybrid-darkgreen) |
 [policy/ppo](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
 ppo.py) | ding -m serial_onpolicy -c gym_hybrid_hppo_config.py -s 0 | | 22 |
+[BDQ](https://arxiv.org/pdf/1711.08946.pdf) | ![hybrid](https://img.shields.io/
+badge/-hybrid-darkgreen) | [policy/bdq](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/dqn.py) | python3 -u hopper_bdq_config.py | | 23 | [MDQN]
+(https://arxiv.org/abs/2007.14430) | ![discrete](https://img.shields.io/badge/-
+discrete-brightgreen) | [policy/mdqn](https://github.com/opendilab/DI-engine/
+blob/main/ding/policy/mdqn.py) | python3 -u asterix_mdqn_config.py | | 24 |
 [QMIX](https://arxiv.org/pdf/1803.11485.pdf) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [QMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/qmix.html)
 [policy/qmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 23 | [COMA]
+qmix.py) | ding -m serial -c smac_3s5z_qmix_config.py -s 0 | | 25 | [COMA]
 (https://arxiv.org/pdf/1705.08926.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [COMA doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/coma.html)
 [policy/coma](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 24 | [QTran]
+coma.py) | ding -m serial -c smac_3s5z_coma_config.py -s 0 | | 26 | [QTran]
 (https://arxiv.org/abs/1905.05408) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [policy/qtran](https://github.com/opendilab/DI-engine/blob/main/
-ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 25
+ding/policy/qtran.py) | ding -m serial -c smac_3s5z_qtran_config.py -s 0 | | 27
 | [WQMIX](https://arxiv.org/abs/2006.10800) | ![MARL](https://img.shields.io/
 badge/-MARL-yellow) | [WQMIX doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/wqmix.html)
 [policy/wqmix](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 26 | [CollaQ]
+wqmix.py) | ding -m serial -c smac_3s5z_wqmix_config.py -s 0 | | 28 | [CollaQ]
 (https://arxiv.org/pdf/2010.08531.pdf) | ![MARL](https://img.shields.io/badge/-
 MARL-yellow) | [CollaQ doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/collaq.html)
 [policy/collaq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 27 |
+collaq.py) | ding -m serial -c smac_3s5z_collaq_config.py -s 0 | | 29 |
 [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) | ![MARL](https://
 img.shields.io/badge/-MARL-yellow) | [MADDPG doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/ddpg.html)
 [policy/ddpg](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 28 | [GAIL](https://
+ddpg.py) | ding -m serial -c ant_maddpg_config.py -s 0 | | 30 | [GAIL](https://
 arxiv.org/pdf/1606.03476.pdf) | ![IL](https://img.shields.io/badge/-IL-purple)
 | [GAIL doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 gail.html)
 [reward_model/gail](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/gail_irl_model.py) | ding -m serial_gail -
-c cartpole_dqn_gail_config.py -s 0 | | 29 | [SQIL](https://arxiv.org/pdf/
+c cartpole_dqn_gail_config.py -s 0 | | 31 | [SQIL](https://arxiv.org/pdf/
 1905.11108.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [SQIL doc]
 (https://di-engine-docs.readthedocs.io/en/latest/12_policies/sqil.html)
 [entry/sqil](https://github.com/opendilab/DI-engine/blob/main/ding/entry/
 serial_entry_sqil.py) | ding -m serial_sqil -c cartpole_sqil_config.py -s 0 | |
-30 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
+32 | [DQFD](https://arxiv.org/pdf/1704.03732.pdf) | ![IL](https://
 img.shields.io/badge/-IL-purple) | [DQFD doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/dqfd.html)
 [policy/dqfd](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 31 | [R2D3]
+dqfd.py) | ding -m serial_dqfd -c cartpole_dqfd_config.py -s 0 | | 33 | [R2D3]
 (https://arxiv.org/pdf/1909.01387.pdf) | ![IL](https://img.shields.io/badge/-
 IL-purple) | [R2D3 doc](https://di-engine-docs.readthedocs.io/en/latest/
 12_policies/r2d3.html)
 [R2D3ä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/r2d3_zh.html)
 [policy/r2d3](https://di-engine-docs.readthedocs.io/zh_CN/latest/12_policies/
-r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 32 | [Guided Cost
+r2d3_zh.html) | python3 -u pong_r2d3_r2d2expert_config.py | | 34 | [Guided Cost
 Learning](https://arxiv.org/pdf/1603.00448.pdf) | ![IL](https://img.shields.io/
 badge/-IL-purple) | [Guided Cost Learningä¸­æææ¡£](https://di-engine-
 docs.readthedocs.io/zh_CN/latest/12_policies/guided_cost_zh.html)
 [reward_model/guided_cost](https://github.com/opendilab/DI-engine/blob/main/
 ding/reward_model/guided_cost_reward_model.py) | python3
-lunarlander_gcl_config.py | | 33 | [TREX](https://arxiv.org/abs/1904.06387) | !
+lunarlander_gcl_config.py | | 35 | [TREX](https://arxiv.org/abs/1904.06387) | !
 [IL](https://img.shields.io/badge/-IL-purple) | [TREX doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/trex.html)
 [reward_model/trex](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 34 |
+reward_model/trex_reward_model.py) | python3 mujoco_trex_main.py | | 36 |
 [Implicit Behavorial Cloning](https://implicitbc.github.io/) (DFO+MCMC) | ![IL]
 (https://img.shields.io/badge/-IL-purple) | [policy/ibc](https://github.com/
 opendilab/DI-engine/blob/main/ding/policy/ibc.py)
 [model/template/ebm](https://github.com/opendilab/DI-engine/blob/main/ding/
 model/template/ebm.py) | python3 d4rl_ibc_main.py -s 0 -
-c pen_human_ibc_mcmc_config.py | | 35 | [BCO](https://arxiv.org/pdf/
+c pen_human_ibc_mcmc_config.py | | 37 | [BCO](https://arxiv.org/pdf/
 1805.01954.pdf) | ![IL](https://img.shields.io/badge/-IL-purple) | [entry/bco]
 (https://github.com/opendilab/DI-engine/blob/main/ding/entry/
-serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 36 | [HER](https:/
+serial_entry_bco.py) | python3 -u cartpole_bco_config.py | | 38 | [HER](https:/
 /arxiv.org/pdf/1707.01495.pdf) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [HER doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/her.html)
 [reward_model/her](https://github.com/opendilab/DI-engine/blob/main/ding/
-reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 37 |
+reward_model/her_reward_model.py) | python3 -u bitflip_her_dqn.py | | 39 |
 [RND](https://arxiv.org/abs/1810.12894) | ![exp](https://img.shields.io/badge/-
 exploration-orange) | [RND doc](https://di-engine-docs.readthedocs.io/en/
 latest/12_policies/rnd.html)
 [reward_model/rnd](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/rnd_reward_model.py) | python3 -u cartpole_rnd_onppo_config.py | |
-38 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
+40 | [ICM](https://arxiv.org/pdf/1705.05363.pdf) | ![exp](https://
 img.shields.io/badge/-exploration-orange) | [ICM doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/icm.html)
 [ICMä¸­æææ¡£](https://di-engine-docs.readthedocs.io/zh_CN/latest/
 12_policies/icm_zh.html)
 [reward_model/icm](https://github.com/opendilab/DI-engine/blob/main/ding/
 reward_model/icm_reward_model.py) | python3 -u cartpole_ppo_icm_config.py | |
-39 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
+41 | [CQL](https://arxiv.org/pdf/2006.04779.pdf) | ![offline](https://
 img.shields.io/badge/-offlineRL-darkblue) | [CQL doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/cql.html)
 [policy/cql](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-cql.py) | python3 -u d4rl_cql_main.py | | 40 | [TD3BC](https://arxiv.org/pdf/
+cql.py) | python3 -u d4rl_cql_main.py | | 42 | [TD3BC](https://arxiv.org/pdf/
 2106.06860.pdf) | ![offline](https://img.shields.io/badge/-offlineRL-darkblue)
 | [TD3BC doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/
 td3_bc.html)
 [policy/td3_bc](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 41 | [Decision Transformer]
+td3_bc.py) | python3 -u d4rl_td3_bc_main.py | | 43 | [Decision Transformer]
 (https://arxiv.org/pdf/2106.01345.pdf) | ![offline](https://img.shields.io/
 badge/-offlineRL-darkblue) | [policy/dt](https://github.com/opendilab/DI-
 engine/blob/main/ding/policy/decision_transformer.py) | python3 -
-u d4rl_dt_main.py | | 42 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
+u d4rl_dt_main.py | | 44 | MBSAC([SAC](https://arxiv.org/abs/1801.01290)+[MVE]
 (https://arxiv.org/abs/1803.00101)+[SVG](https://arxiv.org/abs/1510.09142)) | !
 [continuous](https://img.shields.io/badge/-continous-green)![mbrl](https://
 img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/mbpolicy/mbsac](https:/
 /github.com/opendilab/DI-engine/blob/main/ding/policy/mbpolicy/mbsac.py) |
 python3 -u pendulum_mbsac_mbpo_config.py \ python3 -
-u pendulum_mbsac_ddppo_config.py | | 43 | STEVESAC([SAC](https://arxiv.org/abs/
+u pendulum_mbsac_ddppo_config.py | | 45 | STEVESAC([SAC](https://arxiv.org/abs/
 1801.01290)+[STEVE](https://arxiv.org/abs/1807.01675)+[SVG](https://arxiv.org/
 abs/1510.09142)) | ![continuous](https://img.shields.io/badge/-continous-
 green)![mbrl](https://img.shields.io/badge/-ModelBasedRL-lightblue) | [policy/
 mbpolicy/mbsac](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 44 |
+mbpolicy/mbsac.py) | python3 -u pendulum_stevesac_mbpo_config.py | | 46 |
 [MBPO](https://arxiv.org/pdf/1906.08253.pdf) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [MBPO doc](https://di-engine-
 docs.readthedocs.io/en/latest/12_policies/mbpo.html)
 [world_model/mbpo](https://github.com/opendilab/DI-engine/blob/main/ding/
-world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 45 | [DDPPO]
+world_model/mbpo.py) | python3 -u pendulum_sac_mbpo_config.py | | 47 | [DDPPO]
 (https://openreview.net/forum?id=rzvOQrnclO0) | ![mbrl](https://img.shields.io/
 badge/-ModelBasedRL-lightblue) | [world_model/ddppo](https://github.com/
 opendilab/DI-engine/blob/main/ding/world_model/ddppo.py) | python3 -
-u pendulum_mbsac_ddppo_config.py | | 46 | [PER](https://arxiv.org/pdf/
+u pendulum_mbsac_ddppo_config.py | | 48 | [PER](https://arxiv.org/pdf/
 1511.05952.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [worker/replay_buffer](https://github.com/opendilab/DI-engine/blob/main/ding/
-worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 47 | [GAE](https:
+worker/replay_buffer/advanced_buffer.py) | `rainbow demo` | | 49 | [GAE](https:
 //arxiv.org/pdf/1506.02438.pdf) | ![other](https://img.shields.io/badge/-other-
 lightgrey) | [rl_utils/gae](https://github.com/opendilab/DI-engine/blob/main/
-ding/rl_utils/gae.py) | `ppo demo` | | 48 | [ST-DIM](https://arxiv.org/pdf/
+ding/rl_utils/gae.py) | `ppo demo` | | 50 | [ST-DIM](https://arxiv.org/pdf/
 1906.08226.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/loss/contrastive_loss](https://github.com/opendilab/DI-engine/
 blob/main/ding/torch_utils/loss/contrastive_loss.py) | ding -m serial -
-c cartpole_dqn_stdim_config.py -s 0 | | 49 | [PLR](https://arxiv.org/pdf/
+c cartpole_dqn_stdim_config.py -s 0 | | 51 | [PLR](https://arxiv.org/pdf/
 2010.03934.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [PLR doc](https://di-engine-docs.readthedocs.io/en/latest/12_policies/plr.html)
 [data/level_replay/level_sampler](https://github.com/opendilab/DI-engine/blob/
 main/ding/data/level_replay/level_sampler.py) | python3 -
-u bigfish_plr_config.py -s 0 | | 50 | [PCGrad](https://arxiv.org/pdf/
+u bigfish_plr_config.py -s 0 | | 52 | [PCGrad](https://arxiv.org/pdf/
 2001.06782.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
 [torch_utils/optimizer_helper/PCGrad](https://github.com/opendilab/DI-engine/
 blob/main/ding/data/torch_utils/optimizer_helper.py) | python3 -
-u multi_mnist_pcgrad_main.py -s 0 | | 51 | [BDQ](https://arxiv.org/pdf/
-1711.08946.pdf) | ![other](https://img.shields.io/badge/-other-lightgrey) |
-[policy/bdq](https://github.com/opendilab/DI-engine/blob/main/ding/policy/
-dqn.py) | python3 -u hopper_bdq_config.py |  ### Environment Versatility
-(Click to Collapse) | No | Environment | Label | Visualization | Code and Doc
-Links | | :--: | :--------------------------------------: | :------------------
----------------: | :--------------------------------:|:------------------------
----------------------------------: | | 1 | [Atari](https://github.com/openai/
-gym/tree/master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-
-discrete-brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link]
-(https://github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
+u multi_mnist_pcgrad_main.py -s 0 |  ### Environment Versatility  (Click to
+Collapse) | No | Environment | Label | Visualization | Code and Doc Links | | :
+--: | :--------------------------------------: | :-----------------------------
+----: | :--------------------------------:|:-----------------------------------
+----------------------: | | 1 | [Atari](https://github.com/openai/gym/tree/
+master/gym/envs/atari) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/atari/atari.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/atari/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 atari.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 atari_zh.html) | | 2 | [box2d/bipedalwalker](https://github.com/openai/gym/
 tree/master/gym/envs/box2d) | ![continuous](https://img.shields.io/badge/-
 continous-green) | ![original](./dizoo/box2d/bipedalwalker/original.gif) |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/box2d/
@@ -538,18 +562,20 @@
 atari | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | |
 [dizoo link](https://github.com/opendilab/DI-engine/tree/main/dizoo/pomdp/envs)
 | | 18 | [bsuite](https://github.com/deepmind/bsuite) | ![discrete](https://
 img.shields.io/badge/-discrete-brightgreen) | ![original](./dizoo/bsuite/
 bsuite.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/bsuite/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs//
-bsuite.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https://
-img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/image_classification/
-imagenet.png) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
-dizoo/image_classification)
+bsuite.html)
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+bsuite_zh.html) | | 19 | [ImageNet](https://www.image-net.org/) | ![IL](https:/
+/img.shields.io/badge/-IL/SL-purple) | ![original](./dizoo/
+image_classification/imagenet.png) | [dizoo link](https://github.com/opendilab/
+DI-engine/tree/main/dizoo/image_classification)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 image_cls_zh.html) | | 20 | [slime_volleyball](https://github.com/hardmaru/
 slimevolleygym) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen)![selfplay](https://img.shields.io/badge/-selfplay-blue) | ![ori]
 (dizoo/slime_volley/slime_volley.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/slime_volley)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
@@ -593,18 +619,19 @@
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 sokoban.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 sokoban_zh.html) | | 27 |[gym_anytrading](https://github.com/AminHP/gym-
 anytrading) | ![discrete](https://img.shields.io/badge/-discrete-brightgreen) |
 ![original](./dizoo/gym_anytrading/envs/position.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/gym_anytrading)
-ç¯å¢æå | | 28 |[mario](https://github.com/Kautenja/gym-super-mario-bros)
-| ![discrete](https://img.shields.io/badge/-discrete-brightgreen) | ![original]
-(./dizoo/mario/mario.gif) | [dizoo link](https://github.com/opendilab/DI-
-engine/tree/main/dizoo/mario)
+[env tutorial](https://github.com/opendilab/DI-engine/blob/main/dizoo/
+gym_anytrading/envs/README.md) | | 28 |[mario](https://github.com/Kautenja/gym-
+super-mario-bros) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) | ![original](./dizoo/mario/mario.gif) | [dizoo link](https://
+github.com/opendilab/DI-engine/tree/main/dizoo/mario)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 gym_super_mario_bros.html)
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 gym_super_mario_bros_zh.html) | | 29 |[dmc2gym](https://github.com/denisyarats/
 dmc2gym) | ![continuous](https://img.shields.io/badge/-continous-green) | !
 [original](./dizoo/dmc2gym/dmc2gym_cheetah.png) | [dizoo link](https://
 github.com/opendilab/DI-engine/tree/main/dizoo/dmc2gym)
@@ -613,57 +640,96 @@
 [ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
 dmc2gym_zh.html) | | 30 |[evogym](https://github.com/EvolutionGym/evogym) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/evogym/evogym.gif) | [dizoo link](https://github.com/opendilab/DI-engine/
 tree/main/dizoo/evogym/envs)
 [env tutorial](https://di-engine-docs.readthedocs.io/en/latest/13_envs/
 evogym.html)
-ç¯å¢æå | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+Evogym_zh.html) | | 31 |[gym-pybullet-drones](https://github.com/utiasDSL/gym-
 pybullet-drones) | ![continuous](https://img.shields.io/badge/-continous-green)
 | ![original](./dizoo/gym_pybullet_drones/gym_pybullet_drones.gif) | [dizoo
 link](https://github.com/opendilab/DI-engine/tree/main/dizoo/
 gym_pybullet_drones/envs)
 ç¯å¢æå | | 32 |[beergame](https://github.com/OptMLGroup/
 DeepBeerInventory-RL) | ![discrete](https://img.shields.io/badge/-discrete-
 brightgreen) | ![original](./dizoo/beergame/beergame.png) | [dizoo link](https:
 //github.com/opendilab/DI-engine/tree/main/dizoo/beergame/envs)
 ç¯å¢æå | | 33 |[classic_control/acrobot](https://github.com/openai/gym/
 tree/master/gym/envs/classic_control) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen) | ![original](./dizoo/classic_control/acrobot/
 acrobot.gif) | [dizoo link](https://github.com/opendilab/DI-engine/tree/main/
 dizoo/classic_control/acrobot/envs)
-ç¯å¢æå | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+acrobot_zh.html) | | 34 |[box2d/car_racing](https://github.com/openai/gym/blob/
 master/gym/envs/box2d/car_racing.py) | ![discrete](https://img.shields.io/
 badge/-discrete-brightgreen)
 ![continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/box2d/carracing/car_racing.gif) | [dizoo link](https://github.com/
 opendilab/DI-engine/tree/main/dizoo/box2d/carracing/envs)
 ç¯å¢æå | | 35 |[metadrive](https://github.com/metadriverse/metadrive) | !
 [continuous](https://img.shields.io/badge/-continous-green) | ![original](./
 dizoo/metadrive/metadrive_env.gif) | [dizoo link](https://github.com/opendilab/
 DI-engine/tree/main/dizoo/metadrive/env)
-ç¯å¢æå | ![discrete](https://img.shields.io/badge/-discrete-brightgreen)
-means discrete action space ![continuous](https://img.shields.io/badge/-
-continous-green) means continuous action space ![hybrid](https://
+[ç¯å¢æå](https://di-engine-docs.readthedocs.io/zh_CN/latest/13_envs/
+metadrive_zh.html) | ![discrete](https://img.shields.io/badge/-discrete-
+brightgreen) means discrete action space ![continuous](https://img.shields.io/
+badge/-continous-green) means continuous action space ![hybrid](https://
 img.shields.io/badge/-hybrid-darkgreen) means hybrid (discrete + continuous)
 action space ![MARL](https://img.shields.io/badge/-MARL-yellow) means multi-
 agent RL environment ![sparse](https://img.shields.io/badge/-sparse%20reward-
 orange) means environment which is related to exploration and sparse reward !
 [offline](https://img.shields.io/badge/-offlineRL-darkblue) means offline RL
 environment ![IL](https://img.shields.io/badge/-IL/SL-purple) means Imitation
 Learning or Supervised Learning Dataset ![selfplay](https://img.shields.io/
 badge/-selfplay-blue) means environment that allows agent VS agent battle P.S.
 some enviroments in Atari, such as **MontezumaRevenge**, are also sparse reward
-type  ## Feedback and Contribution - [File an issue](https://github.com/
-opendilab/DI-engine/issues/new/choose) on Github - Open or participate in our
-[forum](https://github.com/opendilab/DI-engine/discussions) - Discuss on DI-
-engine [slack communication channel](https://join.slack.com/t/opendilab/
-shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-engine's QQ
-group (700157520) or add us on WeChat ![WeChat](https://github.com/opendilab/
-DI-engine/blob/main/assets/wechat.png) - Contact our email
+type  ### General Data Container: TreeTensor DI-engine utilizes [TreeTensor]
+(https://github.com/opendilab/DI-treetensor) as the basic data container in
+various components, which is ease of use and consistent across different code
+modules such as environment definition, data processing and DRL optimization.
+Here are some concrete code examples: - TreeTensor can easily extend all the
+operations of `torch.Tensor` to nested data:  (Click for Details) ```python
+import treetensor.torch as ttorch # create random tensor data = ttorch.randn(
+{'a': (3, 2), 'b': {'c': (3, )}}) # clone+detach tensor data_clone = data.clone
+().detach() # access tree structure like attribute a = data.a c = data.b.c #
+stack/cat/split stacked_data = ttorch.stack([data, data_clone], 0) cat_data =
+ttorch.cat([data, data_clone], 0) data, data_clone = ttorch.split(stacked_data,
+1) # reshape data = data.unsqueeze(-1) data = data.squeeze(-1) flatten_data =
+data.view(-1) # indexing data_0 = data[0] data_1to2 = data[1:2] # execute math
+calculations data = data.sin() data.b.c.cos_().clamp_(-1, 1) data += data ** 2
+# backward data.requires_grad_(True) loss = data.arctan().mean() loss.backward
+() # print shape print(data.shape) # result #
+x7fbd3346ddc0> # âââ 'a' --> torch.Size([1, 3, 2]) # âââ 'b' -->
+x7fbd3346dd00> # âââ 'c' --> torch.Size([1, 3]) ```  - TreeTensor can
+make it simple yet effective to implement classic deep reinforcement learning
+pipeline  (Click for Details) ```diff import torch import treetensor.torch as
+ttorch B = 4 def get_item(): return { 'obs': { 'scalar': torch.randn(12),
+'image': torch.randn(3, 32, 32), }, 'action': torch.randint(0, 10, size=(1,)),
+'reward': torch.rand(1), 'done': False, } data = [get_item() for _ in range(B)]
+# execute `stack` op - def stack(data, dim): - elem = data[0] - if isinstance
+(elem, torch.Tensor): - return torch.stack(data, dim) - elif isinstance(elem,
+dict): - return {k: stack([item[k] for item in data], dim) for k in elem.keys
+()} - elif isinstance(elem, bool): - return torch.BoolTensor(data) - else: -
+raise TypeError("not support elem type: {}".format(type(elem))) - stacked_data
+= stack(data, dim=0) + data = [ttorch.tensor(d) for d in data] + stacked_data =
+ttorch.stack(data, dim=0) # validate - assert stacked_data['obs']
+['image'].shape == (B, 3, 32, 32) - assert stacked_data['action'].shape == (B,
+1) - assert stacked_data['reward'].shape == (B, 1) - assert stacked_data
+['done'].shape == (B,) - assert stacked_data['done'].dtype == torch.bool +
+assert stacked_data.obs.image.shape == (B, 3, 32, 32) + assert
+stacked_data.action.shape == (B, 1) + assert stacked_data.reward.shape == (B,
+1) + assert stacked_data.done.shape == (B,) + assert stacked_data.done.dtype ==
+torch.bool ```  ## Feedback and Contribution - [File an issue](https://
+github.com/opendilab/DI-engine/issues/new/choose) on Github - Open or
+participate in our [forum](https://github.com/opendilab/DI-engine/discussions)
+- Discuss on DI-engine [slack communication channel](https://join.slack.com/t/
+opendilab/shared_invite/zt-v9tmv4fp-nUBAQEH1_Kuyu_q4plBssQ) - Discuss on DI-
+engine's QQ group (700157520) or add us on WeChat ![WeChat](https://github.com/
+opendilab/DI-engine/blob/main/assets/wechat.png) - Contact our email
 (opendilab@pjlab.org.cn) - Contributes to our future plan [Roadmap](https://
 github.com/opendilab/DI-engine/issues/548) We appreciate all the feedbacks and
 contributions to improve DI-engine, both algorithms and system designs. And
 `CONTRIBUTING.md` offers some necessary information. ## Supporters ### ↳
 Stargazers [![Stargazers repo roster for @opendilab/DI-engine](https://
 reporoster.com/stars/opendilab/DI-engine)](https://github.com/opendilab/DI-
 engine/stargazers) ### ↳ Forkers [![Forkers repo roster for @opendilab/DI-
```

### Comparing `DI-engine-0.4.6/ding/bonus/demo.py` & `DI-engine-0.4.7/ding/bonus/demo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/bonus/model.py` & `DI-engine-0.4.7/ding/bonus/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 from easydict import EasyDict
 import torch
 import torch.nn as nn
 import treetensor.torch as ttorch
 from copy import deepcopy
 from ding.utils import SequenceType, squeeze
 from ding.model.common import ReparameterizationHead, RegressionHead, MultiHead, \
-    FCEncoder, ConvEncoder, IMPALAConvEncoder
+    FCEncoder, ConvEncoder, IMPALAConvEncoder, PopArtVHead
 from ding.torch_utils import MLP, fc_block
 
 
 class DiscretePolicyHead(nn.Module):
 
     def __init__(
             self,
@@ -53,14 +53,15 @@
         critic_head_layer_num: int = 1,
         activation: Optional[nn.Module] = nn.ReLU(),
         norm_type: Optional[str] = None,
         sigma_type: Optional[str] = 'independent',
         fixed_sigma_value: Optional[int] = 0.3,
         bound_type: Optional[str] = None,
         encoder: Optional[torch.nn.Module] = None,
+        popart_head=False,
     ) -> None:
         super(PPOFModel, self).__init__()
         obs_shape = squeeze(obs_shape)
         action_shape = squeeze(action_shape)
         self.obs_shape, self.action_shape = obs_shape, action_shape
         self.share_encoder = share_encoder
 
@@ -104,17 +105,23 @@
                 else:
                     raise ValueError("illegal encoder instance.")
             else:
                 self.actor_encoder = new_encoder(actor_head_hidden_size)
                 self.critic_encoder = new_encoder(critic_head_hidden_size)
 
         # Head Type
-        self.critic_head = RegressionHead(
-            critic_head_hidden_size, 1, critic_head_layer_num, activation=activation, norm_type=norm_type
-        )
+        if not popart_head:
+            self.critic_head = RegressionHead(
+                critic_head_hidden_size, 1, critic_head_layer_num, activation=activation, norm_type=norm_type
+            )
+        else:
+            self.critic_head = PopArtVHead(
+                critic_head_hidden_size, 1, critic_head_layer_num, activation=activation, norm_type=norm_type
+            )
+
         self.action_space = action_space
         assert self.action_space in ['discrete', 'continuous', 'hybrid'], self.action_space
         if self.action_space == 'continuous':
             self.multi_head = False
             self.actor_head = ReparameterizationHead(
                 actor_head_hidden_size,
                 action_shape,
@@ -203,28 +210,36 @@
 
     def compute_critic(self, x: ttorch.Tensor) -> ttorch.Tensor:
         if self.share_encoder:
             x = self.encoder(x)
         else:
             x = self.critic_encoder(x)
         x = self.critic_head(x)
-        return x['pred']
+        return x
 
     def compute_actor_critic(self, x: ttorch.Tensor) -> ttorch.Tensor:
         if self.share_encoder:
             actor_embedding = critic_embedding = self.encoder(x)
         else:
             actor_embedding = self.actor_encoder(x)
             critic_embedding = self.critic_encoder(x)
 
-        value = self.critic_head(critic_embedding)['pred']
+        value = self.critic_head(critic_embedding)
 
         if self.action_space == 'discrete':
             logit = self.actor_head(actor_embedding)
-            return ttorch.as_tensor({'logit': logit, 'value': value})
+            return ttorch.as_tensor({'logit': logit, 'value': value['pred']})
         elif self.action_space == 'continuous':
             x = self.actor_head(actor_embedding)
-            return ttorch.as_tensor({'logit': x, 'value': value})
+            return ttorch.as_tensor({'logit': x, 'value': value['pred']})
         elif self.action_space == 'hybrid':
             action_type = self.actor_head[0](actor_embedding)
             action_args = self.actor_head[1](actor_embedding)
-            return ttorch.as_tensor({'logit': {'action_type': action_type, 'action_args': action_args}, 'value': value})
+            return ttorch.as_tensor(
+                {
+                    'logit': {
+                        'action_type': action_type,
+                        'action_args': action_args
+                    },
+                    'value': value['pred']
+                }
+            )
```

### Comparing `DI-engine-0.4.6/ding/bonus/ppof.py` & `DI-engine-0.4.7/ding/bonus/ppof.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,119 +1,176 @@
+from dataclasses import dataclass
 from typing import Optional, Union
 from ditk import logging
 from easydict import EasyDict
+from functools import partial
 import os
 import gym
+import gymnasium
 import torch
 from ding.framework import task, OnlineRLContext
 from ding.framework.middleware import interaction_evaluator_ttorch, PPOFStepCollector, multistep_trainer, CkptSaver, \
     wandb_online_logger, offline_data_saver, termination_checker, ppof_adv_estimator
 from ding.envs import BaseEnv, BaseEnvManagerV2, SubprocessEnvManagerV2
 from ding.policy import PPOFPolicy, single_env_forward_wrapper_ttorch
 from ding.utils import set_pkg_seed
 from ding.config import save_config_py
 from .model import PPOFModel
 from .config import get_instance_config, get_instance_env, get_hybrid_shape
 
 
+@dataclass
+class TrainingReturn:
+    '''
+    Attributions
+    wandb_url: The weight & biases (wandb) project url of the trainning experiment.
+    '''
+    wandb_url: str
+
+
 class PPOF:
     supported_env_list = [
         # common
         'lunarlander_discrete',
         'lunarlander_continuous',
         'bipedalwalker',
+        'acrobot',
         # ch2: action
         'rocket_landing',
         'drone_fly',
         'hybrid_moving',
         # ch3: obs
         'evogym_carrier',
         'mario',
         'di_sheep',
         'procgen_bigfish',
+        # ch4: reward
+        'minigrid_fourroom',
+        'metadrive',
+        # atari
+        'atari_qbert',
+        'atari_kangaroo',
+        'atari_bowling',
     ]
 
     def __init__(
             self,
             env: Union[str, BaseEnv],
             seed: int = 0,
             exp_name: str = 'default_experiment',
             model: Optional[torch.nn.Module] = None,
-            cfg: Optional[EasyDict] = None
+            cfg: Optional[EasyDict] = None,
+            policy_state_dict: str = None,
     ) -> None:
         if isinstance(env, str):
             assert env in PPOF.supported_env_list, "Please use supported envs: {}".format(PPOF.supported_env_list)
             self.env = get_instance_env(env)
-            assert cfg is None, 'It should be default env tuned config'
-            self.cfg = get_instance_config(env)
+            if cfg is None:
+                # 'It should be default env tuned config'
+                self.cfg = get_instance_config(env, algorithm="PPO")
+            else:
+                self.cfg = cfg
         elif isinstance(env, BaseEnv):
             self.cfg = cfg
             raise NotImplementedError
         else:
             raise TypeError("not support env type: {}, only strings and instances of `BaseEnv` now".format(type(env)))
         logging.getLogger().setLevel(logging.INFO)
         self.seed = seed
         set_pkg_seed(self.seed)
         self.exp_name = exp_name
         if not os.path.exists(self.exp_name):
             os.makedirs(self.exp_name)
         save_config_py(self.cfg, os.path.join(self.exp_name, 'policy_config.py'))
 
         action_space = self.env.action_space
-        if isinstance(action_space, gym.spaces.Discrete):
-            action_shape = action_space.n
-        elif isinstance(action_space, gym.spaces.Tuple):
+        if isinstance(action_space, (gym.spaces.Discrete, gymnasium.spaces.Discrete)):
+            action_shape = int(action_space.n)
+        elif isinstance(action_space, (gym.spaces.Tuple, gymnasium.spaces.Tuple)):
             action_shape = get_hybrid_shape(action_space)
         else:
             action_shape = action_space.shape
+
+        # Three types of value normalization is supported currently
+        assert self.cfg.value_norm in ['popart', 'value_rescale', 'symlog']
         if model is None:
-            model = PPOFModel(
-                self.env.observation_space.shape, action_shape, action_space=self.cfg.action_space, **self.cfg.model
-            )
+            if self.cfg.value_norm != 'popart':
+                model = PPOFModel(
+                    self.env.observation_space.shape,
+                    action_shape,
+                    action_space=self.cfg.action_space,
+                    **self.cfg.model
+                )
+            else:
+                model = PPOFModel(
+                    self.env.observation_space.shape,
+                    action_shape,
+                    action_space=self.cfg.action_space,
+                    popart_head=True,
+                    **self.cfg.model
+                )
         self.policy = PPOFPolicy(self.cfg, model=model)
+        if policy_state_dict is not None:
+            self.policy.load_state_dict(policy_state_dict)
 
     def train(
             self,
             step: int = int(1e7),
             collector_env_num: int = 4,
             evaluator_env_num: int = 4,
             n_iter_log_show: int = 500,
             n_iter_save_ckpt: int = 1000,
             context: Optional[str] = None,
+            reward_model: Optional[str] = None,
             debug: bool = False
-    ) -> None:
+    ) -> TrainingReturn:
         if debug:
             logging.getLogger().setLevel(logging.DEBUG)
         logging.debug(self.policy._model)
         # define env and policy
-        collector_env = self._setup_env_manager(collector_env_num, context, debug)
-        evaluator_env = self._setup_env_manager(evaluator_env_num, context, debug)
+        collector_env = self._setup_env_manager(collector_env_num, context, debug, 'collector')
+        evaluator_env = self._setup_env_manager(evaluator_env_num, context, debug, 'evaluator')
+
+        if reward_model is not None:
+            # self.reward_model = create_reward_model(reward_model, self.cfg.reward_model)
+            pass
 
         with task.start(ctx=OnlineRLContext()):
             task.use(interaction_evaluator_ttorch(self.seed, self.policy, evaluator_env))
             task.use(PPOFStepCollector(self.seed, self.policy, collector_env, self.cfg.n_sample))
             task.use(ppof_adv_estimator(self.policy))
             task.use(multistep_trainer(self.policy, log_freq=n_iter_log_show))
             task.use(CkptSaver(self.policy, save_dir=self.exp_name, train_freq=n_iter_save_ckpt))
-            task.use(wandb_online_logger(self.exp_name, metric_list=self.policy.monitor_vars(), anonymous=True))
+            task.use(
+                wandb_online_logger(
+                    metric_list=self.policy.monitor_vars(),
+                    model=self.policy._model,
+                    anonymous=True,
+                    project_name=self.exp_name
+                )
+            )
             task.use(termination_checker(max_env_step=step))
             task.run()
 
-    def deploy(self, ckpt_path: str = None, enable_save_replay: bool = False, debug: bool = False) -> None:
+        return TrainingReturn(wandb_url=task.ctx.wandb_url)
+
+    def deploy(self, enable_save_replay: bool = False, replay_save_path: str = None, debug: bool = False) -> None:
         if debug:
             logging.getLogger().setLevel(logging.DEBUG)
         # define env and policy
         env = self.env.clone()
         env.seed(self.seed, dynamic_seed=False)
-        if enable_save_replay:
+
+        if enable_save_replay and replay_save_path:
+            env.enable_save_replay(replay_path=replay_save_path)
+        elif enable_save_replay:
             env.enable_save_replay(replay_path=os.path.join(self.exp_name, 'videos'))
-        if ckpt_path is None:
-            ckpt_path = os.path.join(self.exp_name, 'ckpt/eval.pth.tar')
-        state_dict = torch.load(ckpt_path, map_location='cpu')
-        self.policy.load_state_dict(state_dict)
+        else:
+            logging.warning('No video would be generated during the deploy.')
+
         forward_fn = single_env_forward_wrapper_ttorch(self.policy.eval)
 
         # main loop
         return_ = 0.
         step = 0
         obs = env.reset()
         while True:
@@ -124,68 +181,70 @@
             if done:
                 break
         logging.info(f'PPOF deploy is finished, final episode return with {step} steps is: {return_}')
 
     def collect_data(
             self,
             env_num: int = 8,
-            ckpt_path: Optional[str] = None,
             save_data_path: Optional[str] = None,
             n_sample: Optional[int] = None,
             n_episode: Optional[int] = None,
             context: Optional[str] = None,
             debug: bool = False
     ) -> None:
         if debug:
             logging.getLogger().setLevel(logging.DEBUG)
         if n_episode is not None:
             raise NotImplementedError
         # define env and policy
-        env = self._setup_env_manager(env_num, context, debug)
-        if ckpt_path is None:
-            ckpt_path = os.path.join(self.exp_name, 'ckpt/eval.pth.tar')
+        env = self._setup_env_manager(env_num, context, debug, 'collector')
         if save_data_path is None:
             save_data_path = os.path.join(self.exp_name, 'demo_data')
-        state_dict = torch.load(ckpt_path, map_location='cpu')
-        self.policy.load_state_dict(state_dict)
 
         # main execution task
         with task.start(ctx=OnlineRLContext()):
             task.use(PPOFStepCollector(self.seed, self.policy, env, n_sample))
             task.use(offline_data_saver(save_data_path, data_type='hdf5'))
             task.run(max_step=1)
         logging.info(
             f'PPOF collecting is finished, more than {n_sample} samples are collected and saved in `{save_data_path}`'
         )
 
     def batch_evaluate(
             self,
             env_num: int = 4,
-            ckpt_path: Optional[str] = None,
             n_evaluator_episode: int = 4,
             context: Optional[str] = None,
-            debug: bool = False
+            debug: bool = False,
     ) -> None:
         if debug:
             logging.getLogger().setLevel(logging.DEBUG)
         # define env and policy
-        env = self._setup_env_manager(env_num, context, debug)
-        if ckpt_path is None:
-            ckpt_path = os.path.join(self.exp_name, 'ckpt/eval.pth.tar')
-        state_dict = torch.load(ckpt_path, map_location='cpu')
-        self.policy.load_state_dict(state_dict)
+        env = self._setup_env_manager(env_num, context, debug, 'evaluator')
 
         # main execution task
         with task.start(ctx=OnlineRLContext()):
-            task.use(interaction_evaluator_ttorch(self.seed, self.policy, env, n_evaluator_episode))
+            task.use(interaction_evaluator_ttorch(
+                self.seed,
+                self.policy,
+                env,
+                n_evaluator_episode,
+            ))
             task.run(max_step=1)
 
-    def _setup_env_manager(self, env_num: int, context: Optional[str] = None, debug: bool = False) -> BaseEnvManagerV2:
+    def _setup_env_manager(
+            self,
+            env_num: int,
+            context: Optional[str] = None,
+            debug: bool = False,
+            caller: str = 'collector'
+    ) -> BaseEnvManagerV2:
+        assert caller in ['evaluator', 'collector']
         if debug:
             env_cls = BaseEnvManagerV2
             manager_cfg = env_cls.default_config()
         else:
             env_cls = SubprocessEnvManagerV2
             manager_cfg = env_cls.default_config()
             if context is not None:
                 manager_cfg.context = context
-        return env_cls([self.env.clone for _ in range(env_num)], manager_cfg)
+        return env_cls([partial(self.env.clone, caller) for _ in range(env_num)], manager_cfg)
```

### Comparing `DI-engine-0.4.6/ding/config/config.py` & `DI-engine-0.4.7/ding/config/config.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import shutil
 import sys
 import time
 import tempfile
 import subprocess
 import datetime
 from importlib import import_module
-from typing import Optional, Tuple, NoReturn
+from typing import Optional, Tuple
 from easydict import EasyDict
 from copy import deepcopy
 
 from ding.utils import deep_merge_dicts
 from ding.envs import get_env_cls, get_env_manager_cls, BaseEnvManager
 from ding.policy import get_policy_cls
 from ding.worker import BaseLearner, InteractionSerialEvaluator, BaseSerialCommander, Coordinator, \
@@ -127,28 +127,28 @@
     """
     with open(path, "r") as f:
         config_ = yaml.safe_load(f)
 
     return EasyDict(config_)
 
 
-def save_config_yaml(config_: dict, path: str) -> NoReturn:
+def save_config_yaml(config_: dict, path: str) -> None:
     """
     Overview:
         save configuration to path
     Arguments:
         - config (:obj:`dict`): Config dict
         - path (:obj:`str`): Path of target yaml
     """
     config_string = json.dumps(config_)
     with open(path, "w") as f:
         yaml.safe_dump(json.loads(config_string), f)
 
 
-def save_config_py(config_: dict, path: str) -> NoReturn:
+def save_config_py(config_: dict, path: str) -> None:
     """
     Overview:
         save configuration to python file
     Arguments:
         - config (:obj:`dict`): Config dict
         - path (:obj:`str`): Path of target yaml
     """
@@ -214,15 +214,15 @@
         assert "create_config" in cfg, "Please make sure a 'create_config' variable is declared in config python file!"
         assert "system_config" in cfg, "Please make sure a 'system_config' variable is declared in config python file!"
         return cfg['main_config'], cfg['create_config'], cfg['system_config']
     else:
         raise KeyError("invalid config file suffix: {}".format(suffix))
 
 
-def save_config(config_: dict, path: str, type_: str = 'py', save_formatted: bool = False) -> NoReturn:
+def save_config(config_: dict, path: str, type_: str = 'py', save_formatted: bool = False) -> None:
     """
     Overview:
         save configuration to python file or yaml file
     Arguments:
         - config (:obj:`dict`): Config dict
         - path (:obj:`str`): Path of target yaml or target python file
         - type (:obj:`str`): If type is ``yaml`` , save configuration to yaml file. If type is ``py`` , save\
```

### Comparing `DI-engine-0.4.6/ding/config/utils.py` & `DI-engine-0.4.7/ding/config/utils.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Optional, List, NoReturn
+from typing import Optional, List
 import copy
 from easydict import EasyDict
 
 from ding.utils import find_free_port, find_free_port_slurm, node_to_partition, node_to_host, pretty_print, \
     DEFAULT_K8S_COLLECTOR_PORT, DEFAULT_K8S_LEARNER_PORT, DEFAULT_K8S_COORDINATOR_PORT
 from dizoo.classic_control.cartpole.config.parallel import cartpole_dqn_config
 
@@ -236,15 +236,15 @@
     # learner/collector is created by opereator, so the following field is placeholder
     cfg.system.coordinator.collector = {}
     cfg.system.coordinator.learner = {}
     pretty_print(cfg)
     return cfg
 
 
-def save_config_formatted(config_: dict, path: str = 'formatted_total_config.py') -> NoReturn:
+def save_config_formatted(config_: dict, path: str = 'formatted_total_config.py') -> None:
     """
     Overview:
         save formatted configuration to python file that can be read by serial_pipeline directly.
     Arguments:
         - config (:obj:`dict`): Config dict
         - path (:obj:`str`): Path of python file
     """
```

### Comparing `DI-engine-0.4.6/ding/data/buffer/buffer.py` & `DI-engine-0.4.7/ding/data/buffer/buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/deque_buffer.py` & `DI-engine-0.4.7/ding/data/buffer/deque_buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/deque_buffer_wrapper.py` & `DI-engine-0.4.7/ding/data/buffer/deque_buffer_wrapper.py`

 * *Files 2% similar despite different names*

```diff
@@ -49,15 +49,14 @@
         if self.cfg.max_use != float("inf"):
             self.buffer.use(use_time_check(self.buffer, max_use=self.cfg.max_use))
         # priority middleware
         if self.cfg.priority:
             self.buffer.use(
                 PriorityExperienceReplay(
                     self.buffer,
-                    self.cfg.replay_buffer_size,
                     IS_weight=self.cfg.priority_IS_weight,
                     priority_power_factor=self.cfg.priority_power_factor,
                     IS_weight_power_factor=self.cfg.IS_weight_power_factor,
                     IS_weight_anneal_train_iter=self.cfg.IS_weight_anneal_train_iter
                 )
             )
             self.last_sample_index = None
```

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/clone_object.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/clone_object.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/group_sample.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/group_sample.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/padding.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/padding.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/priority.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/priority.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/sample_range_view.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/sample_range_view.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/staleness_check.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/staleness_check.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/buffer/middleware/use_time_check.py` & `DI-engine-0.4.7/ding/data/buffer/middleware/use_time_check.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/level_replay/level_sampler.py` & `DI-engine-0.4.7/ding/data/level_replay/level_sampler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/model_loader.py` & `DI-engine-0.4.7/ding/data/model_loader.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/shm_buffer.py` & `DI-engine-0.4.7/ding/data/shm_buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/storage/file.py` & `DI-engine-0.4.7/ding/data/storage/file.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/data/storage_loader.py` & `DI-engine-0.4.7/ding/data/storage_loader.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/__init__.py` & `DI-engine-0.4.7/ding/entry/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -23,7 +23,8 @@
 from .serial_entry_preference_based_irl \
       import serial_pipeline_preference_based_irl
 from .serial_entry_preference_based_irl_onpolicy \
       import serial_pipeline_preference_based_irl_onpolicy
 from .application_entry_drex_collect_data import drex_collecting_data
 from .serial_entry_mbrl import serial_pipeline_dyna, serial_pipeline_dream
 from .serial_entry_bco import serial_pipeline_bco
+from .serial_entry_pc import serial_pipeline_pc
```

### Comparing `DI-engine-0.4.6/ding/entry/application_entry.py` & `DI-engine-0.4.7/ding/entry/application_entry.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/application_entry_drex_collect_data.py` & `DI-engine-0.4.7/ding/entry/application_entry_drex_collect_data.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/application_entry_trex_collect_data.py` & `DI-engine-0.4.7/ding/entry/application_entry_trex_collect_data.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/cli.py` & `DI-engine-0.4.7/ding/entry/cli.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/cli_ditask.py` & `DI-engine-0.4.7/ding/entry/cli_ditask.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/cli_parsers/k8s_parser.py` & `DI-engine-0.4.7/ding/entry/cli_parsers/k8s_parser.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/cli_parsers/slurm_parser.py` & `DI-engine-0.4.7/ding/entry/cli_parsers/slurm_parser.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/dist_entry.py` & `DI-engine-0.4.7/ding/entry/dist_entry.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/parallel_entry.py` & `DI-engine-0.4.7/ding/entry/parallel_entry.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/predefined_config.py` & `DI-engine-0.4.7/ding/entry/predefined_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry.py` & `DI-engine-0.4.7/ding/entry/serial_entry.py`

 * *Files 4% similar despite different names*

```diff
@@ -18,28 +18,30 @@
 def serial_pipeline(
         input_cfg: Union[str, Tuple[dict, dict]],
         seed: int = 0,
         env_setting: Optional[List[Any]] = None,
         model: Optional[torch.nn.Module] = None,
         max_train_iter: Optional[int] = int(1e10),
         max_env_step: Optional[int] = int(1e10),
+        dynamic_seed: Optional[bool] = True,
 ) -> 'Policy':  # noqa
     """
     Overview:
         Serial pipeline entry for off-policy RL.
     Arguments:
         - input_cfg (:obj:`Union[str, Tuple[dict, dict]]`): Config in dict type. \
             ``str`` type means config file path. \
             ``Tuple[dict, dict]`` type means [user_config, create_cfg].
         - seed (:obj:`int`): Random seed.
         - env_setting (:obj:`Optional[List[Any]]`): A list with 3 elements: \
             ``BaseEnv`` subclass, collector env config, and evaluator env config.
         - model (:obj:`Optional[torch.nn.Module]`): Instance of torch.nn.Module.
         - max_train_iter (:obj:`Optional[int]`): Maximum policy update iterations in training.
         - max_env_step (:obj:`Optional[int]`): Maximum collected environment interaction steps.
+        - dynamic_seed(:obj:`Optional[bool]`): set dynamic seed for collector.
     Returns:
         - policy (:obj:`Policy`): Converged policy.
     """
     if isinstance(input_cfg, str):
         cfg, create_cfg = read_config(input_cfg)
     else:
         cfg, create_cfg = deepcopy(input_cfg)
@@ -49,15 +51,15 @@
     # Create main components: env, policy
     if env_setting is None:
         env_fn, collector_env_cfg, evaluator_env_cfg = get_vec_env_setting(cfg.env)
     else:
         env_fn, collector_env_cfg, evaluator_env_cfg = env_setting
     collector_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in collector_env_cfg])
     evaluator_env = create_env_manager(cfg.env.manager, [partial(env_fn, cfg=c) for c in evaluator_env_cfg])
-    collector_env.seed(cfg.seed)
+    collector_env.seed(cfg.seed, dynamic_seed=dynamic_seed)
     evaluator_env.seed(cfg.seed, dynamic_seed=False)
     set_pkg_seed(cfg.seed, use_cuda=cfg.policy.cuda)
     policy = create_policy(cfg.policy, model=model, enable_field=['learn', 'collect', 'eval', 'command'])
 
     # Create worker components: learner, collector, evaluator, replay buffer, commander.
     tb_logger = SummaryWriter(os.path.join('./{}/log/'.format(cfg.exp_name), 'serial'))
     learner = BaseLearner(cfg.policy.learn.learner, policy.learn_mode, tb_logger, exp_name=cfg.exp_name)
```

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_bc.py` & `DI-engine-0.4.7/ding/entry/serial_entry_bc.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_bco.py` & `DI-engine-0.4.7/ding/entry/serial_entry_bco.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_decision_transformer.py` & `DI-engine-0.4.7/ding/entry/serial_entry_decision_transformer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_dqfd.py` & `DI-engine-0.4.7/ding/entry/serial_entry_dqfd.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_gail.py` & `DI-engine-0.4.7/ding/entry/serial_entry_gail.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_guided_cost.py` & `DI-engine-0.4.7/ding/entry/serial_entry_guided_cost.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_mbrl.py` & `DI-engine-0.4.7/ding/entry/serial_entry_mbrl.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_ngu.py` & `DI-engine-0.4.7/ding/entry/serial_entry_ngu.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_offline.py` & `DI-engine-0.4.7/ding/entry/serial_entry_offline.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_onpolicy.py` & `DI-engine-0.4.7/ding/entry/serial_entry_onpolicy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_onpolicy_ppg.py` & `DI-engine-0.4.7/ding/entry/serial_entry_onpolicy_ppg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_plr.py` & `DI-engine-0.4.7/ding/entry/serial_entry_plr.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_preference_based_irl.py` & `DI-engine-0.4.7/ding/entry/serial_entry_preference_based_irl.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_preference_based_irl_onpolicy.py` & `DI-engine-0.4.7/ding/entry/serial_entry_preference_based_irl_onpolicy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_r2d3.py` & `DI-engine-0.4.7/ding/entry/serial_entry_r2d3.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_reward_model_offpolicy.py` & `DI-engine-0.4.7/ding/entry/serial_entry_reward_model_offpolicy.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from typing import Union, Optional, List, Any, Tuple
 import os
+import numpy as np
 import torch
 from ditk import logging
 from functools import partial
 from tensorboardX import SummaryWriter
 from copy import deepcopy
 
 from ding.envs import get_vec_env_setting, create_env_manager
@@ -83,31 +84,39 @@
     # ==========
     # Learner's before_run hook.
     learner.call_hook('before_run')
 
     # Accumulate plenty of data at the beginning of training.
     if cfg.policy.get('random_collect_size', 0) > 0:
         random_collect(cfg.policy, policy, collector, collector_env, commander, replay_buffer)
+    count = 0
+    best_reward = -np.inf
     while True:
         collect_kwargs = commander.step()
         # Evaluate policy performance
         if evaluator.should_eval(learner.train_iter):
             stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)
+            reward_mean = np.array([r['eval_episode_return'] for r in reward]).mean()
+            if reward_mean >= best_reward:
+                reward_model.save(path=cfg.exp_name, name='best')
+                best_reward = reward_mean
             if stop:
                 break
         new_data_count, target_new_data_count = 0, cfg.reward_model.get('target_new_data_count', 1)
         while new_data_count < target_new_data_count:
             new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs=collect_kwargs)
             new_data_count += len(new_data)
             # collect data for reward_model training
             reward_model.collect_data(new_data)
             replay_buffer.push(new_data, cur_collector_envstep=collector.envstep)
         # update reward_model
         reward_model.train()
-        reward_model.clear_data()
+        # clear buffer per fix iters to make sure replay buffer's data count isn't too few.
+        if count % cfg.reward_model.clear_buffer_per_iters == 0:
+            reward_model.clear_data()
         # Learn policy from collected data
         for i in range(cfg.policy.learn.update_per_collect):
             # Learner will train ``update_per_collect`` times in one iteration.
             train_data = replay_buffer.sample(learner.policy.get_attribute('batch_size'), learner.train_iter)
             if train_data is None:
                 # It is possible that replay buffer's data count is too few to train ``update_per_collect`` times
                 logging.warning(
@@ -118,11 +127,13 @@
             # update train_data reward using the augmented reward
             train_data_augmented = reward_model.estimate(train_data)
             learner.train(train_data_augmented, collector.envstep)
             if learner.policy.get_attribute('priority'):
                 replay_buffer.update(learner.priority_info)
         if collector.envstep >= max_env_step or learner.train_iter >= max_train_iter:
             break
+        count += 1
 
     # Learner's after_run hook.
     learner.call_hook('after_run')
+    reward_model.save(path=cfg.exp_name, name='last')
     return policy
```

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_reward_model_onpolicy.py` & `DI-engine-0.4.7/ding/entry/serial_entry_reward_model_onpolicy.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 from typing import Union, Optional, List, Any, Tuple
 import os
+import numpy as np
 import torch
 from ditk import logging
 from functools import partial
 from tensorboardX import SummaryWriter
 from copy import deepcopy
 
 from ding.envs import get_vec_env_setting, create_env_manager
@@ -84,19 +85,24 @@
     # Learner's before_run hook.
     learner.call_hook('before_run')
 
     # Accumulate plenty of data at the beginning of training.
     if cfg.policy.get('random_collect_size', 0) > 0:
         random_collect(cfg.policy, policy, collector, collector_env, commander, replay_buffer)
     count = 0
+    best_reward = -np.inf
     while True:
         collect_kwargs = commander.step()
         # Evaluate policy performance
         if evaluator.should_eval(learner.train_iter):
             stop, reward = evaluator.eval(learner.save_checkpoint, learner.train_iter, collector.envstep)
+            reward_mean = np.array([r['eval_episode_return'] for r in reward]).mean()
+            if reward_mean >= best_reward:
+                reward_model.save(path=cfg.exp_name, name='best')
+                best_reward = reward_mean
             if stop:
                 break
         new_data_count, target_new_data_count = 0, cfg.reward_model.get('target_new_data_count', 1)
         while new_data_count < target_new_data_count:
             new_data = collector.collect(train_iter=learner.train_iter, policy_kwargs=collect_kwargs)
             new_data_count += len(new_data)
             # collect data for reward_model training
@@ -123,8 +129,9 @@
                 replay_buffer.update(learner.priority_info)
         if collector.envstep >= max_env_step or learner.train_iter >= max_train_iter:
             break
         count += 1
 
     # Learner's after_run hook.
     learner.call_hook('after_run')
+    reward_model.save(path=cfg.exp_name, name='last')
     return policy
```

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_sqil.py` & `DI-engine-0.4.7/ding/entry/serial_entry_sqil.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/serial_entry_td3_vae.py` & `DI-engine-0.4.7/ding/entry/serial_entry_td3_vae.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/entry/utils.py` & `DI-engine-0.4.7/ding/entry/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/common/common_function.py` & `DI-engine-0.4.7/ding/envs/common/common_function.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/common/env_element.py` & `DI-engine-0.4.7/ding/envs/common/env_element.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/common/env_element_runner.py` & `DI-engine-0.4.7/ding/envs/common/env_element_runner.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env/base_env.py` & `DI-engine-0.4.7/ding/envs/env/base_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env/default_wrapper.py` & `DI-engine-0.4.7/ding/envs/env/default_wrapper.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,15 +1,16 @@
 from easydict import EasyDict
 from typing import Optional, List
 import copy
 
 eval_episode_return_wrapper = EasyDict(type='eval_episode_return')
 
 
-def get_default_wrappers(env_wrapper_name: str, env_id: Optional[str] = None) -> List[dict]:
+def get_default_wrappers(env_wrapper_name: str, env_id: Optional[str] = None, caller: str = 'collector') -> List[dict]:
+    assert caller == 'collector' or 'evaluator'
     if env_wrapper_name == 'mujoco_default':
         return [
             EasyDict(type='delay_reward', kwargs=dict(delay_reward_step=3)),
             copy.deepcopy(eval_episode_return_wrapper),
         ]
     elif env_wrapper_name == 'atari_default':
         wrapper_list = []
@@ -17,15 +18,16 @@
         wrapper_list.append(EasyDict(type='max_and_skip', kwargs=dict(skip=4)))
         wrapper_list.append(EasyDict(type='episodic_life'))
         if env_id is not None:
             if 'Pong' in env_id or 'Qbert' in env_id or 'SpaceInvader' in env_id or 'Montezuma' in env_id:
                 wrapper_list.append(EasyDict(type='fire_reset'))
         wrapper_list.append(EasyDict(type='warp_frame'))
         wrapper_list.append(EasyDict(type='scaled_float_frame'))
-        wrapper_list.append(EasyDict(type='clip_reward'))
+        if caller == 'collector':
+            wrapper_list.append(EasyDict(type='clip_reward'))
         wrapper_list.append(EasyDict(type='frame_stack', kwargs=dict(n_frames=4)))
         wrapper_list.append(copy.deepcopy(eval_episode_return_wrapper))
         return wrapper_list
     elif env_wrapper_name == 'gym_hybrid_default':
         return [
             EasyDict(type='gym_hybrid_dict_action'),
             copy.deepcopy(eval_episode_return_wrapper),
```

### Comparing `DI-engine-0.4.6/ding/envs/env/ding_env_wrapper.py` & `DI-engine-0.4.7/ding/envs/env/ding_env_wrapper.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,92 +1,111 @@
 from typing import List, Optional, Union, Dict
 from easydict import EasyDict
 import gym
+import gymnasium
 import copy
 import numpy as np
 import treetensor.numpy as tnp
 
 from ding.envs.common.common_function import affine_transform
 from ding.envs.env_wrappers import create_env_wrapper
 from ding.torch_utils import to_ndarray
 from ding.utils import CloudPickleWrapper
 from .base_env import BaseEnv, BaseEnvTimestep
 from .default_wrapper import get_default_wrappers
 
 
 class DingEnvWrapper(BaseEnv):
 
-    def __init__(self, env: gym.Env = None, cfg: dict = None, seed_api: bool = True) -> None:
+    def __init__(self, env: gym.Env = None, cfg: dict = None, seed_api: bool = True, caller: str = 'collector') -> None:
         """
         You can pass in either an env instance, or a config to create an env instance:
             - An env instance: Parameter `env` must not be `None`, but should be the instance.
                                Do not support subprocess env manager; Thus usually used in simple env.
             - A config to create an env instance: Parameter `cfg` dict must contain `env_id`.
         """
+        self._env = None
         self._raw_env = env
         self._cfg = cfg
         self._seed_api = seed_api  # some env may disable `env.seed` api
+        self._caller = caller
         if self._cfg is None:
             self._cfg = dict()
         self._cfg = EasyDict(self._cfg)
         if 'act_scale' not in self._cfg:
             self._cfg.act_scale = False
         if 'env_wrapper' not in self._cfg:
             self._cfg.env_wrapper = 'default'
         if 'env_id' not in self._cfg:
             self._cfg.env_id = None
         if env is not None:
-            self._init_flag = True
             self._env = env
-            self._wrap_env()
+            self._wrap_env(caller)
             self._observation_space = self._env.observation_space
             self._action_space = self._env.action_space
             self._action_space.seed(0)  # default seed
             self._reward_space = gym.spaces.Box(
                 low=self._env.reward_range[0], high=self._env.reward_range[1], shape=(1, ), dtype=np.float32
             )
+            self._init_flag = True
         else:
             assert 'env_id' in self._cfg
             self._init_flag = False
             self._observation_space = None
             self._action_space = None
             self._reward_space = None
         # Only if user specifies the replay_path, will the video be saved. So its inital value is None.
         self._replay_path = None
 
     # override
     def reset(self) -> None:
         if not self._init_flag:
             self._env = gym.make(self._cfg.env_id)
-            self._wrap_env()
+            self._wrap_env(self._caller)
             self._observation_space = self._env.observation_space
             self._action_space = self._env.action_space
             self._reward_space = gym.spaces.Box(
                 low=self._env.reward_range[0], high=self._env.reward_range[1], shape=(1, ), dtype=np.float32
             )
             self._init_flag = True
         if self._replay_path is not None:
             self._env = gym.wrappers.RecordVideo(
                 self._env,
                 video_folder=self._replay_path,
                 episode_trigger=lambda episode_id: True,
                 name_prefix='rl-video-{}'.format(id(self))
             )
             self._replay_path = None
-        if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:
-            np_seed = 100 * np.random.randint(1, 1000)
-            if self._seed_api:
-                self._env.seed(self._seed + np_seed)
-            self._action_space.seed(self._seed + np_seed)
-        elif hasattr(self, '_seed'):
-            if self._seed_api:
-                self._env.seed(self._seed)
-            self._action_space.seed(self._seed)
-        obs = self._env.reset()
-        obs = to_ndarray(obs, np.float32)
+        if isinstance(self._env, gym.Env):
+            if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:
+                np_seed = 100 * np.random.randint(1, 1000)
+                if self._seed_api:
+                    self._env.seed(self._seed + np_seed)
+                self._action_space.seed(self._seed + np_seed)
+            elif hasattr(self, '_seed'):
+                if self._seed_api:
+                    self._env.seed(self._seed)
+                self._action_space.seed(self._seed)
+            obs = self._env.reset()
+        elif isinstance(self._env, gymnasium.Env):
+            if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:
+                np_seed = 100 * np.random.randint(1, 1000)
+                self._action_space.seed(self._seed + np_seed)
+                obs = self._env.reset(seed=self._seed + np_seed)
+            elif hasattr(self, '_seed'):
+                self._action_space.seed(self._seed)
+                obs = self._env.reset(seed=self._seed)
+            else:
+                obs = self._env.reset()
+        else:
+            raise RuntimeError("not support env type: {}".format(type(self._env)))
+        if self.observation_space.dtype == np.float32:
+            obs = to_ndarray(obs, dtype=np.float32)
+        else:
+            obs = to_ndarray(obs)
         return obs
 
     # override
     def close(self) -> None:
         try:
             self._env.close()
             del self._env
@@ -101,15 +120,18 @@
 
     # override
     def step(self, action: Union[np.int64, np.ndarray]) -> BaseEnvTimestep:
         action = self._judge_action_type(action)
         if self._cfg.act_scale:
             action = affine_transform(action, min_val=self._env.action_space.low, max_val=self._env.action_space.high)
         obs, rew, done, info = self._env.step(action)
-        obs = to_ndarray(obs, np.float32)
+        if self.observation_space.dtype == np.float32:
+            obs = to_ndarray(obs, dtype=np.float32)
+        else:
+            obs = to_ndarray(obs)
         rew = to_ndarray([rew], np.float32)
         return BaseEnvTimestep(obs, rew, done, info)
 
     def _judge_action_type(self, action: Union[np.ndarray, dict]) -> Union[np.ndarray, dict]:
         if isinstance(action, int):
             return action
         elif isinstance(action, np.int64):
@@ -145,19 +167,19 @@
             raise TypeError(
                 '`random_action` should be either int/np.ndarray or dict of int/np.ndarray, but get {}: {}'.format(
                     type(random_action), random_action
                 )
             )
         return random_action
 
-    def _wrap_env(self) -> None:
+    def _wrap_env(self, caller: str = 'collector') -> None:
         # wrapper_cfgs: Union[str, List]
         wrapper_cfgs = self._cfg.env_wrapper
         if isinstance(wrapper_cfgs, str):
-            wrapper_cfgs = get_default_wrappers(wrapper_cfgs, self._cfg.env_id)
+            wrapper_cfgs = get_default_wrappers(wrapper_cfgs, self._cfg.env_id, caller)
         # self._wrapper_cfgs: List[Union[Callable, Dict]]
         self._wrapper_cfgs = wrapper_cfgs
         for wrapper in self._wrapper_cfgs:
             # wrapper: Union[Callable, Dict]
             if isinstance(wrapper, Dict):
                 self._env = create_env_wrapper(self._env, wrapper)
             else:  # Callable, such as lambda anonymous function
@@ -183,26 +205,28 @@
     def enable_save_replay(self, replay_path: Optional[str] = None) -> None:
         if replay_path is None:
             replay_path = './video'
         self._replay_path = replay_path
 
     @property
     def observation_space(self) -> gym.spaces.Space:
+        if self._observation_space.dtype == np.float64:
+            self._observation_space.dtype = np.float32
         return self._observation_space
 
     @property
     def action_space(self) -> gym.spaces.Space:
         return self._action_space
 
     @property
     def reward_space(self) -> gym.spaces.Space:
         return self._reward_space
 
-    def clone(self) -> BaseEnv:
+    def clone(self, caller: str = 'collector') -> BaseEnv:
         try:
             spec = copy.deepcopy(self._raw_env.spec)
             raw_env = CloudPickleWrapper(self._raw_env)
             raw_env = copy.deepcopy(raw_env).data
             raw_env.__setattr__('spec', spec)
         except Exception:
             raw_env = self._raw_env
-        return DingEnvWrapper(raw_env, self._cfg, self._seed_api)
+        return DingEnvWrapper(raw_env, self._cfg, self._seed_api, caller)
```

### Comparing `DI-engine-0.4.6/ding/envs/env/env_implementation_check.py` & `DI-engine-0.4.7/ding/envs/env/env_implementation_check.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env/tests/demo_env.py` & `DI-engine-0.4.7/ding/envs/env/tests/demo_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env/tests/test_ding_env_wrapper.py` & `DI-engine-0.4.7/ding/envs/env/tests/test_ding_env_wrapper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env/tests/test_env_implementation_check.py` & `DI-engine-0.4.7/ding/envs/env/tests/test_env_implementation_check.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/base_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/base_env_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -475,14 +475,26 @@
         """
         active_env = [i for i, s in self._env_states.items() if s == EnvState.RUN]
         obs = [self._ready_obs[i] for i in active_env]
         if isinstance(obs[0], dict):
             obs = [tnp.array(o) for o in obs]
         return tnp.stack(obs)
 
+    @property
+    def ready_imgs(self, render_mode: Optional[str] = 'rgb_array') -> Dict[int, Any]:
+        """
+        Overview:
+            Get the next ready renderd frame and corresponding env id.
+        Return:
+            - ready_imgs (:obj:`Dict[int, np.ndarray]:`): Dict with env_id keys and rendered frames.
+        """
+        from ding.utils import render
+        assert render_mode in ['rgb_array', 'depth_array']
+        return {i: render(self._envs[i], render_mode) for i in range(self.ready_obs.shape[0])}
+
     def step(self, actions: List[tnp.ndarray]) -> List[tnp.ndarray]:
         """
         Overview:
             Execute env step according to input actions. And reset an env if done.
         Arguments:
             - actions (:obj:`List[tnp.ndarray]`): actions came from outer caller like policy
         Returns:
```

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/env_supervisor.py` & `DI-engine-0.4.7/ding/envs/env_manager/env_supervisor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/envpool_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/envpool_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/gym_vector_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/gym_vector_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/subprocess_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/subprocess_env_manager.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 from typing import Any, Union, List, Tuple, Dict, Callable, Optional
 from multiprocessing import connection, get_context
 from collections import namedtuple
 from ditk import logging
 import platform
 import time
 import copy
+import gymnasium
 import gym
 import traceback
 import torch
 import pickle
 import numpy as np
 import treetensor.numpy as tnp
 from easydict import EasyDict
@@ -100,15 +101,15 @@
             Fork/spawn sub-processes(Call ``_create_env_subprocess``) and create pipes to transfer the data.
         """
         self._env_episode_count = {env_id: 0 for env_id in range(self.env_num)}
         self._ready_obs = {env_id: None for env_id in range(self.env_num)}
         self._reset_param = {i: {} for i in range(self.env_num)}
         if self._shared_memory:
             obs_space = self._observation_space
-            if isinstance(obs_space, gym.spaces.Dict):
+            if isinstance(obs_space, (gym.spaces.Dict, gymnasium.spaces.Dict)):
                 # For multi_agent case, such as multiagent_mujoco and petting_zoo mpe.
                 # Now only for the case that each agent in the team have the same obs structure
                 # and corresponding shape.
                 shape = {k: v.shape for k, v in obs_space.spaces.items()}
                 dtype = {k: v.dtype for k, v in obs_space.spaces.items()}
             else:
                 shape = obs_space.shape
```

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/conftest.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/conftest.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_base_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_base_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_env_supervisor.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_env_supervisor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_envpool_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_envpool_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_gym_vector_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_gym_vector_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_shm.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_shm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_manager/tests/test_subprocess_env_manager.py` & `DI-engine-0.4.7/ding/envs/env_manager/tests/test_subprocess_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/envs/env_wrappers/env_wrappers.py` & `DI-engine-0.4.7/ding/envs/env_wrappers/env_wrappers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,18 @@
 # Borrow a lot from openai baselines:
 # https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py
 
 from typing import Union, List, Tuple
 from easydict import EasyDict
+from functools import reduce
 from collections import deque
 import copy
+import operator
 import gym
+import gymnasium
 import numpy as np
 from torch import float32
 
 from ding.torch_utils import to_ndarray
 from ding.utils import ENV_WRAPPER_REGISTRY, import_module
 '''
 Env Wrapper List:
@@ -24,14 +27,15 @@
     - ObsNormWrapper: Normalize observations according to running mean and std.
     - RewardNormWrapper: Normalize reward according to running std.
     - RamWrapper: Wrap ram env into image-like env
     - EpisodicLifeWrapper: Make end-of-life == end-of-episode, but only reset on true game over.
     - FireResetWrapper: Take fire action at environment reset.
     - GymHybridDictActionWrapper: Transform Gym-Hybrid's original ``gym.spaces.Tuple`` action space
         to ``gym.spaces.Dict``.
+    - FlatObsWrapper: Flatten image and language observation into a vector.
 '''
 
 
 @ENV_WRAPPER_REGISTRY.register('noop_reset')
 class NoopResetWrapper(gym.Wrapper):
     """
     Overview:
@@ -1084,14 +1088,96 @@
             info['time_limit'] = True
         else:
             info['time_limit'] = False
         info['time_count'] = self.time_count
         return obs, reward, done, info
 
 
+class FlatObsWrapper(gym.Wrapper):
+    """
+    Note: only suitable for these envs like minigrid.
+    """
+
+    def __init__(self, env, maxStrLen=96):
+        super().__init__(env)
+
+        self.maxStrLen = maxStrLen
+        self.numCharCodes = 28
+
+        imgSpace = env.observation_space.spaces["image"]
+        imgSize = reduce(operator.mul, imgSpace.shape, 1)
+
+        self.observation_space = gym.spaces.Box(
+            low=0,
+            high=255,
+            shape=(imgSize + self.numCharCodes * self.maxStrLen, ),
+            dtype="float32",
+        )
+
+        self.cachedStr: str = None
+
+    def observation(self, obs):
+        if isinstance(obs, tuple):  # for compatibility of gymnasium
+            obs = obs[0]
+        image = obs["image"]
+        mission = obs["mission"]
+
+        # Cache the last-encoded mission string
+        if mission != self.cachedStr:
+            assert (len(mission) <= self.maxStrLen), f"mission string too long ({len(mission)} chars)"
+            mission = mission.lower()
+
+            strArray = np.zeros(shape=(self.maxStrLen, self.numCharCodes), dtype="float32")
+
+            for idx, ch in enumerate(mission):
+                if ch >= "a" and ch <= "z":
+                    chNo = ord(ch) - ord("a")
+                elif ch == " ":
+                    chNo = ord("z") - ord("a") + 1
+                elif ch == ",":
+                    chNo = ord("z") - ord("a") + 2
+                else:
+                    raise ValueError(f"Character {ch} is not available in mission string.")
+                assert chNo < self.numCharCodes, "%s : %d" % (ch, chNo)
+                strArray[idx, chNo] = 1
+
+            self.cachedStr = mission
+            self.cachedArray = strArray
+
+        obs = np.concatenate((image.flatten(), self.cachedArray.flatten()))
+
+        return obs
+
+    def reset(self, *args, **kwargs):
+        obs = self.env.reset(*args, **kwargs)
+        return self.observation(obs)
+
+    def step(self, *args, **kwargs):
+        o, r, d, i = self.env.step(*args, **kwargs)
+        o = self.observation(o)
+        return o, r, d, i
+
+
+class GymToGymnasiumWrapper(gym.Wrapper):
+
+    def __init__(self, env):
+        assert isinstance(env, gymnasium.Env), type(env)
+        super().__init__(env)
+        self._seed = None
+
+    def seed(self, seed):
+        self._seed = seed
+
+    def reset(self):
+        if self.seed is not None:
+            return self.env.reset(seed=self._seed)
+        else:
+            return self.env.reset()
+
+
 def update_shape(obs_shape, act_shape, rew_shape, wrapper_names):
     """
     Overview:
         Get new shape of observation, acton, and reward given the wrapper.
     Arguments:
         obs_shape (:obj:`Any`), act_shape (:obj:`Any`), rew_shape (:obj:`Any`), wrapper_names (:obj:`Any`)
     Returns:
```

### Comparing `DI-engine-0.4.6/ding/example/c51_nstep.py` & `DI-engine-0.4.7/ding/example/c51_nstep.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/collect_demo_data.py` & `DI-engine-0.4.7/ding/example/collect_demo_data.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/cql.py` & `DI-engine-0.4.7/ding/example/cql.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/d4pg.py` & `DI-engine-0.4.7/ding/example/d4pg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/ddpg.py` & `DI-engine-0.4.7/ding/example/ddpg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn.py` & `DI-engine-0.4.7/ding/example/dqn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn_her.py` & `DI-engine-0.4.7/ding/example/dqn_her.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn_new_env.py` & `DI-engine-0.4.7/ding/example/dqn_new_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn_nstep.py` & `DI-engine-0.4.7/ding/example/dqn_nstep.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn_per.py` & `DI-engine-0.4.7/ding/example/dqn_per.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/dqn_rnd.py` & `DI-engine-0.4.7/ding/example/dqn_rnd.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/iqn_nstep.py` & `DI-engine-0.4.7/ding/example/iqn_nstep.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/pdqn.py` & `DI-engine-0.4.7/ding/example/pdqn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/ppg_offpolicy.py` & `DI-engine-0.4.7/ding/example/ppg_offpolicy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/ppo.py` & `DI-engine-0.4.7/ding/example/ppo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/ppo_offpolicy.py` & `DI-engine-0.4.7/ding/example/ppo_offpolicy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/qrdqn_nstep.py` & `DI-engine-0.4.7/ding/example/qrdqn_nstep.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/r2d2.py` & `DI-engine-0.4.7/ding/example/r2d2.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/sac.py` & `DI-engine-0.4.7/ding/example/sac.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/sqil.py` & `DI-engine-0.4.7/ding/example/sqil.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/sqil_continuous.py` & `DI-engine-0.4.7/ding/example/sqil_continuous.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/sql.py` & `DI-engine-0.4.7/ding/example/sql.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/td3.py` & `DI-engine-0.4.7/ding/example/td3.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/example/trex.py` & `DI-engine-0.4.7/ding/example/trex.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/context.py` & `DI-engine-0.4.7/ding/framework/context.py`

 * *Files 12% similar despite different names*

```diff
@@ -61,21 +61,24 @@
     episodes: List = None
     trajectory_end_idx: List = dataclasses.field(default_factory=list)
     action: Dict = None
     inference_output: Dict = None
     # eval
     eval_value: float = -np.inf
     last_eval_iter: int = -1
+    last_eval_value: int = -np.inf
     eval_output: List = dataclasses.field(default_factory=dict)
+    # wandb
+    wandb_url: str = ""
 
     def __post_init__(self):
         # This method is called just after __init__ method. Here, concretely speaking,
         # this method is called just after the object initialize its fields.
         # We use this method here to keep the fields needed for each iteration.
-        self.keep('env_step', 'env_episode', 'train_iter', 'last_eval_iter')
+        self.keep('env_step', 'env_episode', 'train_iter', 'last_eval_iter', 'last_eval_value', 'wandb_url')
 
 
 @dataclasses.dataclass
 class OfflineRLContext(Context):
 
     # common
     total_step: int = 0
@@ -83,13 +86,15 @@
     train_iter: int = 0
     train_data: Union[Dict, List] = None
     train_output: Union[Dict, List[Dict]] = None
     # eval
     eval_value: float = -np.inf
     last_eval_iter: int = -1
     eval_output: List = dataclasses.field(default_factory=dict)
+    # wandb
+    wandb_url: str = ""
 
     def __post_init__(self):
         # This method is called just after __init__ method. Here, concretely speaking,
         # this method is called just after the object initialize its fields.
         # We use this method here to keep the fields needed for each iteration.
-        self.keep('train_iter', 'last_eval_iter')
+        self.keep('train_iter', 'last_eval_iter', 'wandb_url')
```

### Comparing `DI-engine-0.4.6/ding/framework/event_loop.py` & `DI-engine-0.4.7/ding/framework/event_loop.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/message_queue/mq.py` & `DI-engine-0.4.7/ding/framework/message_queue/mq.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/message_queue/nng.py` & `DI-engine-0.4.7/ding/framework/message_queue/nng.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/message_queue/redis.py` & `DI-engine-0.4.7/ding/framework/message_queue/redis.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/ckpt_handler.py` & `DI-engine-0.4.7/ding/framework/middleware/ckpt_handler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/collector.py` & `DI-engine-0.4.7/ding/framework/middleware/collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/distributer.py` & `DI-engine-0.4.7/ding/framework/middleware/distributer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/__init__.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/advantage_estimator.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/advantage_estimator.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,15 +39,15 @@
                 which should be shorter than the length of `ctx.trajectories`.
         Output of ctx:
             - train_data (:obj:`List[treetensor.torch.Tensor]`): The processed data if `buffer_` is None.
         """
         data = ctx.trajectories  # list
         data = ttorch_collate(data)
         with torch.no_grad():
-            if cfg.policy.get("cuda", False):
+            if cfg.policy.cuda:
                 data = data.cuda()
             value = model.forward(data.obs, mode='compute_critic')['value']
             next_value = model.forward(data.next_obs, mode='compute_critic')['value']
             data.value = value
 
         traj_flag = data.done.clone()
         traj_flag[ctx.trajectory_end_idx] = True
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/collector.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,8 @@
 from typing import TYPE_CHECKING, Callable, List, Tuple, Any
-from easydict import EasyDict
 from functools import reduce
 import treetensor.torch as ttorch
 from ding.envs import BaseEnvManager
 from ding.policy import Policy
 from ding.torch_utils import to_ndarray, get_shape0
 
 if TYPE_CHECKING:
@@ -46,15 +45,15 @@
 
 
 def inferencer(seed: int, policy: Policy, env: BaseEnvManager) -> Callable:
     """
     Overview:
         The middleware that executes the inference process.
     Arguments:
-        - seed (:int:`int`): Random seed.
+        - seed (:obj:`int`): Random seed.
         - policy (:obj:`Policy`): The policy to be inferred.
         - env (:obj:`BaseEnvManager`): The env where the inference process is performed. \
             The env.ready_obs (:obj:`tnp.array`) will be used as model input.
     """
 
     env.seed(seed)
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/ctx_helper.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/ctx_helper.py`

 * *Files 8% similar despite different names*

```diff
@@ -7,19 +7,21 @@
     from ding.framework import Context
 
 
 def final_ctx_saver(name: str) -> Callable:
 
     def _save(ctx: "Context"):
         if task.finish:
+            # make sure the items to be recorded are all kept in the context
             with open(os.path.join(name, 'result.pkl'), 'wb') as f:
                 final_data = {
                     'total_step': ctx.total_step,
                     'train_iter': ctx.train_iter,
-                    'eval_value': ctx.eval_value,
+                    'last_eval_iter': ctx.last_eval_iter,
+                    'eval_value': ctx.last_eval_value,
                 }
                 if ctx.has_attr('env_step'):
                     final_data['env_step'] = ctx.env_step
                     final_data['env_episode'] = ctx.env_episode
                 pickle.dump(final_data, f)
 
     return _save
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/data_processor.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/data_processor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/enhancer.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/enhancer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/evaluator.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/evaluator.py`

 * *Files 0% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 import treetensor.torch as ttorch
 from easydict import EasyDict
 from ding.envs import BaseEnvManager
 from ding.framework.context import Context, OfflineRLContext, OnlineRLContext
 from ding.policy import Policy
 from ding.data import Dataset, DataLoader
 from ding.framework import task
-from ding.torch_utils import tensor_to_list, to_list, to_ndarray, get_shape0
+from ding.torch_utils import to_ndarray, get_shape0
 from ding.utils import lists_to_dicts
 
 
 class IMetric(ABC):
 
     @abstractmethod
     def eval(self, inputs: Any, label: Any) -> dict:
@@ -278,15 +278,16 @@
             )
         elif isinstance(ctx, OfflineRLContext):
             logging.info('Evaluation: Train Iter({})\tEval Reward({:.3f})'.format(ctx.train_iter, episode_return))
         else:
             raise TypeError("not supported ctx type: {}".format(type(ctx)))
         ctx.last_eval_iter = ctx.train_iter
         ctx.eval_value = episode_return
-        ctx.eval_output = {'reward': episode_return}
+        ctx.last_eval_value = ctx.eval_value
+        ctx.eval_output = {'episode_return': episode_return}
         episode_info = eval_monitor.get_episode_info()
         if episode_info is not None:
             ctx.eval_output['episode_info'] = episode_info
         if render:
             ctx.eval_output['replay_video'] = eval_monitor.get_episode_video()
             ctx.eval_output['output'] = eval_monitor.get_episode_output()
         else:
@@ -301,15 +302,15 @@
 def interaction_evaluator_ttorch(
         seed: int,
         policy: Policy,
         env: BaseEnvManager,
         n_evaluator_episode: Optional[int] = None,
         stop_value: float = np.inf,
         eval_freq: int = 1000,
-        render: bool = False
+        render: bool = False,
 ) -> Callable:
     """
     Overview:
         The middleware that executes the evaluation with ttorch data.
     Arguments:
         - policy (:obj:`Policy`): The policy to be evaluated.
         - env (:obj:`BaseEnvManager`): The env for the evaluation.
@@ -349,15 +350,15 @@
         while not eval_monitor.is_finished():
             obs = ttorch.as_tensor(env.ready_obs).to(dtype=ttorch.float32)
             obs = obs.to(device)
             inference_output = policy.eval(obs)
             inference_output = inference_output.cpu()
             if render:
                 eval_monitor.update_video(env.ready_imgs)
-                eval_monitor.update_output(inference_output)
+                # eval_monitor.update_output(inference_output)
             action = inference_output.action.numpy()
             timesteps = env.step(action)
             for timestep in timesteps:
                 env_id = timestep.env_id.item()
                 if timestep.done:
                     policy.reset([env_id])
                     reward = timestep.info.eval_episode_return
@@ -370,14 +371,15 @@
         logging.info(
             'Evaluation: Train Iter({})\tEnv Step({})\tMean Episode Return({:.3f})'.format(
                 ctx.train_iter, ctx.env_step, episode_return_mean
             )
         )
         ctx.last_eval_iter = ctx.train_iter
         ctx.eval_value = episode_return_mean
+        ctx.last_eval_value = ctx.eval_value
         ctx.eval_output = {'episode_return': episode_return}
         episode_info = eval_monitor.get_episode_info()
         if episode_info is not None:
             ctx.eval_output['episode_info'] = episode_info
         if render:
             ctx.eval_output['replay_video'] = eval_monitor.get_episode_video()
             ctx.eval_output['output'] = eval_monitor.get_episode_output()
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/explorer.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/explorer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/logger.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/logger.py`

 * *Files 16% similar despite different names*

```diff
@@ -117,164 +117,231 @@
                 else:
                     writer.add_scalar('basic/train_{}-train_iter'.format(k), v, ctx.train_iter)
 
     return _logger
 
 
 def wandb_online_logger(
-        record_path: str,
-        cfg: Union[str, EasyDict] = 'default',
+        record_path: str = None,
+        cfg: Union[dict, EasyDict] = None,
         metric_list: Optional[List[str]] = None,
         env: Optional[BaseEnvManagerV2] = None,
         model: Optional[torch.nn.Module] = None,
-        anonymous: bool = False
+        anonymous: bool = False,
+        project_name: str = 'default-project',
 ) -> Callable:
     '''
     Overview:
         Wandb visualizer to track the experiment.
     Arguments:
         - record_path (:obj:`str`): The path to save the replay of simulation.
-        - cfg (:obj:`Union[str, EasyDict]`): Config, a dict of following settings:
+        - cfg (:obj:`Union[dict, EasyDict]`): Config, a dict of following settings:
             - gradient_logger: boolean. Whether to track the gradient.
             - plot_logger: boolean. Whether to track the metrics like reward and loss.
-            - action_logger: `q_value` or `action probability`.
+            - video_logger: boolean. Whether to upload the rendering video replay.
+            - action_logger: boolean. `q_value` or `action probability`.
+            - return_logger: boolean. Whether to track the return value.
         - metric_list (:obj:`Optional[List[str]]`): Logged metric list, specialized by different policies.
         - env (:obj:`BaseEnvManagerV2`): Evaluator environment.
         - model (:obj:`nn.Module`): Policy neural network model.
         - anonymous (:obj:`bool`): Open the anonymous mode of wandb or not.
             The anonymous mode allows visualization of data without wandb count.
+        - project_name (:obj:`str`): The name of wandb project.
     '''
     if task.router.is_active and not task.has_role(task.role.LEARNER):
         return task.void()
     color_list = ["orange", "red", "blue", "purple", "green", "darkcyan"]
     if metric_list is None:
-        metric_list = ["q_value", "target q_value", "loss", "lr", "entropy"]
+        metric_list = ["q_value", "target q_value", "loss", "lr", "entropy", "target_q_value", "td_error"]
     # Initialize wandb with default settings
     # Settings can be covered by calling wandb.init() at the top of the script
     if anonymous:
-        wandb.init(anonymous="must")
+        wandb.init(project=project_name, reinit=True, anonymous="must")
     else:
-        wandb.init()
-    if cfg == 'default':
+        wandb.init(project=project_name, reinit=True)
+    if cfg is None:
         cfg = EasyDict(
             dict(
                 gradient_logger=False,
                 plot_logger=True,
                 video_logger=False,
                 action_logger=False,
                 return_logger=False,
             )
         )
+    else:
+        if not isinstance(cfg, EasyDict):
+            cfg = EasyDict(cfg)
+        assert set(cfg.keys()
+                   ) == set(["gradient_logger", "plot_logger", "video_logger", "action_logger", "return_logger"])
+        assert all(value in [True, False] for value in cfg.values())
+
     # The visualizer is called to save the replay of the simulation
     # which will be uploaded to wandb later
-    if env is not None:
+    if env is not None and cfg.video_logger is True and record_path is not None:
         env.enable_save_replay(replay_path=record_path)
     if cfg.gradient_logger:
         wandb.watch(model)
     else:
         one_time_warning(
             "If you want to use wandb to visualize the gradient, please set gradient_logger = True in the config."
         )
 
+    first_plot = True
+
     def _plot(ctx: "OnlineRLContext"):
-        if not cfg.plot_logger:
+        nonlocal first_plot
+        if first_plot:
+            first_plot = False
+            ctx.wandb_url = wandb.run.get_project_url()
+
+        info_for_logging = {}
+
+        if cfg.plot_logger:
+            for metric in metric_list:
+                if metric in ctx.train_output[0]:
+                    metric_value_list = []
+                    for item in ctx.train_output:
+                        if isinstance(item[metric], torch.Tensor):
+                            metric_value_list.append(item[metric].cpu().detach().numpy())
+                        else:
+                            metric_value_list.append(item[metric])
+                    metric_value = np.mean(metric_value_list)
+                    info_for_logging.update({metric: metric_value})
+        else:
             one_time_warning(
                 "If you want to use wandb to visualize the result, please set plot_logger = True in the config."
             )
-            return
-        for metric in metric_list:
-            if metric in ctx.train_output[0]:
-                metric_value = np.mean([item[metric] for item in ctx.train_output])
-                wandb.log({metric: metric_value})
 
         if ctx.eval_value != -np.inf:
-            wandb.log({"reward": ctx.eval_value, "train iter": ctx.train_iter, "env step": ctx.env_step})
+            info_for_logging.update(
+                {
+                    "episode return mean": ctx.eval_value,
+                    "train iter": ctx.train_iter,
+                    "env step": ctx.env_step
+                }
+            )
 
             eval_output = ctx.eval_output['output']
             episode_return = ctx.eval_output['episode_return']
             episode_return = np.array(episode_return)
             if len(episode_return.shape) == 2:
                 episode_return = episode_return.squeeze(1)
 
             if cfg.video_logger:
                 file_list = []
                 for p in os.listdir(record_path):
                     if os.path.splitext(p)[-1] == ".mp4":
                         file_list.append(p)
                 file_list.sort(key=lambda fn: os.path.getmtime(os.path.join(record_path, fn)))
                 video_path = os.path.join(record_path, file_list[-2])
-                wandb.log({"video": wandb.Video(video_path, format="mp4")})
+                info_for_logging.update({"video": wandb.Video(video_path, format="mp4")})
 
-            action_path = os.path.join(record_path, (str(ctx.env_step) + "_action.gif"))
-            return_path = os.path.join(record_path, (str(ctx.env_step) + "_return.gif"))
-            if cfg.action_logger in ['q_value', 'action probability']:
-                if isinstance(eval_output, tnp.ndarray):
-                    action_prob = softmax(eval_output.logit)
-                else:
-                    action_prob = [softmax(to_ndarray(v['logit'])) for v in eval_output]
-                fig, ax = plt.subplots()
-                plt.ylim([-1, 1])
-                action_dim = len(action_prob[1])
-                x_range = [str(x + 1) for x in range(action_dim)]
-                ln = ax.bar(x_range, [0 for x in range(action_dim)], color=color_list[:action_dim])
-                ani = animation.FuncAnimation(
-                    fig, action_prob, fargs=(action_prob, ln), blit=True, save_count=len(action_prob)
-                )
-                ani.save(action_path, writer='pillow')
-                wandb.log({cfg.action_logger: wandb.Video(action_path, format="gif")})
-                plt.clf()
+            if cfg.action_logger:
+                action_path = os.path.join(record_path, (str(ctx.env_step) + "_action.gif"))
+                if all(['logit' in v for v in eval_output]) or hasattr(eval_output, "logit"):
+                    if isinstance(eval_output, tnp.ndarray):
+                        action_prob = softmax(eval_output.logit)
+                    else:
+                        action_prob = [softmax(to_ndarray(v['logit'])) for v in eval_output]
+                    fig, ax = plt.subplots()
+                    plt.ylim([-1, 1])
+                    action_dim = len(action_prob[1])
+                    x_range = [str(x + 1) for x in range(action_dim)]
+                    ln = ax.bar(x_range, [0 for x in range(action_dim)], color=color_list[:action_dim])
+                    ani = animation.FuncAnimation(
+                        fig, action_prob, fargs=(action_prob, ln), blit=True, save_count=len(action_prob)
+                    )
+                    ani.save(action_path, writer='pillow')
+                    info_for_logging.update({"action": wandb.Video(action_path, format="gif")})
+
+                elif all(['action' in v for v in eval_output[0]]):
+                    for i, action_trajectory in enumerate(eval_output):
+                        fig, ax = plt.subplots()
+                        fig_data = np.array([[i + 1, *v['action']] for i, v in enumerate(action_trajectory)])
+                        steps = fig_data[:, 0]
+                        actions = fig_data[:, 1:]
+                        plt.ylim([-1, 1])
+                        for j in range(actions.shape[1]):
+                            ax.scatter(steps, actions[:, j])
+                        info_for_logging.update({"actions_of_trajectory_{}".format(i): fig})
 
             if cfg.return_logger:
+                return_path = os.path.join(record_path, (str(ctx.env_step) + "_return.gif"))
                 fig, ax = plt.subplots()
                 ax = plt.gca()
                 ax.set_ylim([0, 1])
                 hist, x_dim = return_distribution(episode_return)
                 assert len(hist) == len(x_dim)
                 ln_return = ax.bar(x_dim, hist, width=1, color='r', linewidth=0.7)
                 ani = animation.FuncAnimation(fig, return_prob, fargs=(hist, ln_return), blit=True, save_count=1)
                 ani.save(return_path, writer='pillow')
-                wandb.log({"return distribution": wandb.Video(return_path, format="gif")})
+                info_for_logging.update({"return distribution": wandb.Video(return_path, format="gif")})
+
+        if bool(info_for_logging):
+            wandb.log(data=info_for_logging, step=ctx.env_step)
+        plt.clf()
 
     return _plot
 
 
 def wandb_offline_logger(
-        cfg: EasyDict,
-        env: BaseEnvManagerV2,
-        model: torch.nn.Module,
-        datasetpath: str,
-        anonymous: bool = False
+        dataset_path: str,
+        record_path: str = None,
+        cfg: Union[dict, EasyDict] = None,
+        metric_list: Optional[List[str]] = None,
+        env: Optional[BaseEnvManagerV2] = None,
+        model: Optional[torch.nn.Module] = None,
+        anonymous: bool = False,
+        project_name: str = 'default-project',
 ) -> Callable:
     '''
     Overview:
         Wandb visualizer to track the experiment.
     Arguments:
-        - cfg (:obj:`EasyDict`): Config, a dict of following settings:
-            - record_path: string. The path to save the replay of simulation.
+        - datasetpath (:obj:`str`): The path to save the replay of simulation.
+        - record_path (:obj:`str`): The path to save the replay of simulation.
+        - cfg (:obj:`Union[dict, EasyDict]`): Config, a dict of following settings:
             - gradient_logger: boolean. Whether to track the gradient.
             - plot_logger: boolean. Whether to track the metrics like reward and loss.
-            - action_logger: `q_value` or `action probability`.
+            - video_logger: boolean. Whether to upload the rendering video replay.
+            - action_logger: boolean. `q_value` or `action probability`.
+            - return_logger: boolean. Whether to track the return value.
+        - metric_list (:obj:`Optional[List[str]]`): Logged metric list, specialized by different policies.
         - env (:obj:`BaseEnvManagerV2`): Evaluator environment.
-        - model (:obj:`nn.Module`): Model.
-        - datasetpath (:obj:`str`): The path of offline dataset.
+        - model (:obj:`nn.Module`): Policy neural network model.
         - anonymous (:obj:`bool`): Open the anonymous mode of wandb or not.
             The anonymous mode allows visualization of data without wandb count.
+        - project_name (:obj:`str`): The name of wandb project.
     '''
-
+    if task.router.is_active and not task.has_role(task.role.LEARNER):
+        return task.void()
     color_list = ["orange", "red", "blue", "purple", "green", "darkcyan"]
-    metric_list = ["q_value", "target q_value", "loss", "lr", "entropy", "target_q_value", "td_error"]
+    if metric_list is None:
+        metric_list = ["q_value", "target q_value", "loss", "lr", "entropy", "target_q_value", "td_error"]
     # Initialize wandb with default settings
     # Settings can be covered by calling wandb.init() at the top of the script
     if anonymous:
         wandb.init(anonymous="must")
     else:
         wandb.init()
+    if cfg == 'default':
+        cfg = EasyDict(
+            dict(
+                gradient_logger=False,
+                plot_logger=True,
+                video_logger=False,
+                action_logger=False,
+                return_logger=False,
+            )
+        )
     # The visualizer is called to save the replay of the simulation
     # which will be uploaded to wandb later
-    env.enable_save_replay(replay_path=cfg.record_path)
+    if env is not None:
+        env.enable_save_replay(replay_path=record_path)
     if cfg.gradient_logger:
         wandb.watch(model)
     else:
         one_time_warning(
             "If you want to use wandb to visualize the gradient, please set gradient_logger = True in the config."
         )
 
@@ -320,66 +387,97 @@
         axes[1].set_title('state-action')
         axes[2].set_title('stateAction-reward')
         plt.savefig('dataset.png')
 
         wandb.log({"dataset": wandb.Image("dataset.png")})
 
     if cfg.vis_dataset is True:
-        _vis_dataset(datasetpath)
+        _vis_dataset(dataset_path)
+
+    def _plot(ctx: "OnlineRLContext"):
+        info_for_logging = {}
 
-    def _plot(ctx: "OfflineRLContext"):
         if not cfg.plot_logger:
             one_time_warning(
                 "If you want to use wandb to visualize the result, please set plot_logger = True in the config."
             )
             return
         for metric in metric_list:
-            if metric in ctx.train_output:
-                metric_value = ctx.train_output[metric]
-                wandb.log({metric: metric_value})
+            if metric in ctx.train_output[0]:
+                metric_value_list = []
+                for item in ctx.train_output:
+                    if isinstance(item[metric], torch.Tensor):
+                        metric_value_list.append(item[metric].cpu().detach().numpy())
+                    else:
+                        metric_value_list.append(item[metric])
+                metric_value = np.mean(metric_value_list)
+                info_for_logging.update({metric: metric_value})
 
         if ctx.eval_value != -np.inf:
-            wandb.log({"reward": ctx.eval_value, "train iter": ctx.train_iter})
+            info_for_logging.update(
+                {
+                    "episode return mean": ctx.eval_value,
+                    "train iter": ctx.train_iter,
+                    "env step": ctx.env_step
+                }
+            )
 
             eval_output = ctx.eval_output['output']
             episode_return = ctx.eval_output['episode_return']
-            if 'logit' in eval_output[0]:
-                action_value = [to_ndarray(F.softmax(v['logit'], dim=-1)) for v in eval_output]
+            episode_return = np.array(episode_return)
+            if len(episode_return.shape) == 2:
+                episode_return = episode_return.squeeze(1)
+
+            if cfg.video_logger:
+                file_list = []
+                for p in os.listdir(record_path):
+                    if os.path.splitext(p)[-1] == ".mp4":
+                        file_list.append(p)
+                file_list.sort(key=lambda fn: os.path.getmtime(os.path.join(record_path, fn)))
+                video_path = os.path.join(record_path, file_list[-2])
+                info_for_logging.update({"video": wandb.Video(video_path, format="mp4")})
 
-            file_list = []
-            for p in os.listdir(cfg.record_path):
-                if os.path.splitext(p)[-1] == ".mp4":
-                    file_list.append(p)
-            file_list.sort(key=lambda fn: os.path.getmtime(os.path.join(cfg.record_path, fn)))
-
-            video_path = os.path.join(cfg.record_path, file_list[-2])
-            action_path = os.path.join(cfg.record_path, (str(ctx.train_iter) + "_action.gif"))
-            return_path = os.path.join(cfg.record_path, (str(ctx.train_iter) + "_return.gif"))
-            if cfg.action_logger in ['q_value', 'action probability']:
+            action_path = os.path.join(record_path, (str(ctx.env_step) + "_action.gif"))
+            return_path = os.path.join(record_path, (str(ctx.env_step) + "_return.gif"))
+            if cfg.action_logger:
+                if all(['logit' in v for v in eval_output]) or hasattr(eval_output, "logit"):
+                    if isinstance(eval_output, tnp.ndarray):
+                        action_prob = softmax(eval_output.logit)
+                    else:
+                        action_prob = [softmax(to_ndarray(v['logit'])) for v in eval_output]
+                    fig, ax = plt.subplots()
+                    plt.ylim([-1, 1])
+                    action_dim = len(action_prob[1])
+                    x_range = [str(x + 1) for x in range(action_dim)]
+                    ln = ax.bar(x_range, [0 for x in range(action_dim)], color=color_list[:action_dim])
+                    ani = animation.FuncAnimation(
+                        fig, action_prob, fargs=(action_prob, ln), blit=True, save_count=len(action_prob)
+                    )
+                    ani.save(action_path, writer='pillow')
+                    info_for_logging.update({"action": wandb.Video(action_path, format="gif")})
+
+                elif all(['action' in v for v in eval_output[0]]):
+                    for i, action_trajectory in enumerate(eval_output):
+                        fig, ax = plt.subplots()
+                        fig_data = np.array([[i + 1, *v['action']] for i, v in enumerate(action_trajectory)])
+                        steps = fig_data[:, 0]
+                        actions = fig_data[:, 1:]
+                        plt.ylim([-1, 1])
+                        for j in range(actions.shape[1]):
+                            ax.scatter(steps, actions[:, j])
+                        info_for_logging.update({"actions_of_trajectory_{}".format(i): fig})
+
+            if cfg.return_logger:
                 fig, ax = plt.subplots()
-                plt.ylim([-1, 1])
-                action_dim = len(action_value[0])
-                x_range = [str(x + 1) for x in range(action_dim)]
-                ln = ax.bar(x_range, [0 for x in range(action_dim)], color=color_list[:action_dim])
-                ani = animation.FuncAnimation(
-                    fig, action_prob, fargs=(action_value, ln), blit=True, save_count=len(action_value)
-                )
-                ani.save(action_path, writer='pillow')
-                wandb.log({cfg.action_logger: wandb.Video(action_path, format="gif")})
-                plt.clf()
-
-            fig, ax = plt.subplots()
-            ax = plt.gca()
-            ax.set_ylim([0, 1])
-            hist, x_dim = return_distribution(episode_return)
-            assert len(hist) == len(x_dim)
-            ln_return = ax.bar(x_dim, hist, width=1, color='r', linewidth=0.7)
-            ani = animation.FuncAnimation(fig, return_prob, fargs=(hist, ln_return), blit=True, save_count=1)
-            ani.save(return_path, writer='pillow')
-            wandb.log(
-                {
-                    "video": wandb.Video(video_path, format="mp4"),
-                    "return distribution": wandb.Video(return_path, format="gif")
-                }
-            )
+                ax = plt.gca()
+                ax.set_ylim([0, 1])
+                hist, x_dim = return_distribution(episode_return)
+                assert len(hist) == len(x_dim)
+                ln_return = ax.bar(x_dim, hist, width=1, color='r', linewidth=0.7)
+                ani = animation.FuncAnimation(fig, return_prob, fargs=(hist, ln_return), blit=True, save_count=1)
+                ani.save(return_path, writer='pillow')
+                info_for_logging.update({"return distribution": wandb.Video(return_path, format="gif")})
+
+        wandb.log(data=info_for_logging, step=ctx.env_step)
+        plt.clf()
 
     return _plot
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/termination_checker.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/termination_checker.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/timer.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/timer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/functional/trainer.py` & `DI-engine-0.4.7/ding/framework/middleware/functional/trainer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/learner.py` & `DI-engine-0.4.7/ding/framework/middleware/learner.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,15 +48,14 @@
     def __call__(self, ctx: "OnlineRLContext") -> None:
         """
         Output of ctx:
             - train_output (:obj:`Deque`): The training output in deque.
         """
         train_output_queue = []
         for _ in range(self.cfg.policy.learn.update_per_collect):
-            print('train_iter', ctx.train_iter)
             self._fetcher(ctx)
             if ctx.train_data is None:
                 break
             if self._reward_estimator:
                 self._reward_estimator(ctx)
             self._trainer(ctx)
             train_output_queue.append(ctx.train_output)
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/mock_for_test.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/mock_for_test.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_advantage_estimator.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_advantage_estimator.py`

 * *Files 1% similar despite different names*

```diff
@@ -58,15 +58,15 @@
     traj_flag[ctx.trajectory_end_idx] = True
     ctx.trajectories_copy.traj_flag = traj_flag
 
     with patch("ding.policy.Policy", MockPolicy):
         gae_estimator(cfg, MockPolicy(TheModelClass()), buffer)(ctx)
 
     if buffer is not None:
-        train_data = [d.data for d in list(buffer.storage)]
+        train_data = [d.data for d in buffer.export_data()]
         for d in train_data:
             d.logit = d.logit[0]
             d.next_obs = d.next_obs[0]
             d.obs = d.obs[0]
         ctx.train_data = ttorch_collate(train_data)
 
     assert ctx.trajectories is None
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_ckpt_handler.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_ckpt_handler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_collector.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_data_processor.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_data_processor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_distributer.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_distributer.py`

 * *Files 2% similar despite different names*

```diff
@@ -100,15 +100,15 @@
         finally:
             storage_loader.shutdown()
             sleep(1)
             if path.exists(tempdir):
                 shutil.rmtree(tempdir)
 
 
-@pytest.mark.unittest
+@pytest.mark.tmp
 def test_context_exchanger_with_storage_loader():
     Parallel.runner(n_parallel_workers=2)(context_exchanger_with_storage_loader_main)
 
 
 class MockPolicy:
 
     def __init__(self) -> None:
@@ -214,10 +214,10 @@
         finally:
             model_loader.shutdown()
             sleep(0.3)
             if path.exists(tempdir):
                 shutil.rmtree(tempdir)
 
 
-@pytest.mark.unittest
+@pytest.mark.tmp
 def test_model_exchanger_with_model_loader():
     Parallel.runner(n_parallel_workers=2, startup_interval=0)(model_exchanger_main_with_model_loader)
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_enhancer.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_enhancer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_evaluator.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_explorer.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_explorer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_logger.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_logger.py`

 * *Files 10% similar despite different names*

```diff
@@ -188,67 +188,74 @@
 
     def __getitem__(self, index):
         return [[1]] * 50
 
 
 @pytest.mark.unittest
 def test_wandb_online_logger():
-
+    record_path = './video_qbert_dqn'
     cfg = EasyDict(
         dict(
-            record_path='./video_qbert_dqn',
             gradient_logger=True,
             plot_logger=True,
-            action_logger='action probability',
+            action_logger=True,
             return_logger=True,
             video_logger=True,
         )
     )
     env = TheEnvClass()
     ctx = OnlineRLContext()
     ctx.train_output = [{'reward': 1, 'q_value': [1.0]}]
     model = TheModelClass()
     wandb.init(config=cfg, anonymous="must")
 
-    def mock_metric_logger(metric_dict):
+    def mock_metric_logger(metric_dict, step):
         metric_list = [
-            "q_value", "target q_value", "loss", "lr", "entropy", "reward", "q value", "video", "q value distribution",
-            "train iter"
+            "q_value",
+            "target q_value",
+            "loss",
+            "lr",
+            "entropy",
+            "reward",
+            "q value",
+            "video",
+            "q value distribution",
+            "train iter",
+            "episode return mean",
+            "env step",
+            "action",
+            "actions_of_trajectory_0",
+            "actions_of_trajectory_1",
+            "actions_of_trajectory_2",
+            "actions_of_trajectory_3",
+            "return distribution",
         ]
-        assert set(metric_dict.keys()) < set(metric_list)
+        assert set(metric_dict.keys()) <= set(metric_list)
 
     def mock_gradient_logger(input_model):
         assert input_model == model
 
     def test_wandb_online_logger_metric():
         with patch.object(wandb, 'log', new=mock_metric_logger):
-            wandb_online_logger(cfg.record_path, cfg, env=env, model=model, anonymous=True)(ctx)
+            wandb_online_logger(record_path, cfg, env=env, model=model, anonymous=True)(ctx)
 
     def test_wandb_online_logger_gradient():
         with patch.object(wandb, 'watch', new=mock_gradient_logger):
-            wandb_online_logger(cfg.record_path, cfg, env=env, model=model, anonymous=True)(ctx)
+            wandb_online_logger(record_path, cfg, env=env, model=model, anonymous=True)(ctx)
 
     test_wandb_online_logger_metric()
     test_wandb_online_logger_gradient()
 
 
 # @pytest.mark.unittest
 # TODO(nyz): fix CI bug when py=3.8.15
 @pytest.mark.tmp
 def test_wandb_offline_logger(mocker):
-
-    cfg = EasyDict(
-        dict(
-            record_path='./video_pendulum_cql',
-            gradient_logger=True,
-            plot_logger=True,
-            action_logger='action probability',
-            vis_dataset=True
-        )
-    )
+    record_path = './video_pendulum_cql'
+    cfg = EasyDict(dict(gradient_logger=True, plot_logger=True, action_logger=True, vis_dataset=True))
     env = TheEnvClass()
     ctx = OnlineRLContext()
     ctx.train_output = [{'reward': 1, 'q_value': [1.0]}]
     model = TheModelClass()
     wandb.init(config=cfg, anonymous="must")
 
     def mock_metric_logger(metric_dict):
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware/tests/test_trainer.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_trainer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/ckpt_handler.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/ckpt_handler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/collector.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/__init__.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/advantage_estimator.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/advantage_estimator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/collector.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/ctx_helper.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/ctx_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/data_processor.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/data_processor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/enhancer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/enhancer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/evaluator.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/explorer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/explorer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/termination_checker.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/termination_checker.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/functional/trainer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/functional/trainer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/learner.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/learner.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/mock_for_test.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/mock_for_test.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_advantage_estimator.py` & `DI-engine-0.4.7/ding/framework/middleware/tests/test_advantage_estimator.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,15 +29,15 @@
         self._model = model
 
     def get_attribute(self, name: str) -> Any:
         return self._model
 
 
 def call_gae_estimator(batch_size: int = 32, trajectory_end_idx_size: int = 5, buffer: Optional[Buffer] = None):
-    cfg = EasyDict({'policy': {'collect': {'discount_factor': 0.9, 'gae_lambda': 0.95}}})
+    cfg = EasyDict({'policy': {'collect': {'discount_factor': 0.9, 'gae_lambda': 0.95}, 'cuda': False}})
 
     ctx = OnlineRLContext()
     assert trajectory_end_idx_size <= batch_size
 
     ctx.trajectory_end_idx = treetensor.torch.randint(low=0, high=batch_size, size=(trajectory_end_idx_size, ))
     ctx.trajectories = [
         treetensor.torch.Tensor(
@@ -58,15 +58,15 @@
     traj_flag[ctx.trajectory_end_idx] = True
     ctx.trajectories_copy.traj_flag = traj_flag
 
     with patch("ding.policy.Policy", MockPolicy):
         gae_estimator(cfg, MockPolicy(TheModelClass()), buffer)(ctx)
 
     if buffer is not None:
-        train_data = [d.data for d in buffer.export_data()]
+        train_data = [d.data for d in list(buffer.storage)]
         for d in train_data:
             d.logit = d.logit[0]
             d.next_obs = d.next_obs[0]
             d.obs = d.obs[0]
         ctx.train_data = ttorch_collate(train_data)
 
     assert ctx.trajectories is None
```

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_ckpt_handler.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_ckpt_handler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_collector.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_data_processor.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_data_processor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_enhancer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_enhancer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_evaluator.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_explorer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_explorer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/middleware_v3/tests/test_trainer.py` & `DI-engine-0.4.7/ding/framework/middleware_v3/tests/test_trainer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/parallel.py` & `DI-engine-0.4.7/ding/framework/parallel.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/supervisor.py` & `DI-engine-0.4.7/ding/framework/supervisor.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/framework/task.py` & `DI-engine-0.4.7/ding/framework/task.py`

 * *Files 0% similar despite different names*

```diff
@@ -63,15 +63,15 @@
 
     def __call__(self, _):
         return
 
 
 class Task:
     """
-    Tash will manage the execution order of the entire pipeline, register new middleware,
+    Task will manage the execution order of the entire pipeline, register new middleware,
     and generate new context objects.
     """
     role = Role
 
     def __init__(self) -> None:
         self.router = Parallel()
         self._finish = False
```

### Comparing `DI-engine-0.4.6/ding/framework/wrapper/step_timer.py` & `DI-engine-0.4.7/ding/framework/wrapper/step_timer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/hpc_rl/wrapper.py` & `DI-engine-0.4.7/ding/hpc_rl/wrapper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/base/app.py` & `DI-engine-0.4.7/ding/interaction/base/app.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/base/common.py` & `DI-engine-0.4.7/ding/interaction/base/common.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/base/network.py` & `DI-engine-0.4.7/ding/interaction/base/network.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/base/threading.py` & `DI-engine-0.4.7/ding/interaction/base/threading.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/exception/__init__.py` & `DI-engine-0.4.7/ding/interaction/exception/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/exception/base.py` & `DI-engine-0.4.7/ding/interaction/exception/base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/exception/master.py` & `DI-engine-0.4.7/ding/interaction/exception/master.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/exception/slave.py` & `DI-engine-0.4.7/ding/interaction/exception/slave.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/master/connection.py` & `DI-engine-0.4.7/ding/interaction/master/connection.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/master/master.py` & `DI-engine-0.4.7/ding/interaction/master/master.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/master/task.py` & `DI-engine-0.4.7/ding/interaction/master/task.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/slave/action.py` & `DI-engine-0.4.7/ding/interaction/slave/action.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/slave/slave.py` & `DI-engine-0.4.7/ding/interaction/slave/slave.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/base/test_app.py` & `DI-engine-0.4.7/ding/interaction/tests/base/test_app.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/base/test_common.py` & `DI-engine-0.4.7/ding/interaction/tests/base/test_common.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/base/test_network.py` & `DI-engine-0.4.7/ding/interaction/tests/base/test_network.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/base/test_threading.py` & `DI-engine-0.4.7/ding/interaction/tests/base/test_threading.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/exception/test_base.py` & `DI-engine-0.4.7/ding/interaction/tests/exception/test_base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/exception/test_master.py` & `DI-engine-0.4.7/ding/interaction/tests/exception/test_master.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/exception/test_slave.py` & `DI-engine-0.4.7/ding/interaction/tests/exception/test_slave.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/interaction/bases.py` & `DI-engine-0.4.7/ding/interaction/tests/interaction/bases.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/interaction/test_errors.py` & `DI-engine-0.4.7/ding/interaction/tests/interaction/test_errors.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/interaction/test_simple.py` & `DI-engine-0.4.7/ding/interaction/tests/interaction/test_simple.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/interaction/tests/test_utils/stream.py` & `DI-engine-0.4.7/ding/interaction/tests/test_utils/stream.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/algorithm.py` & `DI-engine-0.4.7/ding/league/algorithm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/base_league.py` & `DI-engine-0.4.7/ding/league/base_league.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/metric.py` & `DI-engine-0.4.7/ding/league/metric.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/one_vs_one_league.py` & `DI-engine-0.4.7/ding/league/one_vs_one_league.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/player.py` & `DI-engine-0.4.7/ding/league/player.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/shared_payoff.py` & `DI-engine-0.4.7/ding/league/shared_payoff.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/league/starcraft_player.py` & `DI-engine-0.4.7/ding/league/starcraft_player.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/common/encoder.py` & `DI-engine-0.4.7/ding/model/common/encoder.py`

 * *Files 0% similar despite different names*

```diff
@@ -69,14 +69,15 @@
         assert len(set(hidden_size_list[3:-1])) <= 1, "Please indicate the same hidden size for res block parts"
         for i in range(3, len(self.hidden_size_list) - 1):
             layers.append(ResBlock(self.hidden_size_list[i], activation=self.act, norm_type=norm_type))
         layers.append(Flatten())
         self.main = nn.Sequential(*layers)
 
         flatten_size = self._get_flatten_size()
+        self.output_size = hidden_size_list[-1]
         self.mid = nn.Linear(flatten_size, hidden_size_list[-1])
 
     def _get_flatten_size(self) -> int:
         """
         Overview:
             Get the encoding size after ``self.main`` to get the number of ``in-features`` to feed to ``nn.Linear``.
         Returns:
```

### Comparing `DI-engine-0.4.6/ding/model/common/head.py` & `DI-engine-0.4.7/ding/model/common/head.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 import math
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 from torch.distributions import Normal, Independent
 
-from ding.torch_utils import fc_block, noise_block, NoiseLinearLayer, MLP
+from ding.torch_utils import fc_block, noise_block, NoiseLinearLayer, MLP, PopArt
 from ding.rl_utils import beta_function_map
 from ding.utils import lists_to_dicts, SequenceType
 
 
 class DiscreteHead(nn.Module):
     """
         Overview:
@@ -1012,21 +1012,22 @@
             Input is a (:obj:`torch.Tensor`) of shape ``(B, N)`` and returns a (:obj:`Dict`) containing \
             output ``pred``.
         Interfaces:
             ``__init__``, ``forward``.
     """
 
     def __init__(
-            self,
-            hidden_size: int,
-            output_size: int,
-            layer_num: int = 2,
-            final_tanh: Optional[bool] = False,
-            activation: Optional[nn.Module] = nn.ReLU(),
-            norm_type: Optional[str] = None
+        self,
+        input_size: int,
+        output_size: int,
+        layer_num: int = 2,
+        final_tanh: Optional[bool] = False,
+        activation: Optional[nn.Module] = nn.ReLU(),
+        norm_type: Optional[str] = None,
+        hidden_size: int = None,
     ) -> None:
         """
         Overview:
             Init the ``RegressionHead`` layers according to the provided arguments.
         Arguments:
             - hidden_size (:obj:`int`): The ``hidden_size`` of the MLP connected to ``RegressionHead``.
             - output_size (:obj:`int`): The number of outputs.
@@ -1034,15 +1035,17 @@
             - final_tanh (:obj:`bool`): If ``True`` apply ``tanh`` to output. Default ``False``.
             - activation (:obj:`nn.Module`): The type of activation function to use in MLP. \
                 If ``None``, then default set activation to ``nn.ReLU()``. Default ``None``.
             - norm_type (:obj:`str`): The type of normalization to use. See ``ding.torch_utils.network.fc_block`` \
                 for more details. Default ``None``.
         """
         super(RegressionHead, self).__init__()
-        self.main = MLP(hidden_size, hidden_size, hidden_size, layer_num, activation=activation, norm_type=norm_type)
+        if hidden_size is None:
+            hidden_size = input_size
+        self.main = MLP(input_size, hidden_size, hidden_size, layer_num, activation=activation, norm_type=norm_type)
         self.last = nn.Linear(hidden_size, output_size)  # for convenience of special initialization
         self.final_tanh = final_tanh
         if self.final_tanh:
             self.tanh = nn.Tanh()
 
     def forward(self, x: torch.Tensor) -> Dict:
         """
@@ -1082,23 +1085,24 @@
             ``__init__``, ``forward``.
     """
 
     default_sigma_type = ['fixed', 'independent', 'conditioned']
     default_bound_type = ['tanh', None]
 
     def __init__(
-        self,
-        hidden_size: int,
-        output_size: int,
-        layer_num: int = 2,
-        sigma_type: Optional[str] = None,
-        fixed_sigma_value: Optional[float] = 1.0,
-        activation: Optional[nn.Module] = nn.ReLU(),
-        norm_type: Optional[str] = None,
-        bound_type: Optional[str] = None,
+            self,
+            input_size: int,
+            output_size: int,
+            layer_num: int = 2,
+            sigma_type: Optional[str] = None,
+            fixed_sigma_value: Optional[float] = 1.0,
+            activation: Optional[nn.Module] = nn.ReLU(),
+            norm_type: Optional[str] = None,
+            bound_type: Optional[str] = None,
+            hidden_size: int = None
     ) -> None:
         """
         Overview:
             Init the ``ReparameterizationHead`` layers according to the provided arguments.
         Arguments:
             - hidden_size (:obj:`int`): The ``hidden_size`` of the MLP connected to ``ReparameterizationHead``.
             - output_size (:obj:`int`): The number of outputs.
@@ -1111,23 +1115,25 @@
                 If ``None``, then default set activation to ``nn.ReLU()``. Default ``None``.
             - norm_type (:obj:`str`): The type of normalization to use. See ``ding.torch_utils.network.fc_block`` \
                 for more details. Default ``None``.
             - bound_type (:obj:`str`): Bound type to apply to output ``mu``. Choose among ``['tanh', None]``. \
                 Default is ``None``.
         """
         super(ReparameterizationHead, self).__init__()
+        if hidden_size is None:
+            hidden_size = input_size
         self.sigma_type = sigma_type
         assert sigma_type in self.default_sigma_type, "Please indicate sigma_type as one of {}".format(
             self.default_sigma_type
         )
         self.bound_type = bound_type
         assert bound_type in self.default_bound_type, "Please indicate bound_type as one of {}".format(
             self.default_bound_type
         )
-        self.main = MLP(hidden_size, hidden_size, hidden_size, layer_num, activation=activation, norm_type=norm_type)
+        self.main = MLP(input_size, hidden_size, hidden_size, layer_num, activation=activation, norm_type=norm_type)
         self.mu = nn.Linear(hidden_size, output_size)
         if self.sigma_type == 'fixed':
             self.sigma = torch.full((1, output_size), fixed_sigma_value)
         elif self.sigma_type == 'independent':  # independent parameter
             self.log_sigma_param = nn.Parameter(torch.zeros(1, output_size))
         elif self.sigma_type == 'conditioned':
             self.log_sigma_layer = nn.Linear(hidden_size, output_size)
@@ -1166,14 +1172,86 @@
             sigma = torch.exp(log_sigma)
         elif self.sigma_type == 'conditioned':
             log_sigma = self.log_sigma_layer(x)
             sigma = torch.exp(torch.clamp(log_sigma, -20, 2))
         return {'mu': mu, 'sigma': sigma}
 
 
+class PopArtVHead(nn.Module):
+    """
+        Overview:
+            The ``PopArtVHead`` used to output discrete actions logit with the last layer as popart. \
+            Input is a (:obj:`torch.Tensor`) of shape ``(B, N)`` and returns a (:obj:`Dict`) containing \
+            output ``logit``.
+        Interfaces:
+            ``__init__``, ``forward``.
+    """
+
+    def __init__(
+        self,
+        hidden_size: int,
+        output_size: int,
+        layer_num: int = 1,
+        activation: Optional[nn.Module] = nn.ReLU(),
+        norm_type: Optional[str] = None,
+        noise: Optional[bool] = False,
+    ) -> None:
+        """
+        Overview:
+            Init the ``PopArtVHead`` layers according to the provided arguments.
+        Arguments:
+            - hidden_size (:obj:`int`): The ``hidden_size`` of the MLP connected to ``PopArtVHead``.
+            - output_size (:obj:`int`): The number of outputs.
+            - layer_num (:obj:`int`): The number of layers used in the network to compute Q value output.
+            - activation (:obj:`nn.Module`): The type of activation function to use in MLP. \
+                If ``None``, then default set activation to ``nn.ReLU()``. Default ``None``.
+            - norm_type (:obj:`str`): The type of normalization to use. See ``ding.torch_utils.network.fc_block`` \
+                for more details. Default ``None``.
+            - noise (:obj:`bool`): Whether use ``NoiseLinearLayer`` as ``layer_fn`` in Q networks' MLP. \
+                Default ``False``.
+        """
+        super(PopArtVHead, self).__init__()
+        layer = NoiseLinearLayer if noise else nn.Linear
+        self.popart = PopArt(hidden_size, output_size)
+        self.Q = nn.Sequential(
+            MLP(
+                hidden_size,
+                hidden_size,
+                hidden_size,
+                layer_num,
+                layer_fn=layer,
+                activation=activation,
+                norm_type=norm_type
+            ), self.popart
+        )
+
+    def forward(self, x: torch.Tensor) -> Dict:
+        """
+        Overview:
+            Use encoded embedding tensor to run MLP with ``PopArtVHead`` and return the normalized prediction and \
+                the unnormalized prediction dictionary.
+        Arguments:
+            - x (:obj:`torch.Tensor`): Tensor containing input embedding.
+        Returns:
+            - outputs (:obj:`Dict`): Dict containing keyword ``pred`` (:obj:`torch.Tensor`) \
+                and ``unnormalized_pred`` (:obj:`torch.Tensor`).
+        Shapes:
+            - x: :math:`(B, N)`, where ``B = batch_size`` and ``N = hidden_size``.
+            - logit: :math:`(B, M)`, where ``M = output_size``.
+        Examples:
+            >>> head = PopArtVHead(64, 64)
+            >>> inputs = torch.randn(4, 64)
+            >>> outputs = head(inputs)
+            >>> assert isinstance(outputs, dict) and outputs['pred'].shape == torch.Size([4, 64]) and \
+                    outputs['unnormalized_pred'].shape == torch.Size([4, 64])
+        """
+        x = self.Q(x)
+        return x
+
+
 class AttentionPolicyHead(nn.Module):
 
     def __init__(self) -> None:
         super(AttentionPolicyHead, self).__init__()
 
     def forward(self, key: torch.Tensor, query: torch.Tensor) -> torch.Tensor:
         if len(query.shape) == 2 and len(key.shape) == 3:
@@ -1256,10 +1334,11 @@
     'rainbow': RainbowHead,
     'qrdqn': QRDQNHead,
     'quantile': QuantileHead,
     'attention_policy': AttentionPolicyHead,
     # continuous
     'regression': RegressionHead,
     'reparameterization': ReparameterizationHead,
+    'popart': PopArtVHead,
     # multi
     'multi': MultiHead,
 }
```

### Comparing `DI-engine-0.4.6/ding/model/common/utils.py` & `DI-engine-0.4.7/ding/model/common/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/__init__.py` & `DI-engine-0.4.7/ding/model/template/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -18,8 +18,8 @@
 from .mavac import MAVAC
 from .ngu import NGU
 from .qac_dist import QACDIST
 from .maqac import MAQAC, ContinuousMAQAC
 from .madqn import MADQN
 from .vae import VanillaVAE
 from .decision_transformer import DecisionTransformer
-from .procedure_cloning import ProcedureCloning
+from .procedure_cloning import ProcedureCloningMCTS, ProcedureCloningBFS
```

### Comparing `DI-engine-0.4.6/ding/model/template/acer.py` & `DI-engine-0.4.7/ding/model/template/acer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/atoc.py` & `DI-engine-0.4.7/ding/model/template/atoc.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/bc.py` & `DI-engine-0.4.7/ding/model/template/bc.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,23 +9,24 @@
         MultiHead, RegressionHead, ReparameterizationHead
 
 
 @MODEL_REGISTRY.register('discrete_bc')
 class DiscreteBC(nn.Module):
 
     def __init__(
-            self,
-            obs_shape: Union[int, SequenceType],
-            action_shape: Union[int, SequenceType],
-            encoder_hidden_size_list: SequenceType = [128, 128, 64],
-            dueling: bool = True,
-            head_hidden_size: Optional[int] = None,
-            head_layer_num: int = 1,
-            activation: Optional[nn.Module] = nn.ReLU(),
-            norm_type: Optional[str] = None
+        self,
+        obs_shape: Union[int, SequenceType],
+        action_shape: Union[int, SequenceType],
+        encoder_hidden_size_list: SequenceType = [128, 128, 64],
+        dueling: bool = True,
+        head_hidden_size: Optional[int] = None,
+        head_layer_num: int = 1,
+        activation: Optional[nn.Module] = nn.ReLU(),
+        norm_type: Optional[str] = None,
+        strides: Optional[list] = None,
     ) -> None:
         """
         Overview:
             Init the DiscreteBC (encoder + head) Model according to input arguments.
         Arguments:
             - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape, such as 8 or [4, 84, 84].
             - action_shape (:obj:`Union[int, SequenceType]`): Action space shape, such as 6 or [2, 3, 3].
@@ -45,15 +46,22 @@
         if head_hidden_size is None:
             head_hidden_size = encoder_hidden_size_list[-1]
         # FC Encoder
         if isinstance(obs_shape, int) or len(obs_shape) == 1:
             self.encoder = FCEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
         # Conv Encoder
         elif len(obs_shape) == 3:
-            self.encoder = ConvEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
+            if not strides:
+                self.encoder = ConvEncoder(
+                    obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type
+                )
+            else:
+                self.encoder = ConvEncoder(
+                    obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type, stride=strides
+                )
         else:
             raise RuntimeError(
                 "not support obs_shape for pre-defined encoder: {}, please customize your own BC".format(obs_shape)
             )
         # Head Type
         if dueling:
             head_cls = DuelingHead
```

### Comparing `DI-engine-0.4.6/ding/model/template/collaq.py` & `DI-engine-0.4.7/ding/model/template/collaq.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/coma.py` & `DI-engine-0.4.7/ding/model/template/coma.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/decision_transformer.py` & `DI-engine-0.4.7/ding/model/template/decision_transformer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/ebm.py` & `DI-engine-0.4.7/ding/model/template/ebm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/madqn.py` & `DI-engine-0.4.7/ding/model/template/madqn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/maqac.py` & `DI-engine-0.4.7/ding/model/template/maqac.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/mavac.py` & `DI-engine-0.4.7/ding/model/template/mavac.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/ngu.py` & `DI-engine-0.4.7/ding/model/template/ngu.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/pdqn.py` & `DI-engine-0.4.7/ding/model/template/pdqn.py`

 * *Files 5% similar despite different names*

```diff
@@ -162,15 +162,14 @@
         Returns:
             - outputs (:obj:`Dict`): A dict with keys 'logit', 'action_args'
                 -  'logit': the logit value for each discrete action,
                 -  'action_args': the continuous action args(same as the inputs['action_args']) for later usage
         """
         dis_x = self.encoder[0](inputs['state'])  # size (B, encoded_state_shape)
         action_args = inputs['action_args']  # size (B, action_args_shape)
-        print(dis_x.shape, action_args.shape)
 
         if self.multi_pass:  # mpdqn
             # fill_value=-2 is a mask value, which is not in normal acton range
             # (B, action_args_shape, K) where K is the action_type_shape
             mp_action = torch.full(
                 (dis_x.shape[0], self.action_args_shape, self.action_type_shape),
                 fill_value=-2,
@@ -187,12 +186,14 @@
             mp_state_action_cat = torch.cat([mp_state, mp_action], dim=-1)
 
             logit = self.actor_head[0](mp_state_action_cat)['logit']  # (B, K, K)
 
             logit = torch.diagonal(logit, dim1=-2, dim2=-1)  # (B, K)
         else:  # pdqn
             # size (B, encoded_state_shape + action_args_shape)
+            if len(action_args.shape) == 1:  # (B, ) -> (B, 1)
+                action_args = action_args.unsqueeze(1)
             state_action_cat = torch.cat((dis_x, action_args), dim=-1)
             logit = self.actor_head[0](state_action_cat)['logit']  # size (B, K) where K is action_type_shape
 
         outputs = {'logit': logit, 'action_args': action_args}
         return outputs
```

### Comparing `DI-engine-0.4.6/ding/model/template/pg.py` & `DI-engine-0.4.7/ding/model/template/pg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/ppg.py` & `DI-engine-0.4.7/ding/model/template/ppg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/q_learning.py` & `DI-engine-0.4.7/ding/model/template/q_learning.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,24 +27,24 @@
         Overview:
             Init the DQN (encoder + head) Model according to input arguments.
         Arguments:
             - obs_shape (:obj:`Union[int, SequenceType]`): Observation space shape, such as 8 or [4, 84, 84].
             - action_shape (:obj:`Union[int, SequenceType]`): Action space shape, such as 6 or [2, 3, 3].
             - encoder_hidden_size_list (:obj:`SequenceType`): Collection of ``hidden_size`` to pass to ``Encoder``, \
                 the last element must match ``head_hidden_size``.
-            - dueling (:obj:`dueling`): Whether choose ``DuelingHead`` or ``DiscreteHead(default)``.
+            - dueling (:obj:`Optional[bool]`): Whether choose ``DuelingHead`` or ``DiscreteHead(default)``.
             - head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` of head network.
             - head_layer_num (:obj:`int`): The number of layers used in the head network to compute Q value output
             - activation (:obj:`Optional[nn.Module]`): The type of activation function in networks \
                 if ``None`` then default set it to ``nn.ReLU()``
             - norm_type (:obj:`Optional[str]`): The type of normalization in networks, see \
-                ``ding.torch_utils.fc_block`` for more details.
+                ``ding.torch_utils.fc_block`` for more details. you can choose one of ['BN', 'IN', 'SyncBN', 'LN']
         """
         super(DQN, self).__init__()
-        # For compatibility: 1, (1, ), [4, 32, 32]
+        # Squeeze data from tuple, list or dict to single object. For example, from (4, ) to 4
         obs_shape, action_shape = squeeze(obs_shape), squeeze(action_shape)
         if head_hidden_size is None:
             head_hidden_size = encoder_hidden_size_list[-1]
         # FC Encoder
         if isinstance(obs_shape, int) or len(obs_shape) == 1:
             self.encoder = FCEncoder(obs_shape, encoder_hidden_size_list, activation=activation, norm_type=norm_type)
         # Conv Encoder
@@ -71,15 +71,15 @@
             )
         else:
             self.head = head_cls(
                 head_hidden_size, action_shape, head_layer_num, activation=activation, norm_type=norm_type
             )
 
     def forward(self, x: torch.Tensor) -> Dict:
-        r"""
+        """
         Overview:
             DQN forward computation graph, input observation tensor to predict q_value.
         Arguments:
             - x (:obj:`torch.Tensor`): Observation inputs
         Returns:
             - outputs (:obj:`Dict`): DQN forward outputs, such as q_value.
         ReturnsKeys:
```

### Comparing `DI-engine-0.4.6/ding/model/template/qac.py` & `DI-engine-0.4.7/ding/model/template/qac.py`

 * *Files 10% similar despite different names*

```diff
@@ -16,25 +16,27 @@
         The QAC network, which is used in DDPG/TD3/SAC.
     Interfaces:
         ``__init__``, ``forward``, ``compute_actor``, ``compute_critic``
     """
     mode = ['compute_actor', 'compute_critic']
 
     def __init__(
-            self,
-            obs_shape: Union[int, SequenceType],
-            action_shape: Union[int, SequenceType, EasyDict],
-            action_space: str,
-            twin_critic: bool = False,
-            actor_head_hidden_size: int = 64,
-            actor_head_layer_num: int = 1,
-            critic_head_hidden_size: int = 64,
-            critic_head_layer_num: int = 1,
-            activation: Optional[nn.Module] = nn.ReLU(),
-            norm_type: Optional[str] = None,
+        self,
+        obs_shape: Union[int, SequenceType],
+        action_shape: Union[int, SequenceType, EasyDict],
+        action_space: str,
+        twin_critic: bool = False,
+        actor_head_hidden_size: int = 64,
+        actor_head_layer_num: int = 1,
+        critic_head_hidden_size: int = 64,
+        critic_head_layer_num: int = 1,
+        activation: Optional[nn.Module] = nn.ReLU(),
+        norm_type: Optional[str] = None,
+        encoder_hidden_size_list: Optional[SequenceType] = [32, 64, 256],
+        share_encoder: Optional[bool] = False,
     ) -> None:
         """
         Overview:
             Initailize the QAC Model according to input arguments.
         Arguments:
             - obs_shape (:obj:`Union[int, SequenceType]`): Observation's shape, such as 128, (156, ).
             - action_shape (:obj:`Union[int, SequenceType, EasyDict]`): Action's shape, such as 4, (3, ), \
@@ -48,36 +50,70 @@
             - critic_head_hidden_size (:obj:`Optional[int]`): The ``hidden_size`` to pass to critic head.
             - critic_head_layer_num (:obj:`int`): The num of layers used in the network to compute Q value output \
                 for critic head.
             - activation (:obj:`Optional[nn.Module]`): The type of activation function to use in ``MLP`` \
                 after each FC layer, if ``None`` then default set to ``nn.ReLU()``.
             - norm_type (:obj:`Optional[str]`): The type of normalization to after network layer (FC, Conv), \
                 see ``ding.torch_utils.network`` for more details.
+            - share_encoder (:obj:`Optional[bool]`): Whether to share encoder between actor and critic.
         """
         super(QAC, self).__init__()
         obs_shape: int = squeeze(obs_shape)
         action_shape = squeeze(action_shape)
         self.action_shape = action_shape
         self.action_space = action_space
-        assert self.action_space in ['regression', 'reparameterization', 'hybrid']
+        assert self.action_space in ['regression', 'reparameterization', 'hybrid'], self.action_space
+
+        # encoder
+        self.share_encoder = share_encoder
+        if np.isscalar(obs_shape) or len(obs_shape) == 1:
+            assert not self.share_encoder, "Vector observation doesn't need share encoder."
+            self.encoder = None
+            self.input_size = obs_shape
+        elif len(obs_shape) == 3:
+
+            def setup_conv_encoder():
+                kernel_size = [3 for _ in range(len(encoder_hidden_size_list))]
+                stride = [2] + [1 for _ in range(len(encoder_hidden_size_list) - 1)]
+                return ConvEncoder(
+                    obs_shape,
+                    encoder_hidden_size_list,
+                    activation=activation,
+                    norm_type=norm_type,
+                    kernel_size=kernel_size,
+                    stride=stride
+                )
+
+            if self.share_encoder:
+                self.encoder = setup_conv_encoder()
+                self.input_size = self.encoder.output_size
+            else:
+                self.encoder = nn.ModuleDict({
+                    'actor': setup_conv_encoder(),
+                    'critic': setup_conv_encoder(),
+                })
+                self.input_size = self.encoder['actor'].output_size
+        else:
+            raise RuntimeError("not support observation shape: {}".format(obs_shape))
+        # head
         if self.action_space == 'regression':  # DDPG, TD3
             self.actor = nn.Sequential(
-                nn.Linear(obs_shape, actor_head_hidden_size), activation,
+                nn.Linear(self.input_size, actor_head_hidden_size), activation,
                 RegressionHead(
                     actor_head_hidden_size,
                     action_shape,
                     actor_head_layer_num,
                     final_tanh=True,
                     activation=activation,
                     norm_type=norm_type
                 )
             )
         elif self.action_space == 'reparameterization':  # SAC
             self.actor = nn.Sequential(
-                nn.Linear(obs_shape, actor_head_hidden_size), activation,
+                nn.Linear(self.input_size, actor_head_hidden_size), activation,
                 ReparameterizationHead(
                     actor_head_hidden_size,
                     action_shape,
                     actor_head_layer_num,
                     sigma_type='conditioned',
                     activation=activation,
                     norm_type=norm_type
@@ -85,41 +121,41 @@
             )
         elif self.action_space == 'hybrid':  # PADDPG
             # hybrid action space: action_type(discrete) + action_args(continuous),
             # such as {'action_type_shape': torch.LongTensor([0]), 'action_args_shape': torch.FloatTensor([0.1, -0.27])}
             action_shape.action_args_shape = squeeze(action_shape.action_args_shape)
             action_shape.action_type_shape = squeeze(action_shape.action_type_shape)
             actor_action_args = nn.Sequential(
-                nn.Linear(obs_shape, actor_head_hidden_size), activation,
+                nn.Linear(self.input_size, actor_head_hidden_size), activation,
                 RegressionHead(
                     actor_head_hidden_size,
                     action_shape.action_args_shape,
                     actor_head_layer_num,
                     final_tanh=True,
                     activation=activation,
                     norm_type=norm_type
                 )
             )
             actor_action_type = nn.Sequential(
-                nn.Linear(obs_shape, actor_head_hidden_size), activation,
+                nn.Linear(self.input_size, actor_head_hidden_size), activation,
                 DiscreteHead(
                     actor_head_hidden_size,
                     action_shape.action_type_shape,
                     actor_head_layer_num,
                     activation=activation,
                     norm_type=norm_type,
                 )
             )
             self.actor = nn.ModuleList([actor_action_type, actor_action_args])
 
         self.twin_critic = twin_critic
         if self.action_space == 'hybrid':
-            critic_input_size = obs_shape + action_shape.action_type_shape + action_shape.action_args_shape
+            critic_input_size = self.input_size + action_shape.action_type_shape + action_shape.action_args_shape
         else:
-            critic_input_size = obs_shape + action_shape
+            critic_input_size = self.input_size + action_shape
         if self.twin_critic:
             self.critic = nn.ModuleList()
             for _ in range(2):
                 self.critic.append(
                     nn.Sequential(
                         nn.Linear(critic_input_size, critic_head_hidden_size), activation,
                         RegressionHead(
@@ -208,14 +244,19 @@
             >>> # Reparameterization Mode
             >>> model = QAC(64, 64, 'reparameterization')
             >>> obs = torch.randn(4, 64)
             >>> actor_outputs = model(obs,'compute_actor')
             >>> assert actor_outputs['logit'][0].shape == torch.Size([4, 64])  # mu
             >>> actor_outputs['logit'][1].shape == torch.Size([4, 64]) # sigma
         """
+        if self.encoder is not None:
+            if self.share_encoder:
+                obs = self.encoder(obs)
+            else:
+                obs = self.encoder['actor'](obs)
         if self.action_space == 'regression':
             x = self.actor(obs)
             return {'action': x['pred']}
         elif self.action_space == 'reparameterization':
             x = self.actor(obs)
             return {'logit': [x['mu'], x['sigma']]}
         elif self.action_space == 'hybrid':
@@ -254,14 +295,19 @@
             >>> inputs = {'obs': torch.randn(4, 8), 'action': torch.randn(4, 1)}
             >>> model = QAC(obs_shape=(8, ),action_shape=1, action_space='regression')
             >>> model(inputs, mode='compute_critic')['q_value']  # q value
             ... tensor([0.0773, 0.1639, 0.0917, 0.0370], grad_fn=<SqueezeBackward1>)
         """
 
         obs, action = inputs['obs'], inputs['action']
+        if self.encoder is not None:
+            if self.share_encoder:
+                obs = self.encoder(obs)
+            else:
+                obs = self.encoder['critic'](obs)
         assert len(obs.shape) == 2
         if self.action_space == 'hybrid':
             action_type_logit = inputs['logit']
             action_type_logit = torch.softmax(action_type_logit, dim=-1)
             action_args = action['action_args']
             if len(action_args.shape) == 1:
                 action_args = action_args.unsqueeze(1)
```

### Comparing `DI-engine-0.4.6/ding/model/template/qac_dist.py` & `DI-engine-0.4.7/ding/model/template/qac_dist.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/qmix.py` & `DI-engine-0.4.7/ding/model/template/qmix.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/qtran.py` & `DI-engine-0.4.7/ding/model/template/qtran.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/sqn.py` & `DI-engine-0.4.7/ding/model/template/sqn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/vac.py` & `DI-engine-0.4.7/ding/model/template/vac.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/vae.py` & `DI-engine-0.4.7/ding/model/template/vae.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/template/wqmix.py` & `DI-engine-0.4.7/ding/model/template/wqmix.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/wrapper/model_wrappers.py` & `DI-engine-0.4.7/ding/model/wrapper/model_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/model/wrapper/test_model_wrappers.py` & `DI-engine-0.4.7/ding/model/wrapper/test_model_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/policy/__init__.py` & `DI-engine-0.4.7/ding/policy/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from .base_policy import Policy, CommandModePolicy, create_policy, get_policy_cls
 from .common_utils import single_env_forward_wrapper, single_env_forward_wrapper_ttorch
 from .dqn import DQNSTDIMPolicy, DQNPolicy
+from .mdqn import MDQNPolicy
 from .iqn import IQNPolicy
 from .fqf import FQFPolicy
 from .qrdqn import QRDQNPolicy
 from .c51 import C51Policy
 from .rainbow import RainbowDQNPolicy
 from .ddpg import DDPGPolicy
 from .d4pg import D4PGPolicy
@@ -41,9 +42,11 @@
 
 from .policy_factory import PolicyFactory, get_random_policy
 from .pdqn import PDQNPolicy
 
 from .bc import BehaviourCloningPolicy
 from .ibc import IBCPolicy
 
+from .pc import ProcedureCloningBFSPolicy
+
 # new-type policy
 from .ppof import PPOFPolicy
```

### Comparing `DI-engine-0.4.6/ding/policy/a2c.py` & `DI-engine-0.4.7/ding/policy/a2c.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,16 +24,15 @@
         cuda=False,
         # (bool) whether use on-policy training pipeline(behaviour policy and training policy are the same)
         on_policy=True,  # for a2c strictly on policy algorithm, this line should not be seen by users
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # (int) for a2c, update_per_collect must be 1.
             update_per_collect=1,  # fixed value, this line should not be modified by users
             batch_size=64,
             learning_rate=0.001,
             # (List[float])
             betas=(0.9, 0.999),
             # (float)
```

### Comparing `DI-engine-0.4.6/ding/policy/acer.py` & `DI-engine-0.4.7/ding/policy/acer.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,16 +54,15 @@
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
             # (str) the type of gradient clip method
             grad_clip_type=None,
             # (float) max value when ACER use gradient clip
             clip_value=None,
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # (int) collect n_sample data, train model update_per_collect times
             # here we follow ppo serial pipeline
             update_per_collect=4,
             # (int) the number of data for a train iteration
             batch_size=16,
             # (float) loss weight of the value network, the weight of policy network is set to 1
             value_weight=0.5,
```

### Comparing `DI-engine-0.4.6/ding/policy/atoc.py` & `DI-engine-0.4.7/ding/policy/atoc.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,16 +39,14 @@
             communication=True,
             # (int) The number of thought size
             thought_size=8,
             # (int) The number of agent for each communication group
             agent_per_group=2,
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # (int) Collect n_sample data, update model n_iteration time
             update_per_collect=5,
             # (int) The number of data for a train iteration
             batch_size=64,
             # (float) Gradient-descent step size of actor
             learning_rate_actor=0.001,
             # (float) Gradient-descent step size of critic
```

### Comparing `DI-engine-0.4.6/ding/policy/base_policy.py` & `DI-engine-0.4.7/ding/policy/base_policy.py`

 * *Files 5% similar despite different names*

```diff
@@ -3,22 +3,32 @@
 from typing import Optional, List, Dict, Any, Tuple, Union
 
 import torch
 import copy
 from easydict import EasyDict
 
 from ding.model import create_model
-from ding.utils import import_module, allreduce, broadcast, get_rank, allreduce_async, synchronize, POLICY_REGISTRY
+from ding.utils import import_module, allreduce, broadcast, get_rank, allreduce_async, synchronize, deep_merge_dicts, \
+    POLICY_REGISTRY
 
 
 class Policy(ABC):
 
     @classmethod
     def default_config(cls: type) -> EasyDict:
+        if cls == Policy:
+            raise RuntimeError
+
+        base_cls = cls.__base__
+        if base_cls == Policy:
+            base_policy_cfg = EasyDict(copy.deepcopy(Policy.config))
+        else:
+            base_policy_cfg = copy.deepcopy(base_cls.default_config())
         cfg = EasyDict(copy.deepcopy(cls.config))
+        cfg = deep_merge_dicts(base_policy_cfg, cfg)
         cfg.cfg_type = cls.__name__ + 'Dict'
         return cfg
 
     learn_function = namedtuple(
         'learn_function', [
             'forward',
             'reset',
@@ -49,14 +59,22 @@
             'get_attribute',
             'set_attribute',
             'state_dict',
             'load_state_dict',
         ]
     )
     total_field = set(['learn', 'collect', 'eval'])
+    config = dict(
+        on_policy=False,
+        cuda=False,
+        multi_gpu=False,
+        bp_update_sync=True,
+        traj_len_inf=False,
+        model=dict(),
+    )
 
     def __init__(
             self,
             cfg: dict,
             model: Optional[Union[type, torch.nn.Module]] = None,
             enable_field: Optional[List[str]] = None
     ) -> None:
@@ -69,19 +87,20 @@
         assert set(self._enable_field).issubset(self.total_field), self._enable_field
 
         if len(set(self._enable_field).intersection(set(['learn', 'collect', 'eval']))) > 0:
             model = self._create_model(cfg, model)
             self._cuda = cfg.cuda and torch.cuda.is_available()
             # now only support multi-gpu for only enable learn mode
             if len(set(self._enable_field).intersection(set(['learn']))) > 0:
-                self._rank = get_rank() if self._cfg.learn.multi_gpu else 0
+                multi_gpu = self._cfg.multi_gpu
+                self._rank = get_rank() if multi_gpu else 0
                 if self._cuda:
                     model.cuda()
-                if self._cfg.learn.multi_gpu:
-                    bp_update_sync = self._cfg.learn.get('bp_update_sync', True)
+                if multi_gpu:
+                    bp_update_sync = self._cfg.bp_update_sync
                     self._bp_update_sync = bp_update_sync
                     self._init_multi_gpu_setting(model, bp_update_sync)
             else:
                 self._rank = 0
                 if self._cuda:
                     model.cuda()
             self._model = model
@@ -228,15 +247,19 @@
         }
 
     def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
         self._learn_model.load_state_dict(state_dict['model'])
         self._optimizer.load_state_dict(state_dict['optimizer'])
 
     def _get_batch_size(self) -> Union[int, Dict[str, int]]:
-        return self._cfg.learn.batch_size
+        # some specifial algorithms use different batch size for different optimization parts.
+        if 'batch_size' in self._cfg:
+            return self._cfg.batch_size
+        else:  # for compatibility
+            return self._cfg.learn.batch_size
 
     # *************************************** collect function ************************************
 
     @abstractmethod
     def _forward_collect(self, data: dict, **kwargs) -> dict:
         raise NotImplementedError
 
@@ -254,14 +277,26 @@
 
     def _state_dict_collect(self) -> Dict[str, Any]:
         return {'model': self._collect_model.state_dict()}
 
     def _load_state_dict_collect(self, state_dict: Dict[str, Any]) -> None:
         self._collect_model.load_state_dict(state_dict['model'], strict=True)
 
+    def _get_n_sample(self):
+        if 'n_sample' in self._cfg:
+            return self._cfg.n_sample
+        else:  # for compatibility
+            return self._cfg.collect.get('n_sample', None)  # for some adpative collecting data case
+
+    def _get_n_episode(self):
+        if 'n_episode' in self._cfg:
+            return self._cfg.n_episode
+        else:  # for compatibility
+            return self._cfg.collect.get('n_episode', None)  # for some adpative collecting data case
+
     # *************************************** eval function ************************************
 
     @abstractmethod
     def _forward_eval(self, data: dict) -> Dict[str, Any]:
         raise NotImplementedError
 
     # don't need to implement _reset_eval method by force
```

### Comparing `DI-engine-0.4.6/ding/policy/bc.py` & `DI-engine-0.4.7/ding/policy/bc.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,15 +24,14 @@
     config = dict(
         type='bc',
         cuda=False,
         on_policy=False,
         continuous=False,
         action_shape=19,
         learn=dict(
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=32,
             learning_rate=1e-5,
             lr_decay=False,
             decay_epoch=30,
             decay_rate=0.1,
             warmup_lr=1e-4,
@@ -166,15 +165,15 @@
                         )
         forward_time = self._timer.value
         with self._timer:
             self._optimizer.zero_grad()
             loss.backward()
         backward_time = self._timer.value
         with self._timer:
-            if self._cfg.learn.multi_gpu:
+            if self._cfg.multi_gpu:
                 self.sync_gradients(self._learn_model)
         sync_time = self._timer.value
         self._optimizer.step()
         cur_lr = [param_group['lr'] for param_group in self._optimizer.param_groups]
         cur_lr = sum(cur_lr) / len(cur_lr)
         return {
             'cur_lr': cur_lr,
```

### Comparing `DI-engine-0.4.6/ding/policy/bdq.py` & `DI-engine-0.4.7/ding/policy/bdq.py`

 * *Files 1% similar despite different names*

```diff
@@ -41,15 +41,14 @@
         6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
            | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
         7  ``nstep``            int      1,             | N-step reward discount sum for target
                                          [3, 5]         | q_value estimation
         8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
            | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
                                                         | valid in serial training               | means more off-policy
-        9  | ``learn.multi``    bool     False          | whether to use multi gpu during
            | ``_gpu``
         10 | ``learn.batch_``   int      64             | The number of samples of an iteration
            | ``size``
         11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
            | ``_rate``
         12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
            | ``update_freq``
@@ -85,16 +84,15 @@
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         # (float) Discount factor(gamma) for returns
         discount_factor=0.97,
         # (int) The number of step for calculating target q_value
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             # (int) How many samples in a training batch
             batch_size=64,
             # (float) The step size of gradient descent
@@ -220,15 +218,15 @@
         loss, td_error_per_sample = bdq_nstep_td_error(data_n, self._gamma, nstep=self._nstep, value_gamma=value_gamma)
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/c51.py` & `DI-engine-0.4.7/ding/policy/c51.py`

 * *Files 6% similar despite different names*

```diff
@@ -64,16 +64,15 @@
         nstep=1,
         model=dict(
             v_min=-10,
             v_max=10,
             n_atom=51,
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -154,50 +153,60 @@
             data = to_device(data, self._device)
         # ====================
         # Q-learning forward
         # ====================
         self._learn_model.train()
         self._target_model.train()
         # Current q value (main model)
-        q_value = self._learn_model.forward(data['obs'])['distribution']
+        output = self._learn_model.forward(data['obs'])
+        q_value = output['logit']
+        q_value_dist = output['distribution']
         # Target q value
         with torch.no_grad():
-            target_q_value = self._target_model.forward(data['next_obs'])['distribution']
+            target_output = self._target_model.forward(data['next_obs'])
+            target_q_value_dist = target_output['distribution']
+            target_q_value = target_output['logit']
             # Max q value action (main model)
             target_q_action = self._learn_model.forward(data['next_obs'])['action']
 
         data_n = dist_nstep_td_data(
-            q_value, target_q_value, data['action'], target_q_action, data['reward'], data['done'], data['weight']
+            q_value_dist, target_q_value_dist, data['action'], target_q_action, data['reward'], data['done'],
+            data['weight']
         )
         value_gamma = data.get('value_gamma')
         loss, td_error_per_sample = dist_nstep_td_error(
             data_n, self._gamma, self._v_min, self._v_max, self._n_atom, nstep=self._nstep, value_gamma=value_gamma
         )
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
         return {
             'cur_lr': self._optimizer.defaults['lr'],
             'total_loss': loss.item(),
+            'q_value': q_value.mean().item(),
+            'target_q_value': target_q_value.mean().item(),
             'priority': td_error_per_sample.abs().tolist(),
             # Only discrete action satisfying len(data['action'])==1 can return this and draw histogram on tensorboard.
             # '[histogram]action_distribution': data['action'],
         }
 
+    def _monitor_vars_learn(self) -> List[str]:
+        return ['cur_lr', 'total_loss', 'q_value', 'target_q_value']
+
     def _state_dict_learn(self) -> Dict[str, Any]:
         return {
             'model': self._learn_model.state_dict(),
             'target_model': self._target_model.state_dict(),
             'optimizer': self._optimizer.state_dict(),
         }
```

### Comparing `DI-engine-0.4.6/ding/policy/collaq.py` & `DI-engine-0.4.7/ding/policy/collaq.py`

 * *Files 1% similar despite different names*

```diff
@@ -54,16 +54,15 @@
         # (bool) Whether the RL algorithm is on-policy or off-policy.
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # (int) Collect n_episode data, update_model n_iteration times
             update_per_collect=20,
             # (int) The number of data for a train iteration
             batch_size=32,
             # (float) Gradient-descent step size
             learning_rate=0.0005,
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/coma.py` & `DI-engine-0.4.7/ding/policy/coma.py`

 * *Files 1% similar despite different names*

```diff
@@ -57,16 +57,14 @@
         # (bool) Whether the RL algorithm is on-policy or off-policy.
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=20,
             batch_size=32,
             learning_rate=0.0005,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
             # (float) target network update weight, theta * new_w + (1 - theta) * old_w, defaults in [0, 0.1]
```

### Comparing `DI-engine-0.4.6/ding/policy/command_mode_policy_instance.py` & `DI-engine-0.4.7/ding/policy/command_mode_policy_instance.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,13 @@
 from ding.utils import POLICY_REGISTRY
 from ding.rl_utils import get_epsilon_greedy_fn
 from .base_policy import CommandModePolicy
 
 from .dqn import DQNPolicy, DQNSTDIMPolicy
+from .mdqn import MDQNPolicy
 from .c51 import C51Policy
 from .qrdqn import QRDQNPolicy
 from .iqn import IQNPolicy
 from .fqf import FQFPolicy
 from .rainbow import RainbowDQNPolicy
 from .r2d2 import R2D2Policy
 from .r2d2_gtrxl import R2D2GTrXLPolicy
@@ -97,14 +98,19 @@
 
 
 @POLICY_REGISTRY.register('bdq_command')
 class BDQCommandModePolicy(BDQPolicy, EpsCommandModePolicy):
     pass
 
 
+@POLICY_REGISTRY.register('mdqn_command')
+class MDQNCommandModePolicy(MDQNPolicy, EpsCommandModePolicy):
+    pass
+
+
 @POLICY_REGISTRY.register('dqn_command')
 class DQNCommandModePolicy(DQNPolicy, EpsCommandModePolicy):
     pass
 
 
 @POLICY_REGISTRY.register('dqn_stdim_command')
 class DQNSTDIMCommandModePolicy(DQNSTDIMPolicy, EpsCommandModePolicy):
```

### Comparing `DI-engine-0.4.6/ding/policy/common_utils.py` & `DI-engine-0.4.7/ding/policy/common_utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/policy/cql.py` & `DI-engine-0.4.7/ding/policy/cql.py`

 * *Files 2% similar despite different names*

```diff
@@ -105,16 +105,15 @@
             # (int) Hidden size for actor network head.
             actor_head_hidden_size=256,
 
             # (int) Hidden size for critic network head.
             critic_head_hidden_size=256,
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
 
@@ -188,18 +187,14 @@
                 # (int type) max_use: Max use times of one data in the buffer.
                 # Data will be removed once used for too many times.
                 # Default to infinite.
                 # max_use=256,
             ),
         ),
     )
-    r"""
-    Overview:
-        Policy class of SAC algorithm.
-    """
 
     def _init_learn(self) -> None:
         r"""
         Overview:
             Learn mode init method. Called by ``self.__init__``.
             Init q, value and policy's optimizers, algorithm config, main and target models.
         """
@@ -254,15 +249,19 @@
             lr=self._cfg.learn.learning_rate_policy,
         )
 
         # Algorithm config
         self._gamma = self._cfg.learn.discount_factor
         # Init auto alpha
         if self._cfg.learn.auto_alpha:
-            self._target_entropy = self._cfg.learn.get('target_entropy', -np.prod(self._cfg.model.action_shape))
+            if self._cfg.learn.target_entropy is None:
+                assert 'action_shape' in self._cfg.model, "CQL need network model with action_shape variable"
+                self._target_entropy = -np.prod(self._cfg.model.action_shape)
+            else:
+                self._target_entropy = self._cfg.learn.target_entropy
             if self._cfg.learn.log_space:
                 self._log_alpha = torch.log(torch.FloatTensor([self._cfg.learn.alpha]))
                 self._log_alpha = self._log_alpha.to(self._device).requires_grad_()
                 self._alpha_optim = torch.optim.Adam([self._log_alpha], lr=self._cfg.learn.learning_rate_alpha)
                 assert self._log_alpha.shape == torch.Size([1]) and self._log_alpha.requires_grad
                 self._alpha = self._log_alpha.detach().exp()
                 self._auto_alpha = True
@@ -557,16 +556,14 @@
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.97,
         # (int) N-step reward for target q_value estimation
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -680,15 +677,15 @@
         loss += self._min_q_weight * min_q_loss
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/d4pg.py` & `DI-engine-0.4.7/ding/policy/d4pg.py`

 * *Files 1% similar despite different names*

```diff
@@ -96,16 +96,15 @@
             # Default to 10.0.
             v_max=10,
             # (int) Number of atoms in the support set of the
             # value distribution. Default to 51.
             n_atom=51
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
             # Learning rates for actor network(aka. policy).
```

### Comparing `DI-engine-0.4.6/ding/policy/ddpg.py` & `DI-engine-0.4.7/ding/policy/ddpg.py`

 * *Files 2% similar despite different names*

```diff
@@ -86,23 +86,15 @@
         transition_with_policy_data=False,
         # (str) Action space type
         action_space='continuous',  # ['continuous', 'hybrid']
         # (bool) Whether use batch normalization for reward
         reward_batch_norm=False,
         # (bool) Whether to enable multi-agent training setting
         multi_agent=False,
-        model=dict(
-            # (bool) Whether to use two critic networks or only one.
-            # Clipped Double Q-Learning for Actor-Critic in original TD3 paper(https://arxiv.org/pdf/1802.09477.pdf).
-            # Default True for TD3, False for DDPG.
-            twin_critic=False,
-        ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
             # (float) Learning rates for actor network(aka. policy).
@@ -238,39 +230,43 @@
         self._target_model.train()
         next_obs = data['next_obs']
         reward = data['reward']
         if self._reward_batch_norm:
             reward = (reward - reward.mean()) / (reward.std() + 1e-8)
         # current q value
         q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']
-        q_value_dict = {}
-        if self._twin_critic:
-            q_value_dict['q_value'] = q_value[0].mean()
-            q_value_dict['q_value_twin'] = q_value[1].mean()
-        else:
-            q_value_dict['q_value'] = q_value.mean()
+
         # target q value.
         with torch.no_grad():
             next_actor_data = self._target_model.forward(next_obs, mode='compute_actor')
             next_actor_data['obs'] = next_obs
             target_q_value = self._target_model.forward(next_actor_data, mode='compute_critic')['q_value']
+
+        q_value_dict = {}
+        target_q_value_dict = {}
+
         if self._twin_critic:
             # TD3: two critic networks
             target_q_value = torch.min(target_q_value[0], target_q_value[1])  # find min one as target q value
+            q_value_dict['q_value'] = q_value[0].mean().data.item()
+            q_value_dict['q_value_twin'] = q_value[1].mean().data.item()
+            target_q_value_dict['target q_value'] = target_q_value.mean().data.item()
             # critic network1
             td_data = v_1step_td_data(q_value[0], target_q_value, reward, data['done'], data['weight'])
             critic_loss, td_error_per_sample1 = v_1step_td_error(td_data, self._gamma)
             loss_dict['critic_loss'] = critic_loss
             # critic network2(twin network)
             td_data_twin = v_1step_td_data(q_value[1], target_q_value, reward, data['done'], data['weight'])
             critic_twin_loss, td_error_per_sample2 = v_1step_td_error(td_data_twin, self._gamma)
             loss_dict['critic_twin_loss'] = critic_twin_loss
             td_error_per_sample = (td_error_per_sample1 + td_error_per_sample2) / 2
         else:
             # DDPG: single critic network
+            q_value_dict['q_value'] = q_value.mean().data.item()
+            target_q_value_dict['target q_value'] = target_q_value.mean().data.item()
             td_data = v_1step_td_data(q_value, target_q_value, reward, data['done'], data['weight'])
             critic_loss, td_error_per_sample = v_1step_td_error(td_data, self._gamma)
             loss_dict['critic_loss'] = critic_loss
         # ================
         # critic update
         # ================
         self._optimizer_critic.zero_grad()
@@ -310,14 +306,15 @@
             'cur_lr_critic': self._optimizer_critic.defaults['lr'],
             # 'q_value': np.array(q_value).mean(),
             'action': action_log_value,
             'priority': td_error_per_sample.abs().tolist(),
             'td_error': td_error_per_sample.abs().mean(),
             **loss_dict,
             **q_value_dict,
+            **target_q_value_dict,
         }
 
     def _state_dict_learn(self) -> Dict[str, Any]:
         return {
             'model': self._learn_model.state_dict(),
             'target_model': self._target_model.state_dict(),
             'optimizer_actor': self._optimizer_actor.state_dict(),
```

### Comparing `DI-engine-0.4.6/ding/policy/decision_transformer.py` & `DI-engine-0.4.7/ding/policy/decision_transformer.py`

 * *Files 1% similar despite different names*

```diff
@@ -55,16 +55,15 @@
         warmup_steps=10000,
         max_train_iters=200,
         context_len=20,
         n_blocks=3,
         embed_dim=128,
         dropout_p=0.1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # batch_size=64,
             learning_rate=1e-4,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
         ),
         # collect_mode config
```

### Comparing `DI-engine-0.4.6/ding/policy/dqfd.py` & `DI-engine-0.4.7/ding/policy/dqfd.py`

 * *Files 2% similar despite different names*

```diff
@@ -79,16 +79,15 @@
             lambda1=1.0,  # n-step return
             lambda2=1.0,  # supervised loss
             lambda3=1e-5,  # L2
             # margin function in JE, here we implement this as a constant
             margin_function=0.8,
             # number of pertraining iterations
             per_train_iter_k=10,
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -227,15 +226,15 @@
         )
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/dqn.py` & `DI-engine-0.4.7/ding/policy/dqn.py`

 * *Files 3% similar despite different names*

```diff
@@ -11,124 +11,148 @@
 
 from .base_policy import Policy
 from .common_utils import default_preprocess_learn
 
 
 @POLICY_REGISTRY.register('dqn')
 class DQNPolicy(Policy):
-    r"""
+    """
     Overview:
         Policy class of DQN algorithm, extended by Double DQN/Dueling DQN/PER/multi-step TD.
 
     Config:
-        == ==================== ======== ============== ======================================== =======================
-        ID Symbol               Type     Default Value  Description                              Other(Shape)
-        == ==================== ======== ============== ======================================== =======================
-        1  ``type``             str      dqn            | RL policy register name, refer to      | This arg is optional,
-                                                        | registry ``POLICY_REGISTRY``           | a placeholder
-        2  ``cuda``             bool     False          | Whether to use cuda for network        | This arg can be diff-
+        == ===================== ======== ============== ======================================= =======================
+        ID Symbol                Type     Default Value  Description                              Other(Shape)
+        == ===================== ======== ============== ======================================= =======================
+        1  ``type``              str      dqn            | RL policy register name, refer to     | This arg is optional,
+                                                         | registry ``POLICY_REGISTRY``          | a placeholder
+        2  ``cuda``              bool     False          | Whether to use cuda for network       | This arg can be diff-
                                                                                                  | erent from modes
-        3  ``on_policy``        bool     False          | Whether the RL algorithm is on-policy
-                                                        | or off-policy
-        4  ``priority``         bool     False          | Whether use priority(PER)              | Priority sample,
+        3  ``on_policy``         bool     False          | Whether the RL algorithm is on-policy
+                                                         | or off-policy
+        4  ``priority``          bool     False          | Whether use priority(PER)             | Priority sample,
                                                                                                  | update priority
-        5  | ``priority_IS``    bool     False          | Whether use Importance Sampling Weight
-           | ``_weight``                                | to correct biased update. If True,
-                                                        | priority must be True.
-        6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
-           | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
-        7  ``nstep``            int      1,             | N-step reward discount sum for target
-                                         [3, 5]         | q_value estimation
-        8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
-           | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
-                                                        | valid in serial training               | means more off-policy
-        9  | ``learn.multi``    bool     False          | whether to use multi gpu during
-           | ``_gpu``
-        10 | ``learn.batch_``   int      64             | The number of samples of an iteration
+        5  | ``priority_IS``     bool     False          | Whether use Importance Sampling
+           | ``_weight``                                 | Weight to correct biased update. If
+                                                         | True, priority must be True.
+        6  | ``discount_``       float    0.97,          | Reward's future discount factor, aka. | May be 1 when sparse
+           | ``factor``                   [0.95, 0.999]  | gamma                                 | reward env
+        7  ``nstep``             int      1,             | N-step reward discount sum for target
+                                          [3, 5]         | q_value estimation
+        8  | ``model.dueling``   bool     True           | dueling head architecture
+        9  | ``model.encoder``   list     [32, 64,       | Sequence of ``hidden_size`` of        | default kernel_size
+           | ``_hidden``         (int)    64, 128]       | subsequent conv layers and the        | is [8, 4, 3]
+           | ``_size_list``                               | final dense layer.                   | default stride is
+                                                                                                 | [4, 2 ,1]
+        10 | ``learn.update``    int      3              | How many updates(iterations) to train | This args can be vary
+           | ``per_collect``                             | after collector's one collection.     | from envs. Bigger val
+                                                         | Only valid in serial training         | means more off-policy
+        11 | ``learn.batch_``    int      64             | The number of samples of an iteration
            | ``size``
-        11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
+        12 | ``learn.learning``  float    0.001          | Gradient step length of an iteration.
            | ``_rate``
-        12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
+        13 | ``learn.target_``   int      100            | Frequence of target network update.   | Hard(assign) update
            | ``update_freq``
-        13 | ``learn.ignore_``  bool     False          | Whether ignore done for target value   | Enable it for some
-           | ``done``                                   | calculation.                           | fake termination env
-        14 ``collect.n_sample`` int      [8, 128]       | The number of training samples of a    | It varies from
-                                                        | call of collector.                     | different envs
-        15 | ``collect.unroll`` int      1              | unroll length of an iteration          | In RNN, unroll_len>1
+        14 | ``learn.target_``   float    0.005          | Frequence of target network update.   | Soft(assign) update
+           | ``theta``                                   | Only one of [target_update_freq,
+           |                                             | target_theta] should be set
+        15 | ``learn.ignore_``   bool     False          | Whether ignore done for target value  | Enable it for some
+           | ``done``                                    | calculation.                          | fake termination env
+        16 ``collect.n_sample``  int      [8, 128]       | The number of training samples of a   | It varies from
+                                                         | call of collector.                    | different envs
+        17 ``collect.n_episode`` int      8              | The number of training episodes of a  | only one of [n_sample
+                                                         | call of collector                     | ,n_episode] should
+                                                         |                                       | be set
+        18 | ``collect.unroll``  int      1              | unroll length of an iteration         | In RNN, unroll_len>1
            | ``_len``
-        16 | ``other.eps.type`` str      exp            | exploration rate decay type            | Support ['exp',
+        19 | ``other.eps.type``  str      exp            | exploration rate decay type           | Support ['exp',
                                                                                                  | 'linear'].
-        17 | ``other.eps.``     float    0.95           | start value of exploration rate        | [0,1]
+        20 | ``other.eps.``      float    0.95           | start value of exploration rate       | [0,1]
            | ``start``
-        18 | ``other.eps.``     float    0.1            | end value of exploration rate          | [0,1]
+        21 | ``other.eps.``      float    0.1            | end value of exploration rate         | [0,1]
            | ``end``
-        19 | ``other.eps.``     int      10000          | decay length of exploration            | greater than 0. set
+        22 | ``other.eps.``      int      10000          | decay length of exploration           | greater than 0. set
            | ``decay``                                                                           | decay=10000 means
                                                                                                  | the exploration rate
                                                                                                  | decay from start
                                                                                                  | value to end value
                                                                                                  | during decay length.
-        == ==================== ======== ============== ======================================== =======================
+        == ===================== ======== ============== ======================================= =======================
     """
 
     config = dict(
+        # (str) RL policy register name (refer to function "POLICY_REGISTRY").
         type='dqn',
-        # (bool) Whether use cuda in policy
+        # (bool) Whether use cuda in policy.
         cuda=False,
-        # (bool) Whether learning policy is the same as collecting data policy(on-policy)
+        # (bool) Whether learning policy is the same as collecting data policy(on-policy).
         on_policy=False,
-        # (bool) Whether enable priority experience sample
+        # (bool) Whether enable priority experience sample.
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
-        # (float) Discount factor(gamma) for returns
+        # (float) Discount factor(gamma) for returns.
         discount_factor=0.97,
-        # (int) The number of step for calculating target q_value
+        # (int) The number of step for calculating target q_value.
         nstep=1,
+        model=dict(
+            #(list(int)) Sequence of ``hidden_size`` of subsequent conv layers and the final dense layer.
+            encoder_hidden_size_list=[128, 128, 64],
+        ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
+            # (int) How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
-            # (int) How many samples in a training batch
+            # (int) How many samples in a training batch.
             batch_size=64,
-            # (float) The step size of gradient descent
+            # (float) The step size of gradient descent.
             learning_rate=0.001,
-            # ==============================================================
-            # The following configs are algorithm-specific
-            # ==============================================================
             # (int) Frequence of target network update.
+            # Only one of [target_update_freq, target_theta] should be set.
             target_update_freq=100,
-            # (bool) Whether ignore done(usually for max step termination env)
+            # (float) : Used for soft update of the target network.
+            # aka. Interpolation factor in EMA update for target network.
+            # Only one of [target_update_freq, target_theta] should be set.
+            target_theta=0.005,
+            # (bool) Whether ignore done(usually for max step termination env).
+            # Note: Gym wraps the MuJoCo envs by default with TimeLimit environment wrappers.
+            # These limit HalfCheetah, and several other MuJoCo envs, to max length of 1000.
+            # However, interaction with HalfCheetah always gets done with done is False,
+            # Since we inplace done==True with done==False to keep
+            # TD-error accurate computation(``gamma * (1 - done) * next_v + reward``),
+            # when the episode step is greater than max episode step.
             ignore_done=False,
         ),
         # collect_mode config
         collect=dict(
-            # (int) Only one of [n_sample, n_episode] shoule be set
-            # n_sample=8,
-            # (int) Cut trajectories into pieces with length "unroll_len".
+            # (int) How many training samples collected in one collection procedure.
+            # Only one of [n_sample, n_episode] shoule be set.
+            n_sample=8,
+            # (int) Split episodes or trajectories into pieces with length `unroll_len`.
             unroll_len=1,
         ),
         eval=dict(),
         # other config
         other=dict(
             # Epsilon greedy with decay.
             eps=dict(
                 # (str) Decay type. Support ['exp', 'linear'].
                 type='exp',
-                # (float) Epsilon start value
+                # (float) Epsilon start value.
                 start=0.95,
-                # (float) Epsilon end value
+                # (float) Epsilon end value.
                 end=0.1,
-                # (int) Decay length(env step)
+                # (int) Decay length(env step).
                 decay=10000,
             ),
-            replay_buffer=dict(replay_buffer_size=10000, ),
+            replay_buffer=dict(
+                # (int) Maximum size of replay buffer. Usually, larger buffer size is good.
+                replay_buffer_size=10000,
+            ),
         ),
     )
 
     def default_model(self) -> Tuple[str, List[str]]:
         """
         Overview:
             Return this algorithm default model setting for demonstration.
@@ -185,15 +209,15 @@
             - data (:obj:`Dict[str, Any]`): Dict type data, a batch of data for training, values are torch.Tensor or \
                 np.ndarray or dict/list combinations.
         Returns:
             - info_dict (:obj:`Dict[str, Any]`): Dict type data, a info dict indicated training result, which will be \
                 recorded in text log and tensorboard, values are python scalar or a list of scalars.
         ArgumentsKeys:
             - necessary: ``obs``, ``action``, ``reward``, ``next_obs``, ``done``
-            - optional: ``value_gamma``, ``IS``
+            - optional: ``value_gamma``
         ReturnsKeys:
             - necessary: ``cur_lr``, ``total_loss``, ``priority``
             - optional: ``action_distribution``
         """
         data = default_preprocess_learn(
             data,
             use_priority=self._priority,
@@ -223,15 +247,15 @@
         loss, td_error_per_sample = q_nstep_td_error(data_n, self._gamma, nstep=self._nstep, value_gamma=value_gamma)
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
@@ -242,15 +266,15 @@
             'target_q_value': target_q_value.mean().item(),
             'priority': td_error_per_sample.abs().tolist(),
             # Only discrete action satisfying len(data['action'])==1 can return this and draw histogram on tensorboard.
             # '[histogram]action_distribution': data['action'],
         }
 
     def _monitor_vars_learn(self) -> List[str]:
-        return ['cur_lr', 'total_loss', 'q_value']
+        return ['cur_lr', 'total_loss', 'q_value', 'target_q_value']
 
     def _state_dict_learn(self) -> Dict[str, Any]:
         """
         Overview:
             Return the state_dict of learn mode, usually including model and optimizer.
         Returns:
             - state_dict (:obj:`Dict[str, Any]`): the dict of current policy learn state, for saving and restoring.
@@ -418,15 +442,14 @@
         6  | ``discount_``      float    0.97,          | Reward's future discount factor, aka.  | May be 1 when sparse
            | ``factor``                  [0.95, 0.999]  | gamma                                  | reward env
         7  ``nstep``            int      1,             | N-step reward discount sum for target
                                          [3, 5]         | q_value estimation
         8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
            | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
                                                         | valid in serial training               | means more off-policy
-        9  | ``learn.multi``    bool     False          | whether to use multi gpu during
            | ``_gpu``
         10 | ``learn.batch_``   int      64             | The number of samples of an iteration
            | ``size``
         11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
            | ``_rate``
         12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
            | ``update_freq``
@@ -465,16 +488,15 @@
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         # (float) Discount factor(gamma) for returns
         discount_factor=0.97,
         # (int) The number of step for calculating target q_value
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             # (int) How many samples in a training batch
             batch_size=64,
             # (float) The step size of gradient descent
@@ -599,15 +621,15 @@
             x_no_grad, y_no_grad = self._model_encode(data)
         # the forward function of the auxiliary network
         self._aux_model.train()
         aux_loss_learn = self._aux_model.forward(x_no_grad, y_no_grad)
         # the BP process of the auxiliary network
         self._aux_optimizer.zero_grad()
         aux_loss_learn.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._aux_model)
         self._aux_optimizer.step()
 
         # ====================
         # Q-learning forward
         # ====================
         self._learn_model.train()
@@ -637,15 +659,15 @@
         loss = aux_loss_eval + bellman_loss
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/fqf.py` & `DI-engine-0.4.7/ding/policy/fqf.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,16 +55,15 @@
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.97,
         # (int) N-step reward for target q_value estimation
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate_fraction=2.5e-9,
             learning_rate_quantile=0.00005,
@@ -197,26 +196,26 @@
             return torch.norm(torch.stack([torch.norm(p.grad.detach(), 2.0) for p in model.parameters()]), 2.0)
 
         # ====================
         # fraction_proposal network update
         # ====================
         self._fraction_loss_optimizer.zero_grad()
         fraction_loss.backward(retain_graph=True)
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         with torch.no_grad():
             total_norm_quantiles_proposal = compute_grad_norm(self._model.head.quantiles_proposal)
         self._fraction_loss_optimizer.step()
 
         # ====================
         # Q-learning update
         # ====================
         self._quantile_loss_optimizer.zero_grad()
         quantile_loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         with torch.no_grad():
             total_norm_Q = compute_grad_norm(self._model.head.Q)
             total_norm_fqf_fc = compute_grad_norm(self._model.head.fqf_fc)
             total_norm_encoder = compute_grad_norm(self._model.encoder)
         self._quantile_loss_optimizer.step()
```

### Comparing `DI-engine-0.4.6/ding/policy/ibc.py` & `DI-engine-0.4.7/ding/policy/ibc.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,14 @@
         cuda=False,
         on_policy=False,
         continuous=True,
         model=dict(stochastic_optim=dict(type='mcmc', )),
         learn=dict(
             train_epoch=30,
             batch_size=256,
-            multi_gpu=False,
             optim=dict(
                 learning_rate=1e-5,
                 weight_decay=0.0,
                 beta1=0.9,
                 beta2=0.999,
             ),
         ),
@@ -120,15 +119,15 @@
                 loss += grad_penalty
                 loss_dict['grad_penalty'] = grad_penalty.item()
             loss_dict['total_loss'] = loss.item()
 
             self._optimizer.zero_grad()
             loss.backward()
             with self._sync_timer:
-                if self._cfg.learn.multi_gpu:
+                if self._cfg.multi_gpu:
                     self.sync_gradients(self._learn_model)
             sync_time = self._sync_timer.value
             self._optimizer.step()
 
         total_time = self._timer.value
 
         return {
```

### Comparing `DI-engine-0.4.6/ding/policy/il.py` & `DI-engine-0.4.7/ding/policy/il.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,15 +26,15 @@
         cuda=True,
         # (bool) whether use on-policy training pipeline(behaviour policy and training policy are the same)
         on_policy=False,
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            multi_gpu=False,
+
             # (int) collect n_episode data, train model n_iteration time
             update_per_collect=20,
             # (int) the number of data for a train iteration
             batch_size=64,
             # (float) gradient-descent step size
             learning_rate=0.0002,
         ),
```

### Comparing `DI-engine-0.4.6/ding/policy/impala.py` & `DI-engine-0.4.7/ding/policy/impala.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,16 +51,15 @@
         # (str) Which kind of action space used in IMPALAPolicy, ['discrete', 'continuous']
         action_space='discrete',
         # (int) the trajectory length to calculate v-trace target
         unroll_len=32,
         # (bool) Whether to need policy data in process transition
         transition_with_policy_data=True,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # (int) collect n_sample data, train model update_per_collect times
             # here we follow ppo serial pipeline
             update_per_collect=4,
             # (int) the number of data for a train iteration
             batch_size=16,
             learning_rate=0.0005,
             # (float) loss weight of the value network, the weight of policy network is set to 1
```

### Comparing `DI-engine-0.4.6/ding/policy/iqn.py` & `DI-engine-0.4.7/ding/policy/iqn.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,16 +54,14 @@
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.97,
         # (int) N-step reward for target q_value estimation
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -167,15 +165,15 @@
         )
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/madqn.py` & `DI-engine-0.4.7/ding/policy/madqn.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,16 +23,14 @@
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         nstep=3,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=20,
             batch_size=32,
             learning_rate=0.0005,
             clip_value=100,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/mbpolicy/mbsac.py` & `DI-engine-0.4.7/ding/policy/mbpolicy/mbsac.py`

 * *Files 2% similar despite different names*

```diff
@@ -48,21 +48,14 @@
             # (float) Max norm of gradients.
             grad_clip=100,
             # (bool) Whether to sample states or transitions from environment buffer.
             sample_state=True,
         )
     )
 
-    @classmethod
-    def default_config(cls: type) -> EasyDict:
-        cfg = copy.deepcopy(cls.config)
-        cfg = EasyDict(deep_merge_dicts(super().config, cfg))
-        cfg.cfg_type = cls.__name__ + 'Dict'
-        return cfg
-
     def _init_learn(self) -> None:
         super()._init_learn()
         self._target_model.requires_grad_(False)
 
         self._lambda = self._cfg.learn.lambda_
         self._grad_clip = self._cfg.learn.grad_clip
         self._sample_state = self._cfg.learn.sample_state
@@ -94,14 +87,15 @@
 
         def critic_fn(obss: Tensor, actions: Tensor, model: nn.Module):
             eval_data = {'obs': obss, 'action': actions}
             q_values = model.forward(eval_data, mode='compute_critic')['q_value']
             return q_values
 
         self._critic_fn = critic_fn
+        self._forward_learn_cnt = 0
 
     def _forward_learn(self, data: dict, world_model, envstep) -> Dict[str, Any]:
         # preprocess data
         data = default_preprocess_learn(
             data,
             use_priority=self._priority,
             use_priority_IS_weight=self._cfg.priority_IS_weight,
@@ -251,21 +245,14 @@
             # (float) Max norm of gradients.
             grad_clip=100,
             # (int) The number of ensemble world models.
             ensemble_size=1,
         )
     )
 
-    @classmethod
-    def default_config(cls: type) -> EasyDict:
-        cfg = copy.deepcopy(cls.config)
-        cfg = EasyDict(deep_merge_dicts(super().config, cfg))
-        cfg.cfg_type = cls.__name__ + 'Dict'
-        return cfg
-
     def _init_learn(self) -> None:
         super()._init_learn()
         self._target_model.requires_grad_(False)
 
         self._grad_clip = self._cfg.learn.grad_clip
         self._ensemble_size = self._cfg.learn.ensemble_size
         self._auto_alpha = self._cfg.learn.auto_alpha
@@ -290,14 +277,15 @@
 
         def critic_fn(obss: Tensor, actions: Tensor, model: nn.Module):
             eval_data = {'obs': obss, 'action': actions}
             q_values = model.forward(eval_data, mode='compute_critic')['q_value']
             return q_values
 
         self._critic_fn = critic_fn
+        self._forward_learn_cnt = 0
 
     def _forward_learn(self, data: dict, world_model, envstep) -> Dict[str, Any]:
         # preprocess data
         data = default_preprocess_learn(
             data,
             use_priority=self._priority,
             use_priority_IS_weight=self._cfg.priority_IS_weight,
```

### Comparing `DI-engine-0.4.6/ding/policy/mbpolicy/utils.py` & `DI-engine-0.4.7/ding/policy/mbpolicy/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/policy/ngu.py` & `DI-engine-0.4.7/ding/policy/ngu.py`

 * *Files 1% similar despite different names*

```diff
@@ -81,16 +81,14 @@
         # caused by off-policy
         burnin_step=20,
         # (int) <learn_unroll_len> is the total length of [sequence sample] minus
         # the length of burnin part in [sequence sample],
         # i.e., <sequence sample length> = <unroll_len> = <burnin_step> + <learn_unroll_len>
         learn_unroll_len=80,  # set this key according to the episode length
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.0001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
             # (float type) target_update_theta: Used for soft update of the target network,
```

### Comparing `DI-engine-0.4.6/ding/policy/offppo_collect_traj.py` & `DI-engine-0.4.7/ding/policy/offppo_collect_traj.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,16 +33,15 @@
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         # (bool) Whether to use nstep_return for value loss
         nstep_return=False,
         nstep=3,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=5,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/pdqn.py` & `DI-engine-0.4.7/ding/policy/pdqn.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,15 +39,14 @@
         7  ``nstep``            int      1,             | N-step reward discount sum for target
                                          [3, 5]         | q_value estimation
         8  | ``learn.update``   int      3              | How many updates(iterations) to train  | This args can be vary
            | ``per_collect``                            | after collector's one collection. Only | from envs. Bigger val
                                                         | valid in serial training               | means more off-policy
         9  | ``learn.batch_``   int      64             | The number of samples of an iteration
            | ``size``
-        10 | ``learn.multi``    bool     False          | whether to use multi gpu during
            | ``_gpu``
         11 | ``learn.learning`` float    0.001          | Gradient step length of an iteration.
            | ``_rate``
         12 | ``learn.target_``  int      100            | Frequence of target network update.    | Hard(assign) update
            | ``update_freq``
         13 | ``learn.ignore_``  bool     False          | Whether ignore done for target value   | Enable it for some
            | ``done``                                   | calculation.                           | fake termination env
@@ -77,16 +76,15 @@
         on_policy=False,
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         discount_factor=0.97,
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/pg.py` & `DI-engine-0.4.7/ding/policy/pg.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,16 +24,15 @@
         # (bool) whether use on-policy training pipeline(behaviour policy and training policy are the same)
         on_policy=True,  # for pg strictly on policy algorithm, this line should not be modified by users
         # (str) action space type: ['discrete', 'continuous']
         action_space='discrete',
         # (bool) whether to use deterministic action for evaluation.
         deterministic_eval=True,
         learn=dict(
-            # (bool) whether to use multi gpu
-            multi_gpu=False,
+
             # (int) the number of samples for one update.
             batch_size=64,
             # (float) the step size of one gradient descend.
             learning_rate=0.001,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/policy_factory.py` & `DI-engine-0.4.7/ding/policy/policy_factory.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/policy/ppg.py` & `DI-engine-0.4.7/ding/policy/ppg.py`

 * *Files 0% similar despite different names*

```diff
@@ -87,16 +87,14 @@
         cuda=False,
         # (bool) Whether the RL algorithm is on-policy or off-policy. (Note: in practice PPO can be off-policy used)
         on_policy=True,
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             actor_epoch_per_collect=1,
             critic_epoch_per_collect=1,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
@@ -668,16 +666,14 @@
         on_policy=False,
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         # (bool) Whether to need policy data in process transition
         transition_with_policy_data=True,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=5,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
             # (float) The loss weight of value network, policy network weight is set to 1
```

### Comparing `DI-engine-0.4.6/ding/policy/ppo.py` & `DI-engine-0.4.7/ding/policy/ppo.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,16 +40,14 @@
         # (bool) Whether to use nstep return to calculate value target, otherwise, use return = adv + value
         nstep_return=False,
         # (bool) Whether to enable multi-agent training, i.e.: MAPPO
         multi_agent=False,
         # (bool) Whether to need policy data in process transition
         transition_with_policy_data=True,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             epoch_per_collect=10,
             batch_size=64,
             learning_rate=3e-4,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
             # (float) The loss weight of value network, policy network weight is set to 1
@@ -474,16 +472,14 @@
         # (str) Which kind of action space used in PPOPolicy, ['discrete', 'continuous', 'hybrid']
         action_space='discrete',
         # (bool) Whether to enable multi-agent training, i.e.: MAPPO
         multi_agent=False,
         # (bool) Whether to need policy data in process transition
         transition_with_policy_data=True,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             epoch_per_collect=10,
             batch_size=64,
             learning_rate=3e-4,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
             # (float) The loss weight of entropy regularization, policy network weight is set to 1
@@ -685,16 +681,14 @@
         priority_IS_weight=False,
         # (bool) Whether to use nstep_return for value loss
         nstep_return=False,
         nstep=3,
         # (bool) Whether to need policy data in process transition
         transition_with_policy_data=True,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=5,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -1009,16 +1003,14 @@
             heads=[1, 1],
             # (str) the contrastive loss type.
             loss_type='infonce',
             # (float) a parameter to adjust the polarity between positive and negative samples.
             temperature=1.0,
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             epoch_per_collect=10,
             batch_size=64,
             learning_rate=3e-4,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
             # (float) The loss weight of value network, policy network weight is set to 1
@@ -1164,15 +1156,15 @@
                     x_no_grad, y_no_grad = self._model_encode(batch)
                 # the forward function of the auxiliary network
                 self._aux_model.train()
                 aux_loss_learn = self._aux_model.forward(x_no_grad, y_no_grad)
                 # the BP process of the auxiliary network
                 self._aux_optimizer.zero_grad()
                 aux_loss_learn.backward()
-                if self._cfg.learn.multi_gpu:
+                if self._cfg.multi_gpu:
                     self.sync_gradients(self._aux_model)
                 self._aux_optimizer.step()
 
                 output = self._learn_model.forward(batch['obs'], mode='compute_actor_critic')
                 adv = batch['adv']
                 if self._adv_norm:
                     # Normalize advantage in a train_batch
```

### Comparing `DI-engine-0.4.6/ding/policy/ppof.py` & `DI-engine-0.4.7/ding/policy/ppof.py`

 * *Files 9% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import numpy as np
 import torch
 import treetensor.torch as ttorch
 from torch.optim import AdamW
 
 from ding.rl_utils import ppo_data, ppo_error, ppo_policy_error, ppo_policy_data, gae, gae_data, ppo_error_continuous, \
     get_gae, ppo_policy_error_continuous, ArgmaxSampler, MultinomialSampler, ReparameterizationSampler, MuSampler, \
-    HybridStochasticSampler, HybridDeterminsticSampler
+    HybridStochasticSampler, HybridDeterminsticSampler, value_transform, value_inv_transform, symlog, inv_symlog
 from ding.utils import POLICY_REGISTRY, RunningMeanStd
 
 
 @POLICY_REGISTRY.register('ppof')
 class PPOFPolicy:
     config = dict(
         type='ppo',
@@ -28,15 +28,15 @@
         batch_size=64,
         learning_rate=3e-4,
         weight_decay=0,
         value_weight=0.5,
         entropy_weight=0.01,
         clip_ratio=0.2,
         adv_norm=True,
-        value_norm=True,
+        value_norm='symlog',
         ppo_param_init=True,
         grad_norm=0.5,
         # collect
         n_sample=128,
         unroll_len=1,
         # eval
         deterministic_eval=True,
@@ -144,33 +144,56 @@
         bs = self._cfg.batch_size
         data = data[:self._cfg.n_sample // bs * bs]  # rounding
 
         # outer training loop
         for epoch in range(self._cfg.epoch_per_collect):
             # recompute adv
             with torch.no_grad():
+                # get the value dictionary
+                # In popart, the dictionary has two keys: 'pred' and 'unnormalized_pred'
                 value = self._model.compute_critic(data.obs)
                 next_value = self._model.compute_critic(data.next_obs)
-                if self._cfg.value_norm:
-                    value *= self._running_mean_std.std
-                    next_value *= self._running_mean_std.std
+                reward = data.reward
+
+                assert self._cfg.value_norm in ['popart', 'value_rescale', 'symlog'],\
+                    'Not supported value normalization! Value normalization supported: popart, value rescale, symlog'
+
+                if self._cfg.value_norm == 'popart':
+                    unnormalized_value = value['unnormalized_pred']
+                    unnormalized_next_value = value['unnormalized_pred']
+
+                    mu = self._model.critic_head.popart.mu
+                    sigma = self._model.critic_head.popart.sigma
+                    reward = (reward - mu) / sigma
+
+                    value = value['pred']
+                    next_value = next_value['pred']
+                elif self._cfg.value_norm == 'value_rescale':
+                    value = value_inv_transform(value['pred'])
+                    next_value = value_inv_transform(next_value['pred'])
+                elif self._cfg.value_norm == 'symlog':
+                    value = inv_symlog(value['pred'])
+                    next_value = inv_symlog(next_value['pred'])
 
                 traj_flag = data.get('traj_flag', None)  # traj_flag indicates termination of trajectory
-                adv_data = gae_data(value, next_value, data.reward, data.done, traj_flag)
+                adv_data = gae_data(value, next_value, reward, data.done, traj_flag)
                 data.adv = gae(adv_data, self._cfg.discount_factor, self._cfg.gae_lambda)
 
-                unnormalized_returns = value + data.adv
+                unnormalized_returns = value + data.adv  # In popart, this return is normalized
 
-                if self._cfg.value_norm:
-                    data.value = value / self._running_mean_std.std
-                    data.return_ = unnormalized_returns / self._running_mean_std.std
-                    self._running_mean_std.update(unnormalized_returns.cpu().numpy())
-                else:
-                    data.value = value
-                    data.return_ = unnormalized_returns
+                if self._cfg.value_norm == 'popart':
+                    self._model.critic_head.popart.update_parameters((data.reward).unsqueeze(1))
+                elif self._cfg.value_norm == 'value_rescale':
+                    value = value_transform(value)
+                    unnormalized_returns = value_transform(unnormalized_returns)
+                elif self._cfg.value_norm == 'symlog':
+                    value = symlog(value)
+                    unnormalized_returns = symlog(unnormalized_returns)
+                data.value = value
+                data.return_ = unnormalized_returns
 
             # inner training loop
             split_data = ttorch.split(data, self._cfg.batch_size)
             random.shuffle(list(split_data))
             for batch in split_data:
                 output = self._model.compute_actor_critic(batch.obs)
                 adv = batch.adv
```

### Comparing `DI-engine-0.4.6/ding/policy/qmix.py` & `DI-engine-0.4.7/ding/policy/qmix.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,16 +52,14 @@
         # (bool) Whether the RL algorithm is on-policy or off-policy.
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=20,
             batch_size=32,
             learning_rate=0.0005,
             clip_value=100,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/qrdqn.py` & `DI-engine-0.4.7/ding/policy/qrdqn.py`

 * *Files 3% similar despite different names*

```diff
@@ -54,16 +54,15 @@
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.97,
         # (int) N-step reward for target q_value estimation
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -162,15 +161,15 @@
         )
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/qtran.py` & `DI-engine-0.4.7/ding/policy/qtran.py`

 * *Files 1% similar despite different names*

```diff
@@ -50,16 +50,14 @@
         # (bool) Whether the RL algorithm is on-policy or off-policy.
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=20,
             batch_size=32,
             learning_rate=0.0005,
             clip_value=1.5,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/r2d2.py` & `DI-engine-0.4.7/ding/policy/r2d2.py`

 * *Files 0% similar despite different names*

```diff
@@ -82,16 +82,14 @@
         # (int) the timestep of burnin operation, which is designed to RNN hidden state difference
         # caused by off-policy
         burnin_step=20,
         # (int) the trajectory length to unroll the RNN network minus
         # the timestep of burnin operation
         learn_unroll_len=80,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.0001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
             # (int) Frequence of target network update.
```

### Comparing `DI-engine-0.4.6/ding/policy/r2d2_collect_traj.py` & `DI-engine-0.4.7/ding/policy/r2d2_collect_traj.py`

 * *Files 1% similar despite different names*

```diff
@@ -81,16 +81,14 @@
         # (int) the timestep of burnin operation, which is designed to RNN hidden state difference
         # caused by off-policy
         burnin_step=2,
         # (int) the trajectory length to unroll the RNN network minus
         # the timestep of burnin operation
         unroll_len=80,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.0001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
             # (int) Frequence of target network update.
```

### Comparing `DI-engine-0.4.6/ding/policy/r2d2_gtrxl.py` & `DI-engine-0.4.7/ding/policy/r2d2_gtrxl.py`

 * *Files 0% similar despite different names*

```diff
@@ -87,16 +87,14 @@
         # how many steps to use as burnin
         burnin_step=1,
         # (int) trajectory length
         unroll_len=25,
         # (int) training sequence length
         seq_len=20,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.0001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
             # (int) Frequence of target network update.
```

### Comparing `DI-engine-0.4.6/ding/policy/r2d3.py` & `DI-engine-0.4.7/ding/policy/r2d3.py`

 * *Files 0% similar despite different names*

```diff
@@ -81,16 +81,14 @@
         # (int) the timestep of burnin operation, which is designed to RNN hidden state difference
         # caused by off-policy
         burnin_step=2,
         # (int) the trajectory length to unroll the RNN network minus
         # the timestep of burnin operation
         learn_unroll_len=80,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=1,
             batch_size=64,
             learning_rate=0.0001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
             # (int) Frequence of target network update.
```

### Comparing `DI-engine-0.4.6/ding/policy/rainbow.py` & `DI-engine-0.4.7/ding/policy/rainbow.py`

 * *Files 1% similar despite different names*

```diff
@@ -83,16 +83,15 @@
             n_atom=51,
         ),
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.99,
         # (int) N-step reward for target q_value estimation
         nstep=3,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             batch_size=32,
             learning_rate=0.001,
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/sac.py` & `DI-engine-0.4.7/ding/policy/sac.py`

 * *Files 20% similar despite different names*

```diff
@@ -16,211 +16,168 @@
 from .common_utils import default_preprocess_learn
 
 
 @POLICY_REGISTRY.register('sac_discrete')
 class SACDiscretePolicy(Policy):
     r"""
        Overview:
-           Policy class of discrete SAC algorithm.
+           Policy class of discrete SAC algorithm. Paper link: https://arxiv.org/pdf/1910.07207.pdf.
 
        Config:
            == ====================  ========    =============  ================================= =======================
-           ID Symbol                Type        Default Value  Description                       Other(Shape)
+           ID Symbol                Type        Default Value  Description                       Other
            == ====================  ========    =============  ================================= =======================
-           1  ``type``              str         td3            | RL policy register name, refer  | this arg is optional,
+           1  ``type``              str         sac_discrete   | RL policy register name, refer  | this arg is optional,
                                                                | to registry ``POLICY_REGISTRY`` | a placeholder
            2  ``cuda``              bool        True           | Whether to use cuda for network |
-           3  | ``random_``         int         10000          | Number of randomly collected    | Default to 10000 for
+           3  ``on_policy``         bool        False          | SACDiscrete is an off-policy    |
+                                                               | algorithm.                      |
+           4  ``priority``          bool        False          | Whether to use priority         |
+                                                               | sampling in buffer.             |
+           5  | ``priority_IS_``    bool        False          | Whether use Importance Sampling |
+              | ``weight``                                     | weight to correct biased update |
+           6  | ``random_``         int         10000          | Number of randomly collected    | Default to 10000 for
               | ``collect_size``                               | training samples in replay      | SAC, 25000 for DDPG/
               |                                                | buffer when training starts.    | TD3.
-           4  | ``model.policy_``   int         256            | Linear layer size for policy    |
-              | ``embedding_size``                             | network.                        |
-           5  | ``model.soft_q_``   int         256            | Linear layer size for soft q    |
-              | ``embedding_size``                             | network.                        |
-           6  | ``model.value_``    int         256            | Linear layer size for value     | Defalut to None when
-              | ``embedding_size``                             | network.                        | model.value_network
-              |                                                |                                 | is False.
-           7  | ``learn.learning``  float       3e-4           | Learning rate for soft q        | Defalut to 1e-3, when
-              | ``_rate_q``                                    | network.                        | model.value_network
-              |                                                |                                 | is True.
-           8  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to 1e-3, when
-              | ``_rate_policy``                               | network.                        | model.value_network
-              |                                                |                                 | is True.
-           9  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to None when
-              | ``_rate_value``                                | network.                        | model.value_network
-              |                                                |                                 | is False.
-           10 | ``learn.alpha``     float       0.2            | Entropy regularization          | alpha is initiali-
+           7  | ``learn.learning``  float       3e-4           | Learning rate for soft q        | Defalut to 1e-3
+              | ``_rate_q``                                    | network.                        |
+           8  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to 1e-3
+              | ``_rate_policy``                               | network.                        |
+           9  | ``learn.alpha``     float       0.2            | Entropy regularization          | alpha is initiali-
               |                                                | coefficient.                    | zation for auto
               |                                                |                                 | `\alpha`, when
               |                                                |                                 | auto_alpha is True
-           11 | ``learn.repara_``   bool        True           | Determine whether to use        |
-              | ``meterization``                               | reparameterization trick.       |
-           12 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter
+           10 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter
               | ``auto_alpha``                                 | auto temperature parameter      | determines the
               |                                                | `\alpha`.                       | relative importance
               |                                                |                                 | of the entropy term
               |                                                |                                 | against the reward.
-           13 | ``learn.-``         bool        False          | Determine whether to ignore     | Use ignore_done only
-              | ``ignore_done``                                | done flag.                      | in halfcheetah env.
-           14 | ``learn.-``         float       0.005          | Used for soft update of the     | aka. Interpolation
+           11 | ``learn.-``         bool        False          | Determine whether to ignore     | Use ignore_done only
+              | ``ignore_done``                                | done flag.                      | in env like Pendulum
+           12 | ``learn.-``         float       0.005          | Used for soft update of the     | aka. Interpolation
               | ``target_theta``                               | target network.                 | factor in polyak aver
               |                                                |                                 | aging for target
               |                                                |                                 | networks.
            == ====================  ========    =============  ================================= =======================
        """
 
     config = dict(
         # (str) RL policy register name (refer to function "POLICY_REGISTRY").
         type='sac_discrete',
-        # (bool) Whether to use cuda for network.
+        # (bool) Whether to use cuda for network and loss computation.
         cuda=False,
-        # (bool type) on_policy: Determine whether on-policy or off-policy.
-        # on-policy setting influences the behaviour of buffer.
-        # Default False in SAC.
+        # (bool) Whether to belong to on-policy or off-policy algorithm, SACDiscrete is an off-policy algorithm.
         on_policy=False,
-        # (bool type) priority: Determine whether to use priority in buffer sample.
-        # Default False in SAC.
+        # (bool) Whether to use priority sampling in buffer. Default to False in SACDiscrete.
         priority=False,
-        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
+        # (bool) Whether use Importance Sampling weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
-        # (int) Number of training samples(randomly collected) in replay buffer when training starts.
-        # Default 10000 in SAC.
+        # (int) Number of training samples (randomly collected) in replay buffer when training starts.
         random_collect_size=10000,
-        # (bool) Whether to need policy data in process transition
+        # (bool) Whether to need policy-specific data in process transition.
         transition_with_policy_data=True,
-        # (bool) Whether to enable multi-agent training setting
+        # (bool) Whether to enable multi-agent training setting.
         multi_agent=False,
         model=dict(
-            # (bool type) twin_critic: Determine whether to use double-soft-q-net for target q computation.
-            # Please refer to TD3 about Clipped Double-Q Learning trick, which learns two Q-functions instead of one .
-            # Default to True.
+            # (bool) Whether to use double-soft-q-net for target q computation.
+            # For more details, please refer to TD3 about Clipped Double-Q Learning trick.
             twin_critic=True,
-
-            # (bool type) value_network: Determine whether to use value network as the
-            # original SAC paper (arXiv 1801.01290).
-            # using value_network needs to set learning_rate_value, learning_rate_q,
-            # and learning_rate_policy in `cfg.policy.learn`.
-            # Default to False.
-            # value_network=False,
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
+            # (int) How many updates (iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
-            # (int) Minibatch size for gradient descent.
+            # (int) Minibatch size for one gradient descent.
             batch_size=256,
-
-            # (float type) learning_rate_q: Learning rate for soft q network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
+            # (float) Learning rate for soft q network.
             learning_rate_q=3e-4,
-            # (float type) learning_rate_policy: Learning rate for policy network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
+            # (float) Learning rate for policy network.
             learning_rate_policy=3e-4,
-            # (float type) learning_rate_value: Learning rate for value network.
-            # `learning_rate_value` should be initialized, when model.value_network is True.
-            # Please set to 3e-4, when model.value_network is True.
-            learning_rate_value=3e-4,
-
-            # (float type) learning_rate_alpha: Learning rate for auto temperature parameter `\alpha`.
-            # Default to 3e-4.
+            # (float) Learning rate for auto temperature parameter `\alpha`.
             learning_rate_alpha=3e-4,
-            # (float type) target_theta: Used for soft update of the target network,
-            # aka. Interpolation factor in polyak averaging for target networks.
-            # Default to 0.005.
+            # (float) Used for soft update of the target network,
+            # aka. Interpolation factor in EMA update for target network.
             target_theta=0.005,
-            # (float) discount factor for the discounted sum of rewards, aka. gamma.
+            # (float) Discount factor for the discounted sum of rewards, aka. gamma.
             discount_factor=0.99,
-
-            # (float type) alpha: Entropy regularization coefficient.
+            # (float) Entropy regularization coefficient in SAC.
             # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
             # If auto_alpha is set  to `True`, alpha is initialization for auto `\alpha`.
-            # Default to 0.2.
             alpha=0.2,
-
-            # (bool type) auto_alpha: Determine whether to use auto temperature parameter `\alpha` .
-            # Temperature parameter determines the relative importance of the entropy term against the reward.
+            # (bool) Whether to use auto temperature parameter `\alpha` .
+            # Temperature parameter `\alpha` determines the relative importance of the entropy term against the reward.
             # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
-            # Default to False.
-            # Note that: Using auto alpha needs to set learning_rate_alpha in `cfg.policy.learn`.
+            # Note that: Using auto alpha needs to set the above `learning_rate_alpha`.
             auto_alpha=True,
-            # (bool type) log_space: Determine whether to use auto `\alpha` in log space.
+            # (bool) Whether to use auto `\alpha` in log space.
             log_space=True,
+            # (float) Target policy entropy value for auto temperature (alpha) adjustment.
+            target_entropy=None,
             # (bool) Whether ignore done(usually for max step termination env. e.g. pendulum)
             # Note: Gym wraps the MuJoCo envs by default with TimeLimit environment wrappers.
             # These limit HalfCheetah, and several other MuJoCo envs, to max length of 1000.
             # However, interaction with HalfCheetah always gets done with done is False,
             # Since we inplace done==True with done==False to keep
             # TD-error accurate computation(``gamma * (1 - done) * next_v + reward``),
             # when the episode step is greater than max episode step.
             ignore_done=False,
-            # (float) Weight uniform initialization range in the last output layer
+            # (float) Weight uniform initialization max range in the last output layer
             init_w=3e-3,
         ),
         collect=dict(
-            # You can use either "n_sample" or "n_episode" in actor.collect.
-            # Get "n_sample" samples per collect.
-            # Default n_sample to 1.
-            # n_sample=1,
-            # (int) Cut trajectories into pieces with length "unroll_len".
+            # (int) How many training samples collected in one collection procedure.
+            n_sample=1,
+            # (int) Split episodes or trajectories into pieces with length `unroll_len`.
             unroll_len=1,
+            # (bool) Whether to collect logit in `process_transition`.
+            # In some algorithm like guided cost learning, we need to use logit to train the reward model.
+            collector_logit=False,
         ),
-        eval=dict(),
         other=dict(
             replay_buffer=dict(
-                # (int type) replay_buffer_size: Max size of replay buffer.
+                # (int) Maximum size of replay buffer. Usually, larger buffer size is good
+                # for SAC but cost more storage.
                 replay_buffer_size=1000000,
-                # (int type) max_use: Max use times of one data in the buffer.
-                # Data will be removed once used for too many times.
-                # Default to infinite.
-                # max_use=256,
             ),
         ),
     )
-    r"""
-    Overview:
-        Policy class of SAC algorithm.
-    """
 
     def default_model(self) -> Tuple[str, List[str]]:
         if self._cfg.multi_agent:
             return 'maqac', ['ding.model.template.maqac']
         else:
             return 'discrete_qac', ['ding.model.template.qac']
 
     def _init_learn(self) -> None:
-        r"""
+        """
         Overview:
             Learn mode init method. Called by ``self.__init__``.
-            Init q, value and policy's optimizers, algorithm config, main and target models.
+            Init q function and policy's optimizers, algorithm config, main and target models.
         """
-        # Init
         self._priority = self._cfg.priority
         self._priority_IS_weight = self._cfg.priority_IS_weight
-        # self._value_network = False  # TODO self._cfg.model.value_network
         self._twin_critic = self._cfg.model.twin_critic
 
         self._optimizer_q = Adam(
             self._model.critic.parameters(),
             lr=self._cfg.learn.learning_rate_q,
         )
         self._optimizer_policy = Adam(
             self._model.actor.parameters(),
             lr=self._cfg.learn.learning_rate_policy,
         )
 
-        # Algorithm config
+        # Algorithm-Specific Config
         self._gamma = self._cfg.learn.discount_factor
-        # Init auto alpha
         if self._cfg.learn.auto_alpha:
-            self._target_entropy = self._cfg.learn.get('target_entropy', -np.prod(self._cfg.model.action_shape))
+            if self._cfg.learn.target_entropy is None:
+                assert 'action_shape' in self._cfg.model, "SACDiscrete need network model with action_shape variable"
+                self._target_entropy = -np.prod(self._cfg.model.action_shape)
+            else:
+                self._target_entropy = self._cfg.learn.target_entropy
             if self._cfg.learn.log_space:
                 self._log_alpha = torch.log(torch.FloatTensor([self._cfg.learn.alpha]))
                 self._log_alpha = self._log_alpha.to(self._device).requires_grad_()
                 self._alpha_optim = torch.optim.Adam([self._log_alpha], lr=self._cfg.learn.learning_rate_alpha)
                 assert self._log_alpha.shape == torch.Size([1]) and self._log_alpha.requires_grad
                 self._alpha = self._log_alpha.detach().exp()
                 self._auto_alpha = True
@@ -244,25 +201,15 @@
             update_type='momentum',
             update_kwargs={'theta': self._cfg.learn.target_theta}
         )
         self._learn_model = model_wrap(self._model, wrapper_name='base')
         self._learn_model.reset()
         self._target_model.reset()
 
-        self._forward_learn_cnt = 0
-
     def _forward_learn(self, data: dict) -> Dict[str, Any]:
-        r"""
-        Overview:
-            Forward and backward function of learn mode.
-        Arguments:
-            - data (:obj:`dict`): Dict type data, including at least ['obs', 'action', 'reward', 'next_obs']
-        Returns:
-            - info_dict (:obj:`Dict[str, Any]`): Including current lr and loss.
-        """
         loss_dict = {}
         data = default_preprocess_learn(
             data,
             use_priority=self._priority,
             use_priority_IS_weight=self._cfg.priority_IS_weight,
             ignore_done=self._cfg.learn.ignore_done,
             use_nstep=False
@@ -281,15 +228,15 @@
 
         # 1. predict q value
         q_value = self._learn_model.forward({'obs': obs}, mode='compute_critic')['q_value']
         dist = torch.distributions.categorical.Categorical(logits=logit)
         dist_entropy = dist.entropy()
         entropy = dist_entropy.mean()
 
-        # 2. predict target value depend self._value_network.
+        # 2. predict target value
 
         # target q value. SARSA: first predict next action, then calculate next q value
         with torch.no_grad():
             policy_output_next = self._learn_model.forward({'obs': next_obs}, mode='compute_actor')
             if self._cfg.multi_agent:
                 policy_output_next['logit'][policy_output_next['action_mask'] == 0.0] = -1e8
             prob = F.softmax(policy_output_next['logit'], dim=-1)
@@ -320,14 +267,15 @@
         loss_dict['critic_loss'].backward()
         if self._twin_critic:
             loss_dict['twin_critic_loss'].backward()
         self._optimizer_q.step()
 
         # 5. evaluate to get action distribution
         policy_output = self._learn_model.forward({'obs': data['obs']}, mode='compute_actor')
+        # 6. apply discrete action mask in multi_agent setting
         if self._cfg.multi_agent:
             policy_output['logit'][policy_output['action_mask'] == 0.0] = -1e8
         logit = policy_output['logit']
         prob = F.softmax(logit, dim=-1)
         log_prob = F.log_softmax(logit, dim=-1)
 
         with torch.no_grad():
@@ -362,31 +310,26 @@
                 self._alpha_optim.zero_grad()
                 loss_dict['alpha_loss'].backward()
                 self._alpha_optim.step()
                 self._alpha.data = torch.where(self._alpha > 0, self._alpha,
                                                torch.zeros_like(self._alpha)).requires_grad_()
         loss_dict['total_loss'] = sum(loss_dict.values())
 
-        # =============
-        # after update
-        # =============
-        self._forward_learn_cnt += 1
         # target update
         self._target_model.update(self._learn_model.state_dict())
         return {
             'cur_lr_q': self._optimizer_q.defaults['lr'],
             'cur_lr_p': self._optimizer_policy.defaults['lr'],
             'priority': td_error_per_sample.abs().tolist(),
             'td_error': td_error_per_sample.detach().mean().item(),
             'alpha': self._alpha.item(),
             'q_value_1': target_q_value[0].detach().mean().item(),
             'q_value_2': target_q_value[1].detach().mean().item(),
             'target_value': target_value.detach().mean().item(),
             'entropy': entropy.item(),
-            #**loss_dict
         }
 
     def _state_dict_learn(self) -> Dict[str, Any]:
         ret = {
             'model': self._learn_model.state_dict(),
             'optimizer_q': self._optimizer_q.state_dict(),
             'optimizer_policy': self._optimizer_policy.state_dict(),
@@ -394,117 +337,71 @@
         if self._auto_alpha:
             ret.update({'optimizer_alpha': self._alpha_optim.state_dict()})
         return ret
 
     def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
         self._learn_model.load_state_dict(state_dict['model'])
         self._optimizer_q.load_state_dict(state_dict['optimizer_q'])
-        # if self._value_network:
-        #    self._optimizer_value.load_state_dict(state_dict['optimizer_value'])
         self._optimizer_policy.load_state_dict(state_dict['optimizer_policy'])
         if self._auto_alpha:
             self._alpha_optim.load_state_dict(state_dict['optimizer_alpha'])
 
     def _init_collect(self) -> None:
-        r"""
-        Overview:
-            Collect mode init method. Called by ``self.__init__``.
-            Init traj and unroll length, collect model.
-            Use action noise for exploration.
-        """
         self._unroll_len = self._cfg.collect.unroll_len
         # Empirically, we found that eps_greedy_multinomial_sample works better than multinomial_sample
         # and eps_greedy_sample, and we don't divide logit by alpha,
         # for the details please refer to ding/model/wrapper/model_wrappers
         self._collect_model = model_wrap(self._model, wrapper_name='eps_greedy_multinomial_sample')
         self._collect_model.reset()
 
     def _forward_collect(self, data: dict, eps: float) -> dict:
-        r"""
-        Overview:
-            Forward function of collect mode.
-        Arguments:
-            - data (:obj:`dict`): Dict type data, including at least ['obs'].
-        Returns:
-            - output (:obj:`dict`): Dict type data, including at least inferred action according to input obs.
-        """
         data_id = list(data.keys())
         data = default_collate(list(data.values()))
         if self._cuda:
             data = to_device(data, self._device)
         self._collect_model.eval()
         with torch.no_grad():
             output = self._collect_model.forward({'obs': data}, mode='compute_actor', eps=eps)
         if self._cuda:
             output = to_device(output, 'cpu')
         output = default_decollate(output)
         return {i: d for i, d in zip(data_id, output)}
 
     def _process_transition(self, obs: Any, model_output: dict, timestep: namedtuple) -> dict:
-        r"""
-        Overview:
-            Generate dict type transition data from inputs.
-        Arguments:
-            - obs (:obj:`Any`): Env observation
-            - model_output (:obj:`dict`): Output of collect model, including at least ['action']
-            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \
-                (here 'obs' indicates obs after env step, i.e. next_obs).
-        Return:
-            - transition (:obj:`Dict[str, Any]`): Dict type transition data.
-        """
         transition = {
             'obs': obs,
             'next_obs': timestep.obs,
             'action': model_output['action'],
             'logit': model_output['logit'],
             'reward': timestep.reward,
             'done': timestep.done,
         }
         return transition
 
     def _get_train_sample(self, data: list) -> Union[None, List[Any]]:
         return get_train_sample(data, self._unroll_len)
 
     def _init_eval(self) -> None:
-        r"""
-        Overview:
-            Evaluate mode init method. Called by ``self.__init__``.
-            Init eval model. Unlike learn and collect model, eval model does not need noise.
-        """
         self._eval_model = model_wrap(self._model, wrapper_name='argmax_sample')
         self._eval_model.reset()
 
     def _forward_eval(self, data: dict) -> dict:
-        r"""
-        Overview:
-            Forward function for eval mode, similar to ``self._forward_collect``.
-        Arguments:
-            - data (:obj:`dict`): Dict type data, including at least ['obs'].
-        Returns:
-            - output (:obj:`dict`): Dict type data, including at least inferred action according to input obs.
-        """
         data_id = list(data.keys())
         data = default_collate(list(data.values()))
         if self._cuda:
             data = to_device(data, self._device)
         self._eval_model.eval()
         with torch.no_grad():
             output = self._eval_model.forward({'obs': data}, mode='compute_actor')
         if self._cuda:
             output = to_device(output, 'cpu')
         output = default_decollate(output)
         return {i: d for i, d in zip(data_id, output)}
 
     def _monitor_vars_learn(self) -> List[str]:
-        r"""
-        Overview:
-            Return variables' name if variables are to used in monitor.
-        Returns:
-            - vars (:obj:`List[str]`): Variables' name list.
-        """
         twin_critic = ['twin_critic_loss'] if self._twin_critic else []
         if self._auto_alpha:
             return super()._monitor_vars_learn() + [
                 'alpha_loss', 'policy_loss', 'critic_loss', 'cur_lr_q', 'cur_lr_p', 'target_q_value', 'q_value_1',
                 'q_value_2', 'alpha', 'td_error', 'target_value', 'entropy'
             ] + twin_critic
         else:
@@ -514,235 +411,172 @@
             ] + twin_critic
 
 
 @POLICY_REGISTRY.register('sac')
 class SACPolicy(Policy):
     r"""
        Overview:
-           Policy class of continuous SAC algorithm.
-
-           https://arxiv.org/pdf/1801.01290.pdf
+           Policy class of continuous SAC algorithm. Paper link: https://arxiv.org/pdf/1801.01290.pdf
 
        Config:
            == ====================  ========    =============  ================================= =======================
-           ID Symbol                Type        Default Value  Description                       Other(Shape)
+           ID Symbol                Type        Default Value  Description                       Other
            == ====================  ========    =============  ================================= =======================
-           1  ``type``              str         td3            | RL policy register name, refer  | this arg is optional,
+           1  ``type``              str         sac            | RL policy register name, refer  | this arg is optional,
                                                                | to registry ``POLICY_REGISTRY`` | a placeholder
            2  ``cuda``              bool        True           | Whether to use cuda for network |
-           3  | ``random_``         int         10000          | Number of randomly collected    | Default to 10000 for
+           3  ``on_policy``         bool        False          | SAC is an off-policy            |
+                                                               | algorithm.                      |
+           4  ``priority``          bool        False          | Whether to use priority         |
+                                                               | sampling in buffer.             |
+           5  | ``priority_IS_``    bool        False          | Whether use Importance Sampling |
+              | ``weight``                                     | weight to correct biased update |
+           6  | ``random_``         int         10000          | Number of randomly collected    | Default to 10000 for
               | ``collect_size``                               | training samples in replay      | SAC, 25000 for DDPG/
               |                                                | buffer when training starts.    | TD3.
-           4  | ``model.policy_``   int         256            | Linear layer size for policy    |
-              | ``embedding_size``                             | network.                        |
-           5  | ``model.soft_q_``   int         256            | Linear layer size for soft q    |
-              | ``embedding_size``                             | network.                        |
-           6  | ``model.value_``    int         256            | Linear layer size for value     | Defalut to None when
-              | ``embedding_size``                             | network.                        | model.value_network
-              |                                                |                                 | is False.
-           7  | ``learn.learning``  float       3e-4           | Learning rate for soft q        | Defalut to 1e-3, when
-              | ``_rate_q``                                    | network.                        | model.value_network
-              |                                                |                                 | is True.
-           8  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to 1e-3, when
-              | ``_rate_policy``                               | network.                        | model.value_network
-              |                                                |                                 | is True.
-           9  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to None when
-              | ``_rate_value``                                | network.                        | model.value_network
-              |                                                |                                 | is False.
-           10 | ``learn.alpha``     float       0.2            | Entropy regularization          | alpha is initiali-
+           7  | ``learn.learning``  float       3e-4           | Learning rate for soft q        | Defalut to 1e-3
+              | ``_rate_q``                                    | network.                        |
+           8  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to 1e-3
+              | ``_rate_policy``                               | network.                        |
+           9  | ``learn.alpha``     float       0.2            | Entropy regularization          | alpha is initiali-
               |                                                | coefficient.                    | zation for auto
               |                                                |                                 | `\alpha`, when
               |                                                |                                 | auto_alpha is True
-           11 | ``learn.repara_``   bool        True           | Determine whether to use        |
-              | ``meterization``                               | reparameterization trick.       |
-           12 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter
+           10 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter
               | ``auto_alpha``                                 | auto temperature parameter      | determines the
               |                                                | `\alpha`.                       | relative importance
               |                                                |                                 | of the entropy term
               |                                                |                                 | against the reward.
-           13 | ``learn.-``         bool        False          | Determine whether to ignore     | Use ignore_done only
-              | ``ignore_done``                                | done flag.                      | in halfcheetah env.
-           14 | ``learn.-``         float       0.005          | Used for soft update of the     | aka. Interpolation
+           11 | ``learn.-``         bool        False          | Determine whether to ignore     | Use ignore_done only
+              | ``ignore_done``                                | done flag.                      | in env like Pendulum
+           12 | ``learn.-``         float       0.005          | Used for soft update of the     | aka. Interpolation
               | ``target_theta``                               | target network.                 | factor in polyak aver
               |                                                |                                 | aging for target
               |                                                |                                 | networks.
            == ====================  ========    =============  ================================= =======================
        """
 
     config = dict(
         # (str) RL policy register name (refer to function "POLICY_REGISTRY").
         type='sac',
-        # (bool) Whether to use cuda for network.
+        # (bool) Whether to use cuda for network and loss computation.
         cuda=False,
-        # (bool type) on_policy: Determine whether on-policy or off-policy.
-        # on-policy setting influences the behaviour of buffer.
-        # Default False in SAC.
+        # (bool) Whether to belong to on-policy or off-policy algorithm, SAC is an off-policy algorithm.
         on_policy=False,
-        # (bool type) priority: Determine whether to use priority in buffer sample.
-        # Default False in SAC.
+        # (bool) Whether to use priority sampling in buffer. Default to False in SAC.
         priority=False,
-        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
+        # (bool) Whether use Importance Sampling weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
-        # (int) Number of training samples(randomly collected) in replay buffer when training starts.
-        # Default 10000 in SAC.
+        # (int) Number of training samples (randomly collected) in replay buffer when training starts.
         random_collect_size=10000,
-        # (bool) Whether to need policy data in process transition
+        # (bool) Whether to need policy-specific data in process transition.
         transition_with_policy_data=True,
-        # (bool) Whether to enable multi-agent training setting
+        # (bool) Whether to enable multi-agent training setting.
         multi_agent=False,
         model=dict(
-            # (bool type) twin_critic: Determine whether to use double-soft-q-net for target q computation.
-            # Please refer to TD3 about Clipped Double-Q Learning trick, which learns two Q-functions instead of one .
-            # Default to True.
+            # (bool) Whether to use double-soft-q-net for target q computation.
+            # For more details, please refer to TD3 about Clipped Double-Q Learning trick.
             twin_critic=True,
-
-            # (bool type) value_network: Determine whether to use value network as the
-            # original SAC paper (arXiv 1801.01290).
-            # using value_network needs to set learning_rate_value, learning_rate_q,
-            # and learning_rate_policy in `cfg.policy.learn`.
-            # Default to False.
-            # value_network=False,
-
-            # (str type) action_space: Use reparameterization trick for continous action
+            # (str) Use reparameterization trick for continous action.
             action_space='reparameterization',
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
+            # (int) How many updates (iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
-            # (int) Minibatch size for gradient descent.
+            # (int) Minibatch size for one gradient descent.
             batch_size=256,
-
-            # (float type) learning_rate_q: Learning rate for soft q network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
+            # (float) Learning rate for soft q network.
             learning_rate_q=3e-4,
-            # (float type) learning_rate_policy: Learning rate for policy network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
+            # (float) Learning rate for policy network.
             learning_rate_policy=3e-4,
-            # (float type) learning_rate_value: Learning rate for value network.
-            # `learning_rate_value` should be initialized, when model.value_network is True.
-            # Please set to 3e-4, when model.value_network is True.
-            learning_rate_value=3e-4,
-
-            # (float type) learning_rate_alpha: Learning rate for auto temperature parameter `\alpha`.
-            # Default to 3e-4.
+            # (float) Learning rate for auto temperature parameter `\alpha`.
             learning_rate_alpha=3e-4,
-            # (float type) target_theta: Used for soft update of the target network,
-            # aka. Interpolation factor in polyak averaging for target networks.
-            # Default to 0.005.
+            # (float) Used for soft update of the target network,
+            # aka. Interpolation factor in EMA update for target network.
             target_theta=0.005,
             # (float) discount factor for the discounted sum of rewards, aka. gamma.
             discount_factor=0.99,
-
-            # (float type) alpha: Entropy regularization coefficient.
+            # (float) Entropy regularization coefficient in SAC.
             # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
             # If auto_alpha is set  to `True`, alpha is initialization for auto `\alpha`.
-            # Default to 0.2.
             alpha=0.2,
-
-            # (bool type) auto_alpha: Determine whether to use auto temperature parameter `\alpha` .
-            # Temperature parameter determines the relative importance of the entropy term against the reward.
+            # (bool) Whether to use auto temperature parameter `\alpha` .
+            # Temperature parameter `\alpha` determines the relative importance of the entropy term against the reward.
             # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
-            # Default to False.
-            # Note that: Using auto alpha needs to set learning_rate_alpha in `cfg.policy.learn`.
+            # Note that: Using auto alpha needs to set the above `learning_rate_alpha`.
             auto_alpha=True,
-            # (bool type) log_space: Determine whether to use auto `\alpha` in log space.
+            # (bool) Whether to use auto `\alpha` in log space.
             log_space=True,
+            # (float) Target policy entropy value for auto temperature (alpha) adjustment.
+            target_entropy=None,
             # (bool) Whether ignore done(usually for max step termination env. e.g. pendulum)
             # Note: Gym wraps the MuJoCo envs by default with TimeLimit environment wrappers.
             # These limit HalfCheetah, and several other MuJoCo envs, to max length of 1000.
             # However, interaction with HalfCheetah always gets done with False,
             # Since we inplace done==True with done==False to keep
             # TD-error accurate computation(``gamma * (1 - done) * next_v + reward``),
             # when the episode step is greater than max episode step.
             ignore_done=False,
-            # (float) Weight uniform initialization range in the last output layer
+            # (float) Weight uniform initialization max range in the last output layer.
             init_w=3e-3,
         ),
         collect=dict(
-            # If you need the data collected by the collector to contain logit key which reflect the probability of
-            # the action, you can change the key to be True.
-            # In Guided cost Learning, we need to use logit to train the reward model, we change the key to be True.
-            # Default collector_logit to False.
-            collector_logit=False,
-            # You can use either "n_sample" or "n_episode" in actor.collect.
-            # Get "n_sample" samples per collect.
-            # Default n_sample to 1.
+            # (int) How many training samples collected in one collection procedure.
             n_sample=1,
-            # (int) Cut trajectories into pieces with length "unroll_len".
+            # (int) Split episodes or trajectories into pieces with length `unroll_len`.
             unroll_len=1,
-        ),
-        eval=dict(
-            evaluator=dict(
-                # (int) Evaluate every "eval_freq" training iterations.
-                eval_freq=5000,
-            ),
+            # (bool) Whether to collect logit in `process_transition`.
+            # In some algorithm like guided cost learning, we need to use logit to train the reward model.
+            collector_logit=False,
         ),
         other=dict(
             replay_buffer=dict(
-                # (int type) replay_buffer_size: Max size of replay buffer.
+                # (int) Maximum size of replay buffer. Usually, larger buffer size is good
+                # for SAC but cost more storage.
                 replay_buffer_size=1000000,
-                # (int type) max_use: Max use times of one data in the buffer.
-                # Data will be removed once used for too many times.
-                # Default to infinite.
-                # max_use=256,
             ),
         ),
     )
 
     def default_model(self) -> Tuple[str, List[str]]:
         if self._cfg.multi_agent:
             return 'maqac_continuous', ['ding.model.template.maqac']
         else:
             return 'qac', ['ding.model.template.qac']
 
     def _init_learn(self) -> None:
-        r"""
-        Overview:
-            Learn mode init method. Called by ``self.__init__``.
-            Init q, value and policy's optimizers, algorithm config, main and target models.
-        """
-        # Init
         self._priority = self._cfg.priority
         self._priority_IS_weight = self._cfg.priority_IS_weight
-        self._value_network = False  # TODO self._cfg.model.value_network
         self._twin_critic = self._cfg.model.twin_critic
 
         # Weight Init for the last output layer
         init_w = self._cfg.learn.init_w
-        self._model.actor[2].mu.weight.data.uniform_(-init_w, init_w)
-        self._model.actor[2].mu.bias.data.uniform_(-init_w, init_w)
-        self._model.actor[2].log_sigma_layer.weight.data.uniform_(-init_w, init_w)
-        self._model.actor[2].log_sigma_layer.bias.data.uniform_(-init_w, init_w)
+        self._model.actor[-1].mu.weight.data.uniform_(-init_w, init_w)
+        self._model.actor[-1].mu.bias.data.uniform_(-init_w, init_w)
+        self._model.actor[-1].log_sigma_layer.weight.data.uniform_(-init_w, init_w)
+        self._model.actor[-1].log_sigma_layer.bias.data.uniform_(-init_w, init_w)
 
-        # Optimizers
-        if self._value_network:
-            self._optimizer_value = Adam(
-                self._model.value_critic.parameters(),
-                lr=self._cfg.learn.learning_rate_value,
-            )
         self._optimizer_q = Adam(
             self._model.critic.parameters(),
             lr=self._cfg.learn.learning_rate_q,
         )
         self._optimizer_policy = Adam(
             self._model.actor.parameters(),
             lr=self._cfg.learn.learning_rate_policy,
         )
 
-        # Algorithm config
+        # Algorithm-Specific Config
         self._gamma = self._cfg.learn.discount_factor
-        # Init auto alpha
         if self._cfg.learn.auto_alpha:
-            self._target_entropy = self._cfg.learn.get('target_entropy', -np.prod(self._cfg.model.action_shape))
+            if self._cfg.learn.target_entropy is None:
+                assert 'action_shape' in self._cfg.model, "SAC need network model with action_shape variable"
+                self._target_entropy = -np.prod(self._cfg.model.action_shape)
+            else:
+                self._target_entropy = self._cfg.learn.target_entropy
             if self._cfg.learn.log_space:
                 self._log_alpha = torch.log(torch.FloatTensor([self._cfg.learn.alpha]))
                 self._log_alpha = self._log_alpha.to(self._device).requires_grad_()
                 self._alpha_optim = torch.optim.Adam([self._log_alpha], lr=self._cfg.learn.learning_rate_alpha)
                 assert self._log_alpha.shape == torch.Size([1]) and self._log_alpha.requires_grad
                 self._alpha = self._log_alpha.detach().exp()
                 self._auto_alpha = True
@@ -766,26 +600,15 @@
             update_type='momentum',
             update_kwargs={'theta': self._cfg.learn.target_theta}
         )
         self._learn_model = model_wrap(self._model, wrapper_name='base')
         self._learn_model.reset()
         self._target_model.reset()
 
-        self._forward_learn_cnt = 0
-
     def _forward_learn(self, data: dict) -> Dict[str, Any]:
-        """
-        Overview:
-            Forward and backward function of learn mode.
-        Arguments:
-            - data (:obj:`dict`): Dict type data, including at least ['obs', 'action', 'reward', 'next_obs']
-        Returns:
-            - info_dict (:obj:`Dict[str, Any]`): Including current lr, loss, target_q_value and other \
-                running information.
-        """
         loss_dict = {}
         data = default_preprocess_learn(
             data,
             use_priority=self._priority,
             use_priority_IS_weight=self._cfg.priority_IS_weight,
             ignore_done=self._cfg.learn.ignore_done,
             use_nstep=False
@@ -799,59 +622,53 @@
         next_obs = data['next_obs']
         reward = data['reward']
         done = data['done']
 
         # 1. predict q value
         q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']
 
-        # 2. predict target value depend self._value_network.
-        if self._value_network:
-            v_value = self._learn_model.forward(obs, mode='compute_value_critic')['v_value']
-            with torch.no_grad():
-                next_v_value = self._target_model.forward(next_obs, mode='compute_value_critic')['v_value']
-            target_q_value = next_v_value
-        else:
-            # target q value.
-            with torch.no_grad():
-                (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']
-
-                dist = Independent(Normal(mu, sigma), 1)
-                pred = dist.rsample()
-                next_action = torch.tanh(pred)
-                y = 1 - next_action.pow(2) + 1e-6
-                # keep dimension for loss computation (usually for action space is 1 env. e.g. pendulum)
-                next_log_prob = dist.log_prob(pred).unsqueeze(-1)
-                next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)
-
-                next_data = {'obs': next_obs, 'action': next_action}
-                target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']
-                # the value of a policy according to the maximum entropy objective
-                if self._twin_critic:
-                    # find min one as target q value
-                    target_q_value = torch.min(target_q_value[0],
-                                               target_q_value[1]) - self._alpha * next_log_prob.squeeze(-1)
-                else:
-                    target_q_value = target_q_value - self._alpha * next_log_prob.squeeze(-1)
+        # 2. predict target value
+        with torch.no_grad():
+            (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']
+
+            dist = Independent(Normal(mu, sigma), 1)
+            pred = dist.rsample()
+            next_action = torch.tanh(pred)
+            y = 1 - next_action.pow(2) + 1e-6
+            # keep dimension for loss computation (usually for action space is 1 env. e.g. pendulum)
+            next_log_prob = dist.log_prob(pred).unsqueeze(-1)
+            next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)
+
+            next_data = {'obs': next_obs, 'action': next_action}
+            target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']
+            # the value of a policy according to the maximum entropy objective
+            if self._twin_critic:
+                # find min one as target q value
+                target_q_value = torch.min(target_q_value[0],
+                                           target_q_value[1]) - self._alpha * next_log_prob.squeeze(-1)
+            else:
+                target_q_value = target_q_value - self._alpha * next_log_prob.squeeze(-1)
 
         # 3. compute q loss
         if self._twin_critic:
             q_data0 = v_1step_td_data(q_value[0], target_q_value, reward, done, data['weight'])
             loss_dict['critic_loss'], td_error_per_sample0 = v_1step_td_error(q_data0, self._gamma)
             q_data1 = v_1step_td_data(q_value[1], target_q_value, reward, done, data['weight'])
             loss_dict['twin_critic_loss'], td_error_per_sample1 = v_1step_td_error(q_data1, self._gamma)
             td_error_per_sample = (td_error_per_sample0 + td_error_per_sample1) / 2
         else:
             q_data = v_1step_td_data(q_value, target_q_value, reward, done, data['weight'])
             loss_dict['critic_loss'], td_error_per_sample = v_1step_td_error(q_data, self._gamma)
 
         # 4. update q network
         self._optimizer_q.zero_grad()
-        loss_dict['critic_loss'].backward()
         if self._twin_critic:
-            loss_dict['twin_critic_loss'].backward()
+            (loss_dict['critic_loss'] + loss_dict['twin_critic_loss']).backward()
+        else:
+            loss_dict['critic_loss'].backward()
         self._optimizer_q.step()
 
         # 5. evaluate to get action distribution
         (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']
         dist = Independent(Normal(mu, sigma), 1)
         pred = dist.rsample()
         action = torch.tanh(pred)
@@ -861,36 +678,25 @@
         log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)
 
         eval_data = {'obs': obs, 'action': action}
         new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']
         if self._twin_critic:
             new_q_value = torch.min(new_q_value[0], new_q_value[1])
 
-        # 6. (optional) compute value loss and update value network
-        if self._value_network:
-            # new_q_value: (bs, ), log_prob: (bs, act_shape) -> target_v_value: (bs, )
-            target_v_value = (new_q_value.unsqueeze(-1) - self._alpha * log_prob).mean(dim=-1)
-            loss_dict['value_loss'] = F.mse_loss(v_value, target_v_value.detach())
-
-            # update value network
-            self._optimizer_value.zero_grad()
-            loss_dict['value_loss'].backward()
-            self._optimizer_value.step()
-
-        # 7. compute policy loss
+        # 6. compute policy loss
         policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()
 
         loss_dict['policy_loss'] = policy_loss
 
-        # 8. update policy network
+        # 7. update policy network
         self._optimizer_policy.zero_grad()
         loss_dict['policy_loss'].backward()
         self._optimizer_policy.step()
 
-        # 9. compute alpha loss
+        # 8. compute alpha loss
         if self._auto_alpha:
             if self._log_space:
                 log_prob = log_prob + self._target_entropy
                 loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()
 
                 self._alpha_optim.zero_grad()
                 loss_dict['alpha_loss'].backward()
@@ -903,77 +709,52 @@
                 self._alpha_optim.zero_grad()
                 loss_dict['alpha_loss'].backward()
                 self._alpha_optim.step()
                 self._alpha = max(0, self._alpha)
 
         loss_dict['total_loss'] = sum(loss_dict.values())
 
-        # =============
-        # after update
-        # =============
-        self._forward_learn_cnt += 1
         # target update
         self._target_model.update(self._learn_model.state_dict())
         return {
             'cur_lr_q': self._optimizer_q.defaults['lr'],
             'cur_lr_p': self._optimizer_policy.defaults['lr'],
             'priority': td_error_per_sample.abs().tolist(),
             'td_error': td_error_per_sample.detach().mean().item(),
             'alpha': self._alpha.item(),
             'target_q_value': target_q_value.detach().mean().item(),
+            'transformed_log_prob': log_prob.mean().item(),
             **loss_dict
         }
 
     def _state_dict_learn(self) -> Dict[str, Any]:
         ret = {
             'model': self._learn_model.state_dict(),
             'target_model': self._target_model.state_dict(),
             'optimizer_q': self._optimizer_q.state_dict(),
             'optimizer_policy': self._optimizer_policy.state_dict(),
         }
-        if self._value_network:
-            ret.update({'optimizer_value': self._optimizer_value.state_dict()})
         if self._auto_alpha:
             ret.update({'optimizer_alpha': self._alpha_optim.state_dict()})
         return ret
 
     def _load_state_dict_learn(self, state_dict: Dict[str, Any]) -> None:
         self._learn_model.load_state_dict(state_dict['model'])
         self._target_model.load_state_dict(state_dict['target_model'])
         self._optimizer_q.load_state_dict(state_dict['optimizer_q'])
-        if self._value_network:
-            self._optimizer_value.load_state_dict(state_dict['optimizer_value'])
         self._optimizer_policy.load_state_dict(state_dict['optimizer_policy'])
         if self._auto_alpha:
             self._alpha_optim.load_state_dict(state_dict['optimizer_alpha'])
 
     def _init_collect(self) -> None:
-        r"""
-        Overview:
-            Collect mode init method. Called by ``self.__init__``.
-            Init traj and unroll length, collect model.
-            Use action noise for exploration.
-        """
         self._unroll_len = self._cfg.collect.unroll_len
         self._collect_model = model_wrap(self._model, wrapper_name='base')
         self._collect_model.reset()
 
     def _forward_collect(self, data: dict) -> dict:
-        r"""
-        Overview:
-            Forward function of collect mode.
-        Arguments:
-            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
-                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): Dict type data, including at least inferred action according to input obs.
-        ReturnsKeys
-            - necessary: ``action``
-            - optional: ``logit``
-        """
         data_id = list(data.keys())
         data = default_collate(list(data.values()))
         if self._cuda:
             data = to_device(data, self._device)
         self._collect_model.eval()
         with torch.no_grad():
             (mu, sigma) = self._collect_model.forward(data, mode='compute_actor')['logit']
@@ -982,25 +763,14 @@
             output = {'logit': (mu, sigma), 'action': action}
         if self._cuda:
             output = to_device(output, 'cpu')
         output = default_decollate(output)
         return {i: d for i, d in zip(data_id, output)}
 
     def _process_transition(self, obs: Any, policy_output: dict, timestep: namedtuple) -> dict:
-        r"""
-        Overview:
-            Generate dict type transition data from inputs.
-        Arguments:
-            - obs (:obj:`Any`): Env observation
-            - policy_output (:obj:`dict`): Output of policy collect model, including at least ['action']
-            - timestep (:obj:`namedtuple`): Output after env step, including at least ['obs', 'reward', 'done'] \
-                (here 'obs' indicates obs after env step, i.e. next_obs).
-        Return:
-            - transition (:obj:`Dict[str, Any]`): Dict type transition data.
-        """
         if self._cfg.collect.collector_logit:
             transition = {
                 'obs': obs,
                 'next_obs': timestep.obs,
                 'logit': policy_output['logit'],
                 'action': policy_output['action'],
                 'reward': timestep.reward,
@@ -1016,35 +786,18 @@
             }
         return transition
 
     def _get_train_sample(self, data: list) -> Union[None, List[Any]]:
         return get_train_sample(data, self._unroll_len)
 
     def _init_eval(self) -> None:
-        r"""
-        Overview:
-            Evaluate mode init method. Called by ``self.__init__``.
-            Init eval model. Unlike learn and collect model, eval model does not need noise.
-        """
         self._eval_model = model_wrap(self._model, wrapper_name='base')
         self._eval_model.reset()
 
     def _forward_eval(self, data: dict) -> dict:
-        r"""
-        Overview:
-            Forward function of eval mode, similar to ``self._forward_collect``.
-        Arguments:
-            - data (:obj:`Dict[str, Any]`): Dict type data, stacked env data for predicting policy_output(action), \
-                values are torch.Tensor or np.ndarray or dict/list combinations, keys are env_id indicated by integer.
-        Returns:
-            - output (:obj:`Dict[int, Any]`): The dict of predicting action for the interaction with env.
-        ReturnsKeys
-            - necessary: ``action``
-            - optional: ``logit``
-        """
         data_id = list(data.keys())
         data = default_collate(list(data.values()))
         if self._cuda:
             data = to_device(data, self._device)
         self._eval_model.eval()
         with torch.no_grad():
             (mu, sigma) = self._eval_model.forward(data, mode='compute_actor')['logit']
@@ -1052,254 +805,62 @@
             output = {'action': action}
         if self._cuda:
             output = to_device(output, 'cpu')
         output = default_decollate(output)
         return {i: d for i, d in zip(data_id, output)}
 
     def _monitor_vars_learn(self) -> List[str]:
-        r"""
-        Overview:
-            Return variables' name if variables are to used in monitor.
-        Returns:
-            - vars (:obj:`List[str]`): Variables' name list.
-        """
         twin_critic = ['twin_critic_loss'] if self._twin_critic else []
         alpha_loss = ['alpha_loss'] if self._auto_alpha else []
-        value_loss = ['value_loss'] if self._value_network else []
         return [
+            'value_loss'
             'alpha_loss',
             'policy_loss',
             'critic_loss',
             'cur_lr_q',
             'cur_lr_p',
             'target_q_value',
             'alpha',
             'td_error',
-        ] + twin_critic + alpha_loss + value_loss
+            'transformed_log_prob',
+        ] + twin_critic + alpha_loss
 
 
 @POLICY_REGISTRY.register('sqil_sac')
 class SQILSACPolicy(SACPolicy):
-    r"""
-       Overview:
-           Policy class of continuous SAC algorithm.
-
-           https://arxiv.org/pdf/1801.01290.pdf
-
-       Config:
-           == ====================  ========    =============  ================================= =======================
-           ID Symbol                Type        Default Value  Description                       Other(Shape)
-           == ====================  ========    =============  ================================= =======================
-           1  ``type``              str         td3            | RL policy register name, refer  | this arg is optional,
-                                                               | to registry ``POLICY_REGISTRY`` | a placeholder
-           2  ``cuda``              bool        True           | Whether to use cuda for network |
-           3  | ``random_``         int         10000          | Number of randomly collected    | Default to 10000 for
-              | ``collect_size``                               | training samples in replay      | SAC, 25000 for DDPG/
-              |                                                | buffer when training starts.    | TD3.
-           4  | ``model.policy_``   int         256            | Linear layer size for policy    |
-              | ``embedding_size``                             | network.                        |
-           5  | ``model.soft_q_``   int         256            | Linear layer size for soft q    |
-              | ``embedding_size``                             | network.                        |
-           6  | ``model.value_``    int         256            | Linear layer size for value     | Defalut to None when
-              | ``embedding_size``                             | network.                        | model.value_network
-              |                                                |                                 | is False.
-           7  | ``learn.learning``  float       3e-4           | Learning rate for soft q        | Defalut to 1e-3, when
-              | ``_rate_q``                                    | network.                        | model.value_network
-              |                                                |                                 | is True.
-           8  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to 1e-3, when
-              | ``_rate_policy``                               | network.                        | model.value_network
-              |                                                |                                 | is True.
-           9  | ``learn.learning``  float       3e-4           | Learning rate for policy        | Defalut to None when
-              | ``_rate_value``                                | network.                        | model.value_network
-              |                                                |                                 | is False.
-           10 | ``learn.alpha``     float       0.2            | Entropy regularization          | alpha is initiali-
-              |                                                | coefficient.                    | zation for auto
-              |                                                |                                 | `\alpha`, when
-              |                                                |                                 | auto_alpha is True
-           11 | ``learn.repara_``   bool        True           | Determine whether to use        |
-              | ``meterization``                               | reparameterization trick.       |
-           12 | ``learn.``          bool        False          | Determine whether to use        | Temperature parameter
-              | ``auto_alpha``                                 | auto temperature parameter      | determines the
-              |                                                | `\alpha`.                       | relative importance
-              |                                                |                                 | of the entropy term
-              |                                                |                                 | against the reward.
-           13 | ``learn.-``         bool        False          | Determine whether to ignore     | Use ignore_done only
-              | ``ignore_done``                                | done flag.                      | in halfcheetah env.
-           14 | ``learn.-``         float       0.005          | Used for soft update of the     | aka. Interpolation
-              | ``target_theta``                               | target network.                 | factor in polyak aver
-              |                                                |                                 | aging for target
-              |                                                |                                 | networks.
-           == ====================  ========    =============  ================================= =======================
-       """
-
-    config = dict(
-        # (str) RL policy register name (refer to function "POLICY_REGISTRY").
-        type='sqil_sac',
-        # (bool) Whether to use cuda for network.
-        cuda=False,
-        # (bool type) on_policy: Determine whether on-policy or off-policy.
-        # on-policy setting influences the behaviour of buffer.
-        # Default False in SAC.
-        on_policy=False,
-        # (bool type) priority: Determine whether to use priority in buffer sample.
-        # Default False in SAC.
-        priority=False,
-        # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
-        priority_IS_weight=False,
-        # (int) Number of training samples(randomly collected) in replay buffer when training starts.
-        # Default 10000 in SAC.
-        random_collect_size=10000,
-        # (bool) Whether to need policy data in process transition
-        transition_with_policy_data=True,
-        # (bool) Whether to enable multi-agent training setting
-        multi_agent=False,
-        model=dict(
-            # (bool type) twin_critic: Determine whether to use double-soft-q-net for target q computation.
-            # Please refer to TD3 about Clipped Double-Q Learning trick, which learns two Q-functions instead of one .
-            # Default to True.
-            twin_critic=True,
-
-            # (bool type) value_network: Determine whether to use value network as the
-            # original SAC paper (arXiv 1801.01290).
-            # using value_network needs to set learning_rate_value, learning_rate_q,
-            # and learning_rate_policy in `cfg.policy.learn`.
-            # Default to False.
-            # value_network=False,
-
-            # (str type) action_space: Use reparameterization trick for continous action
-            action_space='reparameterization',
-        ),
-        learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
-            # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
-            update_per_collect=1,
-            # (int) Minibatch size for gradient descent.
-            batch_size=256,
-
-            # (float type) learning_rate_q: Learning rate for soft q network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
-            learning_rate_q=3e-4,
-            # (float type) learning_rate_policy: Learning rate for policy network.
-            # Default to 3e-4.
-            # Please set to 1e-3, when model.value_network is True.
-            learning_rate_policy=3e-4,
-            # (float type) learning_rate_value: Learning rate for value network.
-            # `learning_rate_value` should be initialized, when model.value_network is True.
-            # Please set to 3e-4, when model.value_network is True.
-            learning_rate_value=3e-4,
-
-            # (float type) learning_rate_alpha: Learning rate for auto temperature parameter `\alpha`.
-            # Default to 3e-4.
-            learning_rate_alpha=3e-4,
-            # (float type) target_theta: Used for soft update of the target network,
-            # aka. Interpolation factor in polyak averaging for target networks.
-            # Default to 0.005.
-            target_theta=0.005,
-            # (float) discount factor for the discounted sum of rewards, aka. gamma.
-            discount_factor=0.99,
-
-            # (float type) alpha: Entropy regularization coefficient.
-            # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
-            # If auto_alpha is set  to `True`, alpha is initialization for auto `\alpha`.
-            # Default to 0.2.
-            alpha=0.2,
-
-            # (bool type) auto_alpha: Determine whether to use auto temperature parameter `\alpha` .
-            # Temperature parameter determines the relative importance of the entropy term against the reward.
-            # Please check out the original SAC paper (arXiv 1801.01290): Eq 1 for more details.
-            # Default to False.
-            # Note that: Using auto alpha needs to set learning_rate_alpha in `cfg.policy.learn`.
-            auto_alpha=True,
-            # (bool type) log_space: Determine whether to use auto `\alpha` in log space.
-            log_space=True,
-            # (bool) Whether ignore done(usually for max step termination env. e.g. pendulum)
-            # Note: Gym wraps the MuJoCo envs by default with TimeLimit environment wrappers.
-            # These limit HalfCheetah, and several other MuJoCo envs, to max length of 1000.
-            # However, interaction with HalfCheetah always gets done with False,
-            # Since we inplace done==True with done==False to keep
-            # TD-error accurate computation(``gamma * (1 - done) * next_v + reward``),
-            # when the episode step is greater than max episode step.
-            ignore_done=False,
-            # (float) Weight uniform initialization range in the last output layer
-            init_w=3e-3,
-        ),
-        collect=dict(
-            # If you need the data collected by the collector to contain logit key which reflect the probability of
-            # the action, you can change the key to be True.
-            # In Guided cost Learning, we need to use logit to train the reward model, we change the key to be True.
-            # Default collector_logit to False.
-            collector_logit=False,
-            # You can use either "n_sample" or "n_episode" in actor.collect.
-            # Get "n_sample" samples per collect.
-            # Default n_sample to 1.
-            n_sample=1,
-            # (int) Cut trajectories into pieces with length "unroll_len".
-            unroll_len=1,
-        ),
-        eval=dict(
-            evaluator=dict(
-                # (int) Evaluate every "eval_freq" training iterations.
-                eval_freq=5000,
-            ),
-        ),
-        other=dict(
-            replay_buffer=dict(
-                # (int type) replay_buffer_size: Max size of replay buffer.
-                replay_buffer_size=1000000,
-                # (int type) max_use: Max use times of one data in the buffer.
-                # Data will be removed once used for too many times.
-                # Default to infinite.
-                # max_use=256,
-            ),
-        ),
-    )
 
     def _init_learn(self) -> None:
-        r"""
-        Overview:
-            Learn mode init method. Called by ``self.__init__``.
-            Init q, value and policy's optimizers, algorithm config, main and target models.
-        """
-        # Init
         self._priority = self._cfg.priority
         self._priority_IS_weight = self._cfg.priority_IS_weight
-        self._value_network = False  # TODO self._cfg.model.value_network
         self._twin_critic = self._cfg.model.twin_critic
 
         # Weight Init for the last output layer
         init_w = self._cfg.learn.init_w
         self._model.actor[2].mu.weight.data.uniform_(-init_w, init_w)
         self._model.actor[2].mu.bias.data.uniform_(-init_w, init_w)
         self._model.actor[2].log_sigma_layer.weight.data.uniform_(-init_w, init_w)
         self._model.actor[2].log_sigma_layer.bias.data.uniform_(-init_w, init_w)
 
-        # Optimizers
-        if self._value_network:
-            self._optimizer_value = Adam(
-                self._model.value_critic.parameters(),
-                lr=self._cfg.learn.learning_rate_value,
-            )
         self._optimizer_q = Adam(
             self._model.critic.parameters(),
             lr=self._cfg.learn.learning_rate_q,
         )
         self._optimizer_policy = Adam(
             self._model.actor.parameters(),
             lr=self._cfg.learn.learning_rate_policy,
         )
 
-        # Algorithm config
+        # Algorithm-Specific Config
         self._gamma = self._cfg.learn.discount_factor
-        # Init auto alpha
         if self._cfg.learn.auto_alpha:
-            self._target_entropy = self._cfg.learn.get('target_entropy', -np.prod(self._cfg.model.action_shape))
+            if self._cfg.learn.target_entropy is None:
+                assert 'action_shape' in self._cfg.model, "SACDiscrete need network model with action_shape variable"
+                self._target_entropy = -np.prod(self._cfg.model.action_shape)
+            else:
+                self._target_entropy = self._cfg.learn.target_entropy
             if self._cfg.learn.log_space:
                 self._log_alpha = torch.log(torch.FloatTensor([self._cfg.learn.alpha]))
                 self._log_alpha = self._log_alpha.to(self._device).requires_grad_()
                 self._alpha_optim = torch.optim.Adam([self._log_alpha], lr=self._cfg.learn.learning_rate_alpha)
                 assert self._log_alpha.shape == torch.Size([1]) and self._log_alpha.requires_grad
                 self._alpha = self._log_alpha.detach().exp()
                 self._auto_alpha = True
@@ -1323,29 +884,19 @@
             update_type='momentum',
             update_kwargs={'theta': self._cfg.learn.target_theta}
         )
         self._learn_model = model_wrap(self._model, wrapper_name='base')
         self._learn_model.reset()
         self._target_model.reset()
 
-        self._forward_learn_cnt = 0
         # monitor cossimilarity and entropy switch
         self._monitor_cos = True
         self._monitor_entropy = True
 
     def _forward_learn(self, data: dict) -> Dict[str, Any]:
-        """
-        Overview:
-            Forward and backward function of learn mode.
-        Arguments:
-            - data (:obj:`dict`): Dict type data, including at least ['obs', 'action', 'reward', 'next_obs']
-        Returns:
-            - info_dict (:obj:`Dict[str, Any]`): Including current lr, loss, target_q_value and other \
-                running information.
-        """
         loss_dict = {}
         if self._monitor_cos:
             agent_data = default_preprocess_learn(
                 data[0:len(data) // 2],
                 use_priority=self._priority,
                 use_priority_IS_weight=self._cfg.priority_IS_weight,
                 ignore_done=self._cfg.learn.ignore_done,
@@ -1379,58 +930,52 @@
         next_obs = data['next_obs']
         reward = data['reward']
         done = data['done']
 
         # 1. predict q value
         q_value = self._learn_model.forward(data, mode='compute_critic')['q_value']
 
-        # 2. predict target value depend self._value_network.
-        if self._value_network:
-            v_value = self._learn_model.forward(obs, mode='compute_value_critic')['v_value']
-            with torch.no_grad():
-                next_v_value = self._target_model.forward(next_obs, mode='compute_value_critic')['v_value']
-            target_q_value = next_v_value
-        else:
-            # target q value.
-            with torch.no_grad():
-                (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']
-                dist = Independent(Normal(mu, sigma), 1)
-                pred = dist.rsample()
-                next_action = torch.tanh(pred)
-                y = 1 - next_action.pow(2) + 1e-6
-                # keep dimension for loss computation (usually for action space is 1 env. e.g. pendulum)
-                next_log_prob = dist.log_prob(pred).unsqueeze(-1)
-                next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)
-
-                next_data = {'obs': next_obs, 'action': next_action}
-                target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']
-                # the value of a policy according to the maximum entropy objective
-                if self._twin_critic:
-                    # find min one as target q value
-                    target_q_value = torch.min(target_q_value[0],
-                                               target_q_value[1]) - self._alpha * next_log_prob.squeeze(-1)
-                else:
-                    target_q_value = target_q_value - self._alpha * next_log_prob.squeeze(-1)
+        # 2. predict target value
+        with torch.no_grad():
+            (mu, sigma) = self._learn_model.forward(next_obs, mode='compute_actor')['logit']
+            dist = Independent(Normal(mu, sigma), 1)
+            pred = dist.rsample()
+            next_action = torch.tanh(pred)
+            y = 1 - next_action.pow(2) + 1e-6
+            # keep dimension for loss computation (usually for action space is 1 env. e.g. pendulum)
+            next_log_prob = dist.log_prob(pred).unsqueeze(-1)
+            next_log_prob = next_log_prob - torch.log(y).sum(-1, keepdim=True)
+
+            next_data = {'obs': next_obs, 'action': next_action}
+            target_q_value = self._target_model.forward(next_data, mode='compute_critic')['q_value']
+            # the value of a policy according to the maximum entropy objective
+            if self._twin_critic:
+                # find min one as target q value
+                target_q_value = torch.min(target_q_value[0],
+                                           target_q_value[1]) - self._alpha * next_log_prob.squeeze(-1)
+            else:
+                target_q_value = target_q_value - self._alpha * next_log_prob.squeeze(-1)
 
         # 3. compute q loss
         if self._twin_critic:
             q_data0 = v_1step_td_data(q_value[0], target_q_value, reward, done, data['weight'])
             loss_dict['critic_loss'], td_error_per_sample0 = v_1step_td_error(q_data0, self._gamma)
             q_data1 = v_1step_td_data(q_value[1], target_q_value, reward, done, data['weight'])
             loss_dict['twin_critic_loss'], td_error_per_sample1 = v_1step_td_error(q_data1, self._gamma)
             td_error_per_sample = (td_error_per_sample0 + td_error_per_sample1) / 2
         else:
             q_data = v_1step_td_data(q_value, target_q_value, reward, done, data['weight'])
             loss_dict['critic_loss'], td_error_per_sample = v_1step_td_error(q_data, self._gamma)
 
         # 4. update q network
         self._optimizer_q.zero_grad()
-        loss_dict['critic_loss'].backward()
         if self._twin_critic:
-            loss_dict['twin_critic_loss'].backward()
+            (loss_dict['critic_loss'] + loss_dict['twin_critic_loss']).backward()
+        else:
+            loss_dict['critic_loss'].backward()
         self._optimizer_q.step()
 
         # 5. evaluate to get action distribution
         if self._monitor_cos:
             # agent
             (mu, sigma) = self._learn_model.forward(agent_data['obs'], mode='compute_actor')['logit']
             dist = Independent(Normal(mu, sigma), 1)
@@ -1456,15 +1001,14 @@
             expert_log_prob = expert_log_prob - torch.log(y).sum(-1, keepdim=True)
 
             eval_data = {'obs': expert_data['obs'], 'action': action}
             expert_new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']
             if self._twin_critic:
                 expert_new_q_value = torch.min(expert_new_q_value[0], expert_new_q_value[1])
 
-        # all data
         (mu, sigma) = self._learn_model.forward(data['obs'], mode='compute_actor')['logit']
         dist = Independent(Normal(mu, sigma), 1)
         # for monitor the entropy of policy
         if self._monitor_entropy:
             dist_entropy = dist.entropy()
             entropy = dist_entropy.mean()
 
@@ -1476,30 +1020,19 @@
         log_prob = log_prob - torch.log(y).sum(-1, keepdim=True)
 
         eval_data = {'obs': obs, 'action': action}
         new_q_value = self._learn_model.forward(eval_data, mode='compute_critic')['q_value']
         if self._twin_critic:
             new_q_value = torch.min(new_q_value[0], new_q_value[1])
 
-        # 6. (optional) compute value loss and update value network
-        if self._value_network:
-            # new_q_value: (bs, ), log_prob: (bs, act_shape) -> target_v_value: (bs, )
-            target_v_value = (new_q_value.unsqueeze(-1) - self._alpha * log_prob).mean(dim=-1)
-            loss_dict['value_loss'] = F.mse_loss(v_value, target_v_value.detach())
-
-            # update value network
-            self._optimizer_value.zero_grad()
-            loss_dict['value_loss'].backward()
-            self._optimizer_value.step()
-
-        # 7. compute policy loss
+        # 6. compute policy loss
         policy_loss = (self._alpha * log_prob - new_q_value.unsqueeze(-1)).mean()
         loss_dict['policy_loss'] = policy_loss
 
-        # 8. update policy network
+        # 7. update policy network
         if self._monitor_cos:
             agent_policy_loss = (self._alpha * agent_log_prob - agent_new_q_value.unsqueeze(-1)).mean()
             expert_policy_loss = (self._alpha * expert_log_prob - expert_new_q_value.unsqueeze(-1)).mean()
             loss_dict['agent_policy_loss'] = agent_policy_loss
             loss_dict['expert_policy_loss'] = expert_policy_loss
             self._optimizer_policy.zero_grad()
             loss_dict['agent_policy_loss'].backward()
@@ -1509,15 +1042,15 @@
             expert_grad = (list(list(self._learn_model.actor.children())[-1].children())[-1].weight.grad).mean()
             cos = nn.CosineSimilarity(dim=0)
             cos_similarity = cos(agent_grad, expert_grad)
         self._optimizer_policy.zero_grad()
         loss_dict['policy_loss'].backward()
         self._optimizer_policy.step()
 
-        # 9. compute alpha loss
+        # 8. compute alpha loss
         if self._auto_alpha:
             if self._log_space:
                 log_prob = log_prob + self._target_entropy
                 loss_dict['alpha_loss'] = -(self._log_alpha * log_prob.detach()).mean()
 
                 self._alpha_optim.zero_grad()
                 loss_dict['alpha_loss'].backward()
@@ -1530,18 +1063,14 @@
                 self._alpha_optim.zero_grad()
                 loss_dict['alpha_loss'].backward()
                 self._alpha_optim.step()
                 self._alpha = max(0, self._alpha)
 
         loss_dict['total_loss'] = sum(loss_dict.values())
 
-        # =============
-        # after update
-        # =============
-        self._forward_learn_cnt += 1
         # target update
         self._target_model.update(self._learn_model.state_dict())
         var_monitor = {
             'cur_lr_q': self._optimizer_q.defaults['lr'],
             'cur_lr_p': self._optimizer_policy.defaults['lr'],
             'priority': td_error_per_sample.abs().tolist(),
             'td_error': td_error_per_sample.detach().mean().item(),
@@ -1558,34 +1087,28 @@
         if self._monitor_cos:
             var_monitor['cos_similarity'] = cos_similarity.item()
         if self._monitor_entropy:
             var_monitor['entropy'] = entropy.item()
         return var_monitor
 
     def _monitor_vars_learn(self) -> List[str]:
-        r"""
-        Overview:
-            Return variables' name if variables are to used in monitor.
-        Returns:
-            - vars (:obj:`List[str]`): Variables' name list.
-        """
         twin_critic = ['twin_critic_loss'] if self._twin_critic else []
         alpha_loss = ['alpha_loss'] if self._auto_alpha else []
-        value_loss = ['value_loss'] if self._value_network else []
         cos_similarity = ['cos_similarity'] if self._monitor_cos else []
         entropy = ['entropy'] if self._monitor_entropy else []
         return [
+            'value_loss'
             'alpha_loss',
             'policy_loss',
             'critic_loss',
             'cur_lr_q',
             'cur_lr_p',
             'target_q_value',
             'alpha',
             'td_error',
             'agent_td_error',
             'expert_td_error',
             'mu',
             'sigma',
             'q_value0',
             'q_value1',
-        ] + twin_critic + alpha_loss + value_loss + cos_similarity + entropy
+        ] + twin_critic + alpha_loss + cos_similarity + entropy
```

### Comparing `DI-engine-0.4.6/ding/policy/sql.py` & `DI-engine-0.4.7/ding/policy/sql.py`

 * *Files 0% similar despite different names*

```diff
@@ -31,16 +31,15 @@
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (float) Reward's future discount factor, aka. gamma.
         discount_factor=0.97,
         # (int) N-step reward for target q_value estimation
         nstep=1,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=3,  # after the batch data come into the learner, train with the data for 3 times
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
@@ -148,15 +147,15 @@
         )
         record_target_v = record_target_v.mean()
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/ding/policy/sqn.py` & `DI-engine-0.4.7/ding/policy/sqn.py`

 * *Files 1% similar despite different names*

```diff
@@ -27,15 +27,14 @@
         cuda=False,
         type='sqn',
         on_policy=False,
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            multi_gpu=False,
             update_per_collect=16,
             batch_size=64,
             learning_rate_q=0.001,
             learning_rate_alpha=0.001,
             # ==============================================================
             # The following configs are algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/policy/td3.py` & `DI-engine-0.4.7/ding/policy/td3.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,8 @@
+from typing import List
 from ding.utils import POLICY_REGISTRY
 from .ddpg import DDPGPolicy
 
 
 @POLICY_REGISTRY.register('td3')
 class TD3Policy(DDPGPolicy):
     r"""
@@ -88,15 +89,15 @@
         model=dict(
             # (bool) Whether to use two critic networks or only one.
             # Clipped Double Q-Learning for Actor-Critic in original TD3 paper(https://arxiv.org/pdf/1802.09477.pdf).
             # Default True for TD3, False for DDPG.
             twin_critic=True,
         ),
         learn=dict(
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
             # (float) Learning rates for actor network(aka. policy).
@@ -149,7 +150,10 @@
         other=dict(
             replay_buffer=dict(
                 # (int) Maximum size of replay buffer.
                 replay_buffer_size=100000,
             ),
         ),
     )
+
+    def monitor_vars(self) -> List[str]:
+        return ["q_value", "target q_value", "loss", "lr", "entropy", "target_q_value", "td_error"]
```

### Comparing `DI-engine-0.4.6/ding/policy/td3_bc.py` & `DI-engine-0.4.7/ding/policy/td3_bc.py`

 * *Files 0% similar despite different names*

```diff
@@ -107,15 +107,15 @@
             # (int) Hidden size for actor network head.
             actor_head_hidden_size=256,
 
             # (int) Hidden size for critic network head.
             critic_head_hidden_size=256,
         ),
         learn=dict(
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
             # (float) Learning rates for actor network(aka. policy).
```

### Comparing `DI-engine-0.4.6/ding/policy/td3_vae.py` & `DI-engine-0.4.7/ding/policy/td3_vae.py`

 * *Files 0% similar despite different names*

```diff
@@ -101,15 +101,15 @@
         model=dict(
             # (bool) Whether to use two critic networks or only one.
             # Clipped Double Q-Learning for Actor-Critic in original TD3 paper(https://arxiv.org/pdf/1802.09477.pdf).
             # Default True for TD3, False for DDPG.
             twin_critic=True,
         ),
         learn=dict(
-            multi_gpu=False,
+
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=1,
             # (int) Minibatch size for gradient descent.
             batch_size=256,
             # (float) Learning rates for actor network(aka. policy).
```

### Comparing `DI-engine-0.4.6/ding/policy/wqmix.py` & `DI-engine-0.4.7/ding/policy/wqmix.py`

 * *Files 2% similar despite different names*

```diff
@@ -53,16 +53,14 @@
         # (bool) Whether the RL algorithm is on-policy or off-policy.
         on_policy=False,
         # (bool) Whether use priority(priority sample, IS weight, update priority)
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             update_per_collect=20,
             batch_size=32,
             learning_rate=0.0005,
             clip_value=100,
             # ==============================================================
             # The following configs is algorithm-specific
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/ding/reward_model/__init__.py` & `DI-engine-0.4.7/ding/reward_model/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/reward_model/base_reward_model.py` & `DI-engine-0.4.7/ding/reward_model/base_reward_model.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,12 +1,15 @@
 from abc import ABC, abstractmethod
+from typing import Dict
 from easydict import EasyDict
+from ditk import logging
+import os
 import copy
 from typing import Any
-from ding.utils import REWARD_MODEL_REGISTRY, import_module
+from ding.utils import REWARD_MODEL_REGISTRY, import_module, save_file
 
 
 class BaseRewardModel(ABC):
     """
     Overview:
         the base class of reward model
     Interface:
@@ -86,14 +89,36 @@
         """
         train_data_reward_deepcopy = [
             {k: copy.deepcopy(v) if k == 'reward' else v
              for k, v in sample.items()} for sample in train_data
         ]
         return train_data_reward_deepcopy
 
+    def state_dict(self) -> Dict:
+        # this method should be overrided by subclass.
+        return {}
+
+    def load_state_dict(self, _state_dict) -> None:
+        # this method should be overrided by subclass.
+        pass
+
+    def save(self, path: str = None, name: str = 'best'):
+        if path is None:
+            path = self.cfg.exp_name
+        path = os.path.join(path, 'reward_model', 'ckpt')
+        if not os.path.exists(path):
+            try:
+                os.makedirs(path)
+            except FileExistsError:
+                pass
+        path = os.path.join(path, 'ckpt_{}.pth.tar'.format(name))
+        state_dict = self.state_dict()
+        save_file(path, state_dict)
+        logging.info('Saved reward model ckpt in {}'.format(path))
+
 
 def create_reward_model(cfg: dict, device: str, tb_logger: 'SummaryWriter') -> BaseRewardModel:  # noqa
     """
     Overview:
         Reward Estimation Model.
     Arguments:
         - cfg (:obj:`Dict`): Training config
```

### Comparing `DI-engine-0.4.6/ding/reward_model/gail_irl_model.py` & `DI-engine-0.4.7/ding/reward_model/gail_irl_model.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,12 @@
 from typing import List, Dict, Any
 import pickle
 import random
 from collections.abc import Iterable
 from easydict import EasyDict
-import numpy as np
 
 import torch
 import torch.nn as nn
 import torch.optim as optim
 
 from ding.utils import REWARD_MODEL_REGISTRY
 from .base_reward_model import BaseRewardModel
@@ -109,43 +108,59 @@
     """
     Overview:
         The Gail reward model class (https://arxiv.org/abs/1606.03476)
     Interface:
         ``estimate``, ``train``, ``load_expert_data``, ``collect_data``, ``clear_date``, \
             ``__init__``,  ``state_dict``, ``load_state_dict``, ``learn``
     Config:
-           == ====================  ========   =============  ================================= =======================
-           ID Symbol                Type       Default Value  Description                       Other(Shape)
-           == ====================  ========   =============  ================================= =======================
-           1  ``type``              str        gail           | RL policy register name, refer  | this arg is optional,
-                                                              | to registry ``POLICY_REGISTRY`` | a placeholder
-           2  | ``expert_data_``    str        expert_data.   | Path to the expert dataset      | Should be a '.pkl'
-              | ``path``                       .pkl           |                                 | file
-           3  | ``update_per_``     int        100            | Number of updates per collect   |
-              | ``collect``                                   |                                 |
-           4  | ``batch_size``      int        64             | Training batch size             |
-           5  | ``input_size``      int                       | Size of the input:              |
-              |                                               | obs_dim + act_dim               |
-           6  | ``target_new_``     int        64             | Collect steps per iteration     |
-              | ``data_count``                                |                                 |
-           7  | ``hidden_size``     int        128            | Linear model hidden size        |
-           8  | ``collect_count``   int        100000         | Expert dataset size             | One entry is a (s,a)
-              |                                               |                                 | tuple
-           == ====================  ========   =============  ================================= =======================
-
-       """
+        == ====================  ========   =============  =================================== =======================
+        ID Symbol                Type       Default Value  Description                         Other(Shape)
+        == ====================  ========   =============  =================================== =======================
+        1  ``type``              str        gail           | RL policy register name, refer    | this arg is optional,
+                                                           | to registry ``POLICY_REGISTRY``   | a placeholder
+        2  | ``expert_data_``    str        expert_data.   | Path to the expert dataset        | Should be a '.pkl'
+           | ``path``                       .pkl           |                                   | file
+        3  | ``learning_rate``   float      0.001          | The step size of gradient descent |
+        4  | ``update_per_``     int        100            | Number of updates per collect     |
+           | ``collect``                                   |                                   |
+        5  | ``batch_size``      int        64             | Training batch size               |
+        6  | ``input_size``      int                       | Size of the input:                |
+           |                                               | obs_dim + act_dim                 |
+        7  | ``target_new_``     int        64             | Collect steps per iteration       |
+           | ``data_count``                                |                                   |
+        8  | ``hidden_size``     int        128            | Linear model hidden size          |
+        9  | ``collect_count``   int        100000         | Expert dataset size               | One entry is a (s,a)
+           |                                               |                                   | tuple
+        10 | ``clear_buffer_``   int        1              | clear buffer per fixed iters      | make sure replay
+           | ``per_iters``                                                                     | buffer's data count
+           |                                                                                   | isn't too few.
+           |                                                                                   | (code work in entry)
+        == ====================  ========   =============  =================================== =======================
+    """
     config = dict(
+        # (str) RL policy register name, refer to registry ``POLICY_REGISTRY``.
         type='gail',
+        # (float) The step size of gradient descent.
         learning_rate=1e-3,
+        # (int) How many updates(iterations) to train after collector's one collection.
+        # Bigger "update_per_collect" means bigger off-policy.
+        # collect data -> update policy-> collect data -> ...
         update_per_collect=100,
+        # (int) How many samples in a training batch.
         batch_size=64,
+        # (int) Size of the input: obs_dim + act_dim.
         input_size=4,
+        # (int) Collect steps per iteration.
         target_new_data_count=64,
+        # (int) Linear model hidden size.
         hidden_size=128,
+        # (int) Expert dataset size.
         collect_count=100000,
+        # (int) Clear buffer per fixed iters.
+        clear_buffer_per_iters=1,
     )
 
     def __init__(self, config: EasyDict, device: str, tb_logger: 'SummaryWriter') -> None:  # noqa
         """
         Overview:
             Initialize ``self.`` See ``help(type(self))`` for accurate signature.
         Arguments:
```

### Comparing `DI-engine-0.4.6/ding/reward_model/her_reward_model.py` & `DI-engine-0.4.7/ding/reward_model/her_reward_model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/reward_model/ngu_reward_model.py` & `DI-engine-0.4.7/ding/reward_model/ngu_reward_model.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import copy
 import random
-from typing import Union, Tuple, Any, Dict, List
+from typing import Union, Tuple, Dict, List
 
 import numpy as np
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
 from easydict import EasyDict
```

### Comparing `DI-engine-0.4.6/ding/reward_model/pdeil_irl_model.py` & `DI-engine-0.4.7/ding/reward_model/pdeil_irl_model.py`

 * *Files 25% similar despite different names*

```diff
@@ -12,24 +12,50 @@
 from .base_reward_model import BaseRewardModel
 
 
 @REWARD_MODEL_REGISTRY.register('pdeil')
 class PdeilRewardModel(BaseRewardModel):
     """
     Overview:
-        The Pdeil reward model class
+        The Pdeil reward model class (https://arxiv.org/abs/2112.06746)
     Interface:
         ``estimate``, ``train``, ``load_expert_data``, ``collect_data``, ``clear_date``, \
             ``__init__``, ``_train``, ``_batch_mn_pdf``
+    Config:
+        == ====================  =====   =============  =======================================  =======================
+        ID Symbol                Type    Default Value  Description                              Other(Shape)
+        == ====================  =====   =============  =======================================  =======================
+        1  ``type``              str       pdeil        | Reward model register name, refer      |
+                                                        | to registry ``REWARD_MODEL_REGISTRY``  |
+        2  | ``expert_data_``    str     expert_data.   | Path to the expert dataset             | Should be a '.pkl'
+           | ``path``                    .pkl           |                                        | file
+        3  | ``discrete_``       bool      False        | Whether the action is discrete         |
+           | ``action``                                 |                                        |
+        4  | ``alpha``           float     0.5          | coefficient for Probability            |
+           |                                            | Density Estimator                      |
+        5  | ``clear_buffer``    int      1             | clear buffer per fixed iters           | make sure replay
+            ``_per_iters``                                                                       | buffer's data count
+                                                                                                 | isn't too few.
+                                                                                                 | (code work in entry)
+        == ====================  =====   =============  =======================================  =======================
     """
     config = dict(
+        # (str) Reward model register name, refer to registry ``REWARD_MODEL_REGISTRY``.
         type='pdeil',
+        # (str) Path to the expert dataset.
         # expert_data_path='expert_data.pkl',
+        # (bool) Whether the action is discrete.
         discrete_action=False,
+        # (float) Coefficient for Probability Density Estimator.
+        # alpha + beta = 1, alpha is in [0,1]
+        # when alpha is close to 0, the estimator has high variance and low bias;
+        # when alpha is close to 1, the estimator has high bias and low variance.
         alpha=0.5,
+        # (int) Clear buffer per fixed iters.
+        clear_buffer_per_iters=1,
     )
 
     def __init__(self, cfg: dict, device, tb_logger: 'SummaryWriter') -> None:  # noqa
         """
         Overview:
             Initialize ``self.`` See ``help(type(self))`` for accurate signature.
             Some rules in naming the attributes of ``self.``:
```

### Comparing `DI-engine-0.4.6/ding/reward_model/pwil_irl_model.py` & `DI-engine-0.4.7/ding/reward_model/red_irl_model.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,228 +1,214 @@
 from typing import Dict, List
-import math
-import random
 import pickle
+import random
 import torch
+import torch.nn as nn
+import torch.optim as optim
 
-from ding.utils import REWARD_MODEL_REGISTRY
+from ding.utils import REWARD_MODEL_REGISTRY, one_time_warning
 from .base_reward_model import BaseRewardModel
 
 
-def collect_state_action_pairs(iterator):
-    # concat state and action
-    """
-    Overview:
-        Concate state and action pairs from input iterator.
-    Arguments:
-        - iterator (:obj:`Iterable`): Iterables with at least ``obs`` and ``action`` tensor keys.
-    Returns:
-        - res (:obj:`Torch.tensor`): State and action pairs.
-    """
-    res = []
-    for item in iterator:
-        state = item['obs']
-        action = item['action']
-        # s_a = torch.cat([state, action.float()], dim=-1)
-        res.append((state, action))
-    return res
+class SENet(nn.Module):
+    """support estimation network"""
+
+    def __init__(self, input_size: int, hidden_size: int, output_dims: int) -> None:
+        super(SENet, self).__init__()
+        self.l_1 = nn.Linear(input_size, hidden_size)
+        self.l_2 = nn.Linear(hidden_size, output_dims)
+        self.act = nn.Tanh()
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        out = self.l_1(x)
+        out = self.act(out)
+        out = self.l_2(out)
+        out = self.act(out)
+        return out
 
 
-@REWARD_MODEL_REGISTRY.register('pwil')
-class PwilRewardModel(BaseRewardModel):
+@REWARD_MODEL_REGISTRY.register('red')
+class RedRewardModel(BaseRewardModel):
     """
     Overview:
-        The Pwil reward model class (https://arxiv.org/pdf/2006.04678.pdf)
+         The implement of reward model in RED (https://arxiv.org/abs/1905.06750)
     Interface:
         ``estimate``, ``train``, ``load_expert_data``, ``collect_data``, ``clear_date``, \
-            ``__init__``, ``_train``, ``_get_state_distance``, ``_get_action_distance``
+            ``__init__``, ``_train``
+    Config:
+        == ==================  =====   =============  =======================================  =======================
+        ID Symbol              Type    Default Value  Description                              Other(Shape)
+        == ==================  =====   =============  =======================================  =======================
+        1  ``type``             str      red          | Reward model register name, refer       |
+                                                      | to registry ``REWARD_MODEL_REGISTRY``   |
+        2  | ``expert_data_``   str      expert_data  | Path to the expert dataset              | Should be a '.pkl'
+           | ``path``                    .pkl         |                                         | file
+        3  | ``sample_size``    int      1000         | sample data from expert dataset         |
+                                                      | with fixed size                         |
+        4  | ``sigma``          int      5            | hyperparameter of r(s,a)                | r(s,a) = exp(
+                                                                                                | -sigma* L(s,a))
+        5  | ``batch_size``     int      64           | Training batch size                     |
+        6  | ``hidden_size``    int      128          | Linear model hidden size                |
+        7  | ``update_per_``    int      100          | Number of updates per collect           |
+           | ``collect``                              |                                         |
+        8  | ``clear_buffer``   int      1            | clear buffer per fixed iters            | make sure replay
+             ``_per_iters``                                                                     | buffer's data count
+                                                                                                | isn't too few.
+                                                                                                | (code work in entry)
+        == ==================  =====   =============  =======================================  =======================
     Properties:
-        - reward_table (:obj: `Dict`): In this algorithm, reward model is a dictionary.
+        - online_net (:obj: `SENet`): The reward model, in default initialized once as the training begins.
     """
     config = dict(
-        type='pwil',
-        # expert_data_path='expert_data.pkl',
+        # (str) Reward model register name, refer to registry ``REWARD_MODEL_REGISTRY``.
+        type='red',
+        # (int) Linear model input size.
+        # input_size=4,
+        # (int) Sample data from expert dataset with fixed size.
         sample_size=1000,
-        alpha=5,
-        beta=5,
-        # s_size=4,
-        # a_size=2,
+        # (int) Linear model hidden size.
+        hidden_size=128,
+        # (float) The step size of gradient descent.
+        learning_rate=1e-3,
+        # (int) How many updates(iterations) to train after collector's one collection.
+        # Bigger "update_per_collect" means bigger off-policy.
+        # collect data -> update policy-> collect data -> ...
+        update_per_collect=100,
+        # (str) Path to the expert dataset
+        # expert_data_path='expert_data.pkl',
+        # (int) How many samples in a training batch.
+        batch_size=64,
+        # (float) Hyperparameter at estimated score of r(s,a).
+        # r(s,a) = exp(-sigma* L(s,a))
+        sigma=0.5,
+        # (int) Clear buffer per fixed iters.
+        clear_buffer_per_iters=1,
     )
 
     def __init__(self, config: Dict, device: str, tb_logger: 'SummaryWriter') -> None:  # noqa
         """
         Overview:
             Initialize ``self.`` See ``help(type(self))`` for accurate signature.
         Arguments:
             - cfg (:obj:`Dict`): Training config
             - device (:obj:`str`): Device usage, i.e. "cpu" or "cuda"
             - tb_logger (:obj:`str`): Logger, defaultly set as 'SummaryWriter' for model summary
         """
-        super(PwilRewardModel, self).__init__()
+        super(RedRewardModel, self).__init__()
         self.cfg: Dict = config
-        assert device in ["cpu", "cuda"] or "cuda" in device
-        self.device = device
         self.expert_data: List[tuple] = []
-        self.train_data: List[tuple] = []
-        # In this algo, model is a dict
-        self.reward_table: Dict = {}
-        self.T: int = 0
+        self.device = device
+        assert device in ["cpu", "cuda"] or "cuda" in device
+        self.tb_logger = tb_logger
+        self.target_net: SENet = SENet(config.input_size, config.hidden_size, 1)
+        self.online_net: SENet = SENet(config.input_size, config.hidden_size, 1)
+        self.target_net.to(device)
+        self.online_net.to(device)
+        self.opt: optim.Adam = optim.Adam(self.online_net.parameters(), config.learning_rate)
+        self.train_once_flag = False
 
         self.load_expert_data()
 
     def load_expert_data(self) -> None:
         """
         Overview:
-            Getting the expert data from ``config['expert_data_path']`` attribute in self
+            Getting the expert data from ``config['expert_data_path']`` attribute in self.
         Effects:
-            This is a side effect function which updates the expert data attribute (e.g.  ``self.expert_data``); \
-            in this algorithm, also the ``self.expert_s``, ``self.expert_a`` for states and actions are updated.
-
+            This is a side effect function which updates the expert data attribute (e.g.  ``self.expert_data``)
         """
         with open(self.cfg.expert_data_path, 'rb') as f:
             self.expert_data = pickle.load(f)
-            print("the data size is:", len(self.expert_data))
-        sample_size = min(self.cfg.sample_size, len(self.expert_data))
+        sample_size = min(len(self.expert_data), self.cfg.sample_size)
         self.expert_data = random.sample(self.expert_data, sample_size)
-        self.expert_data = [(item['obs'], item['action']) for item in self.expert_data]
-        self.expert_s, self.expert_a = list(zip(*self.expert_data))
-        print('the expert data demonstrations is:', len(self.expert_data))
+        print('the expert data size is:', len(self.expert_data))
 
-    def collect_data(self, data: list) -> None:
+    def _train(self, batch_data: torch.Tensor) -> float:
         """
         Overview:
-            Collecting training data formatted by  ``fn:concat_state_action_pairs``.
+            Helper function for ``train`` which caclulates loss for train data and expert data.
         Arguments:
-            - data (:obj:`list`): Raw training data (e.g. some form of states, actions, obs, etc)
-        Effects:
-            - This is a side effect function which updates the data attribute in ``self``; \
-                in this algorithm, also the ``s_size``, ``a_size`` for states and actions are updated in the \
-                    attribute in ``self.cfg`` Dict; ``reward_factor`` also updated as ``collect_data`` called.
-        """
-        self.train_data.extend(collect_state_action_pairs(data))
-        self.T = len(self.train_data)
-
-        s_size = self.cfg.s_size
-        a_size = self.cfg.a_size
-        beta = self.cfg.beta
-        self.reward_factor = -beta * self.T / math.sqrt(s_size + a_size)
+            - batch_data (:obj:`torch.Tensor`): Data used for training
+        Returns:
+            - Combined loss calculated of reward model from using ``batch_data`` in both target and reward models.
+        """
+        with torch.no_grad():
+            target = self.target_net(batch_data)
+        hat: torch.Tensor = self.online_net(batch_data)
+        loss: torch.Tensor = ((hat - target) ** 2).mean()
+        self.opt.zero_grad()
+        loss.backward()
+        self.opt.step()
+        return loss.item()
 
     def train(self) -> None:
         """
         Overview:
-            Training the Pwil reward model.
+            Training the RED reward model. In default, RED model should be trained once.
+        Effects:
+            - This is a side effect function which updates the reward model and increment the train iteration count.
         """
-        self._train(self.train_data)
+        if self.train_once_flag:
+            one_time_warning('RED model should be trained once, we do not train it anymore')
+        else:
+            for i in range(self.cfg.update_per_collect):
+                sample_batch = random.sample(self.expert_data, self.cfg.batch_size)
+                states_data = []
+                actions_data = []
+                for item in sample_batch:
+                    states_data.append(item['obs'])
+                    actions_data.append(item['action'])
+                states_tensor: torch.Tensor = torch.stack(states_data).float()
+                actions_tensor: torch.Tensor = torch.stack(actions_data).float()
+                states_actions_tensor: torch.Tensor = torch.cat([states_tensor, actions_tensor], dim=1)
+                states_actions_tensor = states_actions_tensor.to(self.device)
+                loss = self._train(states_actions_tensor)
+                self.tb_logger.add_scalar('reward_model/red_loss', loss, i)
+            self.train_once_flag = True
 
     def estimate(self, data: list) -> List[Dict]:
         """
         Overview:
-            Estimate reward by rewriting the reward key in each row of the data.
+            Estimate reward by rewriting the reward key
         Arguments:
             - data (:obj:`list`): the list of data used for estimation, \
                 with at least ``obs`` and ``action`` keys.
         Effects:
-            - This is a side effect function which updates the ``reward_table`` with ``(obs,action)`` \
-                tuples from input.
+            - This is a side effect function which updates the reward values in place.
         """
         # NOTE: deepcopy reward part of data is very important,
         # otherwise the reward of data in the replay buffer will be incorrectly modified.
         train_data_augmented = self.reward_deepcopy(data)
+        states_data = []
+        actions_data = []
         for item in train_data_augmented:
-            s = item['obs']
-            a = item['action']
-            if (s, a) in self.reward_table:
-                item['reward'] = self.reward_table[(s, a)]
-            else:
-                # when (s, a) pair is not trained, set the reward value to default value(e.g.: 0)
-                item['reward'] = torch.zeros_like(item['reward'])
+            states_data.append(item['obs'])
+            actions_data.append(item['action'])
+        states_tensor = torch.stack(states_data).float()
+        actions_tensor = torch.stack(actions_data).float()
+        states_actions_tensor = torch.cat([states_tensor, actions_tensor], dim=1)
+        states_actions_tensor = states_actions_tensor.to(self.device)
+        with torch.no_grad():
+            hat_1 = self.online_net(states_actions_tensor)
+            hat_2 = self.target_net(states_actions_tensor)
+        c = ((hat_1 - hat_2) ** 2).mean(dim=1)
+        r = torch.exp(-self.cfg.sigma * c)
+        for item, rew in zip(train_data_augmented, r):
+            item['reward'] = rew
         return train_data_augmented
 
-    def _get_state_distance(self, s1: list, s2: list) -> torch.Tensor:
-        """
-        Overview:
-            Getting distances of states given 2 state lists. One single state \
-                is of shape ``torch.Size([n])`` (``n`` referred in in-code comments)
-        Arguments:
-            - s1 (:obj:`torch.Tensor list`): the 1st states' list of size M
-            - s2 (:obj:`torch.Tensor list`): the 2nd states' list of size N
-        Returns:
-            - distance (:obj:`torch.Tensor`) Euclidean distance tensor of  \
-                the state tensor lists, of size M x N.
-        """
-        # Format the values in the tensors to be of float type
-        s1 = torch.stack(s1).float()
-        s2 = torch.stack(s2).float()
-        M, N = s1.shape[0], s2.shape[0]
-        # Automatically fill in length
-        s1 = s1.view(M, -1)
-        s2 = s2.view(N, -1)
-        # Automatically fill in & format the tensor size to be (MxNxn)
-        s1 = s1.unsqueeze(1).repeat(1, N, 1)
-        s2 = s2.unsqueeze(0).repeat(M, 1, 1)
-        # Return the distance tensor of size MxN
-        return ((s1 - s2) ** 2).mean(dim=-1)
-
-    def _get_action_distance(self, a1: list, a2: list) -> torch.Tensor:
-        # TODO the metric of action distance maybe different from envs
+    def collect_data(self, data) -> None:
         """
         Overview:
-            Getting distances of actions given 2 action lists. One single action \
-                is of shape ``torch.Size([n])`` (``n`` referred in in-code comments)
-        Arguments:
-            - a1 (:obj:`torch.Tensor list`): the 1st actions' list of size M
-            - a2 (:obj:`torch.Tensor list`): the 2nd actions' list of size N
-        Returns:
-            - distance (:obj:`torch.Tensor`) Euclidean distance tensor of  \
-                the action tensor lists, of size M x N.
-        """
-        a1 = torch.stack(a1).float()
-        a2 = torch.stack(a2).float()
-        M, N = a1.shape[0], a2.shape[0]
-        a1 = a1.view(M, -1)
-        a2 = a2.view(N, -1)
-        a1 = a1.unsqueeze(1).repeat(1, N, 1)
-        a2 = a2.unsqueeze(0).repeat(M, 1, 1)
-        return ((a1 - a2) ** 2).mean(dim=-1)
-
-    def _train(self, data: list):
-        """
-        Overview:
-            Helper function for ``train``, find the min disctance ``s_e``, ``a_e``.
-        Arguments:
-            - data (:obj:`list`): Raw training data (e.g. some form of states, actions, obs, etc)
-        Effects:
-            - This is a side effect function which updates the ``reward_table`` attribute in ``self`` .
+            Collecting training data, not implemented if reward model (i.e. online_net) is only trained ones, \
+                if online_net is trained continuously, there should be some implementations in collect_data method
         """
-        batch_s, batch_a = list(zip(*data))
-        s_distance_matrix = self._get_state_distance(batch_s, self.expert_s)
-        a_distance_matrix = self._get_action_distance(batch_a, self.expert_a)
-        distance_matrix = s_distance_matrix + a_distance_matrix
-        w_e_list = [1 / len(self.expert_data)] * len(self.expert_data)
-        for i, item in enumerate(data):
-            s, a = item
-            w_pi = 1 / self.T
-            c = 0
-            expert_data_idx = torch.arange(len(self.expert_data)).tolist()
-            while w_pi > 0:
-                selected_dist = distance_matrix[i, expert_data_idx]
-                nearest_distance = selected_dist.min().item()
-                nearest_index_selected = selected_dist.argmin().item()
-                nearest_index = expert_data_idx[nearest_index_selected]
-                if w_pi >= w_e_list[nearest_index]:
-                    c = c + nearest_distance * w_e_list[nearest_index]
-                    w_pi = w_pi - w_e_list[nearest_index]
-                    expert_data_idx.pop(nearest_index_selected)
-                else:
-                    c = c + w_pi * nearest_distance
-                    w_e_list[nearest_index] = w_e_list[nearest_index] - w_pi
-                    w_pi = 0
-            reward = self.cfg.alpha * math.exp(self.reward_factor * c)
-            self.reward_table[(s, a)] = torch.FloatTensor([reward])
+        # if online_net is trained continuously, there should be some implementations in collect_data method
+        pass
 
-    def clear_data(self) -> None:
+    def clear_data(self):
         """
         Overview:
-            Clearing training data. \
-            This is a side effect function which clears the data attribute in ``self``
+            Collecting clearing data, not implemented if reward model (i.e. online_net) is only trained ones, \
+                if online_net is trained continuously, there should be some implementations in clear_data method
         """
-        self.train_data.clear()
+        # if online_net is trained continuously, there should be some implementations in clear_data method
+        pass
```

### Comparing `DI-engine-0.4.6/ding/reward_model/red_irl_model.py` & `DI-engine-0.4.7/ding/worker/collector/base_serial_collector.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,179 +1,229 @@
-from typing import Dict, List
-import pickle
-import random
-import torch
-import torch.nn as nn
-import torch.optim as optim
-
-from ding.utils import REWARD_MODEL_REGISTRY, one_time_warning
-from .base_reward_model import BaseRewardModel
-
-
-class SENet(nn.Module):
-    """support estimation network"""
-
-    def __init__(self, input_size: int, hidden_size: int, output_dims: int) -> None:
-        super(SENet, self).__init__()
-        self.l_1 = nn.Linear(input_size, hidden_size)
-        self.l_2 = nn.Linear(hidden_size, output_dims)
-        self.act = nn.Tanh()
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        out = self.l_1(x)
-        out = self.act(out)
-        out = self.l_2(out)
-        out = self.act(out)
-        return out
-
-
-@REWARD_MODEL_REGISTRY.register('red')
-class RedRewardModel(BaseRewardModel):
-    """
-    Overview:
-         The implement of reward model in RED (https://arxiv.org/abs/1905.06750)
-    Interface:
-        ``estimate``, ``train``, ``load_expert_data``, ``collect_data``, ``clear_date``, \
-            ``__init__``, ``_train``
-    Properties:
-        - online_net (:obj: `SENet`): The reward model, in default initialized once as the training begins.
-    """
-    config = dict(
-        type='red',
-        # input_size=4,
-        sample_size=1000,
-        hidden_size=128,
-        learning_rate=1e-3,
-        update_per_collect=100,
-        # expert_data_path='expert_data.pkl',
-        batch_size=64,
-        sigma=0.5,
-    )
+from abc import ABC, abstractmethod, abstractproperty
+from typing import List, Dict, Any, Optional, Union
+from collections import namedtuple
+from easydict import EasyDict
+import copy
 
-    def __init__(self, config: Dict, device: str, tb_logger: 'SummaryWriter') -> None:  # noqa
+from ding.envs import BaseEnvManager
+from ding.utils import SERIAL_COLLECTOR_REGISTRY, import_module
+from ding.torch_utils import to_tensor
+
+INF = float("inf")
+
+
+class ISerialCollector(ABC):
+    """
+    Overview:
+        Abstract baseclass for serial collector.
+    Interfaces:
+        default_config, reset_env, reset_policy, reset, collect
+    Property:
+        envstep
+    """
+
+    @classmethod
+    def default_config(cls: type) -> EasyDict:
+        """
+        Overview:
+            Get collector's default config. We merge collector's default config with other default configs\
+                and user's config to get the final config.
+        Return:
+            cfg: (:obj:`EasyDict`): collector's default config
+        """
+        cfg = EasyDict(copy.deepcopy(cls.config))
+        cfg.cfg_type = cls.__name__ + 'Dict'
+        return cfg
+
+    @abstractmethod
+    def reset_env(self, _env: Optional[BaseEnvManager] = None) -> None:
+        """
+        Overview:
+            Reset collector's environment. In some case, we need collector use the same policy to collect \
+                data in different environments. We can use reset_env to reset the environment.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def reset_policy(self, _policy: Optional[namedtuple] = None) -> None:
+        """
+        Overview:
+            Reset collector's policy. In some case, we need collector work in this same environment but use\
+                different policy to collect data. We can use reset_policy to reset the policy.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def reset(self, _policy: Optional[namedtuple] = None, _env: Optional[BaseEnvManager] = None) -> None:
+        """
+        Overview:
+            Reset collector's policy and environment. Use new policy and environment to collect data.
+        """
+        raise NotImplementedError
+
+    @abstractmethod
+    def collect(self, per_collect_target: Any) -> List[Any]:
+        """
+        Overview:
+            Collect the corresponding data according to the specified target and return. \
+                There are different definitions in episode and sample mode.
+        """
+        raise NotImplementedError
+
+    @abstractproperty
+    def envstep(self) -> int:
+        """
+        Overview:
+            Get the total envstep num.
+        """
+        raise NotImplementedError
+
+
+def create_serial_collector(cfg: EasyDict, **kwargs) -> ISerialCollector:
+    """
+    Overview:
+        Create a specific collector instance based on the config.
+    """
+    import_module(cfg.get('import_names', []))
+    return SERIAL_COLLECTOR_REGISTRY.build(cfg.type, cfg=cfg, **kwargs)
+
+
+def get_serial_collector_cls(cfg: EasyDict) -> type:
+    """
+    Overview:
+        Get the specific collector class according to the config.
+    """
+    assert hasattr(cfg, 'type'), "{}-{}-{}".format(type(cfg), cfg.keys(), cfg['type'])
+    import_module(cfg.get('import_names', []))
+    return SERIAL_COLLECTOR_REGISTRY.get(cfg.type)
+
+
+class CachePool(object):
+    """
+    Overview:
+       CachePool is the repository of cache items.
+    Interfaces:
+        __init__, update, __getitem__, reset
+    """
+
+    def __init__(self, name: str, env_num: int, deepcopy: bool = False) -> None:
         """
         Overview:
-            Initialize ``self.`` See ``help(type(self))`` for accurate signature.
+            Initialization method.
         Arguments:
-            - cfg (:obj:`Dict`): Training config
-            - device (:obj:`str`): Device usage, i.e. "cpu" or "cuda"
-            - tb_logger (:obj:`str`): Logger, defaultly set as 'SummaryWriter' for model summary
-        """
-        super(RedRewardModel, self).__init__()
-        self.cfg: Dict = config
-        self.expert_data: List[tuple] = []
-        self.device = device
-        assert device in ["cpu", "cuda"] or "cuda" in device
-        self.tb_logger = tb_logger
-        self.target_net: SENet = SENet(config.input_size, config.hidden_size, 1)
-        self.online_net: SENet = SENet(config.input_size, config.hidden_size, 1)
-        self.target_net.to(device)
-        self.online_net.to(device)
-        self.opt: optim.Adam = optim.Adam(self.online_net.parameters(), config.learning_rate)
-        self.train_once_flag = False
-
-        self.load_expert_data()
-
-    def load_expert_data(self) -> None:
-        """
-        Overview:
-            Getting the expert data from ``config['expert_data_path']`` attribute in self.
-        Effects:
-            This is a side effect function which updates the expert data attribute (e.g.  ``self.expert_data``)
-        """
-        with open(self.cfg.expert_data_path, 'rb') as f:
-            self.expert_data = pickle.load(f)
-        sample_size = min(len(self.expert_data), self.cfg.sample_size)
-        self.expert_data = random.sample(self.expert_data, sample_size)
-        print('the expert data size is:', len(self.expert_data))
+            - name (:obj:`str`): name of cache
+            - env_num (:obj:`int`): number of environments
+            - deepcopy (:obj:`bool`): whether to deepcopy data
+        """
+        self._pool = [None for _ in range(env_num)]
+        # TODO(nyz) whether must use deepcopy
+        self._deepcopy = deepcopy
 
-    def _train(self, batch_data: torch.Tensor) -> float:
+    def update(self, data: Union[Dict[int, Any], list]) -> None:
         """
         Overview:
-            Helper function for ``train`` which caclulates loss for train data and expert data.
+            Update elements in cache pool.
         Arguments:
-            - batch_data (:obj:`torch.Tensor`): Data used for training
-        Returns:
-            - Combined loss calculated of reward model from using ``batch_data`` in both target and reward models.
-        """
-        with torch.no_grad():
-            target = self.target_net(batch_data)
-        hat: torch.Tensor = self.online_net(batch_data)
-        loss: torch.Tensor = ((hat - target) ** 2).mean()
-        self.opt.zero_grad()
-        loss.backward()
-        self.opt.step()
-        return loss.item()
-
-    def train(self) -> None:
-        """
-        Overview:
-            Training the RED reward model. In default, RED model should be trained once.
-        Effects:
-            - This is a side effect function which updates the reward model and increment the train iteration count.
+            - data (:obj:`Dict[int, Any]`): A dict containing update index-value pairs. Key is index in cache pool, \
+                and value is the new element.
         """
-        if self.train_once_flag:
-            one_time_warning('RED model should be trained once, we do not train it anymore')
-        else:
-            for i in range(self.cfg.update_per_collect):
-                sample_batch = random.sample(self.expert_data, self.cfg.batch_size)
-                states_data = []
-                actions_data = []
-                for item in sample_batch:
-                    states_data.append(item['obs'])
-                    actions_data.append(item['action'])
-                states_tensor: torch.Tensor = torch.stack(states_data).float()
-                actions_tensor: torch.Tensor = torch.stack(actions_data).float()
-                states_actions_tensor: torch.Tensor = torch.cat([states_tensor, actions_tensor], dim=1)
-                states_actions_tensor = states_actions_tensor.to(self.device)
-                loss = self._train(states_actions_tensor)
-                self.tb_logger.add_scalar('reward_model/red_loss', loss, i)
-            self.train_once_flag = True
+        if isinstance(data, dict):
+            data = [data]
+        for index in range(len(data)):
+            for i in data[index].keys():
+                d = data[index][i]
+                if self._deepcopy:
+                    copy_d = copy.deepcopy(d)
+                else:
+                    copy_d = d
+                if index == 0:
+                    self._pool[i] = [copy_d]
+                else:
+                    self._pool[i].append(copy_d)
 
-    def estimate(self, data: list) -> List[Dict]:
+    def __getitem__(self, idx: int) -> Any:
         """
         Overview:
-            Estimate reward by rewriting the reward key
+            Get item in cache pool.
         Arguments:
-            - data (:obj:`list`): the list of data used for estimation, \
-                with at least ``obs`` and ``action`` keys.
-        Effects:
-            - This is a side effect function which updates the reward values in place.
-        """
-        # NOTE: deepcopy reward part of data is very important,
-        # otherwise the reward of data in the replay buffer will be incorrectly modified.
-        train_data_augmented = self.reward_deepcopy(data)
-        states_data = []
-        actions_data = []
-        for item in train_data_augmented:
-            states_data.append(item['obs'])
-            actions_data.append(item['action'])
-        states_tensor = torch.stack(states_data).float()
-        actions_tensor = torch.stack(actions_data).float()
-        states_actions_tensor = torch.cat([states_tensor, actions_tensor], dim=1)
-        states_actions_tensor = states_actions_tensor.to(self.device)
-        with torch.no_grad():
-            hat_1 = self.online_net(states_actions_tensor)
-            hat_2 = self.target_net(states_actions_tensor)
-        c = ((hat_1 - hat_2) ** 2).mean(dim=1)
-        r = torch.exp(-self.cfg.sigma * c)
-        for item, rew in zip(train_data_augmented, r):
-            item['reward'] = rew
-        return train_data_augmented
-
-    def collect_data(self, data) -> None:
-        """
-        Overview:
-            Collecting training data, not implemented if reward model (i.e. online_net) is only trained ones, \
-                if online_net is trained continuously, there should be some implementations in collect_data method
-        """
-        # if online_net is trained continuously, there should be some implementations in collect_data method
-        pass
+            - idx (:obj:`int`): The index of the item we need to get.
+        Return:
+            - item (:obj:`Any`): The item we get.
+        """
+        data = self._pool[idx]
+        if data is not None and len(data) == 1:
+            data = data[0]
+        return data
 
-    def clear_data(self):
+    def reset(self, idx: int) -> None:
         """
         Overview:
-            Collecting clearing data, not implemented if reward model (i.e. online_net) is only trained ones, \
-                if online_net is trained continuously, there should be some implementations in clear_data method
+            Reset the cache pool.
+        Arguments:
+            - idx (:obj:`int`): The index of the position we need to reset.
         """
-        # if online_net is trained continuously, there should be some implementations in clear_data method
-        pass
+        self._pool[idx] = None
+
+
+class TrajBuffer(list):
+    """
+    Overview:
+       TrajBuffer is used to store traj_len pieces of transitions.
+    Interfaces:
+        __init__, append
+    """
+
+    def __init__(self, maxlen: int, *args, deepcopy: bool = False, **kwargs) -> None:
+        """
+        Overview:
+            Initialization trajBuffer.
+        Arguments:
+            - maxlen (:obj:`int`): The maximum length of trajectory buffer.
+            - deepcopy (:obj:`bool`): Whether to deepcopy data when do operation.
+        """
+        self._maxlen = maxlen
+        self._deepcopy = deepcopy
+        super().__init__(*args, **kwargs)
+
+    def append(self, data: Any) -> None:
+        """
+        Overview:
+            Append data to trajBuffer.
+        """
+        if self._maxlen is not None:
+            while len(self) >= self._maxlen:
+                del self[0]
+        if self._deepcopy:
+            data = copy.deepcopy(data)
+        super().append(data)
+
+
+def to_tensor_transitions(data: List[Dict[str, Any]], shallow_copy_next_obs: bool = True) -> List[Dict[str, Any]]:
+    """
+    Overview:
+        Transform ths original transition return from env to tensor format.
+    Argument:
+        - data (:obj:`List[Dict[str, Any]]`): The data that will be transformed to tensor.
+        - shallow_copy_next_obs (:obj:`bool`): Whether to shallow copy next_obs. Default: True.
+    Return:
+        - data (:obj:`List[Dict[str, Any]]`): The transformed tensor-like data.
+
+    .. tip::
+        In order to save memory, If there are next_obs in the passed data, we do special \
+        treatment on next_obs so that the next_obs of each state in the data fragment is \
+        the next state's obs and the next_obs of the last state is its own next_obsself. \
+        Besides, we set transform_scalar to False to avoid the extra ``.item()`` operation.
+    """
+    if 'next_obs' not in data[0]:
+        return to_tensor(data, transform_scalar=False)
+    else:
+        # to_tensor will assign the separate memory to next_obs, if shallow_copy_next_obs is True,
+        # we can add ignore_keys to avoid this data copy for saving memory of next_obs.
+        if shallow_copy_next_obs:
+            data = to_tensor(data, ignore_keys=['next_obs'], transform_scalar=False)
+            for i in range(len(data) - 1):
+                data[i]['next_obs'] = data[i + 1]['obs']
+            data[-1]['next_obs'] = to_tensor(data[-1]['next_obs'], transform_scalar=False)
+            return data
+        else:
+            data = to_tensor(data, transform_scalar=False)
+        return data
```

### Comparing `DI-engine-0.4.6/ding/reward_model/trex_reward_model.py` & `DI-engine-0.4.7/ding/reward_model/trex_reward_model.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,29 +1,21 @@
-from collections.abc import Iterable
 from copy import deepcopy
 from typing import Tuple, Optional, List, Dict
 from easydict import EasyDict
 import pickle
 import os
 import numpy as np
 
 import torch
 import torch.nn as nn
-import torch.nn.functional as F
 import torch.optim as optim
-from torch.distributions import Normal, Independent
-from torch.distributions.categorical import Categorical
 
 from ding.utils import REWARD_MODEL_REGISTRY
-from ding.model.template.q_learning import DQN
-from ding.model.template.vac import VAC
-from ding.model.template.qac import QAC
 from ding.utils import SequenceType
 from ding.model.common import FCEncoder
-from ding.utils.data import offline_data_save_type
 from ding.utils import build_logger
 from ding.utils.data import default_collate
 
 from .base_reward_model import BaseRewardModel
 from .rnd_reward_model import collect_states
 
 
@@ -32,33 +24,30 @@
     Overview:
         The ``Convolution Encoder`` used in models. Used to encoder raw 2-dim observation.
     Interfaces:
         ``__init__``, ``forward``
     """
 
     def __init__(
-            self,
-            obs_shape: SequenceType,
-            hidden_size_list: SequenceType = [16, 16, 16, 16, 64, 1],
-            activation: Optional[nn.Module] = nn.LeakyReLU(),
-            norm_type: Optional[str] = None
+        self,
+        obs_shape: SequenceType,
+        hidden_size_list: SequenceType = [16, 16, 16, 16, 64, 1],
+        activation: Optional[nn.Module] = nn.LeakyReLU()
     ) -> None:
         r"""
         Overview:
             Init the Trex Convolution Encoder according to arguments. TrexConvEncoder is different \
                 from the ConvEncoder in model.common.encoder, their stride and kernel size parameters \
                 are different
         Arguments:
             - obs_shape (:obj:`SequenceType`): Sequence of ``in_channel``, some ``output size``
             - hidden_size_list (:obj:`SequenceType`): The collection of ``hidden_size``
             - activation (:obj:`nn.Module`):
                 The type of activation to use in the conv ``layers``,
-                if ``None`` then default set to ``nn.ReLU()``
-            - norm_type (:obj:`str`):
-                The type of normalization to use, see ``ding.torch_utils.ResBlock`` for more details
+                if ``None`` then default set to ``nn.LeakyReLU()``
         """
         super(TrexConvEncoder, self).__init__()
         self.obs_shape = obs_shape
         self.act = activation
         self.hidden_size_list = hidden_size_list
 
         layers = []
@@ -144,21 +133,40 @@
 class TrexRewardModel(BaseRewardModel):
     """
     Overview:
         The Trex reward model class (https://arxiv.org/pdf/1904.06387.pdf)
     Interface:
         ``estimate``, ``train``, ``load_expert_data``, ``collect_data``, ``clear_date``, \
             ``__init__``, ``_train``,
+    Config:
+        == ====================  ======   =============  ============================================  =============
+        ID Symbol                Type     Default Value  Description                                   Other(Shape)
+        == ====================  ======   =============  ============================================  =============
+        1  ``type``              str       trex          | Reward model register name, refer           |
+                                                         | to registry ``REWARD_MODEL_REGISTRY``       |
+        3  | ``learning_rate``   float     0.00001       | learning rate for optimizer                 |
+        4  | ``update_per_``     int       100           | Number of updates per collect               |
+           | ``collect``                                 |                                             |
+        5  | ``num_trajs``       int       0             | Number of downsampled full trajectories     |
+        6  | ``num_snippets``    int       6000          | Number of short subtrajectories to sample   |
+        == ====================  ======   =============  ============================================  =============
     """
     config = dict(
+        # (str) Reward model register name, refer to registry ``REWARD_MODEL_REGISTRY``.
         type='trex',
+        # (float) The step size of gradient descent.
         learning_rate=1e-5,
+        # (int) How many updates(iterations) to train after collector's one collection.
+        # Bigger "update_per_collect" means bigger off-policy.
+        # collect data -> update policy-> collect data -> ...
         update_per_collect=100,
-        num_trajs=0,  # number of downsampled full trajectories
-        num_snippets=6000,  # number of short subtrajectories to sample
+        # (int) Number of downsampled full trajectories.
+        num_trajs=0,
+        # (int) Number of short subtrajectories to sample.
+        num_snippets=6000,
     )
 
     def __init__(self, config: EasyDict, device: str, tb_logger: 'SummaryWriter') -> None:  # noqa
         """
         Overview:
             Initialize ``self.`` See ``help(type(self))`` for accurate signature.
         Arguments:
```

### Comparing `DI-engine-0.4.6/ding/rl_utils/__init__.py` & `DI-engine-0.4.7/ding/rl_utils/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 from .exploration import get_epsilon_greedy_fn, create_noise_generator
 from .ppo import ppo_data, ppo_loss, ppo_info, ppo_policy_data, ppo_policy_error, ppo_value_data, ppo_value_error,\
     ppo_error, ppo_error_continuous, ppo_policy_error_continuous
 from .ppg import ppg_data, ppg_joint_loss, ppg_joint_error
 from .gae import gae_data, gae
 from .a2c import a2c_data, a2c_error
 from .coma import coma_data, coma_error
-from .td import q_nstep_td_data, q_nstep_td_error, q_1step_td_data, q_1step_td_error, td_lambda_data, td_lambda_error,\
+from .td import q_nstep_td_data, q_nstep_td_error, q_1step_td_data, \
+    q_1step_td_error, m_q_1step_td_data, m_q_1step_td_error, td_lambda_data, td_lambda_error,\
     q_nstep_td_error_with_rescale, v_1step_td_data, v_1step_td_error, v_nstep_td_data, v_nstep_td_error, \
     generalized_lambda_returns, dist_1step_td_data, dist_1step_td_error, dist_nstep_td_error, dist_nstep_td_data, \
     nstep_return_data, nstep_return, iqn_nstep_td_data, iqn_nstep_td_error, qrdqn_nstep_td_data, qrdqn_nstep_td_error,\
     fqf_nstep_td_data, fqf_nstep_td_error, fqf_calculate_fraction_loss, evaluate_quantile_at_action, \
     q_nstep_sql_td_error, dqfd_nstep_td_error, dqfd_nstep_td_data, q_v_1step_td_error, q_v_1step_td_data,\
     dqfd_nstep_td_error_with_rescale, discount_cumsum, bdq_nstep_td_error
 from .vtrace import vtrace_loss, compute_importance_weights
 from .upgo import upgo_loss
 from .adder import get_gae, get_gae_with_default_last_value, get_nstep_return_data, get_train_sample
-from .value_rescale import value_transform, value_inv_transform
+from .value_rescale import value_transform, value_inv_transform, symlog, inv_symlog
 from .vtrace import vtrace_data, vtrace_error_discrete_action, vtrace_error_continuous_action
 from .beta_function import beta_function_map
 from .retrace import compute_q_retraces
 from .acer import acer_policy_error, acer_value_error, acer_trust_region_update
 from .sampler import ArgmaxSampler, MultinomialSampler, MuSampler, ReparameterizationSampler, HybridStochasticSampler, \
     HybridDeterminsticSampler
```

### Comparing `DI-engine-0.4.6/ding/rl_utils/a2c.py` & `DI-engine-0.4.7/ding/rl_utils/a2c.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/acer.py` & `DI-engine-0.4.7/ding/rl_utils/acer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/adder.py` & `DI-engine-0.4.7/ding/rl_utils/adder.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/beta_function.py` & `DI-engine-0.4.7/ding/rl_utils/beta_function.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/coma.py` & `DI-engine-0.4.7/ding/rl_utils/coma.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/exploration.py` & `DI-engine-0.4.7/ding/rl_utils/exploration.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/gae.py` & `DI-engine-0.4.7/ding/rl_utils/gae.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/isw.py` & `DI-engine-0.4.7/ding/rl_utils/isw.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/ppg.py` & `DI-engine-0.4.7/ding/rl_utils/ppg.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/ppo.py` & `DI-engine-0.4.7/ding/rl_utils/ppo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/retrace.py` & `DI-engine-0.4.7/ding/rl_utils/retrace.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/sampler.py` & `DI-engine-0.4.7/ding/rl_utils/sampler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/td.py` & `DI-engine-0.4.7/ding/rl_utils/td.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,21 +36,114 @@
         weight = torch.ones_like(reward)
     q_s_a = q[batch_range, act]
     target_q_s_a = next_q[batch_range, next_act]
     target_q_s_a = gamma * (1 - done) * target_q_s_a + reward
     return (criterion(q_s_a, target_q_s_a.detach()) * weight).mean()
 
 
+m_q_1step_td_data = namedtuple('m_q_1step_td_data', ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight'])
+
+
+def m_q_1step_td_error(
+        data: namedtuple,
+        gamma: float,
+        tau: float,
+        alpha: float,
+        criterion: torch.nn.modules = nn.MSELoss(reduction='none')  # noqa
+) -> torch.Tensor:
+    """
+    Overview:
+        Munchausen td_error for DQN algorithm, support 1 step td error.
+    Arguments:
+        - data (:obj:`m_q_1step_td_data`): The input data, m_q_1step_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+        - tau (:obj:`float`): Entropy factor for Munchausen DQN
+        - alpha (:obj:`float`): Discount factor for Munchausen term
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
+    Returns:
+        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor
+    Shapes:
+        - data (:obj:`m_q_1step_td_data`): the m_q_1step_td_data containing\
+             ['q', 'target_q', 'next_q', 'act', 'reward', 'done', 'weight']
+        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
+        - target_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
+        - next_q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
+        - act (:obj:`torch.LongTensor`): :math:`(B, )`
+        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`
+        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
+        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight
+    """
+    q, target_q, next_q, act, reward, done, weight = data
+    lower_bound = -1
+    assert len(act.shape) == 1, act.shape
+    assert len(reward.shape) == 1, reward.shape
+    batch_range = torch.arange(act.shape[0])
+    if weight is None:
+        weight = torch.ones_like(reward)
+    q_s_a = q[batch_range, act]
+    # calculate muchausen addon
+    # replay_log_policy
+    target_v_s = target_q[batch_range].max(1)[0].unsqueeze(-1)
+
+    logsum = torch.logsumexp((target_q - target_v_s) / tau, 1).unsqueeze(-1)
+    log_pi = target_q - target_v_s - tau * logsum
+    act_get = act.unsqueeze(-1)
+    # same to the last second tau_log_pi_a
+    munchausen_addon = log_pi.gather(1, act_get)
+
+    muchausen_term = alpha * torch.clamp(munchausen_addon, min=lower_bound, max=1)
+
+    # replay_next_log_policy
+    target_v_s_next = next_q[batch_range].max(1)[0].unsqueeze(-1)
+    logsum_next = torch.logsumexp((next_q - target_v_s_next) / tau, 1).unsqueeze(-1)
+    tau_log_pi_next = next_q - target_v_s_next - tau * logsum_next
+    # do stable softmax == replay_next_policy
+    pi_target = F.softmax((next_q - target_v_s_next) / tau)
+    target_q_s_a = (gamma * (pi_target * (next_q - tau_log_pi_next) * (1 - done.unsqueeze(-1))).sum(1)).unsqueeze(-1)
+
+    target_q_s_a = reward.unsqueeze(-1) + muchausen_term + target_q_s_a
+    td_error_per_sample = criterion(q_s_a.unsqueeze(-1), target_q_s_a.detach()).squeeze(-1)
+
+    # calculate action_gap and clipfrac
+    with torch.no_grad():
+        top2_q_s = target_q[batch_range].topk(2, dim=1, largest=True, sorted=True)[0]
+        action_gap = (top2_q_s[:, 0] - top2_q_s[:, 1]).mean()
+
+        clipped = munchausen_addon.gt(1) | munchausen_addon.lt(lower_bound)
+        clipfrac = torch.as_tensor(clipped).float()
+
+    return (td_error_per_sample * weight).mean(), td_error_per_sample, action_gap, clipfrac
+
+
 q_v_1step_td_data = namedtuple('q_v_1step_td_data', ['q', 'v', 'act', 'reward', 'done', 'weight'])
 
 
 def q_v_1step_td_error(
         data: namedtuple, gamma: float, criterion: torch.nn.modules = nn.MSELoss(reduction='none')
 ) -> torch.Tensor:
     # we will use this function in discrete sac algorithm to calculate td error between q and v value.
+    """
+    Overview:
+        td_error between q and v value for SAC algorithm, support 1 step td error.
+    Arguments:
+        - data (:obj:`q_v_1step_td_data`): The input data, q_v_1step_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
+    Returns:
+        - loss (:obj:`torch.Tensor`): 1step td error, 0-dim tensor
+    Shapes:
+        - data (:obj:`q_v_1step_td_data`): the q_v_1step_td_data containing\
+             ['q', 'v', 'act', 'reward', 'done', 'weight']
+        - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
+        - v (:obj:`torch.FloatTensor`): :math:`(B, )`
+        - act (:obj:`torch.LongTensor`): :math:`(B, )`
+        - reward (:obj:`torch.FloatTensor`): :math:`( , B)`
+        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
+        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight
+    """
     q, v, act, reward, done, weight = data
     if len(act.shape) == 1:
         assert len(reward.shape) == 1, reward.shape
         batch_range = torch.arange(act.shape[0])
         if weight is None:
             weight = torch.ones_like(reward)
         q_s_a = q[batch_range, act]
@@ -117,14 +210,33 @@
 def dist_1step_td_error(
         data: namedtuple,
         gamma: float,
         v_min: float,
         v_max: float,
         n_atom: int,
 ) -> torch.Tensor:
+    """
+    Overview:
+        1 step td_error for distributed q-learning based algorithm
+    Arguments:
+        - data (:obj:`dist_1step_td_data`): The input data, dist_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+    Returns:
+        - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
+    Shapes:
+        - data (:obj:`dist_1step_td_data`): the dist_1step_td_data containing\
+            ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']
+        - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]
+        - next_dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)`
+        - act (:obj:`torch.LongTensor`): :math:`(B, )`
+        - next_act (:obj:`torch.LongTensor`): :math:`(B, )`
+        - reward (:obj:`torch.FloatTensor`): :math:`(, B)`
+        - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
+        - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight
+    """
     dist, next_dist, act, next_act, reward, done, weight = data
     device = reward.device
     assert len(reward.shape) == 1, reward.shape
     support = torch.linspace(v_min, v_max, n_atom).to(device)
     delta_z = (v_max - v_min) / (n_atom - 1)
 
     if len(act.shape) == 1:
@@ -207,21 +319,21 @@
         gamma: float,
         v_min: float,
         v_max: float,
         n_atom: int,
         nstep: int = 1,
         value_gamma: Optional[torch.Tensor] = None,
 ) -> torch.Tensor:
-    r"""
+    """
     Overview:
         Multistep (1 step or n step) td_error for distributed q-learning based algorithm, support single\
             agent case and multi agent case.
     Arguments:
-        - data (:obj:`dist_nstep_td_data`): the input data, dist_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`dist_nstep_td_data`): The input data, dist_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
         - data (:obj:`dist_nstep_td_data`): the dist_nstep_td_data containing\
             ['dist', 'next_n_dist', 'act', 'reward', 'done', 'weight']
         - dist (:obj:`torch.FloatTensor`): :math:`(B, N, n_atom)` i.e. [batch_size, action_dim, n_atom]
@@ -340,21 +452,21 @@
         nstep: int = 1,
         criterion: torch.nn.modules = nn.MSELoss(reduction='none')  # noqa
 ) -> torch.Tensor:
     r"""
     Overview:
         Multistep (n step) td_error for distributed value based algorithm
     Arguments:
-        - data (:obj:`dist_nstep_td_data`): the input data, v_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`dist_nstep_td_data`): The input data, v_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
-        - data (:obj:`dist_nstep_td_data`): the v_nstep_td_data containing\
+        - data (:obj:`dist_nstep_td_data`): The v_nstep_td_data containing\
             ['v', 'next_n_v', 'reward', 'done', 'weight', 'value_gamma']
         - v (:obj:`torch.FloatTensor`): :math:`(B, )` i.e. [batch_size, ]
         - next_v (:obj:`torch.FloatTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
         - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight
         - value_gamma (:obj:`torch.Tensor`): If the remaining data in the buffer is less than n_step\
@@ -405,25 +517,25 @@
         value_gamma: Optional[torch.Tensor] = None,
         criterion: torch.nn.modules = nn.MSELoss(reduction='none'),
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error for q-learning based algorithm
     Arguments:
-        - data (:obj:`q_nstep_td_data`): the input data, q_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
-        - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data
-        - value_gamma (:obj:`torch.Tensor`): gamma discount value for target q_value
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
+        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data
+        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
         - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
             ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -469,25 +581,25 @@
         In fact, the original paper only provides the 1-step TD-error calculation method, \
             and here we extend the calculation method of n-step.
                 TD-error:
                     y_d = \sigma_{t=0}^{nstep} \gamma^t * r_t + \gamma^{nstep} * Q_d'(s', argmax Q_d(s', a_d))
                     TD-error = \frac{1}{D} * (y_d - Q_d(s, a_d))^2
                     Loss = mean(TD-error)
     Arguments:
-        - data (:obj:`q_nstep_td_data`): the input data, q_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
-        - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data
-        - value_gamma (:obj:`torch.Tensor`): gamma discount value for target q_value
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
+        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data
+        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
         - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
             ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(B, D, N)` i.e. [batch_size, branch_num, action_bins_per_branch]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, D, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, D)`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, D)`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -544,26 +656,26 @@
     trans_fn: Callable = value_transform,
     inv_trans_fn: Callable = value_inv_transform,
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error with value rescaling
     Arguments:
-        - data (:obj:`q_nstep_td_data`): the input data, q_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
-        - trans_fn (:obj:`Callable`): value transfrom function, default to value_transform\
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
+        - trans_fn (:obj:`Callable`): Value transfrom function, default to value_transform\
             (refer to rl_utils/value_rescale.py)
-        - inv_trans_fn (:obj:`Callable`): value inverse transfrom function, default to value_inv_transform\
+        - inv_trans_fn (:obj:`Callable`): Value inverse transfrom function, default to value_inv_transform\
             (refer to rl_utils/value_rescale.py)
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
         ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -597,19 +709,19 @@
         value_gamma: Optional[torch.Tensor] = None,
         criterion: torch.nn.modules = nn.MSELoss(reduction='none'),
 ) -> torch.Tensor:
     """
     Overview:
         Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd
     Arguments:
-        - data (:obj:`dqfd_nstep_td_data`): the input data, dqfd_nstep_td_data to calculate loss
+        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss
         - gamma (:obj:`float`): discount factor
-        - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data
-        - value_gamma (:obj:`torch.Tensor`): gamma discount value for target q_value
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
+        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data
+        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
         - nstep (:obj:`int`): nstep num, default set to 10
     Returns:
         - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor
         - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error\
             + supervised margin loss, 1-dim tensor
     Shapes:
         - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
@@ -695,26 +807,26 @@
     trans_fn: Callable = value_transform,
     inv_trans_fn: Callable = value_inv_transform,
 ) -> torch.Tensor:
     """
     Overview:
         Multistep n step td_error + 1 step td_error + supervised margin loss or dqfd
     Arguments:
-        - data (:obj:`dqfd_nstep_td_data`): the input data, dqfd_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
-        - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data
-        - value_gamma (:obj:`torch.Tensor`): gamma discount value for target q_value
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
+        - data (:obj:`dqfd_nstep_td_data`): The input data, dqfd_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
+        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data
+        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target q_value
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
         - nstep (:obj:`int`): nstep num, default set to 10
     Returns:
         - loss (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error + supervised margin loss, 0-dim tensor
         - td_error_per_sample (:obj:`torch.Tensor`): Multistep n step td_error + 1 step td_error\
             + supervised margin loss, 1-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
             ['q', 'next_n_q', 'action', 'next_n_action', 'reward', 'done', 'weight'\
                 , 'new_n_q_one_step', 'next_n_action_one_step', 'is_expert']
         - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
@@ -799,21 +911,21 @@
         nstep: int = 1,
         value_gamma: Optional[torch.Tensor] = None,
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error with in QRDQN
     Arguments:
-        - data (:obj:`iqn_nstep_td_data`): the input data, iqn_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
         ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -869,26 +981,26 @@
         value_gamma: Optional[torch.Tensor] = None,
         criterion: torch.nn.modules = nn.MSELoss(reduction='none'),
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error for q-learning based algorithm
     Arguments:
-        - data (:obj:`q_nstep_td_data`): the input data, q_nstep_sql_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`q_nstep_td_data`): The input data, q_nstep_sql_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - Alpha (:obj:｀float`): A parameter to weight entropy term in a policy equation
-        - cum_reward (:obj:`bool`): whether to use cumulative nstep reward, which is figured out when collecting data
-        - value_gamma (:obj:`torch.Tensor`): gamma discount value for target soft_q_value
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
+        - cum_reward (:obj:`bool`): Whether to use cumulative nstep reward, which is figured out when collecting data
+        - value_gamma (:obj:`torch.Tensor`): Gamma discount value for target soft_q_value
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
         - nstep (:obj:`int`): nstep num, default set to 1
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
         - td_error_per_sample (:obj:`torch.Tensor`): nstep td error, 1-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
             ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(B, N)` i.e. [batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -937,23 +1049,23 @@
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error with in IQN, \
             referenced paper Implicit Quantile Networks for Distributional Reinforcement Learning \
             <https://arxiv.org/pdf/1806.06923.pdf>
     Arguments:
-        - data (:obj:`iqn_nstep_td_data`): the input data, iqn_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`iqn_nstep_td_data`): The input data, iqn_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
-        - beta_function (:obj:`Callable`): the risk function
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
+        - beta_function (:obj:`Callable`): The risk function
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
         ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(tau, B, N)` i.e. [tau x batch_size, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(tau', B, N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -1032,23 +1144,23 @@
 ) -> torch.Tensor:
     """
     Overview:
         Multistep (1 step or n step) td_error with in FQF, \
             referenced paper Fully Parameterized Quantile Function for Distributional Reinforcement Learning \
             <https://arxiv.org/pdf/1911.02140.pdf>
     Arguments:
-        - data (:obj:`fqf_nstep_td_data`): the input data, fqf_nstep_td_data to calculate loss
-        - gamma (:obj:`float`): discount factor
+        - data (:obj:`fqf_nstep_td_data`): The input data, fqf_nstep_td_data to calculate loss
+        - gamma (:obj:`float`): Discount factor
         - nstep (:obj:`int`): nstep num, default set to 1
-        - criterion (:obj:`torch.nn.modules`): loss function criterion
-        - beta_function (:obj:`Callable`): the risk function
+        - criterion (:obj:`torch.nn.modules`): Loss function criterion
+        - beta_function (:obj:`Callable`): The risk function
     Returns:
         - loss (:obj:`torch.Tensor`): nstep td error, 0-dim tensor
     Shapes:
-        - data (:obj:`q_nstep_td_data`): the q_nstep_td_data containing\
+        - data (:obj:`q_nstep_td_data`): The q_nstep_td_data containing\
         ['q', 'next_n_q', 'action', 'reward', 'done']
         - q (:obj:`torch.FloatTensor`): :math:`(B, tau, N)` i.e. [batch_size, tau, action_dim]
         - next_n_q (:obj:`torch.FloatTensor`): :math:`(B, tau', N)`
         - action (:obj:`torch.LongTensor`): :math:`(B, )`
         - next_n_action (:obj:`torch.LongTensor`): :math:`(B, )`
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, where T is timestep(nstep)
         - done (:obj:`torch.BoolTensor`) :math:`(B, )`, whether done in last timestep
@@ -1191,16 +1303,16 @@
     Overview:
         Computing TD(lambda) loss given constant gamma and lambda.
         There is no special handling for terminal state value,
         if some state has reached the terminal, just fill in zeros for values and rewards beyond terminal
         (*including the terminal state*, values[terminal] should also be 0)
     Arguments:
         - data (:obj:`namedtuple`): td_lambda input data with fields ['value', 'reward', 'weight']
-        - gamma (:obj:`float`): constant discount factor gamma, should be in [0, 1], defaults to 0.9
-        - lambda (:obj:`float`): constant lambda, should be in [0, 1], defaults to 0.8
+        - gamma (:obj:`float`): Constant discount factor gamma, should be in [0, 1], defaults to 0.9
+        - lambda (:obj:`float`): Constant lambda, should be in [0, 1], defaults to 0.8
     Returns:
         - loss (:obj:`torch.Tensor`): Computed MSE loss, averaged over the batch
     Shapes:
         - value (:obj:`torch.FloatTensor`): :math:`(T+1, B)`, where T is trajectory length and B is batch,\
             which is the estimation of the state value at step 0 to T
         - reward (:obj:`torch.FloatTensor`): :math:`(T, B)`, the returns from time step 0 to T-1
         - weight (:obj:`torch.FloatTensor` or None): :math:`(B, )`, the training sample weight
@@ -1227,21 +1339,21 @@
     Overview:
         Functional equivalent to trfl.value_ops.generalized_lambda_returns
         https://github.com/deepmind/trfl/blob/2c07ac22512a16715cc759f0072be43a5d12ae45/trfl/value_ops.py#L74
         Passing in a number instead of tensor to make the value constant for all samples in batch
     Arguments:
         - bootstrap_values (:obj:`torch.Tensor` or :obj:`float`):
           estimation of the value at step 0 to *T*, of size [T_traj+1, batchsize]
-        - rewards (:obj:`torch.Tensor`): the returns from 0 to T-1, of size [T_traj, batchsize]
+        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]
         - gammas (:obj:`torch.Tensor` or :obj:`float`):
-          discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]
-        - lambda (:obj:`torch.Tensor` or :obj:`float`): determining the mix of bootstrapping
+          Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]
+        - lambda (:obj:`torch.Tensor` or :obj:`float`): Determining the mix of bootstrapping
           vs further accumulation of multistep returns at each timestep, of size [T_traj, batchsize]
         - done (:obj:`torch.Tensor` or :obj:`float`):
-          whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]
+          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]
     Returns:
         - return (:obj:`torch.Tensor`): Computed lambda return value
           for each state from 0 to T-1, of size [T_traj, batchsize]
     """
     if not isinstance(gammas, torch.Tensor):
         gammas = gammas * torch.ones_like(rewards)
     if not isinstance(lambda_, torch.Tensor):
@@ -1266,22 +1378,22 @@
         result[T-1] = rewards[T-1] + gammas[T-1] * bootstrap_values[T]
         for t in 0...T-2 :
         result[t] = rewards[t] + gammas[t]*(lambdas[t]*result[t+1] + (1-lambdas[t])*bootstrap_values[t+1])
         ```
 
         Assuming the first dim of input tensors correspond to the index in batch
     Arguments:
-        - bootstrap_values (:obj:`torch.Tensor`): estimation of the value at *step 1 to T*, of size [T_traj, batchsize]
-        - rewards (:obj:`torch.Tensor`): the returns from 0 to T-1, of size [T_traj, batchsize]
-        - gammas (:obj:`torch.Tensor`): discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]
-        - lambda (:obj:`torch.Tensor`): determining the mix of bootstrapping vs further accumulation of \
+        - bootstrap_values (:obj:`torch.Tensor`): Estimation of the value at *step 1 to T*, of size [T_traj, batchsize]
+        - rewards (:obj:`torch.Tensor`): The returns from 0 to T-1, of size [T_traj, batchsize]
+        - gammas (:obj:`torch.Tensor`): Discount factor for each step (from 0 to T-1), of size [T_traj, batchsize]
+        - lambda (:obj:`torch.Tensor`): Determining the mix of bootstrapping vs further accumulation of \
             multistep returns at each timestep of size [T_traj, batchsize], the element for T-1 is ignored \
             and effectively set to 0, as there is no information about future rewards.
         - done (:obj:`torch.Tensor` or :obj:`float`):
-          whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]
+          Whether the episode done at current step (from 0 to T-1), of size [T_traj, batchsize]
     Returns:
         - ret (:obj:`torch.Tensor`): Computed lambda return value \
             for each state from 0 to T-1, of size [T_traj, batchsize]
     """
     result = torch.empty_like(rewards)
     if done is None:
         done = torch.zeros_like(rewards)
```

### Comparing `DI-engine-0.4.6/ding/rl_utils/upgo.py` & `DI-engine-0.4.7/ding/rl_utils/upgo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/rl_utils/vtrace.py` & `DI-engine-0.4.7/ding/rl_utils/vtrace.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/__init__.py` & `DI-engine-0.4.7/ding/torch_utils/__init__.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from .checkpoint_helper import build_checkpoint_helper, CountVar, auto_checkpoint
 from .data_helper import to_device, to_tensor, to_ndarray, to_list, to_dtype, same_shape, tensor_to_list, \
-    build_log_buffer, CudaFetcher, get_tensor_data, unsqueeze, squeeze, get_null_data, get_shape0
+    build_log_buffer, CudaFetcher, get_tensor_data, unsqueeze, squeeze, get_null_data, get_shape0, to_item
 from .distribution import CategoricalPd, CategoricalPdPytorch
 from .metric import levenshtein_distance, hamming_distance
 from .network import *
 from .loss import *
 from .optimizer_helper import Adam, RMSprop, calculate_grad_norm, calculate_grad_norm_without_bias_two_norm
 from .nn_test_helper import is_differentiable
 from .math_helper import cov
```

### Comparing `DI-engine-0.4.6/ding/torch_utils/checkpoint_helper.py` & `DI-engine-0.4.7/ding/torch_utils/checkpoint_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/data_helper.py` & `DI-engine-0.4.7/ding/torch_utils/data_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -250,17 +250,17 @@
     elif np.isscalar(item):
         return item
     else:
         raise TypeError("not support item type: {}".format(type(item)))
 
 
 def tensor_to_list(item):
-    r"""
+    """
     Overview:
-        Transform `torch.Tensor` to `list`, keep other data types unchanged
+        Transform `torch.Tensor` to `list`, keep other data types unchanged.
     Arguments:
         - item (:obj:`Any`): the item to be transformed
     Returns:
         - item (:obj:`list`): the list after transformation
 
     .. note::
 
@@ -276,14 +276,39 @@
         return {k: tensor_to_list(v) for k, v in item.items()}
     elif np.isscalar(item):
         return item
     else:
         raise TypeError("not support item type: {}".format(type(item)))
 
 
+def to_item(data):
+    """
+    Overview:
+        Transform data into python native scalar (i.e. data item), keep other data types unchanged.
+    Arguments:
+        - data (:obj:`Any`): The data that needs to be transformed.
+    Returns:
+        - data (:obj:`Any`): Transformed data.
+    """
+    if data is None:
+        return data
+    elif isinstance(data, bool) or isinstance(data, str):
+        return data
+    elif np.isscalar(data):
+        return data
+    elif isinstance(data, np.ndarray) or isinstance(data, torch.Tensor) or isinstance(data, ttorch.Tensor):
+        return data.item()
+    elif isinstance(data, list) or isinstance(data, tuple):
+        return [to_item(d) for d in data]
+    elif isinstance(data, dict):
+        return {k: to_item(v) for k, v in data.items()}
+    else:
+        raise TypeError("not support data type: {}".format(data))
+
+
 def same_shape(data: list) -> bool:
     r"""
     Overview:
         Judge whether all data elements in a list have the same shape.
     Arguments:
         - data (:obj:`list`): the list of data
     Returns:
```

### Comparing `DI-engine-0.4.6/ding/torch_utils/distribution.py` & `DI-engine-0.4.7/ding/torch_utils/distribution.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/loss/contrastive_loss.py` & `DI-engine-0.4.7/ding/torch_utils/loss/contrastive_loss.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/loss/cross_entropy_loss.py` & `DI-engine-0.4.7/ding/torch_utils/loss/cross_entropy_loss.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/loss/multi_logits_loss.py` & `DI-engine-0.4.7/ding/torch_utils/loss/multi_logits_loss.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/math_helper.py` & `DI-engine-0.4.7/ding/torch_utils/math_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/metric.py` & `DI-engine-0.4.7/ding/torch_utils/metric.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/__init__.py` & `DI-engine-0.4.7/ding/torch_utils/network/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -6,7 +6,8 @@
 from .rnn import get_lstm, sequence_mask
 from .soft_argmax import SoftArgmax
 from .transformer import Transformer, ScaledDotProductAttention
 from .scatter_connection import ScatterConnection
 from .resnet import resnet18, ResNet
 from .gumbel_softmax import GumbelSoftmax
 from .gtrxl import GTrXL, GRUGatingUnit
+from .popart import PopArt
```

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/activation.py` & `DI-engine-0.4.7/ding/torch_utils/network/activation.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+import math
+
 import torch
 import torch.nn as nn
 import torch.nn.functional as F
 
 
 class GLU(nn.Module):
     r"""
@@ -63,31 +65,47 @@
 
 
 class Swish(nn.Module):
 
     def __init__(self):
         super(Swish, self).__init__()
 
-    def forward(self, x):
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
         x = x * torch.sigmoid(x)
         return x
 
 
+class GELU(nn.Module):
+    r"""
+    Overview:
+        Gaussian Error Linear Units (GELU) activation function, which is widely used in NLP models like GPT, BERT.
+        The original paper can be viewed in: <link https://arxiv.org/pdf/1606.08415.pdf link>
+    Interfaces:
+        forward
+    """
+
+    def __init__(self):
+        super(GELU, self).__init__()
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
+
+
 def build_activation(activation: str, inplace: bool = None) -> nn.Module:
     r"""
     Overview:
         Return the activation module according to the given type.
     Arguments:
-        - actvation (:obj:`str`): the type of activation module, now supports ['relu', 'glu', 'prelu']
+        - activation (:obj:`str`): the type of activation module, now supports ['relu', 'glu', 'prelu']
         - inplace (:obj:`bool`): can optionally do the operation in-place in relu. Default ``None``
     Returns:
         - act_func (:obj:`nn.module`): the corresponding activation module
     """
     if inplace is not None:
         assert activation == 'relu', 'inplace argument is not compatible with {}'.format(activation)
     else:
         inplace = False
-    act_func = {'relu': nn.ReLU(inplace=inplace), 'glu': GLU, 'prelu': nn.PReLU(), 'swish': Swish()}
+    act_func = {'relu': nn.ReLU(inplace=inplace), 'glu': GLU, 'prelu': nn.PReLU(), 'swish': Swish(), 'gelu': GELU()}
     if activation in act_func.keys():
         return act_func[activation]
     else:
         raise KeyError("invalid key for activation: {}".format(activation))
```

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/coverage.xml` & `DI-engine-0.4.7/ding/torch_utils/network/coverage.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/gtrxl.py` & `DI-engine-0.4.7/ding/torch_utils/network/gtrxl.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/gumbel_softmax.py` & `DI-engine-0.4.7/ding/torch_utils/network/gumbel_softmax.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/nn_module.py` & `DI-engine-0.4.7/ding/torch_utils/network/nn_module.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/normalization.py` & `DI-engine-0.4.7/ding/torch_utils/network/normalization.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/res_block.py` & `DI-engine-0.4.7/ding/torch_utils/network/res_block.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/resnet.py` & `DI-engine-0.4.7/ding/torch_utils/network/resnet.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/rnn.py` & `DI-engine-0.4.7/ding/torch_utils/network/rnn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/scatter_connection.py` & `DI-engine-0.4.7/ding/torch_utils/network/scatter_connection.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/soft_argmax.py` & `DI-engine-0.4.7/ding/torch_utils/network/soft_argmax.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/network/transformer.py` & `DI-engine-0.4.7/ding/torch_utils/network/transformer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/nn_test_helper.py` & `DI-engine-0.4.7/ding/torch_utils/nn_test_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/torch_utils/optimizer_helper.py` & `DI-engine-0.4.7/ding/torch_utils/optimizer_helper.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,21 @@
 import torch
 import math
 from torch.nn.utils import clip_grad_norm_, clip_grad_value_
-from torch._six import inf
-from typing import Union, Iterable, Tuple, Callable
+from typing import Union, Iterable, Tuple, Callable, List
 import torch.nn as nn
 import torch.nn.functional as F
 import torch.optim as optim
 import pdb
 import numpy as np
 import copy
 import random
 
+inf = math.inf
+
 
 def calculate_grad_norm(model: torch.nn.Module, norm_type=2) -> float:
     r"""
     Overview:
         calculate grad norm of the parameters whose grad norms are not None in the model.
     Arguments:
         - model: torch.nn.Module
@@ -189,18 +190,18 @@
 
     def _state_init(self, p, amsgrad):
         state = self.state[p]
         state['thre_exp_avg_sq'] = torch.zeros_like(p.data, device=p.data.device)
         # others
         if torch.__version__ < "1.12.0":
             state['step'] = 0
-            #TODO
-            #wait torch upgrad to 1.4, 1.3.1 didn't support memory format state['step'] = 0
+            # TODO
+            # wait torch upgrad to 1.4, 1.3.1 didn't support memory format state['step'] = 0
         else:
-            state['step'] = torch.zeros((1, ), dtype=torch.float, device=p.device) \
+            state['step'] = torch.zeros((1,), dtype=torch.float, device=p.device) \
                 if self.defaults['capturable'] else torch.tensor(0.)
 
         state['exp_avg'] = torch.zeros_like(p.data)
         # Exponential moving average of squared gradient values
         state['exp_avg_sq'] = torch.zeros_like(p.data)
         if amsgrad:
             # Maintains max of all exp. moving avg. of sq. grad. values
@@ -231,15 +232,15 @@
                 for p in group['params']:
                     if p.grad is None:
                         continue
                     state = self.state[p]
                     if len(state) == 0:
                         self._state_init(p, group['amsgrad'])
                     grad = p.grad.data
-                    #should we use same beta group?
+                    # should we use same beta group?
                     beta1, beta2 = group['betas']
                     bias_correction2 = 1 - beta2 ** state['step']
                     state['thre_exp_avg_sq'].mul_(beta2).addcmul_(1 - beta2, grad, grad)
                     if state['step'] >= self._clip_momentum_timestep:  # initial value is inaccurate
                         flag = grad.abs(
                         ) > (state['thre_exp_avg_sq'].sqrt() / math.sqrt(bias_correction2)) * self._clip_coef
                         grad.mul_(~flag).add_(
@@ -255,23 +256,23 @@
                 for p in group['params']:
                     if p.grad is None:
                         continue
                     state = self.state[p]
                     if len(state) == 0:
                         self._state_init(p, group['amsgrad'])
                     grad = p.grad.data
-                    #should we use same beta group?
+                    # should we use same beta group?
                     beta1, beta2 = group['betas']
                     bias_correction2 = 1 - beta2 ** state['step']
                     state['thre_exp_avg_sq'].mul_(beta2).addcmul_(1 - beta2, grad, grad)
                     # sum total_norm
                     param_norm = grad.norm(self._clip_norm_type)
                     total_norm += param_norm.item() ** self._clip_norm_type
 
-                    #sum momentum_norm
+                    # sum momentum_norm
                     momentum = ((state['thre_exp_avg_sq'].sqrt() / math.sqrt(bias_correction2)) *
                                 self._clip_coef).norm(self._clip_norm_type)
                     total_momentum_norm += momentum.item() ** self._clip_norm_type
                     step = min(step, state['step'])
                 if step > self._clip_momentum_timestep:
                     total_norm = total_norm ** (1. / self._clip_norm_type)
                     total_momentum_norm = total_momentum_norm ** (1. / self._clip_norm_type)
@@ -290,15 +291,15 @@
                 for p in group['params']:
                     if p.grad is None:
                         continue
                     state = self.state[p]
                     if len(state) == 0:
                         self._state_init(p, group['amsgrad'])
                     grad = p.grad.data
-                    #should we use same beta group?
+                    # should we use same beta group?
                     beta1, beta2 = group['betas']
                     bias_correction2 = 1 - beta2 ** state['step']
                     state['thre_exp_avg_sq'].mul_(beta2).addcmul_(1 - beta2, grad, grad)
                     if state['step'] >= self._ignore_momentum_timestep:  # initial value is inaccurate
                         if grad.abs() > (state['thre_exp_avg_sq'].sqrt() /
                                          math.sqrt(bias_correction2)) * self._ignore_coef:
                             flag = True
@@ -322,37 +323,37 @@
                 for p in group['params']:
                     if p.grad is None:
                         continue
                     state = self.state[p]
                     if len(state) == 0:
                         self._state_init(p, group['amsgrad'])
                     grad = p.grad.data
-                    #should we use same beta group?
+                    # should we use same beta group?
                     beta1, beta2 = group['betas']
                     bias_correction2 = 1 - beta2 ** state['step']
                     state['thre_exp_avg_sq'].mul_(beta2).addcmul_(1 - beta2, grad, grad)
                     # sum total_norm
                     param_norm = grad.norm(self._ignore_norm_type)
                     total_norm += param_norm.item() ** self._ignore_norm_type
 
-                    #sum momentum_norm
+                    # sum momentum_norm
                     momentum = ((state['thre_exp_avg_sq'].sqrt() / math.sqrt(bias_correction2)) *
                                 self._ignore_coef).norm(self._ignore_norm_type)
                     total_momentum_norm += momentum.item() ** self._ignore_norm_type
                     step = min(step, state['step'])
 
                 if step > self._ignore_momentum_timestep:
                     total_norm = total_norm ** (1. / self._ignore_norm_type)
                     total_momentum_norm = total_momentum_norm ** (1. / self._ignore_norm_type)
                     ignore_coef = total_momentum_norm / (total_norm + 1e-6)
                     if ignore_coef < 1:
                         for p in group['params']:
                             p.grad.zero_()
 
-        #Adam optim type
+        # Adam optim type
         if self._optim_type == 'adamw':
             for group in self.param_groups:
                 for p in group['params']:
                     if p.grad is None:
                         continue
                     p.data = p.data.add(-self._weight_decay * group['lr'], p.data)
             return super().step(closure=closure)
@@ -513,15 +514,15 @@
                     grad = p.grad.data
                     alpha = group['alpha']
                     state['thre_square_avg'].mul_(alpha).addcmul_(1 - alpha, grad, grad)
                     # sum total_norm
                     param_norm = grad.norm(self._clip_norm_type)
                     total_norm += param_norm.item() ** self._clip_norm_type
 
-                    #sum momentum_norm
+                    # sum momentum_norm
                     momentum = (state['thre_square_avg'].sqrt() * self._clip_coef).norm(self._clip_norm_type)
                     total_momentum_norm += momentum.item() ** self._clip_norm_type
                     step = min(step, state['step'])
                 if step > self._clip_momentum_timestep:
                     total_norm = total_norm ** (1. / self._clip_norm_type)
                     total_momentum_norm = total_momentum_norm ** (1. / self._clip_norm_type)
                     clip_coef = total_momentum_norm / (total_norm + 1e-6)
@@ -574,15 +575,15 @@
                     grad = p.grad.data
                     alpha = group['alpha']
                     state['thre_square_avg'].mul_(alpha).addcmul_(1 - alpha, grad, grad)
                     # sum total_norm
                     param_norm = grad.norm(self._ignore_norm_type)
                     total_norm += param_norm.item() ** self._ignore_norm_type
 
-                    #sum momentum_norm
+                    # sum momentum_norm
                     momentum = (state['thre_square_avg'].sqrt() * self._ignore_coef).norm(self._ignore_norm_type)
                     total_momentum_norm += momentum.item() ** self._ignore_norm_type
                     step = min(step, state['step'])
 
                 if step > self._ignore_momentum_timestep:
                     total_norm = total_norm ** (1. / self._ignore_norm_type)
                     total_momentum_norm = total_momentum_norm ** (1. / self._ignore_norm_type)
@@ -726,7 +727,62 @@
                     grad.append(torch.zeros_like(p).to(p.device))
                     has_grad.append(torch.zeros_like(p).to(p.device))
                     continue
                 shape.append(p.grad.shape)
                 grad.append(p.grad.clone())
                 has_grad.append(torch.ones_like(p).to(p.device))
         return grad, shape, has_grad
+
+
+def configure_weight_decay(model: nn.Module, weight_decay: float) -> List:
+    r"""
+    Overview:
+        Separating out all parameters of the model into two buckets: those that will experience
+    weight decay for regularization and those that won't (biases, and layer-norm or embedding weights).
+    Arguments:
+        - model (:obj:`nn.Module`): the given PyTorch model.
+        - weight_decay (:obj:`float`): weight decay value for optimizer.
+    Returns:
+        - optim groups (:obj:`List`): the parameter groups to be set in the latter optimizer.
+    """
+    decay = set()
+    no_decay = set()
+    whitelist_weight_modules = (torch.nn.Linear, )
+    blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)
+    for mn, m in model.named_modules():
+        for pn, p in m.named_parameters():
+            fpn = '%s.%s' % (mn, pn) if mn else pn  # full param name
+            # Because named_modules and named_parameters are recursive
+            # we will see the same tensors p many times. But doing it this way
+            # allows us to know which parent module any tensor p belongs to.
+            if pn.endswith('bias'):
+                # all biases will not be decayed
+                no_decay.add(fpn)
+            elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):
+                # weights of whitelist modules will be weight decayed
+                decay.add(fpn)
+            elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):
+                # weights of blacklist modules will NOT be weight decayed
+                no_decay.add(fpn)
+            else:
+                decay.add(fpn)
+
+    decay = decay - no_decay
+    # validate that we considered every parameter
+    param_dict = {pn: p for pn, p in model.named_parameters()}
+    union_params = decay | no_decay
+    assert len(
+        param_dict.keys() - union_params) == 0, "parameters %s were not separated into either decay/no_decay set!" \
+                                                % (str(param_dict.keys() - union_params),)
+
+    optim_groups = [
+        {
+            "params": [param_dict[pn] for pn in sorted(list(decay))],
+            "weight_decay": weight_decay
+        },
+        {
+            "params": [param_dict[pn] for pn in sorted(list(no_decay))],
+            "weight_decay": 0.0
+        },
+    ]
+
+    return optim_groups
```

### Comparing `DI-engine-0.4.6/ding/torch_utils/reshape_helper.py` & `DI-engine-0.4.7/ding/torch_utils/reshape_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/__init__.py` & `DI-engine-0.4.7/ding/utils/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -25,14 +25,15 @@
 from .segment_tree import SumSegmentTree, MinSegmentTree, SegmentTree
 from .slurm_helper import find_free_port_slurm, node_to_host, node_to_partition
 from .system_helper import get_ip, get_pid, get_task_uid, PropagatingThread, find_free_port
 from .time_helper import build_time_helper, EasyTimer, WatchDog
 from .type_helper import SequenceType
 from .render_helper import render, fps
 from .fast_copy import fastcopy
+from .bfs_helper import get_vi_sequence
 
 if ding.enable_linklink:
     from .linklink_dist_helper import get_rank, get_world_size, dist_mode, dist_init, dist_finalize, \
         allreduce, broadcast, DistContext, allreduce_async, synchronize
 else:
     from .pytorch_ddp_dist_helper import get_rank, get_world_size, dist_mode, dist_init, dist_finalize, \
         allreduce, broadcast, DistContext, allreduce_async, synchronize
```

### Comparing `DI-engine-0.4.6/ding/utils/autolog/base.py` & `DI-engine-0.4.7/ding/utils/autolog/base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/data.py` & `DI-engine-0.4.7/ding/utils/autolog/data.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/model.py` & `DI-engine-0.4.7/ding/utils/autolog/model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/tests/test_data.py` & `DI-engine-0.4.7/ding/utils/autolog/tests/test_data.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/tests/test_model.py` & `DI-engine-0.4.7/ding/utils/autolog/tests/test_model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/tests/test_time.py` & `DI-engine-0.4.7/ding/utils/autolog/tests/test_time.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/time_ctl.py` & `DI-engine-0.4.7/ding/utils/autolog/time_ctl.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/autolog/value.py` & `DI-engine-0.4.7/ding/utils/autolog/value.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/collection_helper.py` & `DI-engine-0.4.7/ding/utils/collection_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/compression_helper.py` & `DI-engine-0.4.7/ding/utils/compression_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/coverage.xml` & `DI-engine-0.4.7/ding/utils/coverage.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/data/base_dataloader.py` & `DI-engine-0.4.7/ding/utils/data/base_dataloader.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/data/collate_fn.py` & `DI-engine-0.4.7/ding/utils/data/collate_fn.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from collections.abc import Sequence, Mapping
 from typing import List, Dict, Union, Any
 
 import torch
 import treetensor.torch as ttorch
 import re
-from torch._six import string_classes
 import collections.abc as container_abcs
 from ding.compatibility import torch_ge_131
 
 int_classes = int
+string_classes = (str, bytes)
 np_str_obj_array_pattern = re.compile(r'[SaUO]')
 
 default_collate_err_msg_format = (
     "default_collate: batch must contain tensors, numpy arrays, numbers, "
     "dicts or lists; found {}"
 )
```

### Comparing `DI-engine-0.4.6/ding/utils/data/dataloader.py` & `DI-engine-0.4.7/ding/utils/data/dataloader.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/data/dataset.py` & `DI-engine-0.4.7/ding/utils/data/dataset.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 from typing import List, Dict, Tuple
 import pickle
+
+import easydict
 import torch
 import numpy as np
 from ditk import logging
 from copy import deepcopy
+from torch.utils.data import Dataset
 from dataclasses import dataclass
 
 from easydict import EasyDict
-from torch.utils.data import Dataset
+from ding.utils.bfs_helper import get_vi_sequence
 
 from ding.utils import DATASET_REGISTRY, import_module
 from ding.rl_utils import discount_cumsum
 
 
 @dataclass
 class DatasetStatistics:
@@ -419,14 +422,86 @@
                 [torch.ones(traj_len, dtype=torch.long),
                  torch.zeros(padding_len, dtype=torch.long)], dim=0
             )
 
         return timesteps, states, actions, returns_to_go, traj_mask
 
 
+class PCDataset(Dataset):
+
+    def __init__(self, all_data):
+        self._data = all_data
+
+    def __getitem__(self, item):
+        return {'obs': self._data[0][item], 'bfs_in': self._data[1][item], 'bfs_out': self._data[2][item]}
+
+    def __len__(self):
+        return self._data[0].shape[0]
+
+
+def load_bfs_datasets(train_seeds=1, test_seeds=5):
+    from dizoo.maze.envs import Maze
+
+    def load_env(seed):
+        ccc = easydict.EasyDict({'size': 16})
+        e = Maze(ccc)
+        e.seed(seed)
+        e.reset()
+        return e
+
+    envs = [load_env(i) for i in range(train_seeds + test_seeds)]
+
+    observations_train = []
+    observations_test = []
+    bfs_input_maps_train = []
+    bfs_input_maps_test = []
+    bfs_output_maps_train = []
+    bfs_output_maps_test = []
+    for idx, env in enumerate(envs):
+        if idx < train_seeds:
+            observations = observations_train
+            bfs_input_maps = bfs_input_maps_train
+            bfs_output_maps = bfs_output_maps_train
+        else:
+            observations = observations_test
+            bfs_input_maps = bfs_input_maps_test
+            bfs_output_maps = bfs_output_maps_test
+
+        start_obs = env.process_states(env._get_obs(), env.get_maze_map())
+        _, track_back = get_vi_sequence(env, start_obs)
+        env_observations = torch.stack([track_back[i][0] for i in range(len(track_back))], dim=0)
+
+        for i in range(env_observations.shape[0]):
+            bfs_sequence, _ = get_vi_sequence(env, env_observations[i].numpy().astype(np.int32))  # [L, W, W]
+            bfs_input_map = env.n_action * np.ones([env.size, env.size], dtype=np.long)
+
+            for j in range(bfs_sequence.shape[0]):
+                bfs_input_maps.append(torch.from_numpy(bfs_input_map))
+                bfs_output_maps.append(torch.from_numpy(bfs_sequence[j]))
+                observations.append(env_observations[i])
+                bfs_input_map = bfs_sequence[j]
+
+    train_data = PCDataset(
+        (
+            torch.stack(observations_train, dim=0),
+            torch.stack(bfs_input_maps_train, dim=0),
+            torch.stack(bfs_output_maps_train, dim=0),
+        )
+    )
+    test_data = PCDataset(
+        (
+            torch.stack(observations_test, dim=0),
+            torch.stack(bfs_input_maps_test, dim=0),
+            torch.stack(bfs_output_maps_test, dim=0),
+        )
+    )
+
+    return train_data, test_data
+
+
 @DATASET_REGISTRY.register('bco')
 class BCODataset(Dataset):
 
     def __init__(self, data=None):
         if data is None:
             raise ValueError('Dataset can not be empty!')
         else:
```

### Comparing `DI-engine-0.4.6/ding/utils/data/structure/cache.py` & `DI-engine-0.4.7/ding/utils/data/structure/cache.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/default_helper.py` & `DI-engine-0.4.7/ding/utils/default_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/design_helper.py` & `DI-engine-0.4.7/ding/utils/design_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/fast_copy.py` & `DI-engine-0.4.7/ding/utils/fast_copy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/file_helper.py` & `DI-engine-0.4.7/ding/utils/file_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/import_helper.py` & `DI-engine-0.4.7/ding/utils/import_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/k8s_helper.py` & `DI-engine-0.4.7/ding/utils/k8s_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/linklink_dist_helper.py` & `DI-engine-0.4.7/ding/utils/linklink_dist_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/__init__.py` & `DI-engine-0.4.7/ding/utils/loader/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/base.py` & `DI-engine-0.4.7/ding/utils/loader/base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/collection.py` & `DI-engine-0.4.7/ding/utils/loader/collection.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/dict.py` & `DI-engine-0.4.7/ding/utils/loader/dict.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/mapping.py` & `DI-engine-0.4.7/ding/utils/loader/mapping.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/norm.py` & `DI-engine-0.4.7/ding/utils/loader/norm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/number.py` & `DI-engine-0.4.7/ding/utils/loader/number.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/string.py` & `DI-engine-0.4.7/ding/utils/loader/string.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_base.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_collection.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_collection.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_dict.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_dict.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_mapping.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_mapping.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_norm.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_norm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_number.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_number.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_string.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_string.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_types.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_types.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/loader/test_utils.py` & `DI-engine-0.4.7/ding/utils/loader/tests/loader/test_utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/tests/test_cartpole_dqn_serial_config_loader.py` & `DI-engine-0.4.7/ding/utils/loader/tests/test_cartpole_dqn_serial_config_loader.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/loader/types.py` & `DI-engine-0.4.7/ding/utils/loader/types.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/lock_helper.py` & `DI-engine-0.4.7/ding/utils/lock_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/log_helper.py` & `DI-engine-0.4.7/ding/utils/log_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/log_writer_helper.py` & `DI-engine-0.4.7/ding/utils/log_writer_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/orchestrator_launcher.py` & `DI-engine-0.4.7/ding/utils/orchestrator_launcher.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/profiler_helper.py` & `DI-engine-0.4.7/ding/utils/profiler_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/pytorch_ddp_dist_helper.py` & `DI-engine-0.4.7/ding/utils/pytorch_ddp_dist_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/registry.py` & `DI-engine-0.4.7/ding/utils/registry.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/registry_factory.py` & `DI-engine-0.4.7/ding/utils/registry_factory.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/render_helper.py` & `DI-engine-0.4.7/ding/utils/render_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/scheduler_helper.py` & `DI-engine-0.4.7/ding/utils/scheduler_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/segment_tree.py` & `DI-engine-0.4.7/ding/utils/segment_tree.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/slurm_helper.py` & `DI-engine-0.4.7/ding/utils/slurm_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/system_helper.py` & `DI-engine-0.4.7/ding/utils/system_helper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/time_helper.py` & `DI-engine-0.4.7/ding/utils/time_helper.py`

 * *Files 2% similar despite different names*

```diff
@@ -101,24 +101,24 @@
     """
 
     # overwrite
     @classmethod
     def start_time(cls):
         r"""
         Overview:
-            Implement and overide the ``start_time`` method in ``TimeWrapper`` class
+            Implement and override the ``start_time`` method in ``TimeWrapper`` class
         """
         cls.start = time.time()
 
     # overwrite
     @classmethod
     def end_time(cls):
         r"""
         Overview:
-            Implement and overide the end_time method in ``TimeWrapper`` class
+            Implement and override the end_time method in ``TimeWrapper`` class
 
         Returns:
             - time(:obj:`float`): The time between ``start_time`` and end_time
         """
         cls.end = time.time()
         return cls.end - cls.start
```

### Comparing `DI-engine-0.4.6/ding/utils/time_helper_base.py` & `DI-engine-0.4.7/ding/utils/time_helper_base.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/utils/time_helper_cuda.py` & `DI-engine-0.4.7/ding/utils/time_helper_cuda.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/adapter/learner_aggregator.py` & `DI-engine-0.4.7/ding/worker/adapter/learner_aggregator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/__init__.py` & `DI-engine-0.4.7/ding/worker/collector/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/base_parallel_collector.py` & `DI-engine-0.4.7/ding/worker/collector/base_parallel_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/base_serial_evaluator.py` & `DI-engine-0.4.7/ding/worker/collector/base_serial_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/battle_episode_serial_collector.py` & `DI-engine-0.4.7/ding/worker/collector/battle_episode_serial_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -256,15 +256,17 @@
                             self._obs_pool[env_id][policy_id], self._policy_output_pool[env_id][policy_id],
                             policy_timestep
                         )
                         transition['collect_iter'] = train_iter
                         self._traj_buffer[env_id][policy_id].append(transition)
                         # prepare data
                         if timestep.done:
-                            transitions = to_tensor_transitions(self._traj_buffer[env_id][policy_id])
+                            transitions = to_tensor_transitions(
+                                self._traj_buffer[env_id][policy_id], not self._deepcopy_obs
+                            )
                             if self._cfg.get_train_sample:
                                 train_sample = self._policy[policy_id].get_train_sample(transitions)
                                 return_data[policy_id].extend(train_sample)
                             else:
                                 return_data[policy_id].append(transitions)
                             self._traj_buffer[env_id][policy_id].clear()
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/battle_interaction_serial_evaluator.py` & `DI-engine-0.4.7/ding/worker/collector/battle_interaction_serial_evaluator.py`

 * *Files 0% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 import copy
 import numpy as np
 import torch
 
 from ding.utils import build_logger, EasyTimer, deep_merge_dicts, lists_to_dicts, dicts_to_lists, \
     SERIAL_EVALUATOR_REGISTRY
 from ding.envs import BaseEnvManager
-from ding.torch_utils import to_tensor, to_ndarray, tensor_to_list
+from ding.torch_utils import to_tensor, to_ndarray, tensor_to_list, to_item
 from .base_serial_evaluator import ISerialEvaluator, VectorEvalMonitor
 
 
 @SERIAL_EVALUATOR_REGISTRY.register('battle_interaction')
 class BattleInteractionSerialEvaluator(ISerialEvaluator):
     """
     Overview:
@@ -268,8 +268,9 @@
         stop_flag = episode_return >= self._stop_value and train_iter > 0
         if stop_flag:
             self._logger.info(
                 "[DI-engine serial pipeline] " +
                 "Current episode_return: {} is greater than stop_value: {}".format(episode_return, self._stop_value) +
                 ", so your RL agent is converged, you can refer to 'log/evaluator/evaluator_logger.txt' for details."
             )
+        return_info = to_item(return_info)
         return stop_flag, return_info
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/battle_sample_serial_collector.py` & `DI-engine-0.4.7/ding/worker/collector/battle_sample_serial_collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -272,15 +272,17 @@
                             self._obs_pool[env_id][policy_id], self._policy_output_pool[env_id][policy_id],
                             policy_timestep
                         )
                         transition['collect_iter'] = train_iter
                         self._traj_buffer[env_id][policy_id].append(transition)
                         # prepare data
                         if timestep.done or len(self._traj_buffer[env_id][policy_id]) == self._traj_len:
-                            transitions = to_tensor_transitions(self._traj_buffer[env_id][policy_id])
+                            transitions = to_tensor_transitions(
+                                self._traj_buffer[env_id][policy_id], not self._deepcopy_obs
+                            )
                             train_sample = self._policy[policy_id].get_train_sample(transitions)
                             return_data[policy_id].extend(train_sample)
                             self._total_train_sample_count += len(train_sample)
                             self._env_info[env_id]['train_sample'] += len(train_sample)
                             collected_sample[policy_id] += len(train_sample)
                             self._traj_buffer[env_id][policy_id].clear()
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/comm/base_comm_collector.py` & `DI-engine-0.4.7/ding/worker/collector/comm/base_comm_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/comm/flask_fs_collector.py` & `DI-engine-0.4.7/ding/worker/collector/comm/flask_fs_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/comm/utils.py` & `DI-engine-0.4.7/ding/worker/collector/comm/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/episode_serial_collector.py` & `DI-engine-0.4.7/ding/worker/collector/episode_serial_collector.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,17 +89,18 @@
             If _policy is not None, replace the old policy in the collector with the new passed in policy.
         Arguments:
             - policy (:obj:`Optional[namedtuple]`): the api namedtuple of collect_mode policy
         """
         assert hasattr(self, '_env'), "please set env first"
         if _policy is not None:
             self._policy = _policy
-            self._default_n_episode = _policy.get_attribute('cfg').collect.get('n_episode', None)
+            self._policy_cfg = self._policy.get_attribute('cfg')
+            self._default_n_episode = _policy.get_attribute('n_episode')
             self._unroll_len = _policy.get_attribute('unroll_len')
-            self._on_policy = _policy.get_attribute('cfg').on_policy
+            self._on_policy = _policy.get_attribute('on_policy')
             self._traj_len = INF
             self._logger.debug(
                 'Set default n_episode mode(n_episode({}), env_num({}), traj_len({}))'.format(
                     self._default_n_episode, self._env_num, self._traj_len
                 )
             )
         self._policy.reset()
@@ -248,15 +249,15 @@
                     # ``train_iter`` passed in from ``serial_entry``, indicates current collecting model's iteration.
                     transition['collect_iter'] = train_iter
                     self._traj_buffer[env_id].append(transition)
                     self._env_info[env_id]['step'] += 1
                     self._total_envstep_count += 1
                     # prepare data
                     if timestep.done:
-                        transitions = to_tensor_transitions(self._traj_buffer[env_id])
+                        transitions = to_tensor_transitions(self._traj_buffer[env_id], not self._deepcopy_obs)
                         if self._cfg.reward_shaping:
                             self._env.reward_shaping(env_id, transitions)
                         if self._cfg.get_train_sample:
                             train_sample = self._policy.get_train_sample(transitions)
                             return_data.extend(train_sample)
                         else:
                             return_data.append(transitions)
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/interaction_serial_evaluator.py` & `DI-engine-0.4.7/ding/worker/collector/interaction_serial_evaluator.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 from typing import Optional, Callable, Tuple
 from collections import namedtuple
 import numpy as np
 import torch
 import torch.distributed as dist
 
 from ding.envs import BaseEnvManager
-from ding.torch_utils import to_tensor, to_ndarray
+from ding.torch_utils import to_tensor, to_ndarray, to_item
 from ding.utils import build_logger, EasyTimer, SERIAL_EVALUATOR_REGISTRY
 from ding.utils import get_world_size, get_rank
 from .base_serial_evaluator import ISerialEvaluator, VectorEvalMonitor
 
 
 @SERIAL_EVALUATOR_REGISTRY.register('interaction')
 class InteractionSerialEvaluator(ISerialEvaluator):
@@ -307,8 +307,9 @@
                 )
 
         if get_world_size() > 1:
             objects = [stop_flag, return_info]
             dist.broadcast_object_list(objects, src=0)
             stop_flag, return_info = objects
 
+        return_info = to_item(return_info)
         return stop_flag, return_info
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/marine_parallel_collector.py` & `DI-engine-0.4.7/ding/worker/collector/marine_parallel_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/metric_serial_evaluator.py` & `DI-engine-0.4.7/ding/worker/collector/metric_serial_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/sample_serial_collector.py` & `DI-engine-0.4.7/ding/worker/collector/sample_serial_collector.py`

 * *Files 4% similar despite different names*

```diff
@@ -43,15 +43,15 @@
             - env (:obj:`BaseEnvManager`): the subclass of vectorized env_manager(BaseEnvManager)
             - policy (:obj:`namedtuple`): the api namedtuple of collect_mode policy
             - tb_logger (:obj:`SummaryWriter`): tensorboard handle
         """
         self._exp_name = exp_name
         self._instance_name = instance_name
         self._collect_print_freq = cfg.collect_print_freq
-        self._deepcopy_obs = cfg.deepcopy_obs  # avoid shallow copy, e.g., ovelap of s_t and s_t+1
+        self._deepcopy_obs = cfg.deepcopy_obs  # whether to deepcopy each data
         self._transform_obs = cfg.transform_obs
         self._cfg = cfg
         self._timer = EasyTimer()
         self._end_flag = False
 
         if tb_logger is not None:
             self._logger, _ = build_logger(
@@ -90,16 +90,17 @@
             If _policy is not None, replace the old policy in the collector with the new passed in policy.
         Arguments:
             - policy (:obj:`Optional[namedtuple]`): the api namedtuple of collect_mode policy
         """
         assert hasattr(self, '_env'), "please set env first"
         if _policy is not None:
             self._policy = _policy
-            self._default_n_sample = _policy.get_attribute('cfg').collect.get('n_sample', None)
-            self._traj_len_inf = _policy.get_attribute('cfg').collect.get('traj_len_inf', False)
+            self._policy_cfg = self._policy.get_attribute('cfg')
+            self._default_n_sample = _policy.get_attribute('n_sample')
+            self._traj_len_inf = self._policy_cfg.traj_len_inf
             self._unroll_len = _policy.get_attribute('unroll_len')
             self._on_policy = _policy.get_attribute('on_policy')
             if self._default_n_sample is not None and not self._traj_len_inf:
                 self._traj_len = max(
                     self._unroll_len,
                     self._default_n_sample // self._env_num + int(self._default_n_sample % self._env_num != 0)
                 )
@@ -255,17 +256,15 @@
                         # If there is an abnormal timestep, reset all the related variables(including this env).
                         # suppose there is no reset param, just reset this env
                         self._env.reset({env_id: None})
                         self._policy.reset([env_id])
                         self._reset_stat(env_id)
                         self._logger.info('Env{} returns a abnormal step, its info is {}'.format(env_id, timestep.info))
                         continue
-                    if 'type' in self._policy.get_attribute('cfg') and \
-                            self._policy.get_attribute('cfg').type == 'ngu_command':
-                        # for NGU policy
+                    if self._policy_cfg.type == 'ngu_command':  # for NGU policy
                         transition = self._policy.process_transition(
                             self._obs_pool[env_id], self._policy_output_pool[env_id], timestep, env_id
                         )
                     else:
                         transition = self._policy.process_transition(
                             self._obs_pool[env_id], self._policy_output_pool[env_id], timestep
                         )
@@ -285,15 +284,16 @@
                         # (please refer to r2d2.py) and in each collect phase,
                         # we collect a total of <n_sample> (sequence) samples.
                         # 3. When timestep is done and we only collected very few transitions in self._traj_buffer,
                         # by going through self._policy.get_train_sample, it will be padded automatically to get the
                         # sequence sample of length <burnin + learn_unroll_len> (please refer to r2d2.py).
 
                         # Episode is done or traj_buffer(maxlen=traj_len) is full.
-                        transitions = to_tensor_transitions(self._traj_buffer[env_id])
+                        # indicate whether to shallow copy next obs, i.e., overlap of s_t and s_t+1
+                        transitions = to_tensor_transitions(self._traj_buffer[env_id], not self._deepcopy_obs)
                         train_sample = self._policy.get_train_sample(transitions)
                         return_data.extend(train_sample)
                         self._total_train_sample_count += len(train_sample)
                         self._env_info[env_id]['train_sample'] += len(train_sample)
                         collected_sample += len(train_sample)
                         self._traj_buffer[env_id].clear()
```

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/fake_cls_policy.py` & `DI-engine-0.4.7/ding/worker/collector/tests/fake_cls_policy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/fake_cpong_dqn_config.py` & `DI-engine-0.4.7/ding/worker/collector/tests/fake_cpong_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/speed_test/fake_env.py` & `DI-engine-0.4.7/ding/worker/collector/tests/speed_test/fake_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/speed_test/fake_policy.py` & `DI-engine-0.4.7/ding/worker/collector/tests/speed_test/fake_policy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/speed_test/test_collector_profile.py` & `DI-engine-0.4.7/ding/worker/collector/tests/speed_test/test_collector_profile.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/test_episode_serial_collector.py` & `DI-engine-0.4.7/ding/worker/collector/tests/test_episode_serial_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/test_marine_parallel_collector.py` & `DI-engine-0.4.7/ding/worker/collector/tests/test_marine_parallel_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/tests/test_metric_serial_evaluator.py` & `DI-engine-0.4.7/ding/worker/collector/tests/test_metric_serial_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/collector/zergling_parallel_collector.py` & `DI-engine-0.4.7/ding/worker/collector/zergling_parallel_collector.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,17 +74,18 @@
     def policy(self) -> Policy:
         return self._policy
 
     # override
     @policy.setter
     def policy(self, _policy: Policy) -> None:
         self._policy = _policy
-        self._n_episode = _policy.get_attribute('cfg').collect.get('n_episode', None)
-        self._n_sample = _policy.get_attribute('cfg').collect.get('n_sample', None)
-        assert any(
+        self._policy_cfg = self._policy.get_attribute('cfg')
+        self._n_sample = _policy.get_attribute('n_sample')
+        self._n_episode = _policy.get_attribute('n_episode')
+        assert not all(
             [t is None for t in [self._n_sample, self._n_episode]]
         ), "n_episode/n_sample in policy cfg can't be not None at the same time"
         # TODO(nyz) the same definition of traj_len in serial and parallel
         if self._n_episode is not None:
             self._traj_len = INF
         elif self._n_sample is not None:
             self._traj_len = self._n_sample
```

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/base_parallel_commander.py` & `DI-engine-0.4.7/ding/worker/coordinator/base_parallel_commander.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/base_serial_commander.py` & `DI-engine-0.4.7/ding/worker/coordinator/base_serial_commander.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/comm_coordinator.py` & `DI-engine-0.4.7/ding/worker/coordinator/comm_coordinator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/coordinator.py` & `DI-engine-0.4.7/ding/worker/coordinator/coordinator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/one_vs_one_parallel_commander.py` & `DI-engine-0.4.7/ding/worker/coordinator/one_vs_one_parallel_commander.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/operator_server.py` & `DI-engine-0.4.7/ding/worker/coordinator/operator_server.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/resource_manager.py` & `DI-engine-0.4.7/ding/worker/coordinator/resource_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/coordinator/solo_parallel_commander.py` & `DI-engine-0.4.7/ding/worker/coordinator/solo_parallel_commander.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/learner/base_learner.py` & `DI-engine-0.4.7/ding/worker/learner/base_learner.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/learner/comm/base_comm_learner.py` & `DI-engine-0.4.7/ding/worker/learner/comm/base_comm_learner.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/learner/comm/flask_fs_learner.py` & `DI-engine-0.4.7/ding/worker/learner/comm/flask_fs_learner.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/learner/comm/utils.py` & `DI-engine-0.4.7/ding/worker/learner/comm/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/learner/learner_hook.py` & `DI-engine-0.4.7/ding/worker/learner/learner_hook.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/replay_buffer/advanced_buffer.py` & `DI-engine-0.4.7/ding/worker/replay_buffer/advanced_buffer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 import os
 import copy
 import time
-from typing import Union, NoReturn, Any, Optional, List, Dict, Tuple
+from typing import Union, Any, Optional, List, Dict, Tuple
 import numpy as np
 import hickle
 
 from ding.worker.replay_buffer import IBuffer
 from ding.utils import SumSegmentTree, MinSegmentTree, BUFFER_REGISTRY
 from ding.utils import LockContext, LockContextType, build_logger
 from ding.utils.autolog import TickTime
@@ -717,15 +717,15 @@
         return self._valid_count
 
     @property
     def beta(self) -> float:
         return self._beta
 
     @beta.setter
-    def beta(self, beta: float) -> NoReturn:
+    def beta(self, beta: float) -> None:
         self._beta = beta
 
     def state_dict(self) -> dict:
         """
         Overview:
             Provide a state dict to keep a record of current buffer.
         Returns:
```

### Comparing `DI-engine-0.4.6/ding/worker/replay_buffer/base_buffer.py` & `DI-engine-0.4.7/ding/worker/replay_buffer/base_buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/replay_buffer/episode_buffer.py` & `DI-engine-0.4.7/ding/worker/replay_buffer/episode_buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/replay_buffer/naive_buffer.py` & `DI-engine-0.4.7/ding/worker/replay_buffer/naive_buffer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/worker/replay_buffer/utils.py` & `DI-engine-0.4.7/ding/worker/replay_buffer/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/world_model/base_world_model.py` & `DI-engine-0.4.7/ding/world_model/base_world_model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/world_model/ddppo.py` & `DI-engine-0.4.7/ding/world_model/ddppo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/world_model/idm.py` & `DI-engine-0.4.7/ding/world_model/idm.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/world_model/mbpo.py` & `DI-engine-0.4.7/ding/world_model/mbpo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/ding/world_model/utils.py` & `DI-engine-0.4.7/ding/world_model/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_impala_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_onppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_qrdqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/enduro/enduro_rainbow_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/enduro/enduro_rainbow_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_a2c_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_a2c_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_acer_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_acer_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_c51_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_c51_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_cql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqfd_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqfd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_envpool_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_envpool_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_multi_gpu_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_multi_gpu_config.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,23 +10,23 @@
         env_id='Pong-v4',
         #'ALE/Pong-v5' is available. But special setting is needed after gym make.
         frame_stack=4,
     ),
     policy=dict(
         cuda=True,
         priority=False,
+        multi_gpu=True,
         model=dict(
             obs_shape=[4, 84, 84],
             action_shape=6,
             encoder_hidden_size_list=[128, 128, 512],
         ),
         nstep=3,
         discount_factor=0.99,
         learn=dict(
-            multi_gpu=True,
             update_per_collect=10,
             batch_size=32,
             learning_rate=0.0001,
             target_update_freq=500,
         ),
         collect=dict(n_sample=96, ),
         eval=dict(evaluator=dict(eval_freq=4000, )),
```

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_render_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_render_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_dqn_stdim_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_dqn_stdim_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_fqf_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_fqf_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_gail_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_gail_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_impala_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_iqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_iqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_ngu_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_ngu_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_onppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_ppg_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_qrdqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_qrdqn_generation_data_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_qrdqn_generation_data_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_gtrxl_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_gtrxl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d2_residual_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d2_residual_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d3_offppoexpert_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d3_offppoexpert_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_r2d3_r2d2expert_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_r2d3_r2d2expert_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_rainbow_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_rainbow_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_sqil_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_sqil_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_sql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_sql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_trex_offppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_trex_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/pong/pong_trex_sql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/pong/pong_trex_sql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_a2c_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_a2c_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_acer_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_acer_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_c51_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_c51_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_cql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_dqfd_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_dqfd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_fqf_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_fqf_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_impala_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_iqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_iqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_offppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_onppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_ppg_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_qrdqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_qrdqn_generation_data_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_qrdqn_generation_data_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_r2d2_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_r2d2_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_r2d2_gtrxl_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_r2d2_gtrxl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_rainbow_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_rainbow_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_sqil_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_sqil_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_sql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_sql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_trex_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_trex_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/qbert/qbert_trex_offppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/qbert/qbert_trex_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_a2c_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_a2c_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_acer_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_acer_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_c51_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_c51_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqfd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_ddp.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_ddp.py`

 * *Files 0% similar despite different names*

```diff
@@ -11,28 +11,28 @@
         env_id='SpaceInvaders-v4',
         #'ALE/SpaceInvaders-v5' is available. But special setting is needed after gym make.
         frame_stack=4,
         manager=dict(shared_memory=False, )
     ),
     policy=dict(
         cuda=True,
+        multi_gpu=True,
         priority=False,
         model=dict(
             obs_shape=[4, 84, 84],
             action_shape=6,
             encoder_hidden_size_list=[128, 128, 512],
         ),
         nstep=3,
         discount_factor=0.99,
         learn=dict(
             update_per_collect=10,
             batch_size=32,
             learning_rate=0.0001,
             target_update_freq=500,
-            multi_gpu=True,
         ),
         collect=dict(n_sample=100, ),
         eval=dict(evaluator=dict(eval_freq=4000, )),
         other=dict(
             eps=dict(
                 type='exp',
                 start=1.,
```

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_dp.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_dqn_config_multi_gpu_dp.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_fqf_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_fqf_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_impala_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_iqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_iqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_ppg_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_qrdqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_gtrxl_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_gtrxl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_residual_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_r2d2_residual_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_rainbow_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_rainbow_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sqil_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sqil_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sql_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_sql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_dqn_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_offppo_config.py` & `DI-engine-0.4.7/dizoo/atari/config/serial/spaceinvaders/spaceinvaders_trex_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/atari_dqn_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/atari_dqn_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/atari_ppg_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/atari_ppg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/phoenix_fqf_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/phoenix_fqf_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/phoenix_iqn_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/phoenix_iqn_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/pong_cql_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/pong_cql_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/pong_dqn_envpool_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/pong_dqn_envpool_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/pong_fqf_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/pong_fqf_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/qbert_cql_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/qbert_cql_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/qbert_fqf_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/qbert_fqf_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_eval.py` & `DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_ddp.py` & `DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_ddp.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_dp.py` & `DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_dqn_main_multi_gpu_dp.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/entry/spaceinvaders_fqf_main.py` & `DI-engine-0.4.7/dizoo/atari/entry/spaceinvaders_fqf_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/envs/atari_env.py` & `DI-engine-0.4.7/dizoo/atari/envs/atari_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/envs/atari_wrappers.py` & `DI-engine-0.4.7/dizoo/atari/envs/atari_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/atari/envs/test_atari_env.py` & `DI-engine-0.4.7/dizoo/atari/envs/test_atari_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/BGAgent.py` & `DI-engine-0.4.7/dizoo/beergame/envs/BGAgent.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/beergame_core.py` & `DI-engine-0.4.7/dizoo/beergame/envs/beergame_core.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/beergame_env.py` & `DI-engine-0.4.7/dizoo/beergame/envs/beergame_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/clBeergame.py` & `DI-engine-0.4.7/dizoo/beergame/envs/clBeergame.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/plotting.py` & `DI-engine-0.4.7/dizoo/beergame/envs/plotting.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/beergame/envs/utils.py` & `DI-engine-0.4.7/dizoo/beergame/envs/utils.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bitflip/config/bitflip_her_dqn_config.py` & `DI-engine-0.4.7/dizoo/bitflip/config/bitflip_her_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bitflip/config/bitflip_pure_dqn_config.py` & `DI-engine-0.4.7/dizoo/bitflip/config/bitflip_pure_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bitflip/entry/bitflip_dqn_main.py` & `DI-engine-0.4.7/dizoo/bitflip/entry/bitflip_dqn_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bitflip/envs/bitflip_env.py` & `DI-engine-0.4.7/dizoo/bitflip/envs/bitflip_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bitflip/envs/test_bitfilp_env.py` & `DI-engine-0.4.7/dizoo/bitflip/envs/test_bitfilp_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_bco_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_bco_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_dt_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_gail_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_impala_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppo_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppopg_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_ppopg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_sac_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/config/bipedalwalker_td3_config.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/config/bipedalwalker_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/entry/bipedalwalker_ppo_eval.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/entry/bipedalwalker_ppo_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/bipedalwalker_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/bipedalwalker/envs/test_bipedalwalker.py` & `DI-engine-0.4.7/dizoo/box2d/bipedalwalker/envs/test_bipedalwalker.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/carracing/config/carracing_dqn_config.py` & `DI-engine-0.4.7/dizoo/box2d/carracing/config/carracing_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/carracing/envs/carracing_env.py` & `DI-engine-0.4.7/dizoo/box2d/carracing/envs/carracing_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/carracing/envs/test_carracing_env.py` & `DI-engine-0.4.7/dizoo/box2d/carracing/envs/test_carracing_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/__init__.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_a2c_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_a2c_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_acer_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_acer_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_bco_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_bco_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_c51_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_qrdqn_config.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,61 +1,53 @@
 from easydict import EasyDict
 
-lunarlander_c51_config = dict(
-    exp_name='lunarlander_c51_seed0',
+lunarlander_qrdqn_config = dict(
+    exp_name='lunarlander_qrdqn_seed0',
     env=dict(
         collector_env_num=8,
         evaluator_env_num=8,
         env_id='LunarLander-v2',
         n_evaluator_episode=8,
         stop_value=200,
     ),
     policy=dict(
         cuda=False,
-        priority=True,
         model=dict(
             obs_shape=8,
             action_shape=4,
-            encoder_hidden_size_list=[128, 128, 64],
-            v_min=-10,
-            v_max=10,
-            n_atom=51,
         ),
+        nstep=1,
         discount_factor=0.97,
-        nstep=3,
         learn=dict(
-            update_per_collect=3,
             batch_size=64,
             learning_rate=0.001,
-            target_update_freq=100,
-        ),
-        collect=dict(
-            n_sample=80,
-            unroll_len=1,
         ),
+        collect=dict(n_sample=128, ),
+        eval=dict(evaluator=dict(eval_freq=50, )),
         other=dict(
             eps=dict(
                 type='exp',
                 start=0.95,
                 end=0.1,
                 decay=10000,
-            ), replay_buffer=dict(replay_buffer_size=20000, )
+            ),
+            replay_buffer=dict(replay_buffer_size=20000, ),
         ),
     ),
 )
-lunarlander_c51_config = EasyDict(lunarlander_c51_config)
-main_config = lunarlander_c51_config
-lunarlander_c51_create_config = dict(
+lunarlander_qrdqn_config = EasyDict(lunarlander_qrdqn_config)
+main_config = lunarlander_qrdqn_config
+lunarlander_qrdqn_create_config = dict(
     env=dict(
         type='lunarlander',
         import_names=['dizoo.box2d.lunarlander.envs.lunarlander_env'],
     ),
-    env_manager=dict(type='base'),
-    policy=dict(type='c51'),
+    env_manager=dict(type='subprocess'),
+    policy=dict(type='qrdqn'),
 )
-lunarlander_c51_create_config = EasyDict(lunarlander_c51_create_config)
-create_config = lunarlander_c51_create_config
+lunarlander_qrdqn_create_config = EasyDict(lunarlander_qrdqn_create_config)
+create_config = lunarlander_qrdqn_create_config
 
-if __name__ == "__main__":
-    # or you can enter `ding -m serial -c lunarlander_c51_config.py -s 0`
+if __name__ == '__main__':
+    # or you can enter `ding -m serial -c lunarlander_qrdqn_config.py -s 0`
     from ding.entry import serial_pipeline
-    serial_pipeline((main_config, create_config), seed=0)
+    serial_pipeline([main_config, create_config], seed=0)
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_ddpg_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_vae_config.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,64 +1,70 @@
 from easydict import EasyDict
 
-lunarlander_td3_config = dict(
-    exp_name='lunarlander_cont_td3_seed0',
+lunarlander_td3vae_config = dict(
+    exp_name='lunarlander_cont_td3_vae_seed0',
     env=dict(
         env_id='LunarLanderContinuous-v2',
         collector_env_num=8,
         evaluator_env_num=8,
         # (bool) Scale output action into legal range.
         act_scale=True,
         n_evaluator_episode=8,
         stop_value=200,
     ),
     policy=dict(
-        cuda=False,
+        cuda=True,
         priority=False,
-        random_collect_size=0,
+        random_collect_size=10000,
+        original_action_shape=2,
         model=dict(
             obs_shape=8,
-            action_shape=2,
+            action_shape=6,  # latent_action_dim
             twin_critic=True,
             action_space='regression',
         ),
         learn=dict(
-            update_per_collect=256,
+            warm_up_update=int(1e4),
+            rl_vae_update_circle=1,  # train rl 1 iter, vae 1 iter
+            update_per_collect_rl=256,
+            update_per_collect_vae=10,
             batch_size=128,
             learning_rate_actor=3e-4,
             learning_rate_critic=3e-4,
+            learning_rate_vae=1e-4,
             ignore_done=False,
+            target_theta=0.005,
             actor_update_freq=2,
             noise=True,
             noise_sigma=0.1,
             noise_range=dict(
                 min=-0.5,
                 max=0.5,
             ),
         ),
         collect=dict(
             n_sample=256,
-            noise_sigma=0.1,
+            unroll_len=1,
+            noise_sigma=0,  # NOTE: add noise in original action in _forward_collect method of td3_vae policy
             collector=dict(collect_print_freq=1000, ),
         ),
         eval=dict(evaluator=dict(eval_freq=100, ), ),
         other=dict(replay_buffer=dict(replay_buffer_size=int(1e5), ), ),
     ),
 )
-lunarlander_td3_config = EasyDict(lunarlander_td3_config)
-main_config = lunarlander_td3_config
+lunarlander_td3vae_config = EasyDict(lunarlander_td3vae_config)
+main_config = lunarlander_td3vae_config
 
-lunarlander_td3_create_config = dict(
+lunarlander_td3vae_create_config = dict(
     env=dict(
         type='lunarlander',
         import_names=['dizoo.box2d.lunarlander.envs.lunarlander_env'],
     ),
     env_manager=dict(type='subprocess'),
-    policy=dict(type='td3'),
+    policy=dict(type='td3_vae'),
 )
-lunarlander_td3_create_config = EasyDict(lunarlander_td3_create_config)
-create_config = lunarlander_td3_create_config
+lunarlander_td3vae_create_config = EasyDict(lunarlander_td3vae_create_config)
+create_config = lunarlander_td3vae_create_config
 
 if __name__ == '__main__':
-    # or you can enter `ding -m serial -c lunarlander_cont_td3_config.py -s 0`
-    from ding.entry import serial_pipeline
-    serial_pipeline([main_config, create_config], seed=0)
+    from ding.entry import serial_pipeline_td3_vae
+    serial_pipeline_td3_vae([main_config, create_config], seed=0)
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_cont_td3_vae_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_cont_sac_config.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,70 +1,55 @@
 from easydict import EasyDict
 
-lunarlander_td3vae_config = dict(
-    exp_name='lunarlander_cont_td3_vae_seed0',
+lunarlander_sac_config = dict(
+    exp_name='lunarlander_cont_sac_seed0',
     env=dict(
         env_id='LunarLanderContinuous-v2',
-        collector_env_num=8,
+        collector_env_num=4,
         evaluator_env_num=8,
         # (bool) Scale output action into legal range.
         act_scale=True,
         n_evaluator_episode=8,
         stop_value=200,
     ),
     policy=dict(
-        cuda=True,
-        priority=False,
+        cuda=False,
         random_collect_size=10000,
-        original_action_shape=2,
         model=dict(
             obs_shape=8,
-            action_shape=6,  # latent_action_dim
+            action_shape=2,
             twin_critic=True,
-            action_space='regression',
+            action_space='reparameterization',
         ),
         learn=dict(
-            warm_up_update=int(1e4),
-            rl_vae_update_circle=1,  # train rl 1 iter, vae 1 iter
-            update_per_collect_rl=256,
-            update_per_collect_vae=10,
+            update_per_collect=256,
             batch_size=128,
-            learning_rate_actor=3e-4,
-            learning_rate_critic=3e-4,
-            learning_rate_vae=1e-4,
-            ignore_done=False,
-            target_theta=0.005,
-            actor_update_freq=2,
-            noise=True,
-            noise_sigma=0.1,
-            noise_range=dict(
-                min=-0.5,
-                max=0.5,
-            ),
+            learning_rate_q=1e-3,
+            learning_rate_policy=3e-4,
+            learning_rate_alpha=3e-4,
+            auto_alpha=True,
         ),
         collect=dict(
             n_sample=256,
-            unroll_len=1,
-            noise_sigma=0,  # NOTE: add noise in original action in _forward_collect method of td3_vae policy
-            collector=dict(collect_print_freq=1000, ),
         ),
-        eval=dict(evaluator=dict(eval_freq=100, ), ),
+        eval=dict(evaluator=dict(eval_freq=1000, ), ),
         other=dict(replay_buffer=dict(replay_buffer_size=int(1e5), ), ),
     ),
 )
-lunarlander_td3vae_config = EasyDict(lunarlander_td3vae_config)
-main_config = lunarlander_td3vae_config
+lunarlander_sac_config = EasyDict(lunarlander_sac_config)
+main_config = lunarlander_sac_config
 
-lunarlander_td3vae_create_config = dict(
+lunarlander_sac_create_config = dict(
     env=dict(
         type='lunarlander',
         import_names=['dizoo.box2d.lunarlander.envs.lunarlander_env'],
     ),
     env_manager=dict(type='subprocess'),
-    policy=dict(type='td3_vae'),
+    policy=dict(type='sac'),
 )
-lunarlander_td3vae_create_config = EasyDict(lunarlander_td3vae_create_config)
-create_config = lunarlander_td3vae_create_config
+lunarlander_sac_create_config = EasyDict(lunarlander_sac_create_config)
+create_config = lunarlander_sac_create_config
 
 if __name__ == '__main__':
-    from ding.entry import serial_pipeline_td3_vae
-    serial_pipeline_td3_vae([main_config, create_config], seed=0)
+    # or you can enter `ding -m serial -c lunarlander_cont_sac_config.py -s 0`
+    from ding.entry import serial_pipeline
+    serial_pipeline([main_config, create_config], seed=0)
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_decision_transformer.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_decision_transformer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_discrete_sac_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_discrete_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqfd_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqfd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqn_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_dqn_deque_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_dqn_deque_config.py`

 * *Files 5% similar despite different names*

```diff
@@ -50,15 +50,15 @@
             eps=dict(
                 # Decay type. Support ['exp', 'linear'].
                 type='exp',
                 start=0.95,
                 end=0.1,
                 decay=50000,
             ),
-            replay_buffer=dict(replay_buffer_size=100000, priority=True, priority_IS_weight=False)
+            replay_buffer=dict(replay_buffer_size=100000, )
         ),
     ),
 )
 lunarlander_dqn_config = EasyDict(lunarlander_dqn_config)
 main_config = lunarlander_dqn_config
 
 lunarlander_dqn_create_config = dict(
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_gail_dqn_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_gail_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_gcl_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_gcl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_ngu_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_ngu_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_offppo_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_pg_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_sql_config.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,53 +1,52 @@
 from easydict import EasyDict
 
-lunarlander_qrdqn_config = dict(
-    exp_name='lunarlander_qrdqn_seed0',
+lunarlander_sql_config = dict(
+    exp_name='lunarlander_sql_seed0',
     env=dict(
         collector_env_num=8,
         evaluator_env_num=8,
         env_id='LunarLander-v2',
         n_evaluator_episode=8,
         stop_value=200,
     ),
     policy=dict(
         cuda=False,
         model=dict(
             obs_shape=8,
             action_shape=4,
+            encoder_hidden_size_list=[128, 128, 64],
+            dueling=True,
         ),
         nstep=1,
         discount_factor=0.97,
-        learn=dict(
-            batch_size=64,
-            learning_rate=0.001,
-        ),
-        collect=dict(n_sample=128, ),
-        eval=dict(evaluator=dict(eval_freq=50, )),
+        learn=dict(batch_size=64, learning_rate=0.001, alpha=0.08),
+        collect=dict(n_sample=64),
+        eval=dict(evaluator=dict(eval_freq=50, )),  # note: this is the times after which you learns to evaluate
         other=dict(
             eps=dict(
                 type='exp',
                 start=0.95,
                 end=0.1,
                 decay=10000,
             ),
             replay_buffer=dict(replay_buffer_size=20000, ),
         ),
     ),
 )
-lunarlander_qrdqn_config = EasyDict(lunarlander_qrdqn_config)
-main_config = lunarlander_qrdqn_config
-lunarlander_qrdqn_create_config = dict(
+lunarlander_sql_config = EasyDict(lunarlander_sql_config)
+main_config = lunarlander_sql_config
+lunarlander_sql_create_config = dict(
     env=dict(
         type='lunarlander',
         import_names=['dizoo.box2d.lunarlander.envs.lunarlander_env'],
     ),
     env_manager=dict(type='subprocess'),
-    policy=dict(type='qrdqn'),
+    policy=dict(type='sql'),
 )
-lunarlander_qrdqn_create_config = EasyDict(lunarlander_qrdqn_create_config)
-create_config = lunarlander_qrdqn_create_config
+lunarlander_sql_create_config = EasyDict(lunarlander_sql_create_config)
+create_config = lunarlander_sql_create_config
 
-if __name__ == '__main__':
-    # or you can enter `ding -m serial -c lunarlander_qrdqn_config.py -s 0`
+if __name__ == "__main__":
+    # or you can enter `ding -m serial -c lunarlander_sql_config.py -s 0`
     from ding.entry import serial_pipeline
     serial_pipeline([main_config, create_config], seed=0)
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d2_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d2_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d2_gtrxl_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d2_gtrxl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d3_ppoexpert_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d3_ppoexpert_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_r2d3_r2d2expert_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_r2d3_r2d2expert_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_rnd_onppo_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_rnd_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_sqil_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_sqil_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_sql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_ar_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,52 +1,58 @@
 from easydict import EasyDict
 
-lunarlander_sql_config = dict(
-    exp_name='lunarlander_sql_seed0',
+cuda = True
+multi_gpu = False
+
+main_config = dict(
+    exp_name='kitchen_complete_ibc_ar_seed0',
     env=dict(
-        collector_env_num=8,
+        env_id='kitchen-complete-v0',
+        norm_obs=dict(
+            use_norm=True, 
+            offline_stats=dict(use_offline_stats=True, ),
+        ),
         evaluator_env_num=8,
-        env_id='LunarLander-v2',
         n_evaluator_episode=8,
-        stop_value=200,
+        use_act_scale=True,
+        stop_value=1e10,
     ),
     policy=dict(
-        cuda=False,
+        cuda=cuda,
         model=dict(
-            obs_shape=8,
-            action_shape=4,
-            encoder_hidden_size_list=[128, 128, 64],
-            dueling=True,
+            obs_shape=60,
+            action_shape=9,
+            stochastic_optim=dict(type='ardfo',)
         ),
-        nstep=1,
-        discount_factor=0.97,
-        learn=dict(batch_size=64, learning_rate=0.001, alpha=0.08),
-        collect=dict(n_sample=64),
-        eval=dict(evaluator=dict(eval_freq=50, )),  # note: this is the times after which you learns to evaluate
-        other=dict(
-            eps=dict(
-                type='exp',
-                start=0.95,
-                end=0.1,
-                decay=10000,
-            ),
-            replay_buffer=dict(replay_buffer_size=20000, ),
+        learn=dict(
+            multi_gpu=multi_gpu,
+            train_epoch=1000,
+            batch_size=256,
+            optim=dict(learning_rate=1e-5,),
+            learner=dict(hook=dict(log_show_after_iter=100)),
         ),
+        collect=dict(
+            data_type='d4rl',
+            data_path=None,
+        ),
+        eval=dict(evaluator=dict(eval_freq=1000,)),
     ),
 )
-lunarlander_sql_config = EasyDict(lunarlander_sql_config)
-main_config = lunarlander_sql_config
-lunarlander_sql_create_config = dict(
+main_config = EasyDict(main_config)
+main_config = main_config
+create_config = dict(
     env=dict(
-        type='lunarlander',
-        import_names=['dizoo.box2d.lunarlander.envs.lunarlander_env'],
+        type='d4rl',
+        import_names=['dizoo.d4rl.envs.d4rl_env'],
+    ),
+    env_manager=dict(type='base',),
+    policy=dict(
+        type='ibc',
+        import_names=['ding.policy.ibc'],
+        model=dict(
+            type='arebm',
+            import_names=['ding.model.template.ebm'],
+        ),
     ),
-    env_manager=dict(type='subprocess'),
-    policy=dict(type='sql'),
 )
-lunarlander_sql_create_config = EasyDict(lunarlander_sql_create_config)
-create_config = lunarlander_sql_create_config
-
-if __name__ == "__main__":
-    # or you can enter `ding -m serial -c lunarlander_sql_config.py -s 0`
-    from ding.entry import serial_pipeline
-    serial_pipeline([main_config, create_config], seed=0)
+create_config = EasyDict(create_config)
+create_config = create_config
```

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_trex_dqn_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_trex_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/lunarlander_trex_offppo_config.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/lunarlander_trex_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/config/t.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/config/t.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/lunarlander_dqn_eval.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/lunarlander_dqn_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/entry/lunarlander_dqn_example.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/entry/lunarlander_dqn_example.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/lunarlander_env.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/lunarlander_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/box2d/lunarlander/envs/test_lunarlander_env.py` & `DI-engine-0.4.7/dizoo/box2d/lunarlander/envs/test_lunarlander_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bsuite/envs/bsuite_env.py` & `DI-engine-0.4.7/dizoo/bsuite/envs/bsuite_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/bsuite/envs/test_bsuite_env.py` & `DI-engine-0.4.7/dizoo/bsuite/envs/test_bsuite_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/acrobot/config/acrobot_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/acrobot/config/acrobot_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/acrobot_env.py` & `DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/acrobot_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/acrobot/envs/test_acrobot_env.py` & `DI-engine-0.4.7/dizoo/classic_control/acrobot/envs/test_acrobot_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/__init__.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -15,8 +15,9 @@
 from .cartpole_rainbow_config import cartpole_rainbow_config, cartpole_rainbow_create_config
 from .cartpole_sqil_config import cartpole_sqil_config, cartpole_sqil_create_config
 from .cartpole_sql_config import cartpole_sql_config, cartpole_sql_create_config
 from .cartpole_sqn_config import cartpole_sqn_config, cartpole_sqn_create_config
 from .cartpole_trex_dqn_config import cartpole_trex_dqn_config, cartpole_trex_dqn_create_config
 from .cartpole_trex_offppo_config import cartpole_trex_offppo_config, cartpole_trex_offppo_create_config
 from .cartpole_trex_onppo_config import cartpole_trex_ppo_onpolicy_config, cartpole_trex_ppo_onpolicy_create_config
+from .cartpole_mdqn_config import cartpole_mdqn_config, cartpole_mdqn_create_config
 # from .cartpole_ppo_default_loader import cartpole_ppo_default_loader
```

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_a2c_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_a2c_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_acer_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_acer_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_bc_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_bco_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_bco_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_c51_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_c51_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_cql_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_decision_transformer.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_decision_transformer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqfd_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqfd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_gail_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_gail_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_rnd_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_rnd_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_dqn_stdim_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_dqn_stdim_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_drex_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_drex_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_fqf_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_fqf_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_gcl_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_gcl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_impala_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_impala_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_iqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_iqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ngu_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ngu_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_pg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_icm_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_icm_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_offpolicy_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_offpolicy_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppo_stdim_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppo_stdim_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_ppopg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_ppopg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_qrdqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_qrdqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_qrdqn_generation_data_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_qrdqn_generation_data_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_gtrxl_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_gtrxl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_r2d2_residual_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_r2d2_residual_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_rainbow_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_rainbow_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_rnd_onppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_rnd_onppo_config.py`

 * *Files 17% similar despite different names*

```diff
@@ -6,20 +6,15 @@
         collector_env_num=8,
         evaluator_env_num=5,
         n_evaluator_episode=5,
         stop_value=195,
     ),
     reward_model=dict(
         intrinsic_reward_type='add',
-        intrinsic_reward_weight=None,
-        # means the relative weight of RND intrinsic_reward.
-        # If intrinsic_reward_weight=None, we will automatically set it based on
-        # the absolute value of the difference between max and min extrinsic reward in the sampled mini-batch
-        # please refer to rnd_reward_model for details.
-        intrinsic_reward_rescale=0.001,
+        intrinsic_reward_weight=0.001,
         # means the rescale value of RND intrinsic_reward only used when intrinsic_reward_weight is None
         # please refer to rnd_reward_model for details.
         learning_rate=5e-4,
         obs_shape=4,
         batch_size=32,
         update_per_collect=4,
         obs_norm=True,
```

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sac_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sqil_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sqil_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sql_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_sqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_sqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_offppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_offppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/cartpole_trex_onppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_trex_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config_k8s.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/parallel/cartpole_dqn_config_k8s.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_c51_deploy.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_c51_deploy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_c51_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_c51_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_cql_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_cql_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_buffer_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_buffer_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_eval.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_dqn_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_fqf_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_fqf_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppg_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppo_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/cartpole_ppo_offpolicy_main.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/cartpole_ppo_offpolicy_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/entry/formatted_total_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/entry/formatted_total_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/cartpole_env.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/cartpole_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/test_cartpole_env.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/test_cartpole_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/cartpole/envs/test_cartpole_env_manager.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/envs/test_cartpole_env_manager.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/mtcar_env.py` & `DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/mtcar_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/mountain_car/envs/test_mtcar_env.py` & `DI-engine-0.4.7/dizoo/classic_control/mountain_car/envs/test_mtcar_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/__init__.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_bdq_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_bdq_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_cql_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_d4pg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_d4pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ddpg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_dqn_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ibc_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ibc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_pg_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_ppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sac_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sac_data_generation_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sac_data_generation_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_sqil_sac_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_sqil_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_bc_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/config/pendulum_td3_data_generation_config.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/config/pendulum_td3_data_generation_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_cql_ddpg_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_cql_ddpg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_cql_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_cql_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_d4pg_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_d4pg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_ddpg_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_ddpg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_dqn_eval.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_dqn_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_ppo_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_td3_bc_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_td3_bc_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/entry/pendulum_td3_main.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/entry/pendulum_td3_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/pendulum_env.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/pendulum_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/classic_control/pendulum/envs/test_pendulum_env.py` & `DI-engine-0.4.7/dizoo/classic_control/pendulum/envs/test_pendulum_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/common/policy/md_dqn.py` & `DI-engine-0.4.7/dizoo/common/policy/md_dqn.py`

 * *Files 1% similar despite different names*

```diff
@@ -83,15 +83,15 @@
             q_value_mean = q_value.mean().item()
 
         # ====================
         # Q-learning update
         # ====================
         self._optimizer.zero_grad()
         loss.backward()
-        if self._cfg.learn.multi_gpu:
+        if self._cfg.multi_gpu:
             self.sync_gradients(self._learn_model)
         self._optimizer.step()
 
         # =============
         # after update
         # =============
         self._target_model.update(self._learn_model.state_dict())
```

### Comparing `DI-engine-0.4.6/dizoo/common/policy/md_ppo.py` & `DI-engine-0.4.7/dizoo/common/policy/md_ppo.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/common/policy/md_rainbow_dqn.py` & `DI-engine-0.4.7/dizoo/common/policy/md_rainbow_dqn.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/competitive_rl/envs/competitive_rl_env.py` & `DI-engine-0.4.7/dizoo/competitive_rl/envs/competitive_rl_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/competitive_rl/envs/competitive_rl_env_wrapper.py` & `DI-engine-0.4.7/dizoo/competitive_rl/envs/competitive_rl_env_wrapper.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/competitive_rl/envs/test_competitive_rl.py` & `DI-engine-0.4.7/dizoo/competitive_rl/envs/test_competitive_rl.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_replay_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_replay_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_medium_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_medium_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/halfcheetah_random_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/halfcheetah_random_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_ar_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_ar_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_ibc_mcmc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_ibc_mcmc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_replay_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_replay_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_medium_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_medium_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/hopper_random_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/hopper_random_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_ar_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_ar_config.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,32 @@
 from easydict import EasyDict
 
 cuda = True
 multi_gpu = False
 
 main_config = dict(
-    exp_name='kitchen_complete_ibc_ar_seed0',
+    exp_name='pen_human_ibc_ar_seed0',
     env=dict(
-        env_id='kitchen-complete-v0',
+        env_id='pen-human-v0',
         norm_obs=dict(
             use_norm=True, 
             offline_stats=dict(use_offline_stats=True, ),
         ),
         evaluator_env_num=8,
         n_evaluator_episode=8,
         use_act_scale=True,
         stop_value=1e10,
     ),
     policy=dict(
         cuda=cuda,
         model=dict(
-            obs_shape=60,
-            action_shape=9,
+            obs_shape=45,
+            action_shape=24,
+            hidden_size=128,
+            hidden_layer_num=4,
             stochastic_optim=dict(type='ardfo',)
         ),
         learn=dict(
             multi_gpu=multi_gpu,
             train_epoch=1000,
             batch_size=256,
             optim=dict(learning_rate=1e-5,),
```

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/kitchen_complete_ibc_mcmc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/kitchen_complete_ibc_mcmc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/pen_human_bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/pen_human_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_ar_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_td3bc_config.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,60 +1,64 @@
+# You can conduct Experiments on D4RL with this config file through the following command:
+# cd ../entry && python d4rl_td3_bc_main.py
 from easydict import EasyDict
 
-cuda = True
-multi_gpu = False
-
 main_config = dict(
-    exp_name='pen_human_ibc_ar_seed0',
+    exp_name='walker2d_random_td3-bc_seed0',
     env=dict(
-        env_id='pen-human-v0',
+        env_id='walker2d-random-v0',
         norm_obs=dict(
             use_norm=True, 
             offline_stats=dict(use_offline_stats=True, ),
         ),
+        collector_env_num=1,
         evaluator_env_num=8,
-        n_evaluator_episode=8,
         use_act_scale=True,
-        stop_value=1e10,
+        n_evaluator_episode=8,
+        stop_value=6000,
     ),
     policy=dict(
-        cuda=cuda,
         model=dict(
-            obs_shape=45,
-            action_shape=24,
-            hidden_size=128,
-            hidden_layer_num=4,
-            stochastic_optim=dict(type='ardfo',)
+            obs_shape=11,
+            action_shape=3,
         ),
         learn=dict(
-            multi_gpu=multi_gpu,
-            train_epoch=1000,
+            train_epoch=30000,
             batch_size=256,
-            optim=dict(learning_rate=1e-5,),
-            learner=dict(hook=dict(log_show_after_iter=100)),
+            learning_rate_actor=0.0003,
+            learning_rate_critic=0.0003,
+            actor_update_freq=2,
+            noise=True,
+            noise_sigma=0.2,
+            noise_range={
+                'min': -0.5,
+                'max': 0.5
+            },
+            alpha=2.5,
         ),
         collect=dict(
             data_type='d4rl',
             data_path=None,
         ),
-        eval=dict(evaluator=dict(eval_freq=1000,)),
+        eval=dict(evaluator=dict(eval_freq=10000, )),
+        other=dict(replay_buffer=dict(replay_buffer_size=2000000, ), ),
     ),
 )
 main_config = EasyDict(main_config)
 main_config = main_config
 create_config = dict(
     env=dict(
         type='d4rl',
         import_names=['dizoo.d4rl.envs.d4rl_env'],
     ),
-    env_manager=dict(type='base',),
+    env_manager=dict(
+        cfg_type='BaseEnvManagerDict',
+        type='base',
+    ),
     policy=dict(
-        type='ibc',
-        import_names=['ding.policy.ibc'],
-        model=dict(
-            type='arebm',
-            import_names=['ding.model.template.ebm'],
-        ),
+        type='td3_bc',
+        import_names=['ding.policy.td3_bc'],
     ),
+    replay_buffer=dict(type='naive', ),
 )
 create_config = EasyDict(create_config)
 create_config = create_config
```

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/pen_human_ibc_mcmc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/pen_human_ibc_mcmc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_expert_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_expert_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_replay_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_replay_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_medium_td3bc_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_medium_td3bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_cql_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_dt_config.py` & `DI-engine-0.4.7/dizoo/d4rl/config/walker2d_random_dt_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/config/walker2d_random_td3bc_config.py` & `DI-engine-0.4.7/dizoo/mario/mario_dqn_config.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,64 +1,49 @@
-# You can conduct Experiments on D4RL with this config file through the following command:
-# cd ../entry && python d4rl_td3_bc_main.py
 from easydict import EasyDict
 
-main_config = dict(
-    exp_name='walker2d_random_td3-bc_seed0',
+mario_dqn_config = dict(
+    exp_name='mario_dqn_seed0',
     env=dict(
-        env_id='walker2d-random-v0',
-        norm_obs=dict(
-            use_norm=True, 
-            offline_stats=dict(use_offline_stats=True, ),
-        ),
-        collector_env_num=1,
+        collector_env_num=8,
         evaluator_env_num=8,
-        use_act_scale=True,
         n_evaluator_episode=8,
-        stop_value=6000,
+        stop_value=100000,
+        replay_path='mario_dqn_seed0/video',
     ),
     policy=dict(
+        cuda=True,
         model=dict(
-            obs_shape=11,
-            action_shape=3,
+            obs_shape=[4, 84, 84],
+            action_shape=2,
+            encoder_hidden_size_list=[128, 128, 256],
+            dueling=True,
         ),
+        nstep=3,
+        discount_factor=0.99,
         learn=dict(
-            train_epoch=30000,
-            batch_size=256,
-            learning_rate_actor=0.0003,
-            learning_rate_critic=0.0003,
-            actor_update_freq=2,
-            noise=True,
-            noise_sigma=0.2,
-            noise_range={
-                'min': -0.5,
-                'max': 0.5
-            },
-            alpha=2.5,
+            update_per_collect=10,
+            batch_size=32,
+            learning_rate=0.0001,
+            target_update_freq=500,
         ),
-        collect=dict(
-            data_type='d4rl',
-            data_path=None,
+        collect=dict(n_sample=96, ),
+        eval=dict(evaluator=dict(eval_freq=2000, )),
+        other=dict(
+            eps=dict(
+                type='exp',
+                start=1.,
+                end=0.05,
+                decay=250000,
+            ),
+            replay_buffer=dict(replay_buffer_size=100000, ),
         ),
-        eval=dict(evaluator=dict(eval_freq=10000, )),
-        other=dict(replay_buffer=dict(replay_buffer_size=2000000, ), ),
     ),
 )
-main_config = EasyDict(main_config)
-main_config = main_config
-create_config = dict(
-    env=dict(
-        type='d4rl',
-        import_names=['dizoo.d4rl.envs.d4rl_env'],
-    ),
-    env_manager=dict(
-        cfg_type='BaseEnvManagerDict',
-        type='base',
-    ),
-    policy=dict(
-        type='td3_bc',
-        import_names=['ding.policy.td3_bc'],
-    ),
-    replay_buffer=dict(type='naive', ),
+mario_dqn_config = EasyDict(mario_dqn_config)
+main_config = mario_dqn_config
+mario_dqn_create_config = dict(
+    env_manager=dict(type='subprocess'),
+    policy=dict(type='dqn'),
 )
-create_config = EasyDict(create_config)
-create_config = create_config
+mario_dqn_create_config = EasyDict(mario_dqn_create_config)
+create_config = mario_dqn_create_config
+# you can run `python3 -u mario_dqn_main.py`
```

### Comparing `DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_cql_main.py` & `DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_cql_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_dt_main.py` & `DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_dt_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_ibc_main.py` & `DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_ibc_main.py`

 * *Files 11% similar despite different names*

```diff
@@ -2,30 +2,33 @@
 from ding.entry import serial_pipeline_offline
 from ding.config import read_config
 from ding.utils import dist_init
 from pathlib import Path
 import torch
 import torch.multiprocessing as mp
 
+
 def offline_worker(rank, config, args):
     dist_init(rank=rank, world_size=torch.cuda.device_count())
     serial_pipeline_offline(config, seed=args.seed)
 
+
 def train(args):
     # launch from anywhere
-    config = Path(__file__).absolute().parent.parent / 'config' / args.config 
+    config = Path(__file__).absolute().parent.parent / 'config' / args.config
     config = read_config(str(config))
     config[0].exp_name = config[0].exp_name.replace('0', str(args.seed))
-    if not config[0].policy.learn.multi_gpu:
+    if not config[0].policy.multi_gpu:
         serial_pipeline_offline(config, seed=args.seed)
     else:
         os.environ["MASTER_ADDR"] = "localhost"
         os.environ["MASTER_PORT"] = "29600"
         mp.spawn(offline_worker, nprocs=torch.cuda.device_count(), args=(config, args))
 
+
 if __name__ == "__main__":
     import argparse
 
     parser = argparse.ArgumentParser()
     parser.add_argument('--seed', '-s', type=int, default=0)
     parser.add_argument('--config', '-c', type=str, default='hopper_medium_expert_ibc_config.py')
     args = parser.parse_args()
```

### Comparing `DI-engine-0.4.6/dizoo/d4rl/entry/d4rl_td3_bc_main.py` & `DI-engine-0.4.7/dizoo/d4rl/entry/d4rl_td3_bc_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/envs/d4rl_env.py` & `DI-engine-0.4.7/dizoo/d4rl/envs/d4rl_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/d4rl/envs/d4rl_wrappers.py` & `DI-engine-0.4.7/dizoo/d4rl/envs/d4rl_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/dmc2gym/envs/dmc2gym_env.py` & `DI-engine-0.4.7/dizoo/dmc2gym/envs/dmc2gym_env.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,19 +2,19 @@
 import gym
 from gym.spaces import Box
 import numpy as np
 from ding.envs import BaseEnv, BaseEnvTimestep
 from ding.torch_utils import to_ndarray
 from ding.utils import ENV_REGISTRY
 import dmc2gym
+from ding.envs import WarpFrameWrapper, ScaledFloatFrameWrapper, ClipRewardWrapper, FrameStackWrapper
 
 
 def dmc2gym_observation_space(dim, minimum=-np.inf, maximum=np.inf, dtype=np.float32) -> Callable:
-
-    def observation_space(from_pixels=True, height=100, width=100, channels_first=True) -> Box:
+    def observation_space(from_pixels=True, height=84, width=84, channels_first=True) -> Box:
         if from_pixels:
             shape = [3, height, width] if channels_first else [height, width, 3]
             return Box(low=0, high=255, shape=shape, dtype=np.uint8)
         else:
             return Box(np.repeat(minimum, dim).astype(dtype), np.repeat(maximum, dim).astype(dtype), dtype=dtype)
 
     return observation_space
@@ -25,25 +25,27 @@
 
 
 def dmc2gym_action_space(dim, minimum=-1, maximum=1, dtype=np.float32) -> Box:
     return Box(np.repeat(minimum, dim).astype(dtype), np.repeat(maximum, dim).astype(dtype), dtype=dtype)
 
 
 def dmc2gym_reward_space(minimum=0, maximum=1, dtype=np.float32) -> Callable:
-
     def reward_space(frame_skip=1) -> Box:
         return Box(
             np.repeat(minimum * frame_skip, 1).astype(dtype),
             np.repeat(maximum * frame_skip, 1).astype(dtype),
             dtype=dtype
         )
 
     return reward_space
 
 
+"""
+default observation, state, action, reward space for dmc2gym env
+"""
 dmc2gym_env_info = {
     "ball_in_cup": {
         "catch": {
             "observation_space": dmc2gym_observation_space(8),
             "state_space": dmc2gym_state_space(8),
             "action_space": dmc2gym_action_space(2),
             "reward_space": dmc2gym_reward_space()
@@ -102,20 +104,25 @@
 class DMC2GymEnv(BaseEnv):
 
     def __init__(self, cfg: dict = {}) -> None:
         assert cfg.domain_name in dmc2gym_env_info, '{}/{}'.format(cfg.domain_name, dmc2gym_env_info.keys())
         assert cfg.task_name in dmc2gym_env_info[
             cfg.domain_name], '{}/{}'.format(cfg.task_name, dmc2gym_env_info[cfg.domain_name].keys())
 
+        # default config for dmc2gym env
         self._cfg = {
-            "frame_skip": 3,
+            "frame_skip": 4,
+            'warp_frame': False,
+            'scale': False,
+            'clip_rewards': False,
+            "frame_stack": 3,
             "from_pixels": True,
             "visualize_reward": False,
-            "height": 100,
-            "width": 100,
+            "height": 84,
+            "width": 84,
             "channels_first": True,
         }
 
         self._cfg.update(cfg)
 
         self._init_flag = False
 
@@ -137,17 +144,32 @@
                 domain_name=self._cfg["domain_name"],
                 task_name=self._cfg["task_name"],
                 seed=1,
                 visualize_reward=self._cfg["visualize_reward"],
                 from_pixels=self._cfg["from_pixels"],
                 height=self._cfg["height"],
                 width=self._cfg["width"],
-                frame_skip=self._cfg["frame_skip"]
+                frame_skip=self._cfg["frame_skip"],
+                channels_first=self._cfg["channels_first"],
             )
 
+            # optional env wrapper
+            if self._cfg['warp_frame']:
+                self._env = WarpFrameWrapper(self._env)
+            if self._cfg['scale']:
+                self._env = ScaledFloatFrameWrapper(self._env)
+            if self._cfg['clip_rewards']:
+                self._env = ClipRewardWrapper(self._env)
+            if self._cfg['frame_stack'] > 1:
+                self._env = FrameStackWrapper(self._env, self._cfg['frame_stack'])
+
+            # set the obs, action space of wrapped env
+            self._observation_space = self._env.observation_space
+            self._action_space = self._env.action_space
+
             if self._replay_path is not None:
                 if gym.version.VERSION > '0.22.0':
                     self._env.metadata.update({'render_modes': ["rgb_array"]})
                 else:
                     self._env.metadata.update({'render.modes': ["rgb_array"]})
                 self._env = gym.wrappers.RecordVideo(
                     self._env,
@@ -164,18 +186,15 @@
             self._env.seed(self._seed + np_seed)
         elif hasattr(self, '_seed'):
             self._env.seed(self._seed)
 
         self._eval_episode_return = 0
         obs = self._env.reset()
 
-        if self._cfg["from_pixels"]:
-            obs = to_ndarray(obs).astype(np.uint8)
-        else:
-            obs = to_ndarray(obs).astype(np.float32)
+        obs = to_ndarray(obs).astype(np.float32)
 
         return obs
 
     def close(self) -> None:
         if self._init_flag:
             self._env.close()
         self._init_flag = False
@@ -188,20 +207,17 @@
     def step(self, action: np.ndarray) -> BaseEnvTimestep:
         action = action.astype('float32')
         obs, rew, done, info = self._env.step(action)
         self._eval_episode_return += rew
         if done:
             info['eval_episode_return'] = self._eval_episode_return
 
-        if self._cfg["from_pixels"]:
-            obs = to_ndarray(obs).astype(np.uint8)
-        else:
-            obs = to_ndarray(obs).astype(np.float32)
+        obs = to_ndarray(obs).astype(np.float32)
+        rew = to_ndarray([rew]).astype(np.float32)  # wrapped to be transferred to a array with shape (1,)
 
-        rew = to_ndarray([rew]).astype(np.float32)  # wrapped to be transfered to a array with shape (1,)
         return BaseEnvTimestep(obs, rew, done, info)
 
     def enable_save_replay(self, replay_path: Optional[str] = None) -> None:
         if replay_path is None:
             replay_path = './video'
         self._replay_path = replay_path
 
@@ -218,8 +234,8 @@
         return self._action_space
 
     @property
     def reward_space(self) -> gym.spaces.Space:
         return self._reward_space
 
     def __repr__(self) -> str:
-        return "DI-engine Deepmind Control Suite to gym Env: " + self._cfg["domain_name"] + ":" + self._cfg["task_name"]
+        return "DI-engine DeepMind Control Suite to gym Env: " + self._cfg["domain_name"] + ":" + self._cfg["task_name"]
```

### Comparing `DI-engine-0.4.6/dizoo/dmc2gym/envs/test_dmc2gym_env.py` & `DI-engine-0.4.7/dizoo/dmc2gym/envs/test_dmc2gym_env.py`

 * *Files 10% similar despite different names*

```diff
@@ -43,7 +43,8 @@
                     100,
                 )
                 assert timestep.reward.shape == (1, )
                 assert timestep.reward >= env.reward_space.low
                 assert timestep.reward <= env.reward_space.high
         print(env.observation_space, env.action_space, env.reward_space)
         env.close()
+
```

### Comparing `DI-engine-0.4.6/dizoo/evogym/envs/evogym_env.py` & `DI-engine-0.4.7/dizoo/evogym/envs/evogym_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_config.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_kaggle5th_main.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_kaggle5th_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_rule_lt0_main.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_rule_lt0_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_bc_rule_main.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_bc_rule_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/gfootball_dqn_config.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/gfootball_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/show_dataset.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/show_dataset.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/entry/test_accuracy.py` & `DI-engine-0.4.7/dizoo/gfootball/entry/test_accuracy.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/envs/fake_dataset.py` & `DI-engine-0.4.7/dizoo/gfootball/envs/fake_dataset.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/envs/gfootball_academy_env.py` & `DI-engine-0.4.7/dizoo/gfootball/envs/gfootball_academy_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/envs/gfootball_env.py` & `DI-engine-0.4.7/dizoo/gfootball/envs/gfootball_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/envs/gfootballsp_env.py` & `DI-engine-0.4.7/dizoo/gfootball/envs/gfootballsp_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/model/bots/kaggle_5th_place_model.py` & `DI-engine-0.4.7/dizoo/gfootball/model/bots/kaggle_5th_place_model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/model/bots/rule_based_bot_model.py` & `DI-engine-0.4.7/dizoo/gfootball/model/bots/rule_based_bot_model.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gfootball/policy/ppo_lstm.py` & `DI-engine-0.4.7/dizoo/gfootball/policy/ppo_lstm.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,16 +32,14 @@
         priority=False,
         # (bool) Whether use Importance Sampling Weight to correct biased update. If True, priority must be True.
         priority_IS_weight=False,
         # (bool) Whether to use nstep_return for value loss
         nstep_return=False,
         nstep=3,
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
             # How many updates(iterations) to train after collector's one collection.
             # Bigger "update_per_collect" means bigger off-policy.
             # collect data -> update policy-> collect data -> ...
             update_per_collect=5,
             batch_size=64,
             learning_rate=0.001,
             # ==============================================================
```

### Comparing `DI-engine-0.4.6/dizoo/gfootball/replay.py` & `DI-engine-0.4.7/dizoo/gfootball/replay.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_anytrading/config/stocks_dqn_config.py` & `DI-engine-0.4.7/dizoo/gym_anytrading/config/stocks_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_anytrading/envs/stocks_env.py` & `DI-engine-0.4.7/dizoo/gym_anytrading/envs/stocks_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_anytrading/envs/test_stocks_env.py` & `DI-engine-0.4.7/dizoo/gym_anytrading/envs/test_stocks_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_anytrading/envs/trading_env.py` & `DI-engine-0.4.7/dizoo/gym_anytrading/envs/trading_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_anytrading/worker/trading_serial_evaluator.py` & `DI-engine-0.4.7/dizoo/gym_anytrading/worker/trading_serial_evaluator.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_ddpg_config.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_hppo_config.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_hppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_mpdqn_config.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_pdqn_config.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from easydict import EasyDict
 
-gym_hybrid_mpdqn_config = dict(
-    exp_name='gym_hybrid_mpdqn_seed0',
+gym_hybrid_pdqn_config = dict(
+    exp_name='gym_hybrid_pdqn_seed0',
     env=dict(
         collector_env_num=8,
         evaluator_env_num=5,
         # (bool) Scale output action into legal range [-1, 1].
         act_scale=True,
         env_id='Moving-v0',  # ['Sliding-v0', 'Moving-v0']
         n_evaluator_episode=5,
@@ -17,37 +17,30 @@
         nstep=1,
         model=dict(
             obs_shape=10,
             action_shape=dict(
                 action_type_shape=3,
                 action_args_shape=2,
             ),
-            multi_pass=True,
-            action_mask=[[1, 0], [0, 1], [0, 0]],
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
-            # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
             update_per_collect=500,  # 10~500
             batch_size=320,
             learning_rate_dis=3e-4,
             learning_rate_cont=3e-4,
             target_theta=0.001,
             update_circle=10,
         ),
         # collect_mode config
         collect=dict(
             # (int) Only one of [n_sample, n_episode] shoule be set
-            n_sample=3200,
+            n_sample=3200,  # 128,
             # (int) Cut trajectories into pieces with length "unroll_len".
             unroll_len=1,
-            noise_sigma=0.1,
+            noise_sigma=0.1,  # 0.05,
             collector=dict(collect_print_freq=1000, ),
         ),
         eval=dict(evaluator=dict(eval_freq=1000, ), ),
         # other config
         other=dict(
             # Epsilon greedy with decay.
             eps=dict(
@@ -59,25 +52,25 @@
                 decay=int(1e5),
             ),
             replay_buffer=dict(replay_buffer_size=int(1e6), ),
         ),
     )
 )
 
-gym_hybrid_mpdqn_config = EasyDict(gym_hybrid_mpdqn_config)
-main_config = gym_hybrid_mpdqn_config
+gym_hybrid_pdqn_config = EasyDict(gym_hybrid_pdqn_config)
+main_config = gym_hybrid_pdqn_config
 
-gym_hybrid_mpdqn_create_config = dict(
+gym_hybrid_pdqn_create_config = dict(
     env=dict(
         type='gym_hybrid',
         import_names=['dizoo.gym_hybrid.envs.gym_hybrid_env'],
     ),
     env_manager=dict(type='subprocess'),
     policy=dict(type='pdqn'),
 )
-gym_hybrid_mpdqn_create_config = EasyDict(gym_hybrid_mpdqn_create_config)
-create_config = gym_hybrid_mpdqn_create_config
+gym_hybrid_pdqn_create_config = EasyDict(gym_hybrid_pdqn_create_config)
+create_config = gym_hybrid_pdqn_create_config
 
 if __name__ == "__main__":
-    # or you can enter `ding -m serial -c gym_hybrid_mpdqn_config.py -s 0`
+    # or you can enter `ding -m serial -c gym_hybrid_pdqn_config.py -s 0`
     from ding.entry import serial_pipeline
     serial_pipeline([main_config, create_config], seed=0, max_env_step=int(1e7))
```

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/config/gym_hybrid_pdqn_config.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/config/gym_hybrid_mpdqn_config.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 from easydict import EasyDict
 
-gym_hybrid_pdqn_config = dict(
-    exp_name='gym_hybrid_pdqn_seed0',
+gym_hybrid_mpdqn_config = dict(
+    exp_name='gym_hybrid_mpdqn_seed0',
     env=dict(
         collector_env_num=8,
         evaluator_env_num=5,
         # (bool) Scale output action into legal range [-1, 1].
         act_scale=True,
         env_id='Moving-v0',  # ['Sliding-v0', 'Moving-v0']
         n_evaluator_episode=5,
@@ -17,35 +17,32 @@
         nstep=1,
         model=dict(
             obs_shape=10,
             action_shape=dict(
                 action_type_shape=3,
                 action_args_shape=2,
             ),
+            multi_pass=True,
+            action_mask=[[1, 0], [0, 1], [0, 0]],
         ),
         learn=dict(
-            # (bool) Whether to use multi gpu
-            multi_gpu=False,
-            # How many updates(iterations) to train after collector's one collection.
-            # Bigger "update_per_collect" means bigger off-policy.
-            # collect data -> update policy-> collect data -> ...
             update_per_collect=500,  # 10~500
             batch_size=320,
             learning_rate_dis=3e-4,
             learning_rate_cont=3e-4,
             target_theta=0.001,
             update_circle=10,
         ),
         # collect_mode config
         collect=dict(
             # (int) Only one of [n_sample, n_episode] shoule be set
-            n_sample=3200,  # 128,
+            n_sample=3200,
             # (int) Cut trajectories into pieces with length "unroll_len".
             unroll_len=1,
-            noise_sigma=0.1,  # 0.05,
+            noise_sigma=0.1,
             collector=dict(collect_print_freq=1000, ),
         ),
         eval=dict(evaluator=dict(eval_freq=1000, ), ),
         # other config
         other=dict(
             # Epsilon greedy with decay.
             eps=dict(
@@ -57,25 +54,25 @@
                 decay=int(1e5),
             ),
             replay_buffer=dict(replay_buffer_size=int(1e6), ),
         ),
     )
 )
 
-gym_hybrid_pdqn_config = EasyDict(gym_hybrid_pdqn_config)
-main_config = gym_hybrid_pdqn_config
+gym_hybrid_mpdqn_config = EasyDict(gym_hybrid_mpdqn_config)
+main_config = gym_hybrid_mpdqn_config
 
-gym_hybrid_pdqn_create_config = dict(
+gym_hybrid_mpdqn_create_config = dict(
     env=dict(
         type='gym_hybrid',
         import_names=['dizoo.gym_hybrid.envs.gym_hybrid_env'],
     ),
     env_manager=dict(type='subprocess'),
     policy=dict(type='pdqn'),
 )
-gym_hybrid_pdqn_create_config = EasyDict(gym_hybrid_pdqn_create_config)
-create_config = gym_hybrid_pdqn_create_config
+gym_hybrid_mpdqn_create_config = EasyDict(gym_hybrid_mpdqn_create_config)
+create_config = gym_hybrid_mpdqn_create_config
 
 if __name__ == "__main__":
-    # or you can enter `ding -m serial -c gym_hybrid_pdqn_config.py -s 0`
+    # or you can enter `ding -m serial -c gym_hybrid_mpdqn_config.py -s 0`
     from ding.entry import serial_pipeline
     serial_pipeline([main_config, create_config], seed=0, max_env_step=int(1e7))
```

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/entry/e.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/entry/e.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_eval.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_main.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/entry/gym_hybrid_ddpg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/envs/gym_hybrid_env.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/envs/gym_hybrid_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_hybrid/envs/test_gym_hybrid_env.py` & `DI-engine-0.4.7/dizoo/gym_hybrid/envs/test_gym_hybrid_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/gym_pybullet_drones_env.py` & `DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/gym_pybullet_drones_env.py`

 * *Files 0% similar despite different names*

```diff
@@ -262,9 +262,9 @@
     def __repr__(self) -> str:
         return "DI-engine gym_pybullet_drones Env: " + self._cfg["env_id"]
 
     def plot_observation_curve(self) -> None:
         if self._cfg["plot_observation"]:
             self.observation_logger.plot()
 
-    def clone(self) -> 'GymPybulletDronesEnv':
+    def clone(self, caller: str) -> 'GymPybulletDronesEnv':
         return GymPybulletDronesEnv(self.raw_cfg)
```

### Comparing `DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/test_ding_env.py` & `DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/test_ding_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_pybullet_drones/envs/test_ori_env.py` & `DI-engine-0.4.7/dizoo/gym_pybullet_drones/envs/test_ori_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_soccer/envs/gym_soccer_env.py` & `DI-engine-0.4.7/dizoo/gym_soccer/envs/gym_soccer_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/gym_soccer/envs/test_gym_soccer_env.py` & `DI-engine-0.4.7/dizoo/gym_soccer/envs/test_gym_soccer_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/image_classification/data/dataset.py` & `DI-engine-0.4.7/dizoo/image_classification/data/dataset.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/image_classification/data/sampler.py` & `DI-engine-0.4.7/dizoo/image_classification/data/sampler.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/image_classification/policy/policy.py` & `DI-engine-0.4.7/dizoo/image_classification/policy/policy.py`

 * *Files 0% similar despite different names*

```diff
@@ -52,15 +52,15 @@
 
         with self._timer:
             self._optimizer.zero_grad()
             loss.backward()
         backward_time = self._timer.value
 
         with self._timer:
-            if self._cfg.learn.multi_gpu:
+            if self._cfg.multi_gpu:
                 self.sync_gradients(self._learn_model)
         sync_time = self._timer.value
         self._optimizer.step()
 
         cur_lr = [param_group['lr'] for param_group in self._optimizer.param_groups]
         cur_lr = sum(cur_lr) / len(cur_lr)
         return {
```

### Comparing `DI-engine-0.4.6/dizoo/league_demo/demo_league.py` & `DI-engine-0.4.7/dizoo/league_demo/demo_league.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/game_env.py` & `DI-engine-0.4.7/dizoo/league_demo/game_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/league_demo_collector.py` & `DI-engine-0.4.7/dizoo/league_demo/league_demo_collector.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/league_demo_ppo_config.py` & `DI-engine-0.4.7/dizoo/league_demo/league_demo_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/league_demo_ppo_main.py` & `DI-engine-0.4.7/dizoo/league_demo/league_demo_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/selfplay_demo_ppo_config.py` & `DI-engine-0.4.7/dizoo/league_demo/selfplay_demo_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/league_demo/selfplay_demo_ppo_main.py` & `DI-engine-0.4.7/dizoo/league_demo/selfplay_demo_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mario/mario_dqn_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/maze_ppo_config.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,49 +1,55 @@
 from easydict import EasyDict
 
-mario_dqn_config = dict(
-    exp_name='mario_dqn_seed0',
+maze_ppo_config = dict(
     env=dict(
-        collector_env_num=8,
-        evaluator_env_num=8,
-        n_evaluator_episode=8,
-        stop_value=100000,
-        replay_path='mario_dqn_seed0/video',
+        # frame_stack=4,
+        is_train=True,
+        env_id='maze',
+        collector_env_num=4,
+        evaluator_env_num=4,
+        n_evaluator_episode=4,
+        stop_value=10,
     ),
     policy=dict(
-        cuda=True,
+        cuda=False,
+        action_space='discrete',
         model=dict(
-            obs_shape=[4, 84, 84],
-            action_shape=2,
-            encoder_hidden_size_list=[128, 128, 256],
-            dueling=True,
+            obs_shape=[3, 64, 64],
+            action_shape=15,
+            action_space='discrete',
+            encoder_hidden_size_list=[32, 32, 64],
         ),
-        nstep=3,
-        discount_factor=0.99,
         learn=dict(
-            update_per_collect=10,
-            batch_size=32,
+            update_per_collect=5,
+            batch_size=64,
+            value_weight=0.5,
+            entropy_weight=0.01,
+            clip_ratio=0.2,
             learning_rate=0.0001,
-            target_update_freq=500,
         ),
-        collect=dict(n_sample=96, ),
-        eval=dict(evaluator=dict(eval_freq=2000, )),
+        collect=dict(n_sample=100, ),
+        eval=dict(evaluator=dict(eval_freq=5000, )),
         other=dict(
             eps=dict(
                 type='exp',
                 start=1.,
                 end=0.05,
                 decay=250000,
             ),
             replay_buffer=dict(replay_buffer_size=100000, ),
         ),
     ),
 )
-mario_dqn_config = EasyDict(mario_dqn_config)
-main_config = mario_dqn_config
-mario_dqn_create_config = dict(
-    env_manager=dict(type='subprocess'),
-    policy=dict(type='dqn'),
+maze_ppo_config = EasyDict(maze_ppo_config)
+main_config = maze_ppo_config
+
+maze_ppo_create_config = dict(
+    env=dict(
+        type='procgen',
+        import_names=['dizoo.procgen.envs.procgen_env'],
+    ),
+    env_manager=dict(type='subprocess', ),
+    policy=dict(type='ppo'),
 )
-mario_dqn_create_config = EasyDict(mario_dqn_create_config)
-create_config = mario_dqn_create_config
-# you can run `python3 -u mario_dqn_main.py`
+maze_ppo_create_config = EasyDict(maze_ppo_create_config)
+create_config = maze_ppo_create_config
```

### Comparing `DI-engine-0.4.6/dizoo/mario/mario_dqn_example.py` & `DI-engine-0.4.7/dizoo/mario/mario_dqn_example.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mario/mario_dqn_main.py` & `DI-engine-0.4.7/dizoo/mario/mario_dqn_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/metadrive/config/metadrive_onppo_config.py` & `DI-engine-0.4.7/dizoo/metadrive/config/metadrive_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/metadrive/config/metadrive_onppo_eval_config.py` & `DI-engine-0.4.7/dizoo/metadrive/config/metadrive_onppo_eval_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/metadrive/env/drive_env.py` & `DI-engine-0.4.7/dizoo/metadrive/env/drive_env.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,25 +1,23 @@
 import copy
+import gym
 import numpy as np
 from ditk import logging
 from typing import Union, Dict, AnyStr, Tuple, Optional
 from gym.envs.registration import register
 from metadrive.manager.traffic_manager import TrafficMode
 from metadrive.obs.top_down_obs_multi_channel import TopDownMultiChannel
-from metadrive.constants import RENDER_MODE_NONE, DEFAULT_AGENT, REPLAY_DONE
+from metadrive.constants import RENDER_MODE_NONE, DEFAULT_AGENT, REPLAY_DONE, TerminationState
 from metadrive.envs.base_env import BaseEnv
 from metadrive.component.map.base_map import BaseMap
 from metadrive.component.map.pg_map import parse_map_config, MapGenerateMethod
-from metadrive.manager.traffic_manager import TrafficMode
 from metadrive.component.pgblock.first_block import FirstPGBlock
-from metadrive.constants import DEFAULT_AGENT, TerminationState
 from metadrive.component.vehicle.base_vehicle import BaseVehicle
 from metadrive.utils import Config, merge_dicts, get_np_random, clip
 from metadrive.envs.base_env import BASE_DEFAULT_CONFIG
-from metadrive.obs.top_down_obs_multi_channel import TopDownMultiChannel
 from metadrive.component.road_network import Road
 from metadrive.component.algorithm.blocks_prob_dist import PGBlockDistConfig
 
 METADRIVE_DEFAULT_CONFIG = dict(
     # ===== Generalization =====
     start_seed=0,
     environment_num=10,
@@ -99,18 +97,42 @@
         config = super(MetaDrivePPOOriginEnv, cls).default_config()
         config.update(METADRIVE_DEFAULT_CONFIG)
         config.register_type("map", str, int)
         config["map_config"].register_type("config", None)
         return config
 
     def __init__(self, config: dict = None):
+        self.raw_cfg = config
         self.default_config_copy = Config(self.default_config(), unchangeable=True)
-        super(MetaDrivePPOOriginEnv, self).__init__(config)
-        self.start_seed = self.config["start_seed"]
-        self.env_num = self.config["environment_num"]
+        self.init_flag = False
+
+    @property
+    def observation_space(self):
+        return gym.spaces.Box(0, 1, shape=(84, 84, 5), dtype=np.float32)
+
+    @property
+    def action_space(self):
+        return gym.spaces.Box(-1, 1, shape=(2, ), dtype=np.float32)
+
+    @property
+    def reward_space(self):
+        return gym.spaces.Box(-100, 100, shape=(1, ), dtype=np.float32)
+
+    def seed(self, seed, dynamic_seed=False):
+        # TODO implement dynamic_seed mechanism
+        super().seed(seed)
+
+    def reset(self):
+        if not self.init_flag:
+            super(MetaDrivePPOOriginEnv, self).__init__(self.raw_cfg)
+            self.start_seed = self.config["start_seed"]
+            self.env_num = self.config["environment_num"]
+            self.init_flag = True
+        obs = super().reset()
+        return obs
 
     def _merge_extra_config(self, config: Union[dict, "Config"]) -> "Config":
         config = self.default_config().update(config, allow_add_new_key=False)
         if config["vehicle_config"]["lidar"]["distance"] > 50:
             config["max_distance"] = config["vehicle_config"]["lidar"]["distance"]
         return config
 
@@ -148,17 +170,14 @@
 
     def step(self, actions: Union[np.ndarray, Dict[AnyStr, np.ndarray]]):
         actions = self._preprocess_actions(actions)
         engine_info = self._step_simulator(actions)
         o, r, d, i = self._get_step_return(actions, engine_info=engine_info)
         return o, r, d, i
 
-    def _get_observations(self):
-        return {DEFAULT_AGENT: self.get_single_observation(self.config["vehicle_config"])}
-
     def cost_function(self, vehicle_id: str):
         vehicle = self.vehicles[vehicle_id]
         step_info = dict()
         step_info["cost"] = 0
         if self._is_out_of_road(vehicle):
             step_info["cost"] = self.config["out_of_road_cost"]
         elif vehicle.crash_vehicle:
@@ -314,16 +333,16 @@
             vehicle.navigation.get_current_lane_width() / 2 >= lat >=
             (0.5 - vehicle.navigation.get_current_lane_num()) * vehicle.navigation.get_current_lane_width()
         )
         return flag
 
     def _reset_global_seed(self, force_seed=None):
         """
-        Current seed is set to force seed if force_seed is not None. 
-        Otherwise, current seed is randomly generated. 
+        Current seed is set to force seed if force_seed is not None.
+        Otherwise, current seed is randomly generated.
         """
         current_seed = force_seed if force_seed is not None else \
             get_np_random(self._DEBUG_RANDOM_SEED).randint(self.start_seed, self.start_seed + self.env_num)
         self.seed(current_seed)
 
     def _get_observations(self):
         return {DEFAULT_AGENT: self.get_single_observation(self.config["vehicle_config"])}
@@ -335,7 +354,11 @@
             self.config["rgb_clip"],
             frame_stack=3,
             post_stack=10,
             frame_skip=1,
             resolution=(84, 84),
             max_distance=36,
         )
+
+    def clone(self, caller: str):
+        cfg = copy.deepcopy(self.raw_cfg)
+        return MetaDrivePPOOriginEnv(cfg)
```

### Comparing `DI-engine-0.4.6/dizoo/metadrive/env/drive_utils.py` & `DI-engine-0.4.7/dizoo/metadrive/env/drive_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import NoReturn, Optional, List
+from typing import Optional, List
 from gym import utils
 from abc import ABC, abstractmethod
 from typing import Any, Dict, Optional
 from easydict import EasyDict
 from itertools import product
 import gym
 import copy
```

### Comparing `DI-engine-0.4.6/dizoo/metadrive/env/drive_wrapper.py` & `DI-engine-0.4.7/dizoo/metadrive/env/drive_wrapper.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,17 +1,14 @@
 from typing import Any, Dict, Optional
 from easydict import EasyDict
-from itertools import product
-from typing import NoReturn, Optional, List
 import matplotlib.pyplot as plt
 import gym
 import copy
 import numpy as np
 from ding.envs.env.base_env import BaseEnvTimestep
-from ding.envs.common.env_element import EnvElementInfo
 from ding.torch_utils.data_helper import to_ndarray
 from ding.utils.default_helper import deep_merge_dicts
 from dizoo.metadrive.env.drive_utils import BaseDriveEnv
 
 
 def draw_multi_channels_top_down_observation(obs, show_time=0.5):
     num_channels = obs.shape[-1]
@@ -83,15 +80,15 @@
         obs = to_ndarray(obs, dtype=np.float32)
         if isinstance(obs, np.ndarray) and len(obs.shape) == 3:
             obs = obs.transpose((2, 0, 1))
         elif isinstance(obs, dict):
             vehicle_state = obs['vehicle_state']
             birdview = obs['birdview'].transpose((2, 0, 1))
             obs = {'vehicle_state': vehicle_state, 'birdview': birdview}
-        self._final_eval_reward = 0.0
+        self._eval_episode_return = 0.0
         self._arrive_dest = False
         return obs
 
     def step(self, action: Any = None) -> BaseEnvTimestep:
         """
         Overview:
             Wrapper of ``step`` method in env. This aims to convert the returns of ``gym.Env`` step method into
@@ -103,28 +100,31 @@
         Returns:
             - BaseEnvTimestep: DI-engine format of env step returns.
         """
         action = to_ndarray(action)
         obs, rew, done, info = self.env.step(action)
         if self.show_bird_view:
             draw_multi_channels_top_down_observation(obs, show_time=0.5)
-        self._final_eval_reward += rew
+        self._eval_episode_return += rew
         obs = to_ndarray(obs, dtype=np.float32)
         if isinstance(obs, np.ndarray) and len(obs.shape) == 3:
             obs = obs.transpose((2, 0, 1))
         elif isinstance(obs, dict):
             vehicle_state = obs['vehicle_state']
             birdview = obs['birdview'].transpose((2, 0, 1))
             obs = {'vehicle_state': vehicle_state, 'birdview': birdview}
         rew = to_ndarray([rew], dtype=np.float32)
         if done:
-            info['final_eval_reward'] = self._final_eval_reward
-            info['eval_episode_return'] = self._final_eval_reward
+            info['eval_episode_return'] = self._eval_episode_return
         return BaseEnvTimestep(obs, rew, done, info)
 
+    @property
+    def observation_space(self):
+        return gym.spaces.Box(0, 1, shape=(5, 84, 84), dtype=np.float32)
+
     def seed(self, seed: int, dynamic_seed: bool = True) -> None:
         self._seed = seed
         self._dynamic_seed = dynamic_seed
         np.random.seed(self._seed)
 
     def enable_save_replay(self, replay_path: Optional[str] = None) -> None:
         if replay_path is None:
@@ -139,7 +139,11 @@
         return copy.deepcopy(cfg)
 
     def __repr__(self) -> str:
         return repr(self.env)
 
     def render(self):
         self.env.render()
+
+    def clone(self, caller: str):
+        cfg = copy.deepcopy(self._cfg)
+        return DriveEnvWrapper(self.env.clone(caller), cfg)
```

### Comparing `DI-engine-0.4.6/dizoo/minigrid/__init__.py` & `DI-engine-0.4.7/dizoo/minigrid/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,13 +1,15 @@
-from gym.envs.registration import register
+from gymnasium.envs.registration import register
 
 register(id='MiniGrid-AKTDT-7x7-1-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_7x7_1')
 
 register(id='MiniGrid-AKTDT-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure')
 
 register(id='MiniGrid-AKTDT-13x13-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_13x13')
 
 register(id='MiniGrid-AKTDT-13x13-1-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_13x13_1')
 
 register(id='MiniGrid-AKTDT-19x19-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_19x19')
 
-register(id='MiniGrid-AKTDT-19x19-3-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_19x19_3')
+register(id='MiniGrid-AKTDT-19x19-3-v0', entry_point='dizoo.minigrid.envs:AppleKeyToDoorTreasure_19x19_3')
+
+register(id='MiniGrid-NoisyTV-v0', entry_point='dizoo.minigrid.envs:NoisyTVEnv')
```

### Comparing `DI-engine-0.4.6/dizoo/minigrid/envs/app_key_to_door_treasure.py` & `DI-engine-0.4.7/dizoo/minigrid/envs/app_key_to_door_treasure.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 #!/usr/bin/env python
 # -*- coding: utf-8 -*-
 
-from MiniGrid.minigrid.minigrid_env import *
-from MiniGrid.minigrid.utils.rendering import *
-from MiniGrid.minigrid.core.world_object import WorldObj
+from minigrid.minigrid_env import *
+from minigrid.utils.rendering import *
+from minigrid.core.world_object import WorldObj
 
 
 class Ball(WorldObj):
 
     def __init__(self, color='blue'):
         super(Ball, self).__init__('ball', color)
 
@@ -24,18 +24,19 @@
     Can specify agent and goal position, if not it set at random.
     """
 
     def __init__(self, agent_pos=None, goal_pos=None, grid_size=19, apple=2):
         self._agent_default_pos = agent_pos
         self._goal_default_pos = goal_pos
         self.apple = apple
-        super().__init__(grid_size=grid_size, max_steps=100)
+        mission_space = MissionSpace(mission_func=lambda: "Reach the goal")
+        super().__init__(mission_space=mission_space, grid_size=grid_size, max_steps=100)
 
     def _gen_grid(
-            self, width, height
+        self, width, height
     ):  # Note that it is inherited from MiniGridEnv that if width and height == None, width = grid_size , height = grid_size
         # Create the grid
         self.grid = Grid(width, height)
 
         # Generate the surrounding walls
         self.grid.horz_wall(0, 0)
         self.grid.horz_wall(0, height - 1)
@@ -180,16 +181,16 @@
         else:
             assert False, "unknown action"
 
         if self.step_count >= self.max_steps:
             done = True
 
         obs = self.gen_obs()
-
-        return obs, reward, done, {}
+        # return is (observation, reward, terminated, truncated, info)
+        return obs, reward, done, done, {}
 
 
 class AppleKeyToDoorTreasure_13x13(AppleKeyToDoorTreasure):
 
     def __init__(self):
         super().__init__(agent_pos=(2, 8), goal_pos=(7, 1), grid_size=13, apple=2)
```

### Comparing `DI-engine-0.4.6/dizoo/minigrid/envs/minigrid_env.py` & `DI-engine-0.4.7/dizoo/minigrid/envs/minigrid_env.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 from typing import Any, List, Union, Optional
 from collections import namedtuple
 from easydict import EasyDict
 import copy
 import os
 import time
-import gym
-import gymnasium
+import gymnasium as gym
 
 import numpy as np
 from matplotlib import animation
 import matplotlib.pyplot as plt
-from MiniGrid.minigrid.wrappers import FlatObsWrapper, RGBImgPartialObsWrapper, ImgObsWrapper, ViewSizeWrapper
+from minigrid.wrappers import FlatObsWrapper, RGBImgPartialObsWrapper, ImgObsWrapper
+from .minigrid_wrapper import ViewSizeWrapper
 from ding.envs import ObsPlusPrevActRewWrapper
 
 from ding.envs import BaseEnv, BaseEnvTimestep
 from ding.torch_utils import to_ndarray, to_list
 from ding.utils import ENV_REGISTRY
 
 
@@ -37,50 +37,55 @@
         self._env_id = cfg.env_id
         self._flat_obs = cfg.flat_obs
         self._save_replay = False
         self._max_step = cfg.max_step
 
     def reset(self) -> np.ndarray:
         if not self._init_flag:
-            self._env = gymnasium.make(self._env_id)  # using the Gymnasium make method
+            if self._save_replay:
+                self._env = gym.make(self._env_id, render_mode="rgb_array")  # using the Gymnasium make method
+            else:
+                self._env = gym.make(self._env_id)
 
             if self._env_id in ['MiniGrid-AKTDT-13x13-v0' or 'MiniGrid-AKTDT-13x13-1-v0']:
                 # customize the agent field of view size, note this must be an odd number
                 # This also related to the observation space, see gym_minigrid.wrappers for more details
                 self._env = ViewSizeWrapper(self._env, agent_view_size=5)
             if self._env_id == 'MiniGrid-AKTDT-7x7-1-v0':
                 self._env = ViewSizeWrapper(self._env, agent_view_size=3)
             if self._flat_obs:
                 self._env = FlatObsWrapper(self._env)
                 # self._env = RGBImgPartialObsWrapper(self._env)
                 # self._env = ImgObsWrapper(self._env)
             if hasattr(self._cfg, 'obs_plus_prev_action_reward') and self._cfg.obs_plus_prev_action_reward:
                 self._env = ObsPlusPrevActRewWrapper(self._env)
             self._init_flag = True
-        self._observation_space = self._env.observation_space
-        # to be compatiable with subprocess env manager
-        if isinstance(self._observation_space, gym.spaces.Dict):
-            self._observation_space['obs'].dtype = np.dtype('float32')
+        if self._flat_obs:
+            self._observation_space = gym.spaces.Box(0, 1, shape=(2835, ), dytpe=np.float32)
         else:
-            self._observation_space.dtype = np.dtype('float32')
+            self._observation_space = self._env.observation_space
+            # to be compatiable with subprocess env manager
+            if isinstance(self._observation_space, gym.spaces.Dict):
+                self._observation_space['obs'].dtype = np.dtype('float32')
+            else:
+                self._observation_space.dtype = np.dtype('float32')
         self._action_space = self._env.action_space
         self._reward_space = gym.spaces.Box(
             low=self._env.reward_range[0], high=self._env.reward_range[1], shape=(1, ), dtype=np.float32
         )
 
         self._eval_episode_return = 0
         if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:
             np_seed = 100 * np.random.randint(1, 1000)
             self._seed = self._seed + np_seed
             obs, _ = self._env.reset(seed=self._seed)  # using the reset method of Gymnasium env
         elif hasattr(self, '_seed'):
             obs, _ = self._env.reset(seed=self._seed)
         else:
             obs, _ = self._env.reset()
-
         obs = to_ndarray(obs)
         self._current_step = 0
         if self._save_replay:
             self._frames = []
 
         return obs
 
@@ -95,15 +100,15 @@
         np.random.seed(self._seed)
 
     def step(self, action: np.ndarray) -> BaseEnvTimestep:
         assert isinstance(action, np.ndarray), type(action)
         if action.shape == (1, ):
             action = action.squeeze()  # 0-dim array
         if self._save_replay:
-            self._frames.append(self._env.render(mode='rgb_array'))
+            self._frames.append(self._env.render())
         # using the step method of Gymnasium env, return is (observation, reward, terminated, truncated, info)
         obs, rew, done, _, info = self._env.step(action)
         rew = float(rew)
         self._eval_episode_return += rew
         self._current_step += 1
         if self._current_step >= self._max_step:
             done = True
```

### Comparing `DI-engine-0.4.6/dizoo/minigrid/envs/test_minigrid_env.py` & `DI-engine-0.4.7/dizoo/minigrid/envs/test_minigrid_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_ddpg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_gail_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_gail_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_ppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_td3_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_trex_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_trex_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/ant_trex_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/ant_trex_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_bco_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_bco_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_bdq_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_bdq_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_d4pg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_d4pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_ddpg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_gail_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_gail_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_gcl_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_gcl_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_sqil_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_sqil_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_td3_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_trex_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_trex_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/halfcheetah_trex_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/halfcheetah_trex_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_bco_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_bco_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_bdq_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_bdq_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_cql_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_cql_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_d4pg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_d4pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_ddpg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_gail_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_gail_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_gcl_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_gcl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_sac_data_generation_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_sac_data_generation_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_sqil_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_sqil_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_bc_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_bc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_td3_data_generation_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_td3_data_generation_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_trex_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_trex_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/hopper_trex_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/hopper_trex_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_d4pg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_d4pg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_ddpg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gail_ddpg_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gail_ddpg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gail_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gail_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_gcl_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_gcl_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_sqil_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_sqil_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_td3_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_td3_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_trex_onppo_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_trex_onppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/config/walker2d_trex_sac_config.py` & `DI-engine-0.4.7/dizoo/mujoco/config/walker2d_trex_sac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_cql_generation_main.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_cql_generation_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_d4pg_main.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_d4pg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ddpg_eval.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ddpg_eval.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ddpg_main.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ddpg_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_ppo_main.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/entry/mujoco_td3_bc_main.py` & `DI-engine-0.4.7/dizoo/mujoco/entry/mujoco_td3_bc_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_disc_env.py` & `DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_disc_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_env.py` & `DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_gym_env.py` & `DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_gym_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/mujoco/envs/mujoco_wrappers.py` & `DI-engine-0.4.7/dizoo/mujoco/envs/mujoco_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/coupled_half_cheetah.xml` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/coupled_half_cheetah.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_ant.xml` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_ant.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_ant__stage1.xml` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_ant__stage1.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer__bckp2.xml` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer__bckp2.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer_bckp.xml` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/assets/manyagent_swimmer_bckp.xml`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/coupled_half_cheetah.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/coupled_half_cheetah.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/manyagent_ant.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/manyagent_ant.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/manyagent_swimmer.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/manyagent_swimmer.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/mujoco_multi.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/mujoco_multi.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/multi_mujoco_env.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/multi_mujoco_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/multiagentenv.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/multiagentenv.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/multiagent_mujoco/envs/obsk.py` & `DI-engine-0.4.7/dizoo/multiagent_mujoco/envs/obsk.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/overcooked/config/overcooked_ppo_config.py` & `DI-engine-0.4.7/dizoo/overcooked/config/overcooked_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/overcooked/envs/overcooked_env.py` & `DI-engine-0.4.7/dizoo/overcooked/envs/overcooked_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/overcooked/envs/test_overcooked_env.py` & `DI-engine-0.4.7/dizoo/overcooked/envs/test_overcooked_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/__init__.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/__init__.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_atoc_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_atoc_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_collaq_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_collaq_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_coma_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_coma_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_madqn_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_madqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_mappo_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_mappo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_masac_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_masac_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_qmix_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_qmix_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_qtran_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_qtran_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_vdn_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_vdn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/config/ptz_simple_spread_wqmix_config.py` & `DI-engine-0.4.7/dizoo/petting_zoo/config/ptz_simple_spread_wqmix_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/envs/petting_zoo_simple_spread_env.py` & `DI-engine-0.4.7/dizoo/petting_zoo/envs/petting_zoo_simple_spread_env.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,17 +1,20 @@
 from typing import Any, List, Union, Optional, Dict
-import gym
+import gymnasium as gym
 import numpy as np
 import pettingzoo
 from functools import reduce
 
 from ding.envs import BaseEnv, BaseEnvTimestep, FrameStackWrapper
 from ding.torch_utils import to_ndarray, to_list
 from ding.envs.common.common_function import affine_transform
 from ding.utils import ENV_REGISTRY, import_module
+from pettingzoo.utils.conversions import parallel_wrapper_fn
+from pettingzoo.mpe._mpe_utils.simple_env import SimpleEnv, make_env
+from pettingzoo.mpe.simple_spread.simple_spread import Scenario
 
 
 @ENV_REGISTRY.register('petting_zoo')
 class PettingZooEnv(BaseEnv):
     # Now only supports simple_spread_v2.
     # All agents' observations should have the same shape.
 
@@ -48,32 +51,33 @@
             self._env = parallel_env(
                 N=self._cfg.n_agent, continuous_actions=self._continuous_actions, max_cycles=self._max_cycles
             )
         # dynamic seed reduces training speed greatly
         # if hasattr(self, '_seed') and hasattr(self, '_dynamic_seed') and self._dynamic_seed:
         #     np_seed = 100 * np.random.randint(1, 1000)
         #     self._env.seed(self._seed + np_seed)
-        if hasattr(self, '_seed'):
-            self._env.seed(self._seed)
         if self._replay_path is not None:
             self._env = gym.wrappers.Monitor(
                 self._env, self._replay_path, video_callable=lambda episode_id: True, force=True
             )
-        obs = self._env.reset()
+        if hasattr(self, '_seed'):
+            obs = self._env.reset(seed=self._seed)
+        else:
+            obs = self._env.reset()
         if not self._init_flag:
             self._agents = self._env.agents
 
             self._action_space = gym.spaces.Dict({agent: self._env.action_space(agent) for agent in self._agents})
             single_agent_obs_space = self._env.action_space(self._agents[0])
             if isinstance(single_agent_obs_space, gym.spaces.Box):
                 self._action_dim = single_agent_obs_space.shape
             elif isinstance(single_agent_obs_space, gym.spaces.Discrete):
                 self._action_dim = (single_agent_obs_space.n, )
             else:
-                raise Exception('Only support `Box` or `Discrte` obs space for single agent.')
+                raise Exception('Only support `Box` or `Discrete` obs space for single agent.')
 
             # only for env 'simple_spread_v2', n_agent = 5
             # now only for the case that each agent in the team have the same obs structure and corresponding shape.
             if not self._cfg.agent_obs_only:
                 self._observation_space = gym.spaces.Dict(
                     {
                         'agent_state': gym.spaces.Box(
@@ -169,15 +173,15 @@
                 # print(action[agent])
                 # print(self.action_space[agent])
                 # print(self.action_space[agent].low, self.action_space[agent].high)
                 action[agent] = affine_transform(
                     action[agent], min_val=self.action_space[agent].low, max_val=self.action_space[agent].high
                 )
 
-        obs, rew, done, info = self._env.step(action)
+        obs, rew, done, trunc, info = self._env.step(action)
         obs_n = self._process_obs(obs)
         rew_n = np.array([sum([rew[agent] for agent in self._agents])])
         # collide_sum = 0
         # for i in range(self._num_agents):
         #     collide_sum += info['n'][i][1]
         # collide_penalty = self._cfg.get('collide_penal', self._num_agent)
         # rew_n += collide_sum * (1.0 - collide_penalty)
@@ -304,26 +308,21 @@
         return self._action_space
 
     @property
     def reward_space(self) -> gym.spaces.Space:
         return self._reward_space
 
 
-from pettingzoo.utils.conversions import parallel_wrapper_fn
-from pettingzoo.mpe._mpe_utils.simple_env import SimpleEnv, make_env
-from pettingzoo.mpe.scenarios.simple_spread import Scenario
-
-
 class simple_spread_raw_env(SimpleEnv):
 
     def __init__(self, N=3, local_ratio=0.5, max_cycles=25, continuous_actions=False):
         assert 0. <= local_ratio <= 1., "local_ratio is a proportion. Must be between 0 and 1."
         scenario = Scenario()
         world = scenario.make_world(N)
-        super().__init__(scenario, world, max_cycles, continuous_actions, local_ratio)
+        super().__init__(scenario, world, max_cycles, continuous_actions=continuous_actions, local_ratio=local_ratio)
         self.metadata['name'] = "simple_spread_v2"
 
     def _execute_world_step(self):
         # set action for each agent
         for i, agent in enumerate(self.world.agents):
             action = self.current_actions[i]
             scenario_action = []
```

### Comparing `DI-engine-0.4.6/dizoo/petting_zoo/envs/test_petting_zoo_simple_spread_env.py` & `DI-engine-0.4.7/dizoo/petting_zoo/envs/test_petting_zoo_simple_spread_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/pomdp/envs/atari_env.py` & `DI-engine-0.4.7/dizoo/pomdp/envs/atari_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/pomdp/envs/atari_wrappers.py` & `DI-engine-0.4.7/dizoo/pomdp/envs/atari_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/pomdp/envs/test_atari_env.py` & `DI-engine-0.4.7/dizoo/pomdp/envs/test_atari_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/bigfish_plr_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/bigfish_plr_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/bigfish_ppg_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/bigfish_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/coinrun_dqn_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/coinrun_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/coinrun_ppg_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/coinrun_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/coinrun_ppo_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/coinrun_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/maze_dqn_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/maze_dqn_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/maze_ppg_config.py` & `DI-engine-0.4.7/dizoo/procgen/config/maze_ppg_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/config/maze_ppo_config.py` & `DI-engine-0.4.7/dizoo/classic_control/cartpole/config/cartpole_mdqn_config.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,55 +1,58 @@
 from easydict import EasyDict
 
-maze_ppo_config = dict(
+cartpole_mdqn_config = dict(
+    exp_name='cartpole_mdqn_seed0',
     env=dict(
-        # frame_stack=4,
-        is_train=True,
-        env_id='maze',
-        collector_env_num=4,
-        evaluator_env_num=4,
-        n_evaluator_episode=4,
-        stop_value=10,
+        collector_env_num=8,
+        evaluator_env_num=5,
+        n_evaluator_episode=5,
+        stop_value=195,
     ),
     policy=dict(
         cuda=False,
-        action_space='discrete',
         model=dict(
-            obs_shape=[3, 64, 64],
-            action_shape=15,
-            action_space='discrete',
-            encoder_hidden_size_list=[32, 32, 64],
+            obs_shape=4,
+            action_shape=2,
+            encoder_hidden_size_list=[128, 128, 64],
+            dueling=True,
         ),
+        nstep=1,
+        discount_factor=0.97,
+        entropy_tau=0.03,
+        m_alpha=0.9,
         learn=dict(
             update_per_collect=5,
             batch_size=64,
-            value_weight=0.5,
-            entropy_weight=0.01,
-            clip_ratio=0.2,
-            learning_rate=0.0001,
+            learning_rate=0.001,
         ),
-        collect=dict(n_sample=100, ),
-        eval=dict(evaluator=dict(eval_freq=5000, )),
+        collect=dict(n_sample=8),
+        eval=dict(evaluator=dict(eval_freq=40, )),
         other=dict(
             eps=dict(
                 type='exp',
-                start=1.,
-                end=0.05,
-                decay=250000,
+                start=0.95,
+                end=0.1,
+                decay=10000,
             ),
-            replay_buffer=dict(replay_buffer_size=100000, ),
+            replay_buffer=dict(replay_buffer_size=20000, ),
         ),
     ),
 )
-maze_ppo_config = EasyDict(maze_ppo_config)
-main_config = maze_ppo_config
-
-maze_ppo_create_config = dict(
+cartpole_mdqn_config = EasyDict(cartpole_mdqn_config)
+main_config = cartpole_mdqn_config
+cartpole_mdqn_create_config = dict(
     env=dict(
-        type='procgen',
-        import_names=['dizoo.procgen.envs.procgen_env'],
+        type='cartpole',
+        import_names=['dizoo.classic_control.cartpole.envs.cartpole_env'],
     ),
-    env_manager=dict(type='subprocess', ),
-    policy=dict(type='ppo'),
+    env_manager=dict(type='base'),
+    policy=dict(type='mdqn'),
+    replay_buffer=dict(type='deque', import_names=['ding.data.buffer.deque_buffer_wrapper']),
 )
-maze_ppo_create_config = EasyDict(maze_ppo_create_config)
-create_config = maze_ppo_create_config
+cartpole_mdqn_create_config = EasyDict(cartpole_mdqn_create_config)
+create_config = cartpole_mdqn_create_config
+
+if __name__ == "__main__":
+    # or you can enter `ding -m serial -c cartpole_mdqn_config.py -s 0`
+    from ding.entry import serial_pipeline
+    serial_pipeline((main_config, create_config), seed=0, dynamic_seed=False)
```

### Comparing `DI-engine-0.4.6/dizoo/procgen/envs/procgen_env.py` & `DI-engine-0.4.7/dizoo/procgen/envs/procgen_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/procgen/envs/test_coinrun_env.py` & `DI-engine-0.4.7/dizoo/procgen/envs/test_coinrun_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/pybullet/envs/pybullet_env.py` & `DI-engine-0.4.7/dizoo/pybullet/envs/pybullet_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/pybullet/envs/pybullet_wrappers.py` & `DI-engine-0.4.7/dizoo/pybullet/envs/pybullet_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/config/rocket_hover_ppo_config.py` & `DI-engine-0.4.7/dizoo/rocket/config/rocket_hover_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/config/rocket_landing_ppo_config.py` & `DI-engine-0.4.7/dizoo/rocket/config/rocket_landing_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/entry/rocket_hover_onppo_main_v2.py` & `DI-engine-0.4.7/dizoo/rocket/entry/rocket_hover_onppo_main_v2.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/entry/rocket_hover_ppo_main.py` & `DI-engine-0.4.7/dizoo/rocket/entry/rocket_hover_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/entry/rocket_landing_onppo_main_v2.py` & `DI-engine-0.4.7/dizoo/rocket/entry/rocket_landing_onppo_main_v2.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/entry/rocket_landing_ppo_main.py` & `DI-engine-0.4.7/dizoo/rocket/entry/rocket_landing_ppo_main.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/rocket/envs/rocket_env.py` & `DI-engine-0.4.7/dizoo/rocket/envs/rocket_env.py`

 * *Files 1% similar despite different names*

```diff
@@ -84,15 +84,15 @@
         self._save_replay_count = 0
 
     def random_action(self) -> np.ndarray:
         random_action = self.action_space.sample()
         random_action = to_ndarray([random_action], dtype=np.int64)
         return random_action
 
-    def clone(self) -> 'RocketEnv':
+    def clone(self, caller: str) -> 'RocketEnv':
         return RocketEnv(copy.deepcopy(self._cfg))
 
     @property
     def observation_space(self) -> gym.spaces.Space:
         return self._observation_space
 
     @property
```

### Comparing `DI-engine-0.4.6/dizoo/rocket/envs/test_rocket_env.py` & `DI-engine-0.4.7/dizoo/rocket/envs/test_rocket_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/slime_volley/config/slime_volley_ppo_config.py` & `DI-engine-0.4.7/dizoo/slime_volley/config/slime_volley_ppo_config.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/slime_volley/envs/slime_volley_env.py` & `DI-engine-0.4.7/dizoo/slime_volley/envs/slime_volley_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/slime_volley/envs/test_slime_volley_env.py` & `DI-engine-0.4.7/dizoo/slime_volley/envs/test_slime_volley_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/fake_smac_env.py` & `DI-engine-0.4.7/dizoo/smac/envs/fake_smac_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/10m_vs_11m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/10m_vs_11m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/1c3s5z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/1c3s5z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/25m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/25m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/27m_vs_30m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/27m_vs_30m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2c_vs_64zg.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2c_vs_64zg.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2m_vs_1z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2m_vs_1z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2s3z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2s3z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/2s_vs_1sc.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/2s_vs_1sc.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s5z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s5z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s5z_vs_3s6z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s5z_vs_3s6z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_3z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_3z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_4z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_4z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_5z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/3s_vs_5z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/5m_vs_6m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/5m_vs_6m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/6h_vs_8z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/6h_vs_8z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/8m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/8m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/8m_vs_9m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/8m_vs_9m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/MMM.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/MMM.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/MMM2.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/MMM2.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/bane_vs_bane.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/bane_vs_bane.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/corridor.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/corridor.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/infestor_viper.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/infestor_viper.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps/so_many_baneling.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps/so_many_baneling.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/3m.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/3m.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/maps/SMAC_Maps_two_player/3s5z.SC2Map` & `DI-engine-0.4.7/dizoo/smac/envs/maps/SMAC_Maps_two_player/3s5z.SC2Map`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/smac_action.py` & `DI-engine-0.4.7/dizoo/smac/envs/smac_action.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/smac_env.py` & `DI-engine-0.4.7/dizoo/smac/envs/smac_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/smac_map.py` & `DI-engine-0.4.7/dizoo/smac/envs/smac_map.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/smac_reward.py` & `DI-engine-0.4.7/dizoo/smac/envs/smac_reward.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/smac/envs/test_smac_env.py` & `DI-engine-0.4.7/dizoo/smac/envs/test_smac_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/sokoban/envs/sokoban_env.py` & `DI-engine-0.4.7/dizoo/sokoban/envs/sokoban_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/sokoban/envs/sokoban_wrappers.py` & `DI-engine-0.4.7/dizoo/sokoban/envs/sokoban_wrappers.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/dizoo/sokoban/envs/test_sokoban_env.py` & `DI-engine-0.4.7/dizoo/sokoban/envs/test_sokoban_env.py`

 * *Files identical despite different names*

### Comparing `DI-engine-0.4.6/setup.py` & `DI-engine-0.4.7/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -74,24 +74,25 @@
         'readerwriterlock',
         'enum_tools',
         'trueskill',
         'h5py',
         'mpire>=2.3.5',
         'pynng',
         'redis',
-        'pettingzoo==1.12.0',
+        'pettingzoo',
         'DI-treetensor>=0.3.0',
         'DI-toolkit>=0.0.2',
         'hbutils>=0.5.0',
         'wandb',
         'matplotlib',
         'MarkupSafe==2.0.1',  # compatibility
         'h5py',
         'scikit-learn',
         'hickle',
+        'gymnasium',
     ],
     extras_require={
         'test': [
             'gym[box2d]>=0.25.0',
             'opencv-python',  # pypy incompatible
             'coverage>=5,<=7.0.1',
             'mock>=4.0.3',
@@ -129,15 +130,15 @@
         'procgen_env': [
             'procgen',
         ],
         'bsuite_env': [
             'bsuite',
         ],
         'minigrid_env': [
-            'minigrid',
+            'minigrid>=2.0.0',
         ],
         # 'd4rl_env': [
         #     'd4rl @ git+https://github.com/rail-berkeley/d4rl@master#egg=d4rl',
         # ],
         # 'pybulletgym_env': [
         #     'pybulletgym @ git+https://github.com/benelot/pybullet-gym@master#egg=pybulletgym',
         # ],
```

