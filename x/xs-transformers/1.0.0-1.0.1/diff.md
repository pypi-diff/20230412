# Comparing `tmp/xs_transformers-1.0.0.tar.gz` & `tmp/xs_transformers-1.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "xs_transformers-1.0.0.tar", last modified: Wed Apr 12 02:41:21 2023, max compression
+gzip compressed data, was "xs_transformers-1.0.1.tar", last modified: Wed Apr 12 02:56:23 2023, max compression
```

## Comparing `xs_transformers-1.0.0.tar` & `xs_transformers-1.0.1.tar`

### file list

```diff
@@ -1,69 +1,1110 @@
-drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:41:21.747653 xs_transformers-1.0.0/
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      187 2023-04-12 02:41:21.747653 xs_transformers-1.0.0/PKG-INFO
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)       38 2023-04-12 02:41:21.747653 xs_transformers-1.0.0/setup.cfg
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      247 2023-04-12 01:50:54.000000 xs_transformers-1.0.0/setup.py
-drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:41:21.747653 xs_transformers-1.0.0/xs_transformers/
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   223999 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/__init__.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6812 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/activations.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4335 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/activations_tf.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    49246 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/configuration_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21413 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/convert_graph_to_onnx.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17091 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/convert_pytorch_checkpoint_to_tf2.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47424 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/convert_slow_tokenizer.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5351 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/convert_slow_tokenizers_checkpoints_to_fast.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2998 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13077 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/debug_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16614 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/deepspeed.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1785 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/dependency_versions_check.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2850 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/dependency_versions_table.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19277 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/dynamic_module_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18793 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/feature_extraction_sequence_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    26074 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/feature_extraction_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3801 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/file_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19715 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_beam_constraints.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    43352 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_beam_search.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11299 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_flax_logits_process.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46784 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_flax_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34568 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_logits_process.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5643 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_stopping_criteria.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    29756 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_tf_logits_process.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   192102 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_tf_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   224569 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/generation_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14912 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/hf_argparser.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2176 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/image_processing_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17004 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/image_transforms.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20927 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/image_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    60864 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/integrations.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21307 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/keras_callbacks.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35838 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modelcard.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    38967 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_flax_outputs.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15499 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_flax_pytorch_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    54610 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_flax_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    63666 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_outputs.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    54164 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_tf_outputs.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21613 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_tf_pytorch_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   153344 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_tf_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   157825 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/modeling_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    28631 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/optimization.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16751 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/optimization_tf.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10814 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/processing_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10634 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/pytorch_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    57749 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/testing_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2644 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/tf_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41317 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/tokenization_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   182439 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/tokenization_utils_base.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34018 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/tokenization_utils_fast.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   184287 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25027 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer_callback.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    48255 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer_pt_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12489 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer_seq2seq.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36701 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer_tf.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    24120 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/trainer_utils.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    88830 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/training_args.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2945 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/training_args_seq2seq.py
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14200 2023-04-12 01:07:21.000000 xs_transformers-1.0.0/xs_transformers/training_args_tf.py
-drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:41:21.747653 xs_transformers-1.0.0/xs_transformers.egg-info/
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      187 2023-04-12 02:41:21.000000 xs_transformers-1.0.0/xs_transformers.egg-info/PKG-INFO
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2452 2023-04-12 02:41:21.000000 xs_transformers-1.0.0/xs_transformers.egg-info/SOURCES.txt
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        1 2023-04-12 02:41:21.000000 xs_transformers-1.0.0/xs_transformers.egg-info/dependency_links.txt
--rw-rw-r--   0 kyungho   (1000) kyungho   (1000)       16 2023-04-12 02:41:21.000000 xs_transformers-1.0.0/xs_transformers.egg-info/top_level.txt
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.181664 xs_transformers-1.0.1/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      187 2023-04-12 02:56:23.181664 xs_transformers-1.0.1/PKG-INFO
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)       38 2023-04-12 02:56:23.181664 xs_transformers-1.0.1/setup.cfg
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      251 2023-04-12 02:55:47.000000 xs_transformers-1.0.1/setup.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/benchmark/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11267 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3869 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark_args.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4755 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark_args_tf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6716 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark_args_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13741 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark_tf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39796 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/benchmark/benchmark_utils.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/commands/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      923 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11510 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/add_new_model.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    67456 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/add_new_model_like.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8368 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/convert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1988 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/download.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3332 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/env.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8107 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/lfs.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20383 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/pt_to_tf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4526 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/run.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8268 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/serving.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6817 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/train.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2061 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/transformers_cli.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7338 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/commands/user.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/data/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1594 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80055 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/data_collator.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/data/datasets/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1080 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/datasets/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6279 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/datasets/glue.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25227 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/datasets/language_modeling.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9762 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/datasets/squad.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/data/metrics/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3802 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/metrics/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30447 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/metrics/squad_metrics.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/data/processors/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1252 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/processors/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    24301 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/processors/glue.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34373 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/processors/squad.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14839 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/processors/utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3570 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/processors/xnli.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3973 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/data/test_generation_utils.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.129664 xs_transformers-1.0.1/xs_transformers/models/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2940 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/__init__.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/albert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5730 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8624 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/configuration_albert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2267 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    63398 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/modeling_albert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    42291 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/modeling_flax_albert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    70841 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/modeling_tf_albert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14924 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/tokenization_albert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10853 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/albert/tokenization_albert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/auto/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13769 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36965 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/auto_factory.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36163 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/configuration_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18145 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/feature_extraction_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    45920 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/modeling_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12825 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/modeling_flax_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    24002 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/modeling_tf_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14297 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/processing_auto.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41058 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/auto/tokenization_auto.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bart/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4493 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20155 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/configuration_bart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5937 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/convert_bart_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    91229 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/modeling_bart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    85211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/modeling_flax_bart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72550 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/modeling_tf_bart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18588 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/tokenization_bart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13902 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bart/tokenization_bart_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/barthez/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2053 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/barthez/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12748 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/barthez/tokenization_barthez.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8997 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/barthez/tokenization_barthez_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bartpho/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1563 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bartpho/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14460 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bartpho/tokenization_bartpho.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/beit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3409 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9732 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/configuration_beit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17990 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/convert_beit_unilm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12935 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/feature_extraction_beit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    56492 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/modeling_beit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    38582 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/beit/modeling_flax_beit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6305 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9982 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/configuration_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10590 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/convert_bert_original_tf2_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2243 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/convert_bert_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4413 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/convert_bert_pytorch_checkpoint_to_original_tf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7923 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    86721 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/modeling_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65584 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/modeling_flax_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    97899 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/modeling_tf_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    24840 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/tokenization_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14895 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/tokenization_bert_fast.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11492 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert/tokenization_bert_tf.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bert_generation/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2480 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_generation/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6119 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_generation/configuration_bert_generation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    49293 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_generation/modeling_bert_generation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6780 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_generation/tokenization_bert_generation.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bert_japanese/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1307 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_japanese/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    40631 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bert_japanese/tokenization_bert_japanese.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bertweet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1143 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bertweet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    27439 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bertweet/tokenization_bertweet.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/big_bird/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4822 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8225 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/configuration_big_bird.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2627 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/convert_bigbird_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   147804 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/modeling_big_bird.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   111277 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/modeling_flax_big_bird.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12818 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/tokenization_big_bird.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11854 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/big_bird/tokenization_big_bird_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/bigbird_pegasus/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2500 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bigbird_pegasus/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20919 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6534 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bigbird_pegasus/convert_bigbird_pegasus_tf_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   152367 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.133664 xs_transformers-1.0.1/xs_transformers/models/blenderbot/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4215 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20532 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/configuration_blenderbot.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3786 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/convert_blenderbot_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    78832 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/modeling_blenderbot.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66972 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/modeling_flax_blenderbot.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73193 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/modeling_tf_blenderbot.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20116 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/tokenization_blenderbot.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14351 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot/tokenization_blenderbot_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4461 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19817 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/configuration_blenderbot_small.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    77585 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/modeling_blenderbot_small.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    67930 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72382 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/modeling_tf_blenderbot_small.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8896 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/tokenization_blenderbot_small.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4056 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/blenderbot_small/tokenization_blenderbot_small_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/bloom/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2768 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bloom/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11095 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bloom/configuration_bloom.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11212 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    56770 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bloom/modeling_bloom.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7307 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bloom/tokenization_bloom_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/bort/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bort/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14281 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/bort/convert_bort_original_gluonnlp_checkpoint_to_pytorch.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/byt5/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1126 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/byt5/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2205 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/byt5/convert_byt5_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11078 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/byt5/tokenization_byt5.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/camembert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4691 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2240 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/configuration_camembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    75897 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/modeling_camembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    85166 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/modeling_tf_camembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13217 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/tokenization_camembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8755 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/camembert/tokenization_camembert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/canine/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2477 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/canine/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6627 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/canine/configuration_canine.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2137 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/canine/convert_canine_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    75154 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/canine/modeling_canine.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9641 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/canine/tokenization_canine.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/clip/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4913 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17611 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/configuration_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5405 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/convert_clip_original_pytorch_to_hf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7855 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/feature_extraction_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47849 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/modeling_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47953 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/modeling_flax_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59525 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/modeling_tf_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5473 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/processing_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21168 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/tokenization_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7055 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/clip/tokenization_clip_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/codegen/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2712 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/codegen/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10835 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/codegen/configuration_codegen.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    32409 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/codegen/modeling_codegen.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/codegen/tokenization_codegen.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10611 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/codegen/tokenization_codegen_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2864 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11286 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/configuration_conditional_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18201 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55431 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/feature_extraction_conditional_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   131896 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/conditional_detr/modeling_conditional_detr.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/convbert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4317 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7395 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/configuration_convbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2214 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/convert_convbert_original_tf1_checkpoint_to_pytorch_and_tf2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    60862 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/modeling_convbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59531 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/modeling_tf_convbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21632 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/tokenization_convbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8789 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convbert/tokenization_convbert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/convnext/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3273 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5108 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/configuration_convnext.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10659 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/convert_convnext_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7894 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/feature_extraction_convnext.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19687 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/modeling_convnext.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25760 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/convnext/modeling_tf_convnext.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/cpm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2021 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cpm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15442 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cpm/tokenization_cpm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10878 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cpm/tokenization_cpm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.137664 xs_transformers-1.0.1/xs_transformers/models/ctrl/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2893 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ctrl/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5344 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ctrl/configuration_ctrl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35700 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ctrl/modeling_ctrl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39041 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ctrl/modeling_tf_ctrl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8665 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ctrl/tokenization_ctrl.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/cvt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2645 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cvt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6858 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cvt/configuration_cvt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14004 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30070 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cvt/modeling_cvt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    38514 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/cvt/modeling_tf_cvt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/data2vec/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5186 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16303 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/configuration_data2vec_audio.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7310 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/configuration_data2vec_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9795 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/configuration_data2vec_vision.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11424 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/convert_data2vec_audio_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10034 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/convert_data2vec_text_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16381 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/convert_data2vec_vision_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    68710 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/modeling_data2vec_audio.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74920 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/modeling_data2vec_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55371 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/modeling_data2vec_vision.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65892 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/data2vec/modeling_tf_data2vec_vision.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/deberta/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3925 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9518 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/configuration_deberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    62000 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/modeling_deberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65812 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/modeling_tf_deberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20426 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/tokenization_deberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13523 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta/tokenization_deberta_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4114 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8841 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/configuration_deberta_v2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    71840 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/modeling_deberta_v2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72651 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/modeling_tf_deberta_v2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21223 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/tokenization_deberta_v2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11082 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deberta_v2/tokenization_deberta_v2_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/decision_transformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2345 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/decision_transformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7921 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/decision_transformer/configuration_decision_transformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44944 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/decision_transformer/modeling_decision_transformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2687 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11740 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/configuration_deformable_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10437 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36834 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/feature_extraction_deformable_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1635 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/load_custom.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   122778 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deformable_detr/modeling_deformable_detr.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/deit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3606 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6052 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/configuration_deit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10006 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/convert_deit_timm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7472 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/feature_extraction_deit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39364 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/modeling_deit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47650 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/deit/modeling_tf_deit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/detr/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2576 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/detr/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10942 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/detr/configuration_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14931 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65922 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/detr/feature_extraction_detr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   115101 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/detr/modeling_detr.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/dialogpt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dialogpt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1535 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dialogpt/convert_dialogpt_original_pytorch_checkpoint_to_pytorch.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/distilbert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5351 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6976 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/configuration_distilbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    51379 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/modeling_distilbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34062 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/modeling_flax_distilbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    50179 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/modeling_tf_distilbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3045 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/tokenization_distilbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4162 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/distilbert/tokenization_distilbert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/dit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10397 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dit/convert_dit_unilm_to_pytorch.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/donut/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2570 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6176 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/configuration_donut_swin.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9519 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/convert_donut_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9345 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/feature_extraction_donut.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44245 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/modeling_donut_swin.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6972 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/donut/processing_donut.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/dpr/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4719 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6786 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/configuration_dpr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6391 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/convert_dpr_original_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    29271 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/modeling_dpr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34349 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/modeling_tf_dpr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20252 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/tokenization_dpr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20719 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpr/tokenization_dpr_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.141664 xs_transformers-1.0.1/xs_transformers/models/dpt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2525 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8414 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpt/configuration_dpt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12458 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpt/convert_dpt_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10916 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpt/feature_extraction_dpt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    45412 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/dpt/modeling_dpt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/electra/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5505 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9925 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/configuration_electra.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3025 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/convert_electra_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    78373 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/modeling_electra.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    64788 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/modeling_flax_electra.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    79315 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/modeling_tf_electra.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22413 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/tokenization_electra.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10519 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/electra/tokenization_electra_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2635 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4907 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/configuration_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36423 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/modeling_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    45055 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    37859 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/ernie/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2600 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ernie/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8716 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ernie/configuration_ernie.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    86718 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ernie/modeling_ernie.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/esm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3206 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15145 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/configuration_esm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17632 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/convert_esm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55575 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/modeling_esm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    88918 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/modeling_esmfold.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65024 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/modeling_tf_esm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      398 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13871 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/chunk_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3687 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/data_transforms.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7707 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/feats.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3728 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/loss.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11356 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/protein.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39432 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/residue_constants.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41261 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/rigid_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3710 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/tensor_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5697 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/esm/tokenization_esm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/flaubert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3757 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flaubert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11704 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flaubert/configuration_flaubert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59549 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flaubert/modeling_flaubert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59161 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flaubert/modeling_tf_flaubert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    24258 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flaubert/tokenization_flaubert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/flava/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3097 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30906 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/configuration_flava.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3561 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/convert_dalle_to_flava_codebook.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4650 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/convert_flava_original_pytorch_to_hf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17456 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/feature_extraction_flava.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    99841 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/modeling_flava.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5417 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/flava/processing_flava.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/fnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3369 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5895 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/configuration_fnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7446 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/convert_fnet_original_flax_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    50939 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/modeling_fnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13451 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/tokenization_fnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8596 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fnet/tokenization_fnet_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/fsmt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1939 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fsmt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10454 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fsmt/configuration_fsmt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11462 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55813 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fsmt/modeling_fsmt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20435 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/fsmt/tokenization_fsmt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/funnel/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4310 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9530 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/configuration_funnel.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2468 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/convert_funnel_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72826 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/modeling_funnel.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    77549 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/modeling_tf_funnel.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    23845 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/tokenization_funnel.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11879 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/funnel/tokenization_funnel_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.145664 xs_transformers-1.0.1/xs_transformers/models/glpn/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2498 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6215 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/configuration_glpn.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8981 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/convert_glpn_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      898 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/feature_extraction_glpn.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9095 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/image_processing_glpn.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    32776 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/glpn/modeling_glpn.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/gpt2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4428 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12393 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/configuration_gpt2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2616 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/convert_gpt2_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    33307 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/modeling_flax_gpt2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    71130 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/modeling_gpt2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    58322 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/modeling_tf_gpt2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14929 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/tokenization_gpt2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8104 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt2/tokenization_gpt2_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2876 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12089 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/configuration_gpt_neo.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2674 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/convert_gpt_neo_mesh_tf_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    29306 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/modeling_flax_gpt_neo.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41052 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neo/modeling_gpt_neo.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/gpt_neox/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5790 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox/configuration_gpt_neox.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    32293 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox/modeling_gpt_neox.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5846 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox/tokenization_gpt_neox_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/gpt_neox_japanese/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2386 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox_japanese/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5945 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    33650 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17650 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/gptj/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3583 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gptj/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9507 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gptj/configuration_gptj.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    29626 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gptj/modeling_flax_gptj.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    49207 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gptj/modeling_gptj.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    49373 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/gptj/modeling_tf_gptj.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/groupvit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3080 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/groupvit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18032 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/groupvit/configuration_groupvit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10288 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    70754 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/groupvit/modeling_groupvit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    85616 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/groupvit/modeling_tf_groupvit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/herbert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1673 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/herbert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25436 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/herbert/tokenization_herbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6634 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/herbert/tokenization_herbert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/hubert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2747 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14627 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/configuration_hubert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9380 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/convert_distilhubert_original_s3prl_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11248 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3139 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/convert_hubert_original_s3prl_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    60014 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/modeling_hubert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73455 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/hubert/modeling_tf_hubert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/ibert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2340 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ibert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7529 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ibert/configuration_ibert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59371 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ibert/modeling_ibert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30870 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/ibert/quant_modules.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/imagegpt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2777 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/imagegpt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8865 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/imagegpt/configuration_imagegpt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2745 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/imagegpt/convert_imagegpt_original_tf2_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7630 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/imagegpt/feature_extraction_imagegpt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55796 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/imagegpt/modeling_imagegpt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/layoutlm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4035 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8625 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/configuration_layoutlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    62407 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/modeling_layoutlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72030 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/modeling_tf_layoutlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21460 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/tokenization_layoutlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8991 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlm/tokenization_layoutlm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3561 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11269 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/configuration_layoutlmv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10786 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/feature_extraction_layoutlmv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    63729 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/modeling_layoutlmv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7961 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/processing_layoutlmv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73585 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/tokenization_layoutlmv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39513 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.149664 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4538 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13528 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/configuration_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11623 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/feature_extraction_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    62960 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/modeling_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73850 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7793 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/processing_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74924 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/tokenization_layoutlmv3.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41659 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/layoutxlm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2221 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutxlm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7944 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutxlm/processing_layoutxlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59055 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutxlm/tokenization_layoutxlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41256 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/layoutxlm/tokenization_layoutxlm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/led/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3272 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7631 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/configuration_led.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   143563 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/modeling_led.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   123588 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/modeling_tf_led.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21127 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/tokenization_led.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15075 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/led/tokenization_led_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/levit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2645 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/levit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5987 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/levit/configuration_levit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6387 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/levit/convert_levit_timm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7133 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/levit/feature_extraction_levit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    31068 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/levit/modeling_levit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/lilt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2093 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lilt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7158 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lilt/configuration_lilt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55728 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lilt/modeling_lilt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/longformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4380 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10711 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/configuration_longformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3112 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/convert_longformer_original_pytorch_lightning_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   118215 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/modeling_longformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   132927 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/modeling_tf_longformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19348 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/tokenization_longformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14625 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longformer/tokenization_longformer_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/longt5/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2815 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longt5/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8629 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longt5/configuration_longt5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12551 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longt5/convert_longt5x_checkpoint_to_flax.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   108736 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longt5/modeling_flax_longt5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   107779 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/longt5/modeling_longt5.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/luke/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2567 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/luke/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6610 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/luke/configuration_luke.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7961 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/luke/convert_luke_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   107032 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/luke/modeling_luke.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73066 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/luke/tokenization_luke.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/lxmert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3580 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9556 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/configuration_lxmert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2193 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/convert_lxmert_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66449 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/modeling_lxmert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66796 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/modeling_tf_lxmert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21183 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/tokenization_lxmert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8461 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/lxmert/tokenization_lxmert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/m2m_100/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2261 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/m2m_100/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14037 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/m2m_100/configuration_m2m_100.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/m2m_100/convert_m2m100_original_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    67548 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/m2m_100/modeling_m2m_100.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17076 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/m2m_100/tokenization_m2m_100.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/marian/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3852 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19831 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/configuration_marian.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    37317 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/convert_marian_tatoeba_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    27476 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/convert_marian_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66300 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/modeling_flax_marian.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    84984 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/modeling_marian.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    73060 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/modeling_tf_marian.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17391 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/marian/tokenization_marian.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/markuplm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3096 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7854 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/configuration_markuplm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6560 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/feature_extraction_markuplm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59421 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/modeling_markuplm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6365 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/processing_markuplm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    71909 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/tokenization_markuplm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44254 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/markuplm/tokenization_markuplm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.153664 xs_transformers-1.0.1/xs_transformers/models/maskformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2572 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/maskformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8940 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/maskformer/configuration_maskformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34655 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/maskformer/convert_maskformer_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44245 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/maskformer/feature_extraction_maskformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   127592 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/maskformer/modeling_maskformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mbart/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4706 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19527 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/configuration_mbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3190 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/convert_mbart_original_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    77211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/modeling_flax_mbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    91348 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/modeling_mbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74045 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/modeling_tf_mbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15229 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/tokenization_mbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12433 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart/tokenization_mbart_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mbart50/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2052 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart50/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16535 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart50/tokenization_mbart50.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12662 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mbart50/tokenization_mbart50_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mctct/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2508 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mctct/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9364 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mctct/configuration_mctct.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16721 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mctct/feature_extraction_mctct.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35539 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mctct/modeling_mctct.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5901 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mctct/processing_mctct.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/megatron_bert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2738 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_bert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6405 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_bert/configuration_megatron_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13962 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    86059 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_bert/modeling_megatron_bert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/megatron_gpt2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      801 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_gpt2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    37262 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13744 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mluke/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1557 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mluke/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10711 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    84922 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mluke/tokenization_mluke.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mmbt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1694 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mmbt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1604 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mmbt/configuration_mmbt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19483 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mmbt/modeling_mmbt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mobilebert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4788 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8584 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/configuration_mobilebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2301 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/convert_mobilebert_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72994 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/modeling_mobilebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80648 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/modeling_tf_mobilebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21080 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/tokenization_mobilebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8415 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilebert/tokenization_mobilebert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mobilevit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3827 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8450 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/configuration_mobilevit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13417 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/convert_mlcvnets_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9237 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/feature_extraction_mobilevit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41796 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/modeling_mobilevit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    48828 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mobilevit/modeling_tf_mobilevit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mpnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4059 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5500 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/configuration_mpnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44489 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/modeling_mpnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55832 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/modeling_tf_mpnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22686 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/tokenization_mpnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8991 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mpnet/tokenization_mpnet_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mt5/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mt5/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7681 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mt5/configuration_mt5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4237 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mt5/modeling_flax_mt5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3960 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mt5/modeling_mt5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3364 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mt5/modeling_tf_mt5.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/mvp/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2805 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mvp/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mvp/configuration_mvp.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    97444 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mvp/modeling_mvp.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17430 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mvp/tokenization_mvp.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12178 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/mvp/tokenization_mvp_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/nezha/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2475 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nezha/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5117 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nezha/configuration_nezha.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    77767 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nezha/modeling_nezha.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/nllb/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2052 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nllb/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19841 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nllb/tokenization_nllb.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16831 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nllb/tokenization_nllb_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/nystromformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2627 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nystromformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6679 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nystromformer/configuration_nystromformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4332 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    51001 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/nystromformer/modeling_nystromformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.157664 xs_transformers-1.0.1/xs_transformers/models/openai/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3890 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7614 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/configuration_openai.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2720 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/convert_openai_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39049 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/modeling_openai.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    42632 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/modeling_tf_openai.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15416 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/tokenization_openai.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3240 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/openai/tokenization_openai_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/opt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3247 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6691 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/configuration_opt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3069 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/convert_opt_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    32331 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/modeling_flax_opt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    58193 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/modeling_opt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47943 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/opt/modeling_tf_opt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/owlvit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2957 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17717 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/configuration_owlvit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14289 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/convert_owlvit_original_flax_to_hf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10582 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/feature_extraction_owlvit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    64313 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/modeling_owlvit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8738 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/owlvit/processing_owlvit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/pegasus/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4375 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7936 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/configuration_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5557 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/convert_pegasus_tf_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    68012 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/modeling_flax_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    83806 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/modeling_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74135 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/modeling_tf_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13548 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/tokenization_pegasus.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10144 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus/tokenization_pegasus_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/pegasus_x/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2097 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus_x/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8664 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus_x/configuration_pegasus_x.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    81580 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/pegasus_x/modeling_pegasus_x.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/perceiver/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3387 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12293 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/configuration_perceiver.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22597 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/convert_perceiver_haiku_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8692 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/feature_extraction_perceiver.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   150319 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/modeling_perceiver.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9421 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/perceiver/tokenization_perceiver.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/phobert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1139 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/phobert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13755 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/phobert/tokenization_phobert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/plbart/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2619 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/plbart/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8764 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/plbart/configuration_plbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3753 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/plbart/convert_plbart_original_checkpoint_to_torch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    84716 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/plbart/modeling_plbart.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21161 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/plbart/tokenization_plbart.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/poolformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2608 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/poolformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5269 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/poolformer/configuration_poolformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8243 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/poolformer/convert_poolformer_original_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7970 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/poolformer/feature_extraction_poolformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18931 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/poolformer/modeling_poolformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/prophetnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2389 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/prophetnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9060 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/prophetnet/configuration_prophetnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7690 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/prophetnet/convert_prophetnet_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   118238 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/prophetnet/modeling_prophetnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21584 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/prophetnet/tokenization_prophetnet.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/qdqbert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2617 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/qdqbert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5819 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/qdqbert/configuration_qdqbert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80300 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/qdqbert/modeling_qdqbert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/rag/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2692 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8921 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/configuration_rag.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    94490 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/modeling_rag.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    94918 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/modeling_tf_rag.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    29275 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/retrieval_rag.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4677 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rag/tokenization_rag.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/realm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2880 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8992 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/configuration_realm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    86312 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/modeling_realm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6878 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/retrieval_realm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25859 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/tokenization_realm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14657 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/realm/tokenization_realm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/reformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3377 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13485 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/configuration_reformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7994 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/convert_reformer_trax_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   121300 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/modeling_reformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6955 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/tokenization_reformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4863 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/reformer/tokenization_reformer_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.161664 xs_transformers-1.0.1/xs_transformers/models/regnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2738 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4117 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/configuration_regnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19683 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/convert_regnet_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18486 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/modeling_regnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22705 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/regnet/modeling_tf_regnet.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/rembert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4683 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6700 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/configuration_rembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2292 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/convert_rembert_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    70319 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/modeling_rembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80526 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/modeling_tf_rembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10673 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/tokenization_rembert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10673 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/rembert/tokenization_rembert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/resnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2840 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/resnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4582 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/resnet/configuration_resnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7585 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/resnet/convert_resnet_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17180 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/resnet/modeling_resnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21799 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/resnet/modeling_tf_resnet.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/retribert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2603 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/retribert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5403 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/retribert/configuration_retribert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9763 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/retribert/modeling_retribert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22346 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/retribert/tokenization_retribert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9039 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/retribert/tokenization_retribert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/roberta/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5339 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3544 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/configuration_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8412 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/convert_roberta_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    58861 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/modeling_flax_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74599 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/modeling_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    83579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/modeling_tf_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18572 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/tokenization_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13773 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roberta/tokenization_roberta_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/roformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5581 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7543 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/configuration_roformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2355 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/convert_roformer_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    40650 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/modeling_flax_roformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    71709 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/modeling_roformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    65493 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/modeling_tf_roformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    23607 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/tokenization_roformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8335 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/tokenization_roformer_fast.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2666 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/roformer/tokenization_utils.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/segformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3770 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7727 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/configuration_segformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20674 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/convert_segformer_original_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12127 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/feature_extraction_segformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    36587 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/modeling_segformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39866 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/segformer/modeling_tf_segformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/sew/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1968 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14106 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew/configuration_sew.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13539 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew/convert_sew_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    54197 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew/modeling_sew.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/sew_d/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1994 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew_d/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16343 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew_d/configuration_sew_d.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14369 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew_d/convert_sew_d_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    75992 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/sew_d/modeling_sew_d.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2262 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5155 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15585 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/convert_mbart_wav2vec2_seq2seq_original_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12504 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/convert_speech_to_text_wav2vec2_seq2seq_original_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46364 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    33425 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4202 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9556 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/configuration_speech_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4575 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/convert_s2t_fairseq_to_tfms.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11847 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/feature_extraction_speech_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    67868 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/modeling_speech_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72043 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/modeling_tf_speech_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4790 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/processing_speech_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11375 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text/tokenization_speech_to_text.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2398 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6797 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/configuration_speech_to_text_2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46938 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4762 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/processing_speech_to_text_2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9387 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/speech_to_text_2/tokenization_speech_to_text_2.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/splinter/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2785 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/splinter/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6118 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/splinter/configuration_splinter.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    55609 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/splinter/modeling_splinter.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22397 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/splinter/tokenization_splinter.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9729 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/splinter/tokenization_splinter_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.165664 xs_transformers-1.0.1/xs_transformers/models/squeezebert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3201 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/squeezebert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7908 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/squeezebert/configuration_squeezebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46706 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/squeezebert/modeling_squeezebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21651 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/squeezebert/tokenization_squeezebert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9421 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/squeezebert/tokenization_squeezebert_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/swin/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2965 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swin/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7054 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swin/configuration_swin.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5965 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swin/convert_swin_timm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    56420 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swin/modeling_swin.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    69119 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swin/modeling_tf_swin.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/swinv2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2088 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swinv2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6526 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swinv2/configuration_swinv2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7886 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    62223 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/swinv2/modeling_swinv2.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/t5/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4491 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7555 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/configuration_t5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2205 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/convert_t5_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11164 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/convert_t5x_checkpoint_to_flax.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    76095 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/modeling_flax_t5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    88105 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/modeling_t5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    79412 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/modeling_tf_t5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15240 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/tokenization_t5.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10721 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/t5/tokenization_t5_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/table_transformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2246 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/table_transformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11288 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/table_transformer/configuration_table_transformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16751 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/table_transformer/convert_table_transformer_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    95224 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/table_transformer/modeling_table_transformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/tapas/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3157 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12898 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/configuration_tapas.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5184 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/convert_tapas_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   113718 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/modeling_tapas.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   112009 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/modeling_tf_tapas.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   123775 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapas/tokenization_tapas.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/tapex/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1151 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapex/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66566 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/tapex/tokenization_tapex.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/time_series_transformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2290 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/time_series_transformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11622 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/time_series_transformer/configuration_time_series_transformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    94928 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/time_series_transformer/modeling_time_series_transformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/trajectory_transformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2297 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trajectory_transformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7927 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trajectory_transformer/configuration_trajectory_transformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3312 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trajectory_transformer/convert_trajectory_transformer_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    27105 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trajectory_transformer/modeling_trajectory_transformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3435 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7916 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/configuration_transfo_xl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5003 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/convert_transfo_xl_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    48939 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_tf_transfo_xl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7973 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_tf_transfo_xl_utilities.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    57872 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_transfo_xl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11232 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_transfo_xl_utilities.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    31367 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/transfo_xl/tokenization_transfo_xl.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/trocr/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2051 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trocr/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6974 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trocr/configuration_trocr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11588 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trocr/convert_trocr_unilm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    48651 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trocr/modeling_trocr.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4531 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/trocr/processing_trocr.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/unispeech/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2256 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17028 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech/configuration_unispeech.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech/convert_unispeech_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72829 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech/modeling_unispeech.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2499 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18451 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/configuration_unispeech_sat.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5183 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/convert_unispeech_original_s3prl_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9953 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/convert_unispeech_sat_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    86796 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/unispeech_sat/modeling_unispeech_sat.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/van/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1975 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/van/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4833 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/van/configuration_van.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10534 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/van/convert_van_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22476 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/van/modeling_van.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/videomae/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2622 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/videomae/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6716 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/videomae/configuration_videomae.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13325 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/videomae/convert_videomae_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7988 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/videomae/feature_extraction_videomae.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    45312 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/videomae/modeling_videomae.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.169664 xs_transformers-1.0.1/xs_transformers/models/vilt/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2902 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6961 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/configuration_vilt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14090 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/convert_vilt_original_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12937 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/feature_extraction_vilt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    67396 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/modeling_vilt.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4603 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vilt/processing_vilt.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2887 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9090 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    42844 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39949 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35665 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2472 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5418 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/configuration_vision_text_dual_encoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    27576 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    25610 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5586 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/visual_bert/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2473 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/visual_bert/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7998 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/visual_bert/configuration_visual_bert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5242 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/visual_bert/convert_visual_bert_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    71837 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/visual_bert/modeling_visual_bert.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/vit/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3820 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5926 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/configuration_vit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9644 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/convert_dino_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10950 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/convert_vit_timm_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6714 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/feature_extraction_vit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    26211 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/modeling_flax_vit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35302 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/modeling_tf_vit.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35813 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit/modeling_vit.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/vit_mae/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2692 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_mae/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6600 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_mae/configuration_vit_mae.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8333 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_mae/convert_vit_mae_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    51155 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_mae/modeling_tf_vit_mae.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    44867 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_mae/modeling_vit_mae.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/vit_msn/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1998 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_msn/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5060 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_msn/configuration_vit_msn.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10751 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_msn/convert_msn_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30976 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/vit_msn/modeling_vit_msn.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4275 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    19663 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/configuration_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11984 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5151 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/convert_wav2vec2_original_s3prl_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11566 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/feature_extraction_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59679 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/modeling_flax_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    74201 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/modeling_tf_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    96035 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/modeling_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7169 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/processing_wav2vec2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39905 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2/tokenization_wav2vec2.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_conformer/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2559 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_conformer/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20786 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_conformer/configuration_wav2vec2_conformer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14182 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_conformer/convert_wav2vec2_conformer_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    99325 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_phoneme/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1177 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_phoneme/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    26633 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_with_lm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1165 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_with_lm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    27285 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/wavlm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2149 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wavlm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18891 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wavlm/configuration_wavlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9036 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wavlm/convert_wavlm_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5127 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wavlm/convert_wavlm_original_s3prl_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80329 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/wavlm/modeling_wavlm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/whisper/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3198 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12952 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/configuration_whisper.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    23313 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/english_normalizer.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13679 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/feature_extraction_whisper.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    66926 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/modeling_tf_whisper.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    59827 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/modeling_whisper.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5746 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/processing_whisper.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17556 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/whisper/tokenization_whisper.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/x_clip/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2237 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/x_clip/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    17928 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/x_clip/configuration_x_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    18904 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    69486 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/x_clip/modeling_x_clip.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5592 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/x_clip/processing_x_clip.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.173664 xs_transformers-1.0.1/xs_transformers/models/xglm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4208 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6077 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/configuration_xglm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2391 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/convert_xglm_original_ckpt_to_trfms.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34748 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/modeling_flax_xglm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46231 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/modeling_tf_xglm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    47065 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/modeling_xglm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13497 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/tokenization_xglm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8128 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xglm/tokenization_xglm_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/xlm/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3561 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11973 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/configuration_xlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3092 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/convert_xlm_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    58421 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/modeling_tf_xlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    56503 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/modeling_xlm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    35215 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm/tokenization_xlm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/xlm_prophetnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2868 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_prophetnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9123 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_prophetnet/configuration_xlm_prophetnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   122492 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14119 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_prophetnet/tokenization_xlm_prophetnet.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5579 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8236 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/configuration_xlm_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5802 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7045 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/modeling_tf_xlm_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    76249 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/modeling_xlm_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14473 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/tokenization_xlm_roberta.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10361 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta/tokenization_xlm_roberta_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta_xl/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2589 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta_xl/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7651 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     8646 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta_xl/convert_xlm_roberta_xl_original_pytorch_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    72886 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/xlnet/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4478 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11253 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/configuration_xlnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3766 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/convert_xlnet_original_tf_checkpoint_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    80719 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/modeling_tf_xlnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    95373 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/modeling_xlnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14691 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/tokenization_xlnet.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10136 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/xlnet/tokenization_xlnet_fast.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/yolos/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2537 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yolos/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7847 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yolos/configuration_yolos.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12336 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yolos/convert_yolos_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    34692 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yolos/feature_extraction_yolos.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    61204 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yolos/modeling_yolos.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/models/yoso/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2322 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yoso/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6958 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yoso/configuration_yoso.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4274 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    57563 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/models/yoso/modeling_yoso.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/onnx/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1576 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7231 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/__main__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    33614 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/config.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22808 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/convert.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    28010 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/features.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3643 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/onnx/utils.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/pipelines/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    41316 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5972 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/audio_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7952 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/audio_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21763 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/automatic_speech_recognition.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    50687 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/base.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13797 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/conversational.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     3850 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/depth_estimation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    22696 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/document_question_answering.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4239 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/feature_extraction.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10210 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/fill_mask.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4454 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/image_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7566 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/image_segmentation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4554 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/image_to_text.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5465 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/object_detection.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    12402 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/pt_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    30408 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/question_answering.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    20352 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/table_question_answering.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    16777 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/text2text_generation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    10077 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/text_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13851 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/text_generation.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    21815 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/token_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5137 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/visual_question_answering.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    11118 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/zero_shot_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5235 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/zero_shot_image_classification.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    13576 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/pipelines/zero_shot_object_detection.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.177664 xs_transformers-1.0.1/xs_transformers/sagemaker/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      901 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/sagemaker/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1043 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/sagemaker/trainer_sm.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5547 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/sagemaker/training_args_sm.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.181664 xs_transformers-1.0.1/xs_transformers/utils/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     6086 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/__init__.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     7629 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/bitsandbytes.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      172 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/constants.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39284 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/doc.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      390 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_detectron2_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    26807 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_flax_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)   140866 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_pt_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     1137 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_scatter_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      342 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_sentencepiece_and_speech_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      300 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_sentencepiece_and_tokenizers_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4889 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_sentencepiece_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      482 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_speech_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      321 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_tensorflow_text_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    61117 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_tf_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2969 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_timm_and_vision_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9910 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_tokenizers_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5258 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/dummy_vision_objects.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    46496 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/fx.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    14542 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/generic.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     5061 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/hp_naming.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    48418 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/hub.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    39864 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/import_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     9659 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/logging.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     2302 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/model_parallel_utils.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    15195 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/notebook.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    50691 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/sentencepiece_model_pb2.py
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)     4555 2023-04-12 01:07:21.000000 xs_transformers-1.0.1/xs_transformers/utils/versions.py
+drwxrwxr-x   0 kyungho   (1000) kyungho   (1000)        0 2023-04-12 02:56:23.181664 xs_transformers-1.0.1/xs_transformers/xs_transformers.egg-info/
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)      187 2023-04-12 02:56:23.000000 xs_transformers-1.0.1/xs_transformers/xs_transformers.egg-info/PKG-INFO
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)    52032 2023-04-12 02:56:23.000000 xs_transformers-1.0.1/xs_transformers/xs_transformers.egg-info/SOURCES.txt
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)        1 2023-04-12 02:56:23.000000 xs_transformers-1.0.1/xs_transformers/xs_transformers.egg-info/dependency_links.txt
+-rw-rw-r--   0 kyungho   (1000) kyungho   (1000)       62 2023-04-12 02:56:23.000000 xs_transformers-1.0.1/xs_transformers/xs_transformers.egg-info/top_level.txt
```

### Comparing `xs_transformers-1.0.0/xs_transformers/configuration_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_tf_transfo_xl.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,1008 +1,1297 @@
 # coding=utf-8
-# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.
+# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.
 # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-""" Configuration base class and utilities."""
-
-
-import copy
-import json
-import os
-import re
-import warnings
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-from packaging import version
-
-from . import __version__
-from .dynamic_module_utils import custom_object_save
-from .utils import (
-    CONFIG_NAME,
-    PushToHubMixin,
-    cached_file,
-    copy_func,
-    download_url,
-    extract_commit_hash,
-    is_remote_url,
-    is_torch_available,
+"""
+ TF 2.0 Transformer XL model.
+"""
+
+from dataclasses import dataclass
+from typing import List, Optional, Tuple, Union
+
+import numpy as np
+import tensorflow as tf
+
+from ...modeling_tf_utils import (
+    TFModelInputType,
+    TFPreTrainedModel,
+    TFSequenceClassificationLoss,
+    get_initializer,
+    keras_serializable,
+    unpack_inputs,
+)
+from ...tf_utils import shape_list, stable_softmax
+from ...utils import (
+    ModelOutput,
+    add_code_sample_docstrings,
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
     logging,
 )
+from .configuration_transfo_xl import TransfoXLConfig
+from .modeling_tf_transfo_xl_utilities import TFAdaptiveSoftmaxMask
 
 logger = logging.get_logger(__name__)
 
-_re_configuration_file = re.compile(r"config\.(.*)\.json")
+_CHECKPOINT_FOR_DOC = "transfo-xl-wt103"
+_CONFIG_FOR_DOC = "TransfoXLConfig"
+_TOKENIZER_FOR_DOC = "TransfoXLTokenizer"
 
+TF_TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "transfo-xl-wt103",
+    # See all Transformer XL models at https://huggingface.co/models?filter=transfo-xl
+]
 
-class PretrainedConfig(PushToHubMixin):
-    r"""
-    Base class for all configuration classes. Handles a few parameters common to all models' configurations as well as
-    methods for loading/downloading/saving configurations.
 
-    <Tip>
+class TFPositionalEmbedding(tf.keras.layers.Layer):
+    def __init__(self, demb, **kwargs):
+        super().__init__(**kwargs)
 
-    A configuration file can be loaded and saved to disk. Loading the configuration file and using this file to
-    initialize a model does **not** load the model weights. It only affects the model's configuration.
+        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))
 
-    </Tip>
+    def call(self, pos_seq, bsz=None):
+        self.inv_freq = tf.cast(self.inv_freq, dtype=pos_seq.dtype)
+        sinusoid_inp = tf.einsum("i,j->ij", pos_seq, self.inv_freq)
+        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)
 
-    Class attributes (overridden by derived classes):
+        if bsz is not None:
+            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])
+        else:
+            return pos_emb[:, None, :]
 
-    - **model_type** (`str`) -- An identifier for the model type, serialized into the JSON file, and used to recreate
-      the correct object in [`~transformers.AutoConfig`].
-    - **is_composition** (`bool`) -- Whether the config class is composed of multiple sub-configs. In this case the
-      config has to be initialized from two or more configs of type [`~transformers.PretrainedConfig`] like:
-      [`~transformers.EncoderDecoderConfig`] or [`~RagConfig`].
-    - **keys_to_ignore_at_inference** (`List[str]`) -- A list of keys to ignore by default when looking at dictionary
-      outputs of the model during inference.
-    - **attribute_map** (`Dict[str, str]`) -- A dict that maps model specific attribute names to the standardized
-      naming of attributes.
-
-    Common attributes (present in all subclasses):
-
-    - **vocab_size** (`int`) -- The number of tokens in the vocabulary, which is also the first dimension of the
-      embeddings matrix (this attribute may be missing for models that don't have a text modality like ViT).
-    - **hidden_size** (`int`) -- The hidden size of the model.
-    - **num_attention_heads** (`int`) -- The number of attention heads used in the multi-head attention layers of the
-      model.
-    - **num_hidden_layers** (`int`) -- The number of blocks in the model.
-
-    Arg:
-        name_or_path (`str`, *optional*, defaults to `""`):
-            Store the string that was passed to [`PreTrainedModel.from_pretrained`] or
-            [`TFPreTrainedModel.from_pretrained`] as `pretrained_model_name_or_path` if the configuration was created
-            with such a method.
-        output_hidden_states (`bool`, *optional*, defaults to `False`):
-            Whether or not the model should return all hidden-states.
-        output_attentions (`bool`, *optional*, defaults to `False`):
-            Whether or not the model should returns all attentions.
-        return_dict (`bool`, *optional*, defaults to `True`):
-            Whether or not the model should return a [`~transformers.utils.ModelOutput`] instead of a plain tuple.
-        is_encoder_decoder (`bool`, *optional*, defaults to `False`):
-            Whether the model is used as an encoder/decoder or not.
-        is_decoder (`bool`, *optional*, defaults to `False`):
-            Whether the model is used as decoder or not (in which case it's used as an encoder).
-        cross_attention_hidden_size** (`bool`, *optional*):
-            The hidden size of the cross-attention layer in case the model is used as a decoder in an encoder-decoder
-            setting and the cross-attention hidden dimension differs from `self.config.hidden_size`.
-        add_cross_attention (`bool`, *optional*, defaults to `False`):
-            Whether cross-attention layers should be added to the model. Note, this option is only relevant for models
-            that can be used as decoder models within the [`EncoderDecoderModel`] class, which consists of all models
-            in `AUTO_MODELS_FOR_CAUSAL_LM`.
-        tie_encoder_decoder (`bool`, *optional*, defaults to `False`):
-            Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder
-            and decoder model to have the exact same parameter names.
-        prune_heads (`Dict[int, List[int]]`, *optional*, defaults to `{}`):
-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of
-            heads to prune in said layer.
-
-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.
-        chunk_size_feed_forward (`int`, *optional*, defaults to `0`):
-            The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that
-            the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <
-            sequence_length embeddings at a time. For more information on feed forward chunking, see [How does Feed
-            Forward Chunking work?](../glossary.html#feed-forward-chunking).
-
-        > Parameters for sequence generation
-
-        max_length (`int`, *optional*, defaults to 20):
-            Maximum length that will be used by default in the `generate` method of the model.
-        min_length (`int`, *optional*, defaults to 10):
-            Minimum length that will be used by default in the `generate` method of the model.
-        do_sample (`bool`, *optional*, defaults to `False`):
-            Flag that will be used by default in the `generate` method of the model. Whether or not to use sampling ;
-            use greedy decoding otherwise.
-        early_stopping (`bool`, *optional*, defaults to `False`):
-            Flag that will be used by default in the `generate` method of the model. Whether to stop the beam search
-            when at least `num_beams` sentences are finished per batch or not.
-        num_beams (`int`, *optional*, defaults to 1):
-            Number of beams for beam search that will be used by default in the `generate` method of the model. 1 means
-            no beam search.
-        num_beam_groups (`int`, *optional*, defaults to 1):
-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams
-            that will be used by default in the `generate` method of the model. 1 means no group beam search.
-        diversity_penalty (`float`, *optional*, defaults to 0.0):
-            Value to control diversity for group beam search. that will be used by default in the `generate` method of
-            the model. 0 means no diversity penalty. The higher the penalty, the more diverse are the outputs.
-        temperature (`float`, *optional*, defaults to 1):
-            The value used to module the next token probabilities that will be used by default in the `generate` method
-            of the model. Must be strictly positive.
-        top_k (`int`, *optional*, defaults to 50):
-            Number of highest probability vocabulary tokens to keep for top-k-filtering that will be used by default in
-            the `generate` method of the model.
-        top_p (`float`, *optional*, defaults to 1):
-            Value that will be used by default in the `generate` method of the model for `top_p`. If set to float < 1,
-            only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation.
-        repetition_penalty (`float`, *optional*, defaults to 1):
-            Parameter for repetition penalty that will be used by default in the `generate` method of the model. 1.0
-            means no penalty.
-        length_penalty (`float`, *optional*, defaults to 1):
-            Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to
-            the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log
-            likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while
-            `length_penalty` < 0.0 encourages shorter sequences.
-        no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by default in the
-            `generate` method of the model for `no_repeat_ngram_size`. If set to int > 0, all ngrams of that size can
-            only occur once.
-        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0) -- Value that will be used by
-            default in the `generate` method of the model for `encoder_no_repeat_ngram_size`. If set to int > 0, all
-            ngrams of that size that occur in the `encoder_input_ids` cannot occur in the `decoder_input_ids`.
-        bad_words_ids (`List[int]`, *optional*):
-            List of token ids that are not allowed to be generated that will be used by default in the `generate`
-            method of the model. In order to get the tokens of the words that should not appear in the generated text,
-            use `tokenizer.encode(bad_word, add_prefix_space=True)`.
-        num_return_sequences (`int`, *optional*, defaults to 1):
-            Number of independently computed returned sequences for each element in the batch that will be used by
-            default in the `generate` method of the model.
-        output_scores (`bool`, *optional*, defaults to `False`):
-            Whether the model should return the logits when used for generation.
-        return_dict_in_generate (`bool`, *optional*, defaults to `False`):
-            Whether the model should return a [`~transformers.utils.ModelOutput`] instead of a `torch.LongTensor`.
-        forced_bos_token_id (`int`, *optional*):
-            The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for
-            multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target
-            language token.
-        forced_eos_token_id (`int`, *optional*):
-            The id of the token to force as the last generated token when `max_length` is reached.
-        remove_invalid_values (`bool`, *optional*):
-            Whether to remove possible _nan_ and _inf_ outputs of the model to prevent the generation method to crash.
-            Note that using `remove_invalid_values` can slow down generation.
-
-        > Parameters for fine-tuning tasks
-
-        architectures (`List[str]`, *optional*):
-            Model architectures that can be used with the model pretrained weights.
-        finetuning_task (`str`, *optional*):
-            Name of the task used to fine-tune the model. This can be used when converting from an original (TensorFlow
-            or PyTorch) checkpoint.
-        id2label (`Dict[int, str]`, *optional*):
-            A map from index (for instance prediction index, or target index) to label.
-        label2id (`Dict[str, int]`, *optional*): A map from label to index for the model.
-        num_labels (`int`, *optional*):
-            Number of labels to use in the last layer added to the model, typically for a classification task.
-        task_specific_params (`Dict[str, Any]`, *optional*):
-            Additional keyword arguments to store for the current task.
-        problem_type (`str`, *optional*):
-            Problem type for `XxxForSequenceClassification` models. Can be one of `"regression"`,
-            `"single_label_classification"` or `"multi_label_classification"`.
-
-        > Parameters linked to the tokenizer
-
-        tokenizer_class (`str`, *optional*):
-            The name of the associated tokenizer class to use (if none is set, will use the tokenizer associated to the
-            model by default).
-        prefix (`str`, *optional*):
-            A specific prompt that should be added at the beginning of each text before calling the model.
-        bos_token_id (`int`, *optional*): The id of the _beginning-of-stream_ token.
-        pad_token_id (`int`, *optional*): The id of the _padding_ token.
-        eos_token_id (`int`, *optional*): The id of the _end-of-stream_ token.
-        decoder_start_token_id (`int`, *optional*):
-            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token.
-        sep_token_id (`int`, *optional*): The id of the _separation_ token.
-
-        > PyTorch specific parameters
-
-        torchscript (`bool`, *optional*, defaults to `False`):
-            Whether or not the model should be used with Torchscript.
-        tie_word_embeddings (`bool`, *optional*, defaults to `True`):
-            Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the
-            model has a output word embedding layer.
-        torch_dtype (`str`, *optional*):
-            The `dtype` of the weights. This attribute can be used to initialize the model to a non-default `dtype`
-            (which is normally `float32`) and thus allow for optimal storage allocation. For example, if the saved
-            model is `float16`, ideally we want to load it back using the minimal amount of memory needed to load
-            `float16` weights. Since the config object is stored in plain text, this attribute contains just the
-            floating type string without the `torch.` prefix. For example, for `torch.float16` ``torch_dtype` is the
-            `"float16"` string.
-
-            This attribute is currently not being used during model loading time, but this may change in the future
-            versions. But we can already start preparing for the future by saving the dtype with save_pretrained.
-
-        > TensorFlow specific parameters
-
-        use_bfloat16 (`bool`, *optional*, defaults to `False`):
-            Whether or not the model should use BFloat16 scalars (only used by some TensorFlow models).
-        tf_legacy_loss (`bool`, *optional*, defaults to `False`):
-            Whether the model should use legacy TensorFlow losses. Legacy losses have variable output shapes and may
-            not be XLA-compatible. This option is here for backward compatibility and will be removed in Transformers
-            v5.
-    """
-    model_type: str = ""
-    is_composition: bool = False
-    attribute_map: Dict[str, str] = {}
-    _auto_class: Optional[str] = None
-
-    def __setattr__(self, key, value):
-        if key in super().__getattribute__("attribute_map"):
-            key = super().__getattribute__("attribute_map")[key]
-        super().__setattr__(key, value)
-
-    def __getattribute__(self, key):
-        if key != "attribute_map" and key in super().__getattribute__("attribute_map"):
-            key = super().__getattribute__("attribute_map")[key]
-        return super().__getattribute__(key)
-
-    def __init__(self, **kwargs):
-        # Attributes with defaults
-        self.return_dict = kwargs.pop("return_dict", True)
-        self.output_hidden_states = kwargs.pop("output_hidden_states", False)
-        self.output_attentions = kwargs.pop("output_attentions", False)
-        self.torchscript = kwargs.pop(
-            "torchscript", False
-        )  # Only used by PyTorch models
-        self.torch_dtype = kwargs.pop(
-            "torch_dtype", None
-        )  # Only used by PyTorch models
-        self.use_bfloat16 = kwargs.pop("use_bfloat16", False)
-        self.tf_legacy_loss = kwargs.pop(
-            "tf_legacy_loss", False
-        )  # Only used by TensorFlow models
-        self.pruned_heads = kwargs.pop("pruned_heads", {})
-        self.tie_word_embeddings = kwargs.pop(
-            "tie_word_embeddings", True
-        )  # Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.
-
-        # Is decoder is used in encoder-decoder models to differentiate encoder from decoder
-        self.is_encoder_decoder = kwargs.pop("is_encoder_decoder", False)
-        self.is_decoder = kwargs.pop("is_decoder", False)
-        self.cross_attention_hidden_size = kwargs.pop(
-            "cross_attention_hidden_size", None
-        )
-        self.add_cross_attention = kwargs.pop("add_cross_attention", False)
-        self.tie_encoder_decoder = kwargs.pop("tie_encoder_decoder", False)
-
-        # Parameters for sequence generation
-        self.max_length = kwargs.pop("max_length", 20)
-        self.min_length = kwargs.pop("min_length", 0)
-        self.do_sample = kwargs.pop("do_sample", False)
-        self.early_stopping = kwargs.pop("early_stopping", False)
-        self.num_beams = kwargs.pop("num_beams", 1)
-        self.num_beam_groups = kwargs.pop("num_beam_groups", 1)
-        self.diversity_penalty = kwargs.pop("diversity_penalty", 0.0)
-        self.temperature = kwargs.pop("temperature", 1.0)
-        self.top_k = kwargs.pop("top_k", 50)
-        self.top_p = kwargs.pop("top_p", 1.0)
-        self.typical_p = kwargs.pop("typical_p", 1.0)
-        self.repetition_penalty = kwargs.pop("repetition_penalty", 1.0)
-        self.length_penalty = kwargs.pop("length_penalty", 1.0)
-        self.no_repeat_ngram_size = kwargs.pop("no_repeat_ngram_size", 0)
-        self.encoder_no_repeat_ngram_size = kwargs.pop(
-            "encoder_no_repeat_ngram_size", 0
-        )
-        self.bad_words_ids = kwargs.pop("bad_words_ids", None)
-        self.num_return_sequences = kwargs.pop("num_return_sequences", 1)
-        self.chunk_size_feed_forward = kwargs.pop("chunk_size_feed_forward", 0)
-        self.output_scores = kwargs.pop("output_scores", False)
-        self.return_dict_in_generate = kwargs.pop("return_dict_in_generate", False)
-        self.forced_bos_token_id = kwargs.pop("forced_bos_token_id", None)
-        self.forced_eos_token_id = kwargs.pop("forced_eos_token_id", None)
-        self.remove_invalid_values = kwargs.pop("remove_invalid_values", False)
-        self.exponential_decay_length_penalty = kwargs.pop(
-            "exponential_decay_length_penalty", None
-        )
-        self.suppress_tokens = kwargs.pop("suppress_tokens", None)
-        self.begin_suppress_tokens = kwargs.pop("begin_suppress_tokens", None)
-
-        # Fine-tuning task arguments
-        self.architectures = kwargs.pop("architectures", None)
-        self.finetuning_task = kwargs.pop("finetuning_task", None)
-        self.id2label = kwargs.pop("id2label", None)
-        self.label2id = kwargs.pop("label2id", None)
-        if self.id2label is not None:
-            num_labels = kwargs.pop("num_labels", None)
-            if num_labels is not None and len(self.id2label) != num_labels:
-                logger.warning(
-                    f"You passed along `num_labels={num_labels}` with an incompatible id to label map: "
-                    f"{self.id2label}. The number of labels wil be overwritten to {self.num_labels}."
-                )
-            self.id2label = dict(
-                (int(key), value) for key, value in self.id2label.items()
-            )
-            # Keys are always strings in JSON so convert ids to int here.
+
+class TFPositionwiseFF(tf.keras.layers.Layer):
+    def __init__(
+        self,
+        d_model,
+        d_inner,
+        dropout,
+        pre_lnorm=False,
+        layer_norm_epsilon=1e-5,
+        init_std=0.02,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
+
+        self.d_model = d_model
+        self.d_inner = d_inner
+        self.dropout = dropout
+
+        self.layer_1 = tf.keras.layers.Dense(
+            d_inner,
+            kernel_initializer=get_initializer(init_std),
+            activation=tf.nn.relu,
+            name="CoreNet_._0",
+        )
+        self.drop_1 = tf.keras.layers.Dropout(dropout)
+        self.layer_2 = tf.keras.layers.Dense(
+            d_model, kernel_initializer=get_initializer(init_std), name="CoreNet_._3"
+        )
+        self.drop_2 = tf.keras.layers.Dropout(dropout)
+
+        self.layer_norm = tf.keras.layers.LayerNormalization(
+            epsilon=layer_norm_epsilon, name="layer_norm"
+        )
+
+        self.pre_lnorm = pre_lnorm
+
+    def call(self, inp, training=False):
+        if self.pre_lnorm:
+            # layer normalization + positionwise feed-forward
+            core_out = self.layer_norm(inp)
+            core_out = self.layer_1(core_out)
+            core_out = self.drop_1(core_out, training=training)
+            core_out = self.layer_2(core_out)
+            core_out = self.drop_2(core_out, training=training)
+
+            # residual connection
+            output = core_out + inp
         else:
-            self.num_labels = kwargs.pop("num_labels", 2)
+            # positionwise feed-forward
+            core_out = self.layer_1(inp)
+            core_out = self.drop_1(core_out, training=training)
+            core_out = self.layer_2(core_out)
+            core_out = self.drop_2(core_out, training=training)
 
-        if self.torch_dtype is not None and isinstance(self.torch_dtype, str):
-            # we will start using self.torch_dtype in v5, but to be consistent with
-            # from_pretrained's torch_dtype arg convert it to an actual torch.dtype object
-            if is_torch_available():
-                import torch
-
-                self.torch_dtype = getattr(torch, self.torch_dtype)
-
-        # Tokenizer arguments TODO: eventually tokenizer and models should share the same config
-        self.tokenizer_class = kwargs.pop("tokenizer_class", None)
-        self.prefix = kwargs.pop("prefix", None)
-        self.bos_token_id = kwargs.pop("bos_token_id", None)
-        self.pad_token_id = kwargs.pop("pad_token_id", None)
-        self.eos_token_id = kwargs.pop("eos_token_id", None)
-        self.sep_token_id = kwargs.pop("sep_token_id", None)
-
-        self.decoder_start_token_id = kwargs.pop("decoder_start_token_id", None)
-
-        # task specific arguments
-        self.task_specific_params = kwargs.pop("task_specific_params", None)
-
-        # regression / multi-label classification
-        self.problem_type = kwargs.pop("problem_type", None)
-        allowed_problem_types = (
-            "regression",
-            "single_label_classification",
-            "multi_label_classification",
-        )
-        if (
-            self.problem_type is not None
-            and self.problem_type not in allowed_problem_types
-        ):
-            raise ValueError(
-                f"The config parameter `problem_type` was not understood: received {self.problem_type} "
-                "but only 'regression', 'single_label_classification' and 'multi_label_classification' are valid."
-            )
+            # residual connection + layer normalization
+            output = self.layer_norm(inp + core_out)
 
-        # TPU arguments
-        if kwargs.pop("xla_device", None) is not None:
-            logger.warning(
-                "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can "
-                "safely remove it from your `config.json` file."
-            )
+        return output
+
+
+class TFRelPartialLearnableMultiHeadAttn(tf.keras.layers.Layer):
+    def __init__(
+        self,
+        n_head,
+        d_model,
+        d_head,
+        dropout,
+        dropatt=0.0,
+        pre_lnorm=False,
+        r_r_bias=None,
+        r_w_bias=None,
+        layer_norm_epsilon=1e-5,
+        init_std=0.02,
+        output_attentions=False,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
+
+        self.n_head = n_head
+        self.d_model = d_model
+        self.d_head = d_head
+        self.dropout = dropout
+        self.output_attentions = output_attentions
+
+        self.qkv_net = tf.keras.layers.Dense(
+            3 * n_head * d_head,
+            kernel_initializer=get_initializer(init_std),
+            use_bias=False,
+            name="qkv_net",
+        )
+
+        self.drop = tf.keras.layers.Dropout(dropout)
+        self.dropatt = tf.keras.layers.Dropout(dropatt)
+        self.o_net = tf.keras.layers.Dense(
+            d_model,
+            kernel_initializer=get_initializer(init_std),
+            use_bias=False,
+            name="o_net",
+        )
+
+        self.layer_norm = tf.keras.layers.LayerNormalization(
+            epsilon=layer_norm_epsilon, name="layer_norm"
+        )
+
+        self.scale = 1 / (d_head**0.5)
 
-        # Name or path to the pretrained checkpoint
-        self._name_or_path = str(kwargs.pop("name_or_path", ""))
-        # Config hash
-        self._commit_hash = kwargs.pop("_commit_hash", None)
-
-        # Drop the transformers version info
-        self.transformers_version = kwargs.pop("transformers_version", None)
-
-        # Deal with gradient checkpointing
-        if kwargs.get("gradient_checkpointing", False):
-            warnings.warn(
-                "Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 "
-                "Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the "
-                "`Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`."
+        self.pre_lnorm = pre_lnorm
+
+        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared
+            self.r_r_bias = r_r_bias
+            self.r_w_bias = r_w_bias
+        else:
+            self.r_r_bias = None
+            self.r_w_bias = None
+
+        self.r_net = tf.keras.layers.Dense(
+            self.n_head * self.d_head,
+            kernel_initializer=get_initializer(init_std),
+            use_bias=False,
+            name="r_net",
+        )
+
+    def build(self, input_shape):
+        if self.r_r_bias is None or self.r_w_bias is None:  # Biases are not shared
+            self.r_r_bias = self.add_weight(
+                shape=(self.n_head, self.d_head),
+                initializer="zeros",
+                trainable=True,
+                name="r_r_bias",
+            )
+            self.r_w_bias = self.add_weight(
+                shape=(self.n_head, self.d_head),
+                initializer="zeros",
+                trainable=True,
+                name="r_w_bias",
             )
+        super().build(input_shape)
 
-        # Additional attributes without default values
-        for key, value in kwargs.items():
-            try:
-                setattr(self, key, value)
-            except AttributeError as err:
-                logger.error(f"Can't set {key} with value {value} for {self}")
-                raise err
-
-    @property
-    def name_or_path(self) -> str:
-        return getattr(self, "_name_or_path", None)
-
-    @name_or_path.setter
-    def name_or_path(self, value):
-        self._name_or_path = str(
-            value
-        )  # Make sure that name_or_path is a string (for JSON encoding)
+    def _rel_shift(self, x):
+        x_size = shape_list(x)
 
-    @property
-    def use_return_dict(self) -> bool:
-        """
-        `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.
-        """
-        # If torchscript is set, force `return_dict=False` to avoid jit errors
-        return self.return_dict and not self.torchscript
+        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])
+        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])
+        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])
+        x = tf.reshape(x, x_size)
+
+        return x
+
+    def call(self, w, r, attn_mask, mems, head_mask, output_attentions, training=False):
+        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]
+
+        if mems is not None:
+            mems = tf.cast(mems, dtype=w.dtype)
+            cat = tf.concat([mems, w], 0)
+            if self.pre_lnorm:
+                w_heads = self.qkv_net(self.layer_norm(cat))
+            else:
+                w_heads = self.qkv_net(cat)
+            r_head_k = self.r_net(r)
 
-    @property
-    def num_labels(self) -> int:
-        """
-        `int`: The number of labels for classification models.
-        """
-        return len(self.id2label)
+            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)
+            w_head_q = w_head_q[-qlen:]
+        else:
+            if self.pre_lnorm:
+                w_heads = self.qkv_net(self.layer_norm(w))
+            else:
+                w_heads = self.qkv_net(w)
+            r_head_k = self.r_net(r)
+
+            w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)
+
+        klen = shape_list(w_head_k)[0]
+
+        w_head_q = tf.reshape(
+            w_head_q, (qlen, bsz, self.n_head, self.d_head)
+        )  # qlen x bsz x n_head x d_head
+        w_head_k = tf.reshape(
+            w_head_k, (klen, bsz, self.n_head, self.d_head)
+        )  # qlen x bsz x n_head x d_head
+        w_head_v = tf.reshape(
+            w_head_v, (klen, bsz, self.n_head, self.d_head)
+        )  # qlen x bsz x n_head x d_head
+
+        r_head_k = tf.reshape(
+            r_head_k, (rlen, self.n_head, self.d_head)
+        )  # qlen x n_head x d_head
+
+        # compute attention score
+        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head
+        AC = tf.einsum(
+            "ibnd,jbnd->ijbn", rw_head_q, w_head_k
+        )  # qlen x klen x bsz x n_head
+
+        rr_head_q = w_head_q + self.r_r_bias
+        BD = tf.einsum(
+            "ibnd,jnd->ijbn", rr_head_q, r_head_k
+        )  # qlen x klen x bsz x n_head
+        BD = self._rel_shift(BD)
+
+        # [qlen x klen x bsz x n_head]
+        attn_score = AC + BD
+        attn_score = attn_score * self.scale
+
+        # compute attention probability
+        if attn_mask is not None:
+            attn_mask_t = attn_mask[:, :, None, None]
+            attn_mask_t = tf.cast(attn_mask_t, dtype=attn_score.dtype)
+            attn_score = attn_score * (1.0 - attn_mask_t) - 1e30 * attn_mask_t
+
+        # [qlen x klen x bsz x n_head]
+        attn_prob = stable_softmax(attn_score, axis=1)
+        attn_prob = self.dropatt(attn_prob, training=training)
+
+        # Mask heads if we want to
+        if head_mask is not None:
+            attn_prob = attn_prob * head_mask
+
+        # compute attention vector
+        attn_vec = tf.einsum("ijbn,jbnd->ibnd", attn_prob, w_head_v)
+
+        # [qlen x bsz x n_head x d_head]
+        attn_vec_sizes = shape_list(attn_vec)
+        attn_vec = tf.reshape(
+            attn_vec, (attn_vec_sizes[0], attn_vec_sizes[1], self.n_head * self.d_head)
+        )
+
+        # linear projection
+        attn_out = self.o_net(attn_vec)
+        attn_out = self.drop(attn_out, training=training)
+
+        if self.pre_lnorm:
+            # residual connection
+            outputs = [w + attn_out]
+        else:
+            # residual connection + layer normalization
+            outputs = [self.layer_norm(w + attn_out)]
+
+        if output_attentions:
+            outputs.append(attn_prob)
+
+        return outputs
 
-    @num_labels.setter
-    def num_labels(self, num_labels: int):
-        if (
-            not hasattr(self, "id2label")
-            or self.id2label is None
-            or len(self.id2label) != num_labels
-        ):
-            self.id2label = {i: f"LABEL_{i}" for i in range(num_labels)}
-            self.label2id = dict(zip(self.id2label.values(), self.id2label.keys()))
 
-    def save_pretrained(
+class TFRelPartialLearnableDecoderLayer(tf.keras.layers.Layer):
+    def __init__(
         self,
-        save_directory: Union[str, os.PathLike],
-        push_to_hub: bool = False,
+        n_head,
+        d_model,
+        d_head,
+        d_inner,
+        dropout,
+        dropatt=0.0,
+        pre_lnorm=False,
+        r_w_bias=None,
+        r_r_bias=None,
+        layer_norm_epsilon=1e-5,
+        init_std=0.02,
+        output_attentions=False,
         **kwargs,
     ):
-        """
-        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the
-        [`~PretrainedConfig.from_pretrained`] class method.
+        super().__init__(**kwargs)
 
-        Args:
-            save_directory (`str` or `os.PathLike`):
-                Directory where the configuration JSON file will be saved (will be created if it does not exist).
-            push_to_hub (`bool`, *optional*, defaults to `False`):
-                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
-                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your
-                namespace).
-            kwargs:
-                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.
-        """
-        if os.path.isfile(save_directory):
-            raise AssertionError(
-                f"Provided path ({save_directory}) should be a directory, not a file"
-            )
+        self.dec_attn = TFRelPartialLearnableMultiHeadAttn(
+            n_head,
+            d_model,
+            d_head,
+            dropout,
+            dropatt=dropatt,
+            pre_lnorm=pre_lnorm,
+            r_w_bias=r_w_bias,
+            r_r_bias=r_r_bias,
+            init_std=init_std,
+            layer_norm_epsilon=layer_norm_epsilon,
+            output_attentions=output_attentions,
+            name="dec_attn",
+        )
+        self.pos_ff = TFPositionwiseFF(
+            d_model,
+            d_inner,
+            dropout,
+            pre_lnorm=pre_lnorm,
+            init_std=init_std,
+            layer_norm_epsilon=layer_norm_epsilon,
+            name="pos_ff",
+        )
 
-        os.makedirs(save_directory, exist_ok=True)
+    def call(
+        self,
+        dec_inp,
+        r,
+        dec_attn_mask,
+        mems,
+        head_mask,
+        output_attentions,
+        training=False,
+    ):
+        attn_outputs = self.dec_attn(
+            dec_inp,
+            r,
+            dec_attn_mask,
+            mems,
+            head_mask,
+            output_attentions,
+            training=training,
+        )
+        ff_output = self.pos_ff(attn_outputs[0], training=training)
 
-        if push_to_hub:
-            commit_message = kwargs.pop("commit_message", None)
-            repo_id = kwargs.pop("repo_id", save_directory.split(os.path.sep)[-1])
-            repo_id, token = self._create_repo(repo_id, **kwargs)
-            files_timestamps = self._get_files_timestamps(save_directory)
-
-        # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be
-        # loaded from the Hub.
-        if self._auto_class is not None:
-            custom_object_save(self, save_directory, config=self)
-
-        # If we save using the predefined names, we can load using `from_pretrained`
-        output_config_file = os.path.join(save_directory, CONFIG_NAME)
-
-        self.to_json_file(output_config_file, use_diff=True)
-        logger.info(f"Configuration saved in {output_config_file}")
-
-        if push_to_hub:
-            self._upload_modified_files(
-                save_directory,
-                repo_id,
-                files_timestamps,
-                commit_message=commit_message,
-                token=token,
-            )
+        outputs = [ff_output] + attn_outputs[1:]
 
-    @classmethod
-    def from_pretrained(
-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs
-    ) -> "PretrainedConfig":
-        r"""
-        Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.
+        return outputs
 
-        Args:
-            pretrained_model_name_or_path (`str` or `os.PathLike`):
-                This can be either:
-
-                - a string, the *model id* of a pretrained model configuration hosted inside a model repo on
-                  huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or
-                  namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
-                - a path to a *directory* containing a configuration file saved using the
-                  [`~PretrainedConfig.save_pretrained`] method, e.g., `./my_model_directory/`.
-                - a path or url to a saved configuration JSON *file*, e.g., `./my_model_directory/configuration.json`.
-            cache_dir (`str` or `os.PathLike`, *optional*):
-                Path to a directory in which a downloaded pretrained model configuration should be cached if the
-                standard cache should not be used.
-            force_download (`bool`, *optional*, defaults to `False`):
-                Whether or not to force to (re-)download the configuration files and override the cached versions if
-                they exist.
-            resume_download (`bool`, *optional*, defaults to `False`):
-                Whether or not to delete incompletely received file. Attempts to resume the download if such a file
-                exists.
-            proxies (`Dict[str, str]`, *optional*):
-                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
-                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
-            use_auth_token (`str` or `bool`, *optional*):
-                The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use
-                the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).
-            revision (`str`, *optional*, defaults to `"main"`):
-                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
-                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
-                identifier allowed by git.
-
-                <Tip>
-
-                To test a pull request you made on the Hub, you can pass `revision="refs/pr/<pr_number>".
-
-                </Tip>
-
-            return_unused_kwargs (`bool`, *optional*, defaults to `False`):
-                If `False`, then this function returns just the final configuration object.
-
-                If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a
-                dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
-                part of `kwargs` which has not been used to update `config` and is otherwise ignored.
-            subfolder (`str`, *optional*, defaults to `""`):
-                In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
-                specify the folder name here.
-            kwargs (`Dict[str, Any]`, *optional*):
-                The values in kwargs of any keys which are configuration attributes will be used to override the loaded
-                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled
-                by the `return_unused_kwargs` keyword parameter.
-
-        Returns:
-            [`PretrainedConfig`]: The configuration object instantiated from this pretrained model.
-
-        Examples:
-
-        ```python
-        # We can't instantiate directly the base class *PretrainedConfig* so let's show the examples on a
-        # derived class: BertConfig
-        config = BertConfig.from_pretrained(
-            "bert-base-uncased"
-        )  # Download configuration from huggingface.co and cache.
-        config = BertConfig.from_pretrained(
-            "./test/saved_model/"
-        )  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*
-        config = BertConfig.from_pretrained("./test/saved_model/my_configuration.json")
-        config = BertConfig.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
-        assert config.output_attentions == True
-        config, unused_kwargs = BertConfig.from_pretrained(
-            "bert-base-uncased", output_attentions=True, foo=False, return_unused_kwargs=True
-        )
-        assert config.output_attentions == True
-        assert unused_kwargs == {"foo": False}
-        ```"""
-        config_dict, kwargs = cls.get_config_dict(
-            pretrained_model_name_or_path, **kwargs
-        )
-        if (
-            "model_type" in config_dict
-            and hasattr(cls, "model_type")
-            and config_dict["model_type"] != cls.model_type
-        ):
-            logger.warning(
-                f"You are using a model of type {config_dict['model_type']} to instantiate a model of type "
-                f"{cls.model_type}. This is not supported for all configurations of models and can yield errors."
-            )
 
-        return cls.from_dict(config_dict, **kwargs)
+class TFTransfoEmbeddings(tf.keras.layers.Layer):
+    def __init__(self, vocab_size, emb_size, init_std, **kwargs):
+        super().__init__(**kwargs)
 
-    @classmethod
-    def get_config_dict(
-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs
-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-        """
-        From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a
-        [`PretrainedConfig`] using `from_dict`.
+        self.vocab_size = vocab_size
+        self.emb_size = emb_size
+        self.init_std = init_std
 
-        Parameters:
-            pretrained_model_name_or_path (`str` or `os.PathLike`):
-                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.
+    def build(self, input_shape):
+        self.weight = self.add_weight(
+            shape=(self.vocab_size, self.emb_size),
+            initializer=get_initializer(self.init_std),
+            name="embeddings",
+        )
 
-        Returns:
-            `Tuple[Dict, Dict]`: The dictionary(ies) that will be used to instantiate the configuration object.
+        super().build(input_shape)
 
-        """
-        original_kwargs = copy.deepcopy(kwargs)
-        # Get config dict associated with the base config file
-        config_dict, kwargs = cls._get_config_dict(
-            pretrained_model_name_or_path, **kwargs
-        )
-        if "_commit_hash" in config_dict:
-            original_kwargs["_commit_hash"] = config_dict["_commit_hash"]
-
-        # That config file may point us toward another config file to use.
-        if "configuration_files" in config_dict:
-            configuration_file = get_configuration_file(
-                config_dict["configuration_files"]
-            )
-            config_dict, kwargs = cls._get_config_dict(
-                pretrained_model_name_or_path,
-                _configuration_file=configuration_file,
-                **original_kwargs,
-            )
+    def call(self, inputs):
+        return tf.gather(self.weight, inputs)
 
-        return config_dict, kwargs
 
-    @classmethod
-    def _get_config_dict(
-        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs
-    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
-        cache_dir = kwargs.pop("cache_dir", None)
-        force_download = kwargs.pop("force_download", False)
-        resume_download = kwargs.pop("resume_download", False)
-        proxies = kwargs.pop("proxies", None)
-        use_auth_token = kwargs.pop("use_auth_token", None)
-        local_files_only = kwargs.pop("local_files_only", False)
-        revision = kwargs.pop("revision", None)
-        trust_remote_code = kwargs.pop("trust_remote_code", None)
-        subfolder = kwargs.pop("subfolder", "")
-        from_pipeline = kwargs.pop("_from_pipeline", None)
-        from_auto_class = kwargs.pop("_from_auto", False)
-        commit_hash = kwargs.pop("_commit_hash", None)
-
-        if trust_remote_code is True:
-            logger.warning(
-                "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is"
-                " ignored."
-            )
+class TFAdaptiveEmbedding(tf.keras.layers.Layer):
+    def __init__(
+        self,
+        n_token,
+        d_embed,
+        d_proj,
+        cutoffs,
+        div_val=1,
+        init_std=0.02,
+        sample_softmax=False,
+        **kwargs,
+    ):
+        super().__init__(**kwargs)
 
-        user_agent = {"file_type": "config", "from_auto_class": from_auto_class}
-        if from_pipeline is not None:
-            user_agent["using_pipeline"] = from_pipeline
-
-        pretrained_model_name_or_path = str(pretrained_model_name_or_path)
-
-        is_local = os.path.isdir(pretrained_model_name_or_path)
-        if os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):
-            # Special case when pretrained_model_name_or_path is a local file
-            resolved_config_file = pretrained_model_name_or_path
-            is_local = True
-        elif is_remote_url(pretrained_model_name_or_path):
-            configuration_file = pretrained_model_name_or_path
-            resolved_config_file = download_url(pretrained_model_name_or_path)
-        else:
-            configuration_file = kwargs.pop("_configuration_file", CONFIG_NAME)
-
-            try:
-                # Load from local folder or from cache or download from model Hub and cache
-                resolved_config_file = cached_file(
-                    pretrained_model_name_or_path,
-                    configuration_file,
-                    cache_dir=cache_dir,
-                    force_download=force_download,
-                    proxies=proxies,
-                    resume_download=resume_download,
-                    local_files_only=local_files_only,
-                    use_auth_token=use_auth_token,
-                    user_agent=user_agent,
-                    revision=revision,
-                    subfolder=subfolder,
-                    _commit_hash=commit_hash,
-                )
-                commit_hash = extract_commit_hash(resolved_config_file, commit_hash)
-            except EnvironmentError:
-                # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to
-                # the original exception.
-                raise
-            except Exception:
-                # For any other exception, we throw a generic error.
-                raise EnvironmentError(
-                    f"Can't load the configuration of '{pretrained_model_name_or_path}'. If you were trying to load it"
-                    " from 'https://huggingface.co/models', make sure you don't have a local directory with the same"
-                    f" name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory"
-                    f" containing a {configuration_file} file"
+        self.n_token = n_token
+        self.d_embed = d_embed
+        self.init_std = init_std
+
+        self.cutoffs = cutoffs + [n_token]
+        self.div_val = div_val
+        self.d_proj = d_proj
+
+        self.emb_scale = d_proj**0.5
+
+        self.cutoff_ends = [0] + self.cutoffs
+
+        self.emb_layers = []
+        self.emb_projs = []
+
+        if div_val == 1:
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+        else:
+            for i in range(len(self.cutoffs)):
+                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
+                d_emb_i = d_embed // (div_val**i)
+                self.emb_layers.append(
+                    TFTransfoEmbeddings(
+                        r_idx - l_idx,
+                        d_emb_i,
+                        init_std,
+                        name=f"emb_layers_._{i}",
+                    )
                 )
 
-        try:
-            # Load config dict
-            config_dict = cls._dict_from_json_file(resolved_config_file)
-            config_dict["_commit_hash"] = commit_hash
-        except (json.JSONDecodeError, UnicodeDecodeError):
-            raise EnvironmentError(
-                f"It looks like the config file at '{resolved_config_file}' is not a valid JSON file."
+    def build(self, input_shape):
+        for i in range(len(self.cutoffs)):
+            d_emb_i = self.d_embed // (self.div_val**i)
+            self.emb_projs.append(
+                self.add_weight(
+                    shape=(d_emb_i, self.d_proj),
+                    initializer=get_initializer(self.init_std),
+                    trainable=True,
+                    name=f"emb_projs_._{i}",
+                )
             )
 
-        if is_local:
-            logger.info(f"loading configuration file {resolved_config_file}")
+        super().build(input_shape)
+
+    def call(self, inp):
+        if self.div_val == 1:
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
         else:
-            logger.info(
-                f"loading configuration file {configuration_file} from cache at {resolved_config_file}"
+            inp_flat = tf.reshape(inp, (-1,))
+            emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])
+            for i in range(len(self.cutoffs)):
+                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
+
+                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)
+
+                inp_i = tf.boolean_mask(inp_flat, mask_i) - l_idx
+                emb_i = self.emb_layers[i](inp_i)
+                emb_i = tf.einsum("id,de->ie", emb_i, self.emb_projs[i])
+
+                mask_idx = tf.where(mask_i)
+                scatter = tf.scatter_nd(mask_idx, emb_i, shape_list(emb_flat))
+                emb_flat = tf.cast(emb_flat, dtype=scatter.dtype)
+                emb_flat += scatter
+
+            embed_shape = shape_list(inp) + [self.d_proj]
+            embed = tf.reshape(emb_flat, embed_shape)
+
+        embed *= self.emb_scale
+
+        return embed
+
+
+@keras_serializable
+class TFTransfoXLMainLayer(tf.keras.layers.Layer):
+    config_class = TransfoXLConfig
+
+    def __init__(self, config, **kwargs):
+        super().__init__(**kwargs)
+
+        self.config = config
+        self.output_hidden_states = config.output_hidden_states
+        self.output_attentions = config.output_attentions
+        self.return_dict = config.use_return_dict
+
+        self.n_token = config.vocab_size
+
+        self.d_embed = config.d_embed
+        self.d_model = config.d_model
+        self.n_head = config.n_head
+        self.d_head = config.d_head
+        self.untie_r = config.untie_r
+
+        self.word_emb = TFAdaptiveEmbedding(
+            config.vocab_size,
+            config.d_embed,
+            config.d_model,
+            config.cutoffs,
+            div_val=config.div_val,
+            init_std=config.init_std,
+            name="word_emb",
+        )
+
+        self.drop = tf.keras.layers.Dropout(config.dropout)
+
+        self.n_layer = config.n_layer
+        self.mem_len = config.mem_len
+        self.attn_type = config.attn_type
+
+        self.layers = []
+        if config.attn_type == 0:  # the default attention
+            for i in range(config.n_layer):
+                self.layers.append(
+                    TFRelPartialLearnableDecoderLayer(
+                        config.n_head,
+                        config.d_model,
+                        config.d_head,
+                        config.d_inner,
+                        config.dropout,
+                        dropatt=config.dropatt,
+                        pre_lnorm=config.pre_lnorm,
+                        r_w_bias=None if self.untie_r else self.r_w_bias,
+                        r_r_bias=None if self.untie_r else self.r_r_bias,
+                        layer_norm_epsilon=config.layer_norm_epsilon,
+                        init_std=config.init_std,
+                        output_attentions=self.output_attentions,
+                        name=f"layers_._{i}",
+                    )
+                )
+        else:  # learnable embeddings and absolute embeddings
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+
+        self.same_length = config.same_length
+        self.clamp_len = config.clamp_len
+
+        if self.attn_type == 0:  # default attention
+            self.pos_emb = TFPositionalEmbedding(self.d_model, name="pos_emb")
+        else:  # learnable embeddings and absolute embeddings
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+
+    def build(self, input_shape):
+        if not self.untie_r:
+            self.r_w_bias = self.add_weight(
+                shape=(self.n_head, self.d_head),
+                initializer="zeros",
+                trainable=True,
+                name="r_w_bias",
+            )
+            self.r_r_bias = self.add_weight(
+                shape=(self.n_head, self.d_head),
+                initializer="zeros",
+                trainable=True,
+                name="r_r_bias",
             )
+        super().build(input_shape)
 
-        return config_dict, kwargs
+    def get_input_embeddings(self):
+        return self.word_emb
 
-    @classmethod
-    def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> "PretrainedConfig":
-        """
-        Instantiates a [`PretrainedConfig`] from a Python dictionary of parameters.
+    def set_input_embeddings(self, value):
+        raise NotImplementedError
 
-        Args:
-            config_dict (`Dict[str, Any]`):
-                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be
-                retrieved from a pretrained checkpoint by leveraging the [`~PretrainedConfig.get_config_dict`] method.
-            kwargs (`Dict[str, Any]`):
-                Additional parameters from which to initialize the configuration object.
+    def backward_compatible(self):
+        self.sample_softmax = -1
 
-        Returns:
-            [`PretrainedConfig`]: The configuration object instantiated from those parameters.
-        """
-        return_unused_kwargs = kwargs.pop("return_unused_kwargs", False)
-        # Those arguments may be passed along for our internal telemetry.
-        # We remove them so they don't appear in `return_unused_kwargs`.
-        kwargs.pop("_from_auto", None)
-        kwargs.pop("_from_pipeline", None)
-        # The commit hash might have been updated in the `config_dict`, we don't want the kwargs to erase that update.
-        if "_commit_hash" in kwargs and "_commit_hash" in config_dict:
-            kwargs["_commit_hash"] = config_dict["_commit_hash"]
-
-        config = cls(**config_dict)
-
-        if hasattr(config, "pruned_heads"):
-            config.pruned_heads = dict(
-                (int(key), value) for key, value in config.pruned_heads.items()
+    def reset_memory_length(self, mem_len):
+        self.mem_len = mem_len
+
+    def _prune_heads(self, heads):
+        raise NotImplementedError
+
+    def init_mems(self, bsz):
+        if self.mem_len > 0:
+            mems = []
+            for i in range(self.n_layer):
+                empty = tf.zeros([self.mem_len, bsz, self.d_model])
+                mems.append(empty)
+
+            return mems
+        else:
+            return None
+
+    def _update_mems(self, hids, mems, mlen, qlen):
+        # does not deal with None
+        if mems is None:
+            return None
+
+        # mems is not None
+        assert len(hids) == len(mems), "len(hids) != len(mems)"
+
+        # There are `mlen + qlen` steps that can be cached into mems
+        new_mems = []
+        end_idx = mlen + tf.math.maximum(0, qlen)
+        beg_idx = tf.math.maximum(0, end_idx - tf.convert_to_tensor(self.mem_len))
+        for i in range(len(hids)):
+            mems[i] = tf.cast(mems[i], dtype=hids[i].dtype)
+            cat = tf.concat([mems[i], hids[i]], axis=0)
+            tf.stop_gradient(cat)
+            new_mems.append(cat[beg_idx:end_idx])
+
+        return new_mems
+
+    @unpack_inputs
+    def call(
+        self,
+        input_ids: Optional[TFModelInputType] = None,
+        mems: Optional[List[tf.Tensor]] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        training: bool = False,
+    ):
+        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library
+        # so we transpose here from shape [bsz, len] to shape [len, bsz]
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError(
+                "You cannot specify both input_ids and inputs_embeds at the same time"
             )
+        elif input_ids is not None:
+            input_ids = tf.transpose(input_ids, perm=(1, 0))
+            qlen, bsz = shape_list(input_ids)
+        elif inputs_embeds is not None:
+            inputs_embeds = tf.transpose(inputs_embeds, perm=(1, 0, 2))
+            qlen, bsz = shape_list(inputs_embeds)[:2]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        if mems is None:
+            mems = self.init_mems(bsz)
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)
+        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]
+        if head_mask is not None:
+            raise NotImplementedError
+        else:
+            head_mask = [None] * self.n_layer
+
+        if inputs_embeds is not None:
+            word_emb = inputs_embeds
+        else:
+            word_emb = self.word_emb(input_ids)
+
+        mlen = shape_list(mems[0])[0] if mems is not None else 0
+        klen = mlen + qlen
+
+        # Compute decoder attention mask
+
+        # ::: PyTorch masking code for reference :::
+        # if self.same_length:
+        #     all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)
+        #     mask_len = klen - self.mem_len
+        #     if mask_len > 0:
+        #         mask_shift_len = qlen - mask_len
+        #     else:
+        #         mask_shift_len = qlen
+        #     dec_attn_mask = (torch.triu(all_ones, 1+mlen)
+        #             + torch.tril(all_ones, -mask_shift_len))[:, :, None] # -1
+        # else:
+        #     dec_attn_mask = torch.triu(
+        #         word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1+mlen)[:,:,None]
+
+        # TensorFlow version
+        dec_attn_mask = 1 - tf.linalg.band_part(
+            tf.ones([qlen, klen], dtype=tf.int32), -1, mlen
+        )  # (q, q): diagonal with 1's
+        if self.same_length:
+            mask_len = klen - self.mem_len
+            if mask_len > 0:
+                mask_shift_len = qlen - mask_len
+            else:
+                mask_shift_len = qlen
+            if mask_shift_len >= 1:
+                dec_attn_mask += 1 - tf.linalg.band_part(
+                    tf.ones([qlen, klen], dtype=tf.int32), mask_shift_len - 1, -1
+                )
+            else:
+                dec_attn_mask += tf.linalg.band_part(
+                    tf.ones([qlen, klen], dtype=tf.int32), -1, -mask_shift_len
+                )
 
-        # Update config with kwargs if needed
-        if "num_labels" in kwargs and "id2label" in kwargs:
-            num_labels = kwargs["num_labels"]
-            id2label = kwargs["id2label"] if kwargs["id2label"] is not None else []
-            if len(id2label) != num_labels:
-                raise ValueError(
-                    f"You passed along `num_labels={num_labels }` with an incompatible id to label map: "
-                    f"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove "
-                    "one of them."
+        hids = []
+        attentions = [] if output_attentions else None
+        if self.attn_type == 0:  # default
+            pos_seq = tf.range(klen - 1, -1, -1.0)
+            if self.clamp_len > 0:
+                pos_seq = tf.minimum(pos_seq, self.clamp_len)
+            pos_emb = self.pos_emb(pos_seq)
+
+            core_out = self.drop(word_emb, training=training)
+            pos_emb = self.drop(pos_emb, training=training)
+
+            for i, layer in enumerate(self.layers):
+                hids.append(core_out)
+                mems_i = None if mems is None else mems[i]
+                layer_outputs = layer(
+                    core_out,
+                    pos_emb,
+                    dec_attn_mask,
+                    mems_i,
+                    head_mask[i],
+                    output_attentions,
+                    training=training,
                 )
-        to_remove = []
-        for key, value in kwargs.items():
-            if hasattr(config, key):
-                setattr(config, key, value)
-                if key != "torch_dtype":
-                    to_remove.append(key)
-        for key in to_remove:
-            kwargs.pop(key, None)
-
-        logger.info(f"Model config {config}")
-        if return_unused_kwargs:
-            return config, kwargs
+                core_out = layer_outputs[0]
+                if output_attentions:
+                    attentions.append(layer_outputs[1])
+        else:  # learnable embeddings and absolute embeddings
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+
+        core_out = self.drop(core_out, training=training)
+
+        new_mems = self._update_mems(hids, mems, mlen, qlen)
+
+        # We transpose back here to shape [bsz, len, hidden_dim]
+        core_out = tf.transpose(core_out, perm=(1, 0, 2))
+
+        if output_hidden_states:
+            # Transpose to library standard shape [bsz, len, hidden_dim] and add last layer
+            hids = tuple(tf.transpose(t, perm=(1, 0, 2)) for t in hids)
+            hids = hids + (core_out,)
         else:
-            return config
+            hids = None
+        if output_attentions:
+            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]
+            attentions = tuple(tf.transpose(t, perm=(2, 3, 0, 1)) for t in attentions)
+
+        if not return_dict:
+            return tuple(
+                v for v in [core_out, new_mems, hids, attentions] if v is not None
+            )
 
-    @classmethod
-    def from_json_file(cls, json_file: Union[str, os.PathLike]) -> "PretrainedConfig":
-        """
-        Instantiates a [`PretrainedConfig`] from the path to a JSON file of parameters.
+        return TFTransfoXLModelOutput(
+            last_hidden_state=core_out,
+            mems=new_mems,
+            hidden_states=hids,
+            attentions=attentions,
+        )
 
-        Args:
-            json_file (`str` or `os.PathLike`):
-                Path to the JSON file containing the parameters.
 
-        Returns:
-            [`PretrainedConfig`]: The configuration object instantiated from that JSON file.
+class TFTransfoXLPreTrainedModel(TFPreTrainedModel):
+    """
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
+    """
 
-        """
-        config_dict = cls._dict_from_json_file(json_file)
-        return cls(**config_dict)
+    config_class = TransfoXLConfig
+    base_model_prefix = "transformer"
 
-    @classmethod
-    def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):
-        with open(json_file, "r", encoding="utf-8") as reader:
-            text = reader.read()
-        return json.loads(text)
+    @tf.function(
+        input_signature=[
+            {
+                "input_ids": tf.TensorSpec((None, None), tf.int64, name="input_ids"),
+            }
+        ]
+    )
+    def serving(self, inputs):
+        output = self.call(inputs)
 
-    def __eq__(self, other):
-        return self.__dict__ == other.__dict__
+        return self.serving_output(output)
 
-    def __repr__(self):
-        return f"{self.__class__.__name__} {self.to_json_string()}"
 
-    def to_diff_dict(self) -> Dict[str, Any]:
-        """
-        Removes all attributes from config which correspond to the default config attributes for better readability and
-        serializes to a Python dictionary.
+@dataclass
+class TFTransfoXLModelOutput(ModelOutput):
+    """
+    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).
 
-        Returns:
-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,
-        """
-        config_dict = self.to_dict()
+    Args:
+        last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):
+            Sequence of hidden-states at the output of the last layer of the model.
+        mems (`List[tf.Tensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape
+            `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
 
-        # get the default config dict
-        default_config_dict = PretrainedConfig().to_dict()
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+    """
 
-        # get class specific config dict
-        class_config_dict = (
-            self.__class__().to_dict() if not self.is_composition else {}
-        )
+    last_hidden_state: tf.Tensor = None
+    mems: List[tf.Tensor] = None
+    hidden_states: Optional[Tuple[tf.Tensor]] = None
+    attentions: Optional[Tuple[tf.Tensor]] = None
 
-        serializable_config_dict = {}
 
-        # only serialize values that differ from the default config
-        for key, value in config_dict.items():
-            if (
-                key not in default_config_dict
-                or key == "transformers_version"
-                or value != default_config_dict[key]
-                or (key in class_config_dict and value != class_config_dict[key])
-            ):
-                serializable_config_dict[key] = value
+@dataclass
+class TFTransfoXLLMHeadModelOutput(ModelOutput):
+    """
+    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).
 
-        self.dict_torch_dtype_to_str(serializable_config_dict)
+    Args:
+        losses (`tf.Tensor` of shape *(batch_size, sequence_length-1)*, *optional*, returned when `labels` is provided):
+            Language modeling losses (not reduced).
+        prediction_scores (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
+            Prediction scores of the language modeling head (scores for each vocabulary token after SoftMax).
+        mems (`List[tf.Tensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape
+            `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
 
-        return serializable_config_dict
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+    """
 
-    def to_dict(self) -> Dict[str, Any]:
-        """
-        Serializes this instance to a Python dictionary.
+    prediction_scores: tf.Tensor = None
+    mems: List[tf.Tensor] = None
+    hidden_states: Optional[Tuple[tf.Tensor]] = None
+    attentions: Optional[Tuple[tf.Tensor]] = None
 
-        Returns:
-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.
-        """
-        output = copy.deepcopy(self.__dict__)
-        if hasattr(self.__class__, "model_type"):
-            output["model_type"] = self.__class__.model_type
-        if "_auto_class" in output:
-            del output["_auto_class"]
-        if "_commit_hash" in output:
-            del output["_commit_hash"]
 
-        # Transformers version when serializing the model
-        output["transformers_version"] = __version__
+@dataclass
+class TFTransfoXLSequenceClassifierOutputWithPast(ModelOutput):
+    """
+    Base class for outputs of sentence classification models.
 
-        self.dict_torch_dtype_to_str(output)
+    Args:
+        loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
+            Classification (or regression if config.num_labels==1) loss.
+        logits (`tf.Tensor` of shape `(batch_size, config.num_labels)`):
+            Classification (or regression if config.num_labels==1) scores (before SoftMax).
+        mems (`List[tf.Tensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(tf.Tensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of shape
+            `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(tf.Tensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
 
-        return output
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+    """
 
-    def to_json_string(self, use_diff: bool = True) -> str:
-        """
-        Serializes this instance to a JSON string.
+    loss: Optional[tf.Tensor] = None
+    logits: tf.Tensor = None
+    mems: List[tf.Tensor] = None
+    hidden_states: Optional[Tuple[tf.Tensor]] = None
+    attentions: Optional[Tuple[tf.Tensor]] = None
+
+
+TRANSFO_XL_START_DOCSTRING = r"""
+
+    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
+    etc.)
+
+    This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
+    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
+    behavior.
 
-        Args:
-            use_diff (`bool`, *optional*, defaults to `True`):
-                If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`
-                is serialized to JSON string.
+    <Tip>
 
-        Returns:
-            `str`: String containing all the attributes that make up this configuration instance in JSON format.
-        """
-        if use_diff is True:
-            config_dict = self.to_diff_dict()
-        else:
-            config_dict = self.to_dict()
-        return json.dumps(config_dict, indent=2, sort_keys=True) + "\n"
+    TensorFlow models and layers in `transformers` accept two formats as input:
 
-    def to_json_file(
-        self, json_file_path: Union[str, os.PathLike], use_diff: bool = True
+    - having all inputs as keyword arguments (like PyTorch models), or
+    - having all inputs as a list, tuple or dict in the first positional argument.
+
+    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
+    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
+    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
+    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
+    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
+    positional argument:
+
+    - a single Tensor with `input_ids` only and nothing else: `model(input_ids)`
+    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
+    `model([input_ids, attention_mask])` or `model([input_ids, attention_mask, token_type_ids])`
+    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
+    `model({"input_ids": input_ids, "token_type_ids": token_type_ids})`
+
+    Note that when creating models and layers with
+    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
+    about any of this, as you can just pass inputs like you would to any other Python function!
+
+    </Tip>
+
+    Parameters:
+        config ([`TransfoXLConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
+"""
+
+TRANSFO_XL_INPUTS_DOCSTRING = r"""
+    Args:
+        input_ids (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length)`):
+            Indices of input sequence tokens in the vocabulary.
+
+            Indices can be obtained using [`BertTokenizer`]. See [`PreTrainedTokenizer.__call__`] and
+            [`PreTrainedTokenizer.encode`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        mems (`List[tf.Tensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (see
+            `mems` output below). Can be used to speed up sequential decoding. The token ids which have their mems
+            given to this model should not be passed as `input_ids` as they have already been computed.
+        head_mask (`tf.Tensor` or `Numpy array` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
+
+            - 1 indicates the head is **not masked**,
+            - 0 indicates the head is **masked**.
+        inputs_embeds (`tf.Tensor` or `Numpy array` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
+            model's internal embedding lookup matrix.
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
+            tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
+            config will be used instead.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
+            used instead.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple. This argument can be used in
+            eager mode, in graph mode the value will always be set to True.
+        training (`bool`, *optional*, defaults to `False`):
+            Whether or not to use the model in training mode (some modules like dropout modules have different
+            behaviors between training and evaluation).
+"""
+
+
+@add_start_docstrings(
+    "The bare Bert Model transformer outputting raw hidden-states without any specific head on top.",
+    TRANSFO_XL_START_DOCSTRING,
+)
+class TFTransfoXLModel(TFTransfoXLPreTrainedModel):
+    def __init__(self, config, *inputs, **kwargs):
+        super().__init__(config, *inputs, **kwargs)
+        self.transformer = TFTransfoXLMainLayer(config, name="transformer")
+
+    @unpack_inputs
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TFTransfoXLModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def call(
+        self,
+        input_ids: Optional[TFModelInputType] = None,
+        mems: Optional[List[tf.Tensor]] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        training: bool = False,
     ):
-        """
-        Save this instance to a JSON file.
+        outputs = self.transformer(
+            input_ids=input_ids,
+            mems=mems,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            training=training,
+        )
 
-        Args:
-            json_file_path (`str` or `os.PathLike`):
-                Path to the JSON file in which this configuration instance's parameters will be saved.
-            use_diff (`bool`, *optional*, defaults to `True`):
-                If set to `True`, only the difference between the config instance and the default `PretrainedConfig()`
-                is serialized to JSON file.
-        """
-        with open(json_file_path, "w", encoding="utf-8") as writer:
-            writer.write(self.to_json_string(use_diff=use_diff))
+        return outputs
 
-    def update(self, config_dict: Dict[str, Any]):
-        """
-        Updates attributes of this class with attributes from `config_dict`.
+    def serving_output(self, output):
+        hs = (
+            tf.convert_to_tensor(output.hidden_states)
+            if self.config.output_hidden_states
+            else None
+        )
+        attns = (
+            tf.convert_to_tensor(output.attentions)
+            if self.config.output_attentions
+            else None
+        )
 
-        Args:
-            config_dict (`Dict[str, Any]`): Dictionary of attributes that should be updated for this class.
-        """
-        for key, value in config_dict.items():
-            setattr(self, key, value)
+        return TFTransfoXLModelOutput(
+            last_hidden_state=output.last_hidden_state,
+            mems=tf.convert_to_tensor(output.mems),
+            hidden_states=hs,
+            attentions=attns,
+        )
 
-    def update_from_string(self, update_str: str):
-        """
-        Updates attributes of this class with attributes from `update_str`.
 
-        The expected format is ints, floats and strings as is, and for booleans use `true` or `false`. For example:
-        "n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index"
+@add_start_docstrings(
+    """
+    The Transformer-XL Model with a language modeling head on top (adaptive softmax with weights tied to the adaptive
+    input embeddings)
+    """,
+    TRANSFO_XL_START_DOCSTRING,
+)
+class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.transformer = TFTransfoXLMainLayer(config, name="transformer")
+        self.sample_softmax = config.sample_softmax
+        assert self.sample_softmax <= 0, (
+            "Sampling from the softmax is not implemented yet. Please look at issue: #3310:"
+            " https://github.com/huggingface/transformers/issues/3310"
+        )
 
-        The keys to change have to already exist in the config object.
+        self.crit = TFAdaptiveSoftmaxMask(
+            config.vocab_size,
+            config.d_embed,
+            config.d_model,
+            config.cutoffs,
+            div_val=config.div_val,
+            name="crit",
+        )
 
-        Args:
-            update_str (`str`): String with attributes that should be updated for this class.
+    def _resize_token_embeddings(self, new_num_tokens):
+        raise NotImplementedError()
 
-        """
+    def get_output_embeddings(self):
+        """Double-check if you are using adaptive softmax."""
+        if len(self.crit.out_layers) > 0:
+            return self.crit.out_layers[-1]
+        return None
+
+    def reset_memory_length(self, mem_len):
+        self.transformer.reset_memory_length(mem_len)
+
+    def init_mems(self, bsz):
+        return self.transformer.init_mems(bsz)
+
+    @unpack_inputs
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TFTransfoXLLMHeadModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def call(
+        self,
+        input_ids: Optional[TFModelInputType] = None,
+        mems: Optional[List[tf.Tensor]] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        training: bool = False,
+    ):
+        if input_ids is not None:
+            bsz, tgt_len = shape_list(input_ids)[:2]
+        else:
+            bsz, tgt_len = shape_list(inputs_embeds)[:2]
 
-        d = dict(x.split("=") for x in update_str.split(","))
-        for k, v in d.items():
-            if not hasattr(self, k):
-                raise ValueError(f"key {k} isn't in the original config dict")
-
-            old_v = getattr(self, k)
-            if isinstance(old_v, bool):
-                if v.lower() in ["true", "1", "y", "yes"]:
-                    v = True
-                elif v.lower() in ["false", "0", "n", "no"]:
-                    v = False
-                else:
-                    raise ValueError(f"can't derive true or false from {v} (key {k})")
-            elif isinstance(old_v, int):
-                v = int(v)
-            elif isinstance(old_v, float):
-                v = float(v)
-            elif not isinstance(old_v, str):
-                raise ValueError(
-                    f"You can only update int, float, bool or string values in the config, got {v} for key {k}"
-                )
+        transformer_outputs = self.transformer(
+            input_ids,
+            mems,
+            head_mask,
+            inputs_embeds,
+            output_attentions,
+            output_hidden_states,
+            return_dict,
+            training=training,
+        )
 
-            setattr(self, k, v)
+        last_hidden = transformer_outputs[0]
+        pred_hid = last_hidden[:, -tgt_len:]
 
-    def dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None:
-        """
-        Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,
-        converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *"float32"*
-        string, which can then be stored in the json format.
-        """
-        if d.get("torch_dtype", None) is not None and not isinstance(
-            d["torch_dtype"], str
-        ):
-            d["torch_dtype"] = str(d["torch_dtype"]).split(".")[1]
-        for value in d.values():
-            if isinstance(value, dict):
-                self.dict_torch_dtype_to_str(value)
+        softmax_output = self.crit(pred_hid, labels, training=training)
+        prediction_scores = softmax_output if labels is None else ()
 
-    @classmethod
-    def register_for_auto_class(cls, auto_class="AutoConfig"):
-        """
-        Register this class with a given auto class. This should only be used for custom configurations as the ones in
-        the library are already mapped with `AutoConfig`.
+        if not return_dict:
+            return (prediction_scores,) + transformer_outputs[1:]
 
-        <Tip warning={true}>
+        return TFTransfoXLLMHeadModelOutput(
+            prediction_scores=prediction_scores,
+            mems=transformer_outputs.mems,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
 
-        This API is experimental and may have some slight breaking changes in the next releases.
+    def serving_output(self, output):
+        hs = (
+            tf.convert_to_tensor(output.hidden_states)
+            if self.config.output_hidden_states
+            else None
+        )
+        attns = (
+            tf.convert_to_tensor(output.attentions)
+            if self.config.output_attentions
+            else None
+        )
 
-        </Tip>
+        return TFTransfoXLLMHeadModelOutput(
+            prediction_scores=output.prediction_scores,
+            mems=tf.convert_to_tensor(output.mems),
+            hidden_states=hs,
+            attentions=attns,
+        )
 
-        Args:
-            auto_class (`str` or `type`, *optional*, defaults to `"AutoConfig"`):
-                The auto class to register this new configuration with.
-        """
-        if not isinstance(auto_class, str):
-            auto_class = auto_class.__name__
+    def prepare_inputs_for_generation(self, input_ids, past=None, **model_kwargs):
+        inputs = {}
 
-        import transformers.models.auto as auto_module
+        # if past is defined in model kwargs then use it for faster decoding
+        if past:
+            input_ids = tf.expand_dims(input_ids[:, -1], axis=-1)
+        else:
+            input_ids = input_ids
 
-        if not hasattr(auto_module, auto_class):
-            raise ValueError(f"{auto_class} is not a valid auto class.")
+        return inputs
 
-        cls._auto_class = auto_class
+    @staticmethod
+    def _reorder_cache(mems: List[tf.Tensor], beam_idx: tf.Tensor) -> List[tf.Tensor]:
+        return [tf.gather(layer_past, beam_idx, axis=1) for layer_past in mems]
 
 
-def get_configuration_file(configuration_files: List[str]) -> str:
+@add_start_docstrings(
     """
-    Get the configuration file to use for this version of transformers.
+    The Transfo XL Model transformer with a sequence classification head on top (linear layer).
 
-    Args:
-        configuration_files (`List[str]`): The list of available configuration files.
+    [`TFTransfoXLForSequenceClassification`] uses the last token in order to do the classification, as other causal
+    models (e.g. GPT-1,GPT-2) do.
 
-    Returns:
-        `str`: The configuration file to use.
-    """
-    configuration_files_map = {}
-    for file_name in configuration_files:
-        search = _re_configuration_file.search(file_name)
-        if search is not None:
-            v = search.groups()[0]
-            configuration_files_map[v] = file_name
-    available_versions = sorted(configuration_files_map.keys())
-
-    # Defaults to FULL_CONFIGURATION_FILE and then try to look at some newer versions.
-    configuration_file = CONFIG_NAME
-    transformers_version = version.parse(__version__)
-    for v in available_versions:
-        if version.parse(v) <= transformers_version:
-            configuration_file = configuration_files_map[v]
-        else:
-            # No point going further since the versions are sorted.
-            break
-
-    return configuration_file
-
-
-PretrainedConfig.push_to_hub = copy_func(PretrainedConfig.push_to_hub)
-PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(
-    object="config", object_class="AutoConfig", object_files="configuration file"
+    Since it does classification on the last token, it requires to know the position of the last token. If a
+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
+    each row of the batch).
+    """,
+    TRANSFO_XL_START_DOCSTRING,
 )
+class TFTransfoXLForSequenceClassification(
+    TFTransfoXLPreTrainedModel, TFSequenceClassificationLoss
+):
+    def __init__(self, config, *inputs, **kwargs):
+        super().__init__(config, *inputs, **kwargs)
+        self.num_labels = config.num_labels
+        self.score = tf.keras.layers.Dense(
+            config.num_labels,
+            kernel_initializer=get_initializer(config.init_range),
+            name="score",
+            use_bias=False,
+        )
+        self.transformer = TFTransfoXLMainLayer(config, name="transformer")
+
+    def get_output_embeddings(self):
+        return self.transformer.word_emb
+
+    @unpack_inputs
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TFTransfoXLSequenceClassifierOutputWithPast,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def call(
+        self,
+        input_ids: Optional[TFModelInputType] = None,
+        mems: Optional[List[tf.Tensor]] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        inputs_embeds: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        training: Optional[bool] = False,
+    ) -> Union[Tuple, TFTransfoXLSequenceClassifierOutputWithPast]:
+        r"""
+        labels (`tf.Tensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the cross entropy classification loss. Indices should be in `[0, ...,
+            config.vocab_size - 1]`.
+        """
+        transformer_outputs = self.transformer(
+            input_ids=input_ids,
+            mems=mems,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            training=training,
+        )
+
+        hidden_states = transformer_outputs[0]
+        logits = self.score(hidden_states)
+        in_logits = None
+        if self.config.pad_token_id is None:
+            sequence_lengths = -1
+        else:
+            if input_ids is not None:
+                sequence_lengths = (
+                    tf.reduce_sum(
+                        tf.cast(
+                            tf.math.not_equal(input_ids, self.config.pad_token_id),
+                            dtype=input_ids.dtype,
+                        ),
+                        -1,
+                        keepdims=False,
+                    )
+                    - 1
+                )
+                in_logits = tf.gather(logits, sequence_lengths, batch_dims=1, axis=1)
+            else:
+                sequence_lengths = -1
+                logger.warning(
+                    f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
+                    "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
+                )
+        loss = None
+
+        if labels is not None:
+            if input_ids is not None:
+                batch_size, sequence_length = shape_list(input_ids)[:2]
+            else:
+                batch_size, sequence_length = shape_list(inputs_embeds)[:2]
+            assert (
+                self.config.pad_token_id is not None or batch_size == 1
+            ), "Cannot handle batch sizes > 1 if no padding token is defined."
+
+            if not tf.is_tensor(sequence_lengths):
+                in_logits = logits[0:batch_size, sequence_lengths]
+
+            loss = self.hf_compute_loss(
+                tf.reshape(labels, [-1, 1]),
+                tf.reshape(in_logits, [-1, self.num_labels]),
+            )
+
+        pooled_logits = in_logits if in_logits is not None else logits
+
+        if not return_dict:
+            output = (pooled_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return TFTransfoXLSequenceClassifierOutputWithPast(
+            loss=loss,
+            logits=pooled_logits,
+            mems=transformer_outputs.mems,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
+
+    def serving_output(self, output):
+        hs = (
+            tf.convert_to_tensor(output.hidden_states)
+            if self.config.output_hidden_states
+            else None
+        )
+        attns = (
+            tf.convert_to_tensor(output.attentions)
+            if self.config.output_attentions
+            else None
+        )
+
+        return TFTransfoXLSequenceClassifierOutputWithPast(
+            logits=output.logits,
+            mems=tf.convert_to_tensor(output.mems),
+            hidden_states=hs,
+            attentions=attns,
+        )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/convert_graph_to_onnx.py` & `xs_transformers-1.0.1/xs_transformers/commands/pt_to_tf.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,619 +1,479 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# Copyright 2022 The HuggingFace Team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-import warnings
-from argparse import ArgumentParser
-from os import listdir, makedirs
-from pathlib import Path
-from typing import Dict, List, Optional, Tuple
-
-from packaging.version import Version, parse
-from transformers.pipelines import Pipeline, pipeline
-from transformers.tokenization_utils import BatchEncoding
-from transformers.utils import ModelOutput, is_tf_available, is_torch_available
-
-# This is the minimal required version to
-# support some ONNX Runtime features
-ORT_QUANTIZE_MINIMUM_VERSION = parse("1.4.0")
-
-
-SUPPORTED_PIPELINES = [
-    "feature-extraction",
-    "ner",
-    "sentiment-analysis",
-    "fill-mask",
-    "question-answering",
-    "text-generation",
-    "translation_en_to_fr",
-    "translation_en_to_de",
-    "translation_en_to_ro",
-]
+import inspect
+import os
+from argparse import ArgumentParser, Namespace
+from importlib import import_module
+
+import huggingface_hub
+import numpy as np
+from packaging import version
+
+from .. import (
+    FEATURE_EXTRACTOR_MAPPING,
+    PROCESSOR_MAPPING,
+    TOKENIZER_MAPPING,
+    AutoConfig,
+    AutoFeatureExtractor,
+    AutoProcessor,
+    AutoTokenizer,
+    is_datasets_available,
+    is_tf_available,
+    is_torch_available,
+)
+from ..utils import TF2_WEIGHTS_INDEX_NAME, TF2_WEIGHTS_NAME, logging
+from . import BaseTransformersCLICommand
 
+if is_tf_available():
+    import tensorflow as tf
 
-class OnnxConverterArgumentParser(ArgumentParser):
+    tf.config.experimental.enable_tensor_float_32_execution(False)
+
+if is_torch_available():
+    import torch
+
+if is_datasets_available():
+    from datasets import load_dataset
+
+
+MAX_ERROR = 5e-5  # larger error tolerance than in our internal tests, to avoid flaky user-facing errors
+
+
+def convert_command_factory(args: Namespace):
     """
-    Wraps all the script arguments supported to export transformers models to ONNX IR
+    Factory function used to convert a model PyTorch checkpoint in a TensorFlow 2 checkpoint.
+
+    Returns: ServeCommand
     """
+    return PTtoTFCommand(
+        args.model_name,
+        args.local_dir,
+        args.max_error,
+        args.new_weights,
+        args.no_pr,
+        args.push,
+        args.extra_commit_description,
+    )
 
-    def __init__(self):
-        super().__init__("ONNX Converter")
 
-        self.add_argument(
-            "--pipeline",
-            type=str,
-            choices=SUPPORTED_PIPELINES,
-            default="feature-extraction",
+class PTtoTFCommand(BaseTransformersCLICommand):
+    @staticmethod
+    def register_subcommand(parser: ArgumentParser):
+        """
+        Register this command to argparse so it's available for the transformer-cli
+
+        Args:
+            parser: Root parser to register command-specific arguments
+        """
+        train_parser = parser.add_parser(
+            "pt-to-tf",
+            help=(
+                "CLI tool to run convert a transformers model from a PyTorch checkpoint to a TensorFlow checkpoint."
+                " Can also be used to validate existing weights without opening PRs, with --no-pr."
+            ),
         )
-        self.add_argument(
-            "--model",
+        train_parser.add_argument(
+            "--model-name",
             type=str,
             required=True,
-            help="Model's id or path (ex: bert-base-cased)",
-        )
-        self.add_argument(
-            "--tokenizer", type=str, help="Tokenizer's id or path (ex: bert-base-cased)"
+            help="The model name, including owner/organization, as seen on the hub.",
         )
-        self.add_argument(
-            "--framework",
+        train_parser.add_argument(
+            "--local-dir",
             type=str,
-            choices=["pt", "tf"],
-            help="Framework for loading the model",
+            default="",
+            help="Optional local directory of the model repository. Defaults to /tmp/{model_name}",
+        )
+        train_parser.add_argument(
+            "--max-error",
+            type=float,
+            default=MAX_ERROR,
+            help=(
+                f"Maximum error tolerance. Defaults to {MAX_ERROR}. This flag should be avoided, use at your own risk."
+            ),
         )
-        self.add_argument("--opset", type=int, default=11, help="ONNX opset to use")
-        self.add_argument(
-            "--check-loading",
+        train_parser.add_argument(
+            "--new-weights",
             action="store_true",
-            help="Check ONNX is able to load the model",
+            help="Optional flag to create new TensorFlow weights, even if they already exist.",
         )
-        self.add_argument(
-            "--use-external-format",
+        train_parser.add_argument(
+            "--no-pr",
             action="store_true",
-            help="Allow exporting model >= than 2Gb",
+            help="Optional flag to NOT open a PR with converted weights.",
         )
-        self.add_argument(
-            "--quantize",
+        train_parser.add_argument(
+            "--push",
             action="store_true",
-            help="Quantize the neural network to be run with int8",
+            help="Optional flag to push the weights directly to `main` (requires permissions)",
         )
-        self.add_argument("output")
-
-
-def generate_identified_filename(filename: Path, identifier: str) -> Path:
-    """
-    Append a string-identifier at the end (before the extension, if any) to the provided filepath
-
-    Args:
-        filename: pathlib.Path The actual path object we would like to add an identifier suffix
-        identifier: The suffix to add
-
-    Returns: String with concatenated identifier at the end of the filename
-    """
-    return filename.parent.joinpath(filename.stem + identifier).with_suffix(
-        filename.suffix
-    )
-
-
-def check_onnxruntime_requirements(minimum_version: Version):
-    """
-    Check onnxruntime is installed and if the installed version match is recent enough
-
-    Raises:
-        ImportError: If onnxruntime is not installed or too old version is found
-    """
-    try:
-        import onnxruntime
-
-        # Parse the version of the installed onnxruntime
-        ort_version = parse(onnxruntime.__version__)
-
-        # We require 1.4.0 minimum
-        if ort_version < ORT_QUANTIZE_MINIMUM_VERSION:
-            raise ImportError(
-                f"We found an older version of onnxruntime ({onnxruntime.__version__}) "
-                f"but we require onnxruntime to be >= {minimum_version} to enable all the conversions options.\n"
-                "Please update onnxruntime by running `pip install --upgrade onnxruntime`"
-            )
-
-    except ImportError:
-        raise ImportError(
-            "onnxruntime doesn't seem to be currently installed. "
-            "Please install the onnxruntime by running `pip install onnxruntime`"
-            " and relaunch the conversion."
+        train_parser.add_argument(
+            "--extra-commit-description",
+            type=str,
+            default="",
+            help="Optional additional commit description to use when opening a PR (e.g. to tag the owner).",
         )
+        train_parser.set_defaults(func=convert_command_factory)
 
+    @staticmethod
+    def find_pt_tf_differences(pt_outputs, tf_outputs):
+        """
+        Compares the TensorFlow and PyTorch outputs, returning a dictionary with all tensor differences.
+        """
+        # 1. All output attributes must be the same
+        pt_out_attrs = set(pt_outputs.keys())
+        tf_out_attrs = set(tf_outputs.keys())
+        if pt_out_attrs != tf_out_attrs:
+            raise ValueError(
+                f"The model outputs have different attributes, aborting. (Pytorch: {pt_out_attrs}, TensorFlow:"
+                f" {tf_out_attrs})"
+            )
 
-def ensure_valid_input(model, tokens, input_names):
-    """
-    Ensure inputs are presented in the correct order, without any Non
-
-    Args:
-        model: The model used to forward the input data
-        tokens: BatchEncoding holding the input data
-        input_names: The name of the inputs
-
-    Returns: Tuple
-
-    """
-    print("Ensuring inputs are in correct order")
-
-    model_args_name = model.forward.__code__.co_varnames
-    model_args, ordered_input_names = [], []
-    for arg_name in model_args_name[1:]:  # start at index 1 to skip "self" argument
-        if arg_name in input_names:
-            ordered_input_names.append(arg_name)
-            model_args.append(tokens[arg_name])
-        else:
-            print(f"{arg_name} is not present in the generated input list.")
-            break
-
-    print(f"Generated inputs order: {ordered_input_names}")
-    return ordered_input_names, tuple(model_args)
-
-
-def infer_shapes(
-    nlp: Pipeline, framework: str
-) -> Tuple[List[str], List[str], Dict, BatchEncoding]:
-    """
-    Attempt to infer the static vs dynamic axes for each input and output tensors for a specific model
-
-    Args:
-        nlp: The pipeline object holding the model to be exported
-        framework: The framework identifier to dispatch to the correct inference scheme (pt/tf)
-
-    Returns:
-
-        - List of the inferred input variable names
-        - List of the inferred output variable names
-        - Dictionary with input/output variables names as key and shape tensor as value
-        - a BatchEncoding reference which was used to infer all the above information
-    """
-
-    def build_shape_dict(name: str, tensor, is_input: bool, seq_len: int):
-        if isinstance(tensor, (tuple, list)):
-            return [build_shape_dict(name, t, is_input, seq_len) for t in tensor]
-
-        else:
-            # Let's assume batch is the first axis with only 1 element (~~ might not be always true ...)
-            axes = {
-                [axis for axis, numel in enumerate(tensor.shape) if numel == 1][
-                    0
-                ]: "batch"
-            }
-            if is_input:
-                if len(tensor.shape) == 2:
-                    axes[1] = "sequence"
-                else:
-                    raise ValueError(
-                        f"Unable to infer tensor axes ({len(tensor.shape)})"
-                    )
+        # 2. For each output attribute, computes the difference
+        def _find_pt_tf_differences(pt_out, tf_out, differences, attr_name=""):
+            # If the current attribute is a tensor, it is a leaf and we make the comparison. Otherwise, we will dig in
+            # recursivelly, keeping the name of the attribute.
+            if isinstance(pt_out, torch.Tensor):
+                tensor_difference = np.max(np.abs(pt_out.numpy() - tf_out.numpy()))
+                differences[attr_name] = tensor_difference
             else:
-                seq_axes = [
-                    dim for dim, shape in enumerate(tensor.shape) if shape == seq_len
-                ]
-                axes.update({dim: "sequence" for dim in seq_axes})
-
-        print(f"Found {'input' if is_input else 'output'} {name} with shape: {axes}")
-        return axes
-
-    tokens = nlp.tokenizer("This is a sample output", return_tensors=framework)
-    seq_len = tokens.input_ids.shape[-1]
-    outputs = nlp.model(**tokens) if framework == "pt" else nlp.model(tokens)
-    if isinstance(outputs, ModelOutput):
-        outputs = outputs.to_tuple()
-    if not isinstance(outputs, (list, tuple)):
-        outputs = (outputs,)
-
-    # Generate input names & axes
-    input_vars = list(tokens.keys())
-    input_dynamic_axes = {
-        k: build_shape_dict(k, v, True, seq_len) for k, v in tokens.items()
-    }
-
-    # flatten potentially grouped outputs (past for gpt2, attentions)
-    outputs_flat = []
-    for output in outputs:
-        if isinstance(output, (tuple, list)):
-            outputs_flat.extend(output)
-        else:
-            outputs_flat.append(output)
-
-    # Generate output names & axes
-    output_names = [f"output_{i}" for i in range(len(outputs_flat))]
-    output_dynamic_axes = {
-        k: build_shape_dict(k, v, False, seq_len)
-        for k, v in zip(output_names, outputs_flat)
-    }
-
-    # Create the aggregated axes representation
-    dynamic_axes = dict(input_dynamic_axes, **output_dynamic_axes)
-    return input_vars, output_names, dynamic_axes, tokens
-
-
-def load_graph_from_args(
-    pipeline_name: str,
-    framework: str,
-    model: str,
-    tokenizer: Optional[str] = None,
-    **models_kwargs,
-) -> Pipeline:
-    """
-    Convert the set of arguments provided through the CLI to an actual pipeline reference (tokenizer + model
-
-    Args:
-        pipeline_name: The kind of pipeline to use (ner, question-answering, etc.)
-        framework: The actual model to convert the pipeline from ("pt" or "tf")
-        model: The model name which will be loaded by the pipeline
-        tokenizer: The tokenizer name which will be loaded by the pipeline, default to the model's value
-
-    Returns: Pipeline object
-
-    """
-    # If no tokenizer provided
-    if tokenizer is None:
-        tokenizer = model
-
-    # Check the wanted framework is available
-    if framework == "pt" and not is_torch_available():
-        raise Exception(
-            "Cannot convert because PyTorch is not installed. Please install torch first."
-        )
-    if framework == "tf" and not is_tf_available():
-        raise Exception(
-            "Cannot convert because TF is not installed. Please install tensorflow first."
-        )
-
-    print(f"Loading pipeline (model: {model}, tokenizer: {tokenizer})")
-
-    # Allocate tokenizer and model
-    return pipeline(
-        pipeline_name,
-        model=model,
-        tokenizer=tokenizer,
-        framework=framework,
-        model_kwargs=models_kwargs,
-    )
-
-
-def convert_pytorch(nlp: Pipeline, opset: int, output: Path, use_external_format: bool):
-    """
-    Export a PyTorch backed pipeline to ONNX Intermediate Representation (IR
-
-    Args:
-        nlp: The pipeline to be exported
-        opset: The actual version of the ONNX operator set to use
-        output: Path where will be stored the generated ONNX model
-        use_external_format: Split the model definition from its parameters to allow model bigger than 2GB
-
-    Returns:
-
-    """
-    if not is_torch_available():
-        raise Exception(
-            "Cannot convert because PyTorch is not installed. Please install torch first."
-        )
-
-    import torch
-    from torch.onnx import export
+                root_name = attr_name
+                for i, pt_item in enumerate(pt_out):
+                    # If it is a named attribute, we keep the name. Otherwise, just its index.
+                    if isinstance(pt_item, str):
+                        branch_name = root_name + pt_item
+                        tf_item = tf_out[pt_item]
+                        pt_item = pt_out[pt_item]
+                    else:
+                        branch_name = root_name + f"[{i}]"
+                        tf_item = tf_out[i]
+                    differences = _find_pt_tf_differences(
+                        pt_item, tf_item, differences, branch_name
+                    )
 
-    from .pytorch_utils import is_torch_less_than_1_11
+            return differences
 
-    print(f"Using framework PyTorch: {torch.__version__}")
+        return _find_pt_tf_differences(pt_outputs, tf_outputs, {})
 
-    with torch.no_grad():
-        input_names, output_names, dynamic_axes, tokens = infer_shapes(nlp, "pt")
-        ordered_input_names, model_args = ensure_valid_input(
-            nlp.model, tokens, input_names
-        )
-
-        # PyTorch deprecated the `enable_onnx_checker` and `use_external_data_format` arguments in v1.11,
-        # so we check the torch version for backwards compatibility
-        if is_torch_less_than_1_11:
-            export(
-                nlp.model,
-                model_args,
-                f=output.as_posix(),
-                input_names=ordered_input_names,
-                output_names=output_names,
-                dynamic_axes=dynamic_axes,
-                do_constant_folding=True,
-                use_external_data_format=use_external_format,
-                enable_onnx_checker=True,
-                opset_version=opset,
+    def __init__(
+        self,
+        model_name: str,
+        local_dir: str,
+        max_error: float,
+        new_weights: bool,
+        no_pr: bool,
+        push: bool,
+        extra_commit_description: str,
+        *args,
+    ):
+        self._logger = logging.get_logger("transformers-cli/pt_to_tf")
+        self._model_name = model_name
+        self._local_dir = local_dir if local_dir else os.path.join("/tmp", model_name)
+        self._max_error = max_error
+        self._new_weights = new_weights
+        self._no_pr = no_pr
+        self._push = push
+        self._extra_commit_description = extra_commit_description
+
+    def get_inputs(self, pt_model, config):
+        """
+        Returns the right inputs for the model, based on its signature.
+        """
+
+        def _get_audio_input():
+            ds = load_dataset(
+                "hf-internal-testing/librispeech_asr_dummy", "clean", split="validation"
             )
+            speech_samples = ds.sort("id").select(range(2))[:2]["audio"]
+            raw_samples = [x["array"] for x in speech_samples]
+            return raw_samples
+
+        model_config_class = type(pt_model.config)
+        if model_config_class in PROCESSOR_MAPPING:
+            processor = AutoProcessor.from_pretrained(self._local_dir)
+            if (
+                model_config_class in TOKENIZER_MAPPING
+                and processor.tokenizer.pad_token is None
+            ):
+                processor.tokenizer.pad_token = processor.tokenizer.eos_token
+        elif model_config_class in FEATURE_EXTRACTOR_MAPPING:
+            processor = AutoFeatureExtractor.from_pretrained(self._local_dir)
+        elif model_config_class in TOKENIZER_MAPPING:
+            processor = AutoTokenizer.from_pretrained(self._local_dir)
+            if processor.pad_token is None:
+                processor.pad_token = processor.eos_token
         else:
-            export(
-                nlp.model,
-                model_args,
-                f=output.as_posix(),
-                input_names=ordered_input_names,
-                output_names=output_names,
-                dynamic_axes=dynamic_axes,
-                do_constant_folding=True,
-                opset_version=opset,
+            raise ValueError(
+                f"Unknown data processing type (model config type: {model_config_class})"
             )
 
-
-def convert_tensorflow(nlp: Pipeline, opset: int, output: Path):
-    """
-    Export a TensorFlow backed pipeline to ONNX Intermediate Representation (IR)
-
-    Args:
-        nlp: The pipeline to be exported
-        opset: The actual version of the ONNX operator set to use
-        output: Path where will be stored the generated ONNX model
-
-    Notes: TensorFlow cannot export model bigger than 2GB due to internal constraint from TensorFlow
-
-    """
-    if not is_tf_available():
-        raise Exception(
-            "Cannot convert because TF is not installed. Please install tensorflow first."
+        model_forward_signature = set(
+            inspect.signature(pt_model.forward).parameters.keys()
         )
+        processor_inputs = {}
+        if "input_ids" in model_forward_signature:
+            processor_inputs.update(
+                {
+                    "text": [
+                        "Hi there!",
+                        "I am a batch with more than one row and different input lengths.",
+                    ],
+                    "padding": True,
+                    "truncation": True,
+                }
+            )
+        if "pixel_values" in model_forward_signature:
+            sample_images = load_dataset("cifar10", "plain_text", split="test")[:2][
+                "img"
+            ]
+            processor_inputs.update({"images": sample_images})
+        if "input_features" in model_forward_signature:
+            feature_extractor_signature = inspect.signature(
+                processor.feature_extractor
+            ).parameters
+            # Pad to the largest input length by default but take feature extractor default
+            # padding value if it exists e.g. "max_length" and is not False or None
+            if "padding" in feature_extractor_signature:
+                default_strategy = feature_extractor_signature["padding"].default
+                if default_strategy is not False and default_strategy is not None:
+                    padding_strategy = default_strategy
+                else:
+                    padding_strategy = True
+            else:
+                padding_strategy = True
+            processor_inputs.update(
+                {"audio": _get_audio_input(), "padding": padding_strategy}
+            )
+        if "input_values" in model_forward_signature:  # Wav2Vec2 audio input
+            processor_inputs.update({"audio": _get_audio_input(), "padding": True})
+        pt_input = processor(**processor_inputs, return_tensors="pt")
+        tf_input = processor(**processor_inputs, return_tensors="tf")
+
+        # Extra input requirements, in addition to the input modality
+        if config.is_encoder_decoder or (
+            hasattr(pt_model, "encoder") and hasattr(pt_model, "decoder")
+        ):
+            decoder_input_ids = np.asarray([[1], [1]], dtype=int) * (
+                pt_model.config.decoder_start_token_id or 0
+            )
+            pt_input.update({"decoder_input_ids": torch.tensor(decoder_input_ids)})
+            tf_input.update(
+                {"decoder_input_ids": tf.convert_to_tensor(decoder_input_ids)}
+            )
 
-    print("/!\\ Please note TensorFlow doesn't support exporting model > 2Gb /!\\")
+        return pt_input, tf_input
 
-    try:
-        import tensorflow as tf
-        import tf2onnx
-        from tf2onnx import __version__ as t2ov
+    def run(self):
+        # hub version 0.9.0 introduced the possibility of programmatically opening PRs with normal write tokens.
+        if version.parse(huggingface_hub.__version__) < version.parse("0.9.0"):
+            raise ImportError(
+                "The huggingface_hub version must be >= 0.9.0 to use this command. Please update your huggingface_hub"
+                " installation."
+            )
+        else:
+            from huggingface_hub import Repository, create_commit
+            from huggingface_hub._commit_api import CommitOperationAdd
 
-        print(f"Using framework TensorFlow: {tf.version.VERSION}, tf2onnx: {t2ov}")
+        # Fetch remote data
+        repo = Repository(local_dir=self._local_dir, clone_from=self._model_name)
 
-        # Build
-        input_names, output_names, dynamic_axes, tokens = infer_shapes(nlp, "tf")
+        # Load config and get the appropriate architecture -- the latter is needed to convert the head's weights
+        config = AutoConfig.from_pretrained(self._local_dir)
+        architectures = config.architectures
+        if architectures is None:  # No architecture defined -- use auto classes
+            pt_class = getattr(import_module("transformers"), "AutoModel")
+            tf_class = getattr(import_module("transformers"), "TFAutoModel")
+            self._logger.warning(
+                "No detected architecture, using AutoModel/TFAutoModel"
+            )
+        else:  # Architecture defined -- use it
+            if len(architectures) > 1:
+                raise ValueError(
+                    f"More than one architecture was found, aborting. (architectures = {architectures})"
+                )
+            self._logger.warning(f"Detected architecture: {architectures[0]}")
+            pt_class = getattr(import_module("transformers"), architectures[0])
+            try:
+                tf_class = getattr(
+                    import_module("transformers"), "TF" + architectures[0]
+                )
+            except AttributeError:
+                raise AttributeError(
+                    f"The TensorFlow equivalent of {architectures[0]} doesn't exist in transformers."
+                )
 
-        # Forward
-        nlp.model.predict(tokens.data)
-        input_signature = [
-            tf.TensorSpec.from_tensor(tensor, name=key)
-            for key, tensor in tokens.items()
-        ]
-        model_proto, _ = tf2onnx.convert.from_keras(
-            nlp.model, input_signature, opset=opset, output_path=output.as_posix()
+        # Load models and acquire a basic input compatible with the model.
+        pt_model = pt_class.from_pretrained(self._local_dir)
+        pt_model.eval()
+
+        tf_from_pt_model = tf_class.from_pretrained(self._local_dir, from_pt=True)
+        pt_input, tf_input = self.get_inputs(pt_model, config)
+
+        with torch.no_grad():
+            pt_outputs = pt_model(**pt_input, output_hidden_states=True)
+        del pt_model  # will no longer be used, and may have a large memory footprint
+
+        tf_from_pt_model = tf_class.from_pretrained(self._local_dir, from_pt=True)
+        tf_from_pt_outputs = tf_from_pt_model(**tf_input, output_hidden_states=True)
+
+        # Confirms that cross loading PT weights into TF worked.
+        crossload_differences = self.find_pt_tf_differences(
+            pt_outputs, tf_from_pt_outputs
+        )
+        output_differences = {
+            k: v for k, v in crossload_differences.items() if "hidden" not in k
+        }
+        hidden_differences = {
+            k: v for k, v in crossload_differences.items() if "hidden" in k
+        }
+        if len(output_differences) == 0 and architectures is not None:
+            raise ValueError(
+                f"Something went wrong -- the config file has architectures ({architectures}), but no model head"
+                " output was found. All outputs start with 'hidden'"
+            )
+        max_crossload_output_diff = (
+            max(output_differences.values()) if output_differences else 0.0
         )
+        max_crossload_hidden_diff = max(hidden_differences.values())
+        if (
+            max_crossload_output_diff > self._max_error
+            or max_crossload_hidden_diff > self._max_error
+        ):
+            raise ValueError(
+                "The cross-loaded TensorFlow model has different outputs, something went wrong!\n"
+                + f"\nList of maximum output differences above the threshold ({self._max_error}):\n"
+                + "\n".join(
+                    [
+                        f"{k}: {v:.3e}"
+                        for k, v in output_differences.items()
+                        if v > self._max_error
+                    ]
+                )
+                + f"\n\nList of maximum hidden layer differences above the threshold ({self._max_error}):\n"
+                + "\n".join(
+                    [
+                        f"{k}: {v:.3e}"
+                        for k, v in hidden_differences.items()
+                        if v > self._max_error
+                    ]
+                )
+            )
 
-    except ImportError as e:
-        raise Exception(
-            f"Cannot import {e.name} required to convert TF model to ONNX. Please install {e.name} first. {e}"
+        # Save the weights in a TF format (if needed) and confirms that the results are still good
+        tf_weights_path = os.path.join(self._local_dir, TF2_WEIGHTS_NAME)
+        tf_weights_index_path = os.path.join(self._local_dir, TF2_WEIGHTS_INDEX_NAME)
+        if (
+            not os.path.exists(tf_weights_path)
+            and not os.path.exists(tf_weights_index_path)
+        ) or self._new_weights:
+            tf_from_pt_model.save_pretrained(self._local_dir)
+        del tf_from_pt_model  # will no longer be used, and may have a large memory footprint
+
+        tf_model = tf_class.from_pretrained(self._local_dir)
+        tf_outputs = tf_model(**tf_input, output_hidden_states=True)
+
+        conversion_differences = self.find_pt_tf_differences(pt_outputs, tf_outputs)
+        output_differences = {
+            k: v for k, v in conversion_differences.items() if "hidden" not in k
+        }
+        hidden_differences = {
+            k: v for k, v in conversion_differences.items() if "hidden" in k
+        }
+        if len(output_differences) == 0 and architectures is not None:
+            raise ValueError(
+                f"Something went wrong -- the config file has architectures ({architectures}), but no model head"
+                " output was found. All outputs start with 'hidden'"
+            )
+        max_conversion_output_diff = (
+            max(output_differences.values()) if output_differences else 0.0
         )
-
-
-def convert(
-    framework: str,
-    model: str,
-    output: Path,
-    opset: int,
-    tokenizer: Optional[str] = None,
-    use_external_format: bool = False,
-    pipeline_name: str = "feature-extraction",
-    **model_kwargs,
-):
-    """
-    Convert the pipeline object to the ONNX Intermediate Representation (IR) format
-
-    Args:
-        framework: The framework the pipeline is backed by ("pt" or "tf")
-        model: The name of the model to load for the pipeline
-        output: The path where the ONNX graph will be stored
-        opset: The actual version of the ONNX operator set to use
-        tokenizer: The name of the model to load for the pipeline, default to the model's name if not provided
-        use_external_format:
-            Split the model definition from its parameters to allow model bigger than 2GB (PyTorch only)
-        pipeline_name: The kind of pipeline to instantiate (ner, question-answering, etc.)
-        model_kwargs: Keyword arguments to be forwarded to the model constructor
-
-    Returns:
-
-    """
-    warnings.warn(
-        "The `transformers.convert_graph_to_onnx` package is deprecated and will be removed in version 5 of"
-        " Transformers",
-        FutureWarning,
-    )
-    print(f"ONNX opset version set to: {opset}")
-
-    # Load the pipeline
-    nlp = load_graph_from_args(
-        pipeline_name, framework, model, tokenizer, **model_kwargs
-    )
-
-    if not output.parent.exists():
-        print(f"Creating folder {output.parent}")
-        makedirs(output.parent.as_posix())
-    elif len(listdir(output.parent.as_posix())) > 0:
-        raise Exception(
-            f"Folder {output.parent.as_posix()} is not empty, aborting conversion"
-        )
-
-    # Export the graph
-    if framework == "pt":
-        convert_pytorch(nlp, opset, output, use_external_format)
-    else:
-        convert_tensorflow(nlp, opset, output)
-
-
-def optimize(onnx_model_path: Path) -> Path:
-    """
-    Load the model at the specified path and let onnxruntime look at transformations on the graph to enable all the
-    optimizations possible
-
-    Args:
-        onnx_model_path: filepath where the model binary description is stored
-
-    Returns: Path where the optimized model binary description has been saved
-
-    """
-    from onnxruntime import InferenceSession, SessionOptions
-
-    # Generate model name with suffix "optimized"
-    opt_model_path = generate_identified_filename(onnx_model_path, "-optimized")
-    sess_option = SessionOptions()
-    sess_option.optimized_model_filepath = opt_model_path.as_posix()
-    _ = InferenceSession(onnx_model_path.as_posix(), sess_option)
-
-    print(f"Optimized model has been written at {opt_model_path}: \N{heavy check mark}")
-    print(
-        "/!\\ Optimized model contains hardware specific operators which might not be portable. /!\\"
-    )
-
-    return opt_model_path
-
-
-def quantize(onnx_model_path: Path) -> Path:
-    """
-    Quantize the weights of the model from float32 to in8 to allow very efficient inference on modern CPU
-
-    Args:
-        onnx_model_path: Path to location the exported ONNX model is stored
-
-    Returns: The Path generated for the quantized
-    """
-    import onnx
-    import onnxruntime
-    from onnx.onnx_pb import ModelProto
-    from onnxruntime.quantization import QuantizationMode
-    from onnxruntime.quantization.onnx_quantizer import ONNXQuantizer
-    from onnxruntime.quantization.registry import IntegerOpsRegistry
-
-    # Load the ONNX model
-    onnx_model = onnx.load(onnx_model_path.as_posix())
-
-    if parse(onnx.__version__) < parse("1.5.0"):
-        print(
-            "Models larger than 2GB will fail to quantize due to protobuf constraint.\n"
-            "Please upgrade to onnxruntime >= 1.5.0."
-        )
-
-    # Copy it
-    copy_model = ModelProto()
-    copy_model.CopyFrom(onnx_model)
-
-    # Construct quantizer
-    # onnxruntime renamed input_qType to activation_qType in v1.13.1, so we
-    # check the onnxruntime version to ensure backward compatibility.
-    # See also: https://github.com/microsoft/onnxruntime/pull/12873
-    if parse(onnxruntime.__version__) < parse("1.13.1"):
-        quantizer = ONNXQuantizer(
-            model=copy_model,
-            per_channel=False,
-            reduce_range=False,
-            mode=QuantizationMode.IntegerOps,
-            static=False,
-            weight_qType=True,
-            input_qType=False,
-            tensors_range=None,
-            nodes_to_quantize=None,
-            nodes_to_exclude=None,
-            op_types_to_quantize=list(IntegerOpsRegistry),
-        )
-    else:
-        quantizer = ONNXQuantizer(
-            model=copy_model,
-            per_channel=False,
-            reduce_range=False,
-            mode=QuantizationMode.IntegerOps,
-            static=False,
-            weight_qType=True,
-            activation_qType=False,
-            tensors_range=None,
-            nodes_to_quantize=None,
-            nodes_to_exclude=None,
-            op_types_to_quantize=list(IntegerOpsRegistry),
-        )
-
-    # Quantize and export
-    quantizer.quantize_model()
-
-    # Append "-quantized" at the end of the model's name
-    quantized_model_path = generate_identified_filename(onnx_model_path, "-quantized")
-
-    # Save model
-    print(
-        f"Quantized model has been written at {quantized_model_path}: \N{heavy check mark}"
-    )
-    onnx.save_model(quantizer.model.model, quantized_model_path.as_posix())
-
-    return quantized_model_path
-
-
-def verify(path: Path):
-    from onnxruntime import InferenceSession, SessionOptions
-    from onnxruntime.capi.onnxruntime_pybind11_state import RuntimeException
-
-    print(f"Checking ONNX model loading from: {path} ...")
-    try:
-        onnx_options = SessionOptions()
-        _ = InferenceSession(
-            path.as_posix(), onnx_options, providers=["CPUExecutionProvider"]
-        )
-        print(f"Model {path} correctly loaded: \N{heavy check mark}")
-    except RuntimeException as re:
-        print(f"Error while loading the model {re}: \N{heavy ballot x}")
-
-
-if __name__ == "__main__":
-    parser = OnnxConverterArgumentParser()
-    args = parser.parse_args()
-
-    # Make sure output is absolute path
-    args.output = Path(args.output).absolute()
-
-    try:
-        print("\n====== Converting model to ONNX ======")
-        # Convert
-        convert(
-            args.framework,
-            args.model,
-            args.output,
-            args.opset,
-            args.tokenizer,
-            args.use_external_format,
-            args.pipeline,
-        )
-
-        if args.quantize:
-            # Ensure requirements for quantization on onnxruntime is met
-            check_onnxruntime_requirements(ORT_QUANTIZE_MINIMUM_VERSION)
-
-            # onnxruntime optimizations doesn't provide the same level of performances on TensorFlow than PyTorch
-            if args.framework == "tf":
-                print(
-                    "\t Using TensorFlow might not provide the same optimization level compared to PyTorch.\n"
-                    "\t For TensorFlow users you can try optimizing the model directly through onnxruntime_tools.\n"
-                    "\t For more information, please refer to the onnxruntime documentation:\n"
-                    "\t\thttps://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/transformers\n"
+        max_conversion_hidden_diff = max(hidden_differences.values())
+        if (
+            max_conversion_output_diff > self._max_error
+            or max_conversion_hidden_diff > self._max_error
+        ):
+            raise ValueError(
+                "The converted TensorFlow model has different outputs, something went wrong!\n"
+                + f"\nList of maximum output differences above the threshold ({self._max_error}):\n"
+                + "\n".join(
+                    [
+                        f"{k}: {v:.3e}"
+                        for k, v in output_differences.items()
+                        if v > self._max_error
+                    ]
                 )
+                + f"\n\nList of maximum hidden layer differences above the threshold ({self._max_error}):\n"
+                + "\n".join(
+                    [
+                        f"{k}: {v:.3e}"
+                        for k, v in hidden_differences.items()
+                        if v > self._max_error
+                    ]
+                )
+            )
 
-            print("\n====== Optimizing ONNX model ======")
-
-            # Quantization works best when using the optimized version of the model
-            args.optimized_output = optimize(args.output)
-
-            # Do the quantization on the right graph
-            args.quantized_output = quantize(args.optimized_output)
-
-        # And verify
-        if args.check_loading:
-            print("\n====== Check exported ONNX model(s) ======")
-            verify(args.output)
-
-            if hasattr(args, "optimized_output"):
-                verify(args.optimized_output)
-
-            if hasattr(args, "quantized_output"):
-                verify(args.quantized_output)
+        commit_message = "Update TF weights" if self._new_weights else "Add TF weights"
+        if self._push:
+            repo.git_add(auto_lfs_track=True)
+            repo.git_commit(commit_message)
+            repo.git_push(blocking=True)  # this prints a progress bar with the upload
+            self._logger.warning(f"TF weights pushed into {self._model_name}")
+        elif not self._no_pr:
+            self._logger.warning("Uploading the weights into a new PR...")
+            commit_descrition = (
+                "Model converted by the [`transformers`' `pt_to_tf`"
+                " CLI](https://github.com/huggingface/transformers/blob/main/src/transformers/commands/pt_to_tf.py). "
+                "All converted model outputs and hidden layers were validated against its Pytorch counterpart.\n\n"
+                f"Maximum crossload output difference={max_crossload_output_diff:.3e}; "
+                f"Maximum crossload hidden layer difference={max_crossload_hidden_diff:.3e};\n"
+                f"Maximum conversion output difference={max_conversion_output_diff:.3e}; "
+                f"Maximum conversion hidden layer difference={max_conversion_hidden_diff:.3e};\n"
+            )
+            if self._max_error > MAX_ERROR:
+                commit_descrition += f"\n\nCAUTION: The maximum admissible error was manually increased to {self._max_error}!"
+            if self._extra_commit_description:
+                commit_descrition += "\n\n" + self._extra_commit_description
+
+            # sharded model -> adds all related files (index and .h5 shards)
+            if os.path.exists(tf_weights_index_path):
+                operations = [
+                    CommitOperationAdd(
+                        path_in_repo=TF2_WEIGHTS_INDEX_NAME,
+                        path_or_fileobj=tf_weights_index_path,
+                    )
+                ]
+                for shard_path in tf.io.gfile.glob(self._local_dir + "/tf_model-*.h5"):
+                    operations += [
+                        CommitOperationAdd(
+                            path_in_repo=os.path.basename(shard_path),
+                            path_or_fileobj=shard_path,
+                        )
+                    ]
+            else:
+                operations = [
+                    CommitOperationAdd(
+                        path_in_repo=TF2_WEIGHTS_NAME, path_or_fileobj=tf_weights_path
+                    )
+                ]
 
-    except Exception as e:
-        print(f"Error while converting the model: {e}")
-        exit(1)
+            hub_pr_url = create_commit(
+                repo_id=self._model_name,
+                operations=operations,
+                commit_message=commit_message,
+                commit_description=commit_descrition,
+                repo_type="model",
+                create_pr=True,
+            )
+            self._logger.warning(f"PR open in {hub_pr_url}")
```

### Comparing `xs_transformers-1.0.0/xs_transformers/convert_tf_hub_seq_to_seq_bert_to_pytorch.py` & `xs_transformers-1.0.1/xs_transformers/models/albert/convert_albert_original_tf_checkpoint_to_pytorch.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,101 +1,73 @@
 # coding=utf-8
-# Copyright 2020 The HuggingFace Inc. team.
+# Copyright 2018 The HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Convert Seq2Seq TF Hub checkpoint."""
+"""Convert ALBERT checkpoint."""
 
 
 import argparse
 
-from . import (
-    BertConfig,
-    BertGenerationConfig,
-    BertGenerationDecoder,
-    BertGenerationEncoder,
-    load_tf_weights_in_bert_generation,
-    logging,
-)
+import torch
+from transformers import AlbertConfig, AlbertForPreTraining, load_tf_weights_in_albert
+from transformers.utils import logging
 
 logging.set_verbosity_info()
 
 
 def convert_tf_checkpoint_to_pytorch(
-    tf_hub_path, pytorch_dump_path, is_encoder_named_decoder, vocab_size, is_encoder
+    tf_checkpoint_path, albert_config_file, pytorch_dump_path
 ):
     # Initialise PyTorch model
-    bert_config = BertConfig.from_pretrained(
-        "bert-large-cased",
-        vocab_size=vocab_size,
-        max_position_embeddings=512,
-        is_decoder=True,
-        add_cross_attention=True,
-    )
-    bert_config_dict = bert_config.to_dict()
-    del bert_config_dict["type_vocab_size"]
-    config = BertGenerationConfig(**bert_config_dict)
-    if is_encoder:
-        model = BertGenerationEncoder(config)
-    else:
-        model = BertGenerationDecoder(config)
+    config = AlbertConfig.from_json_file(albert_config_file)
     print(f"Building PyTorch model from configuration: {config}")
+    model = AlbertForPreTraining(config)
 
     # Load weights from tf checkpoint
-    load_tf_weights_in_bert_generation(
-        model,
-        tf_hub_path,
-        model_class="bert",
-        is_encoder_named_decoder=is_encoder_named_decoder,
-        is_encoder=is_encoder,
-    )
+    load_tf_weights_in_albert(model, config, tf_checkpoint_path)
 
     # Save pytorch-model
-    print(f"Save PyTorch model and config to {pytorch_dump_path}")
-    model.save_pretrained(pytorch_dump_path)
+    print(f"Save PyTorch model to {pytorch_dump_path}")
+    torch.save(model.state_dict(), pytorch_dump_path)
 
 
 if __name__ == "__main__":
     parser = argparse.ArgumentParser()
     # Required parameters
     parser.add_argument(
-        "--tf_hub_path",
+        "--tf_checkpoint_path",
         default=None,
         type=str,
         required=True,
         help="Path to the TensorFlow checkpoint path.",
     )
     parser.add_argument(
-        "--pytorch_dump_path",
+        "--albert_config_file",
         default=None,
         type=str,
         required=True,
-        help="Path to the output PyTorch model.",
+        help=(
+            "The config json file corresponding to the pre-trained ALBERT model. \n"
+            "This specifies the model architecture."
+        ),
     )
     parser.add_argument(
-        "--is_encoder_named_decoder",
-        action="store_true",
-        help="If decoder has to be renamed to encoder in PyTorch model.",
-    )
-    parser.add_argument(
-        "--is_encoder", action="store_true", help="If model is an encoder."
-    )
-    parser.add_argument(
-        "--vocab_size", default=50358, type=int, help="Vocab size of model"
+        "--pytorch_dump_path",
+        default=None,
+        type=str,
+        required=True,
+        help="Path to the output PyTorch model.",
     )
     args = parser.parse_args()
     convert_tf_checkpoint_to_pytorch(
-        args.tf_hub_path,
-        args.pytorch_dump_path,
-        args.is_encoder_named_decoder,
-        args.vocab_size,
-        is_encoder=args.is_encoder,
+        args.tf_checkpoint_path, args.albert_config_file, args.pytorch_dump_path
     )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/debug_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/esm/openfold_utils/chunk_utils.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,362 +1,409 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# Copyright 2021 AlQuraishi Laboratory
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
-#     http://www.apache.org/licenses/LICENSE-2.0
+#      http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-import collections
-
-from .utils import ExplicitEnum, is_torch_available, logging
-
-if is_torch_available():
-    import torch
-
-
-logger = logging.get_logger(__name__)
-
-
-class DebugUnderflowOverflow:
+import logging
+import math
+from functools import partial
+from typing import Any, Callable, Dict, Optional, Sequence, Tuple
+
+import torch
+
+from .tensor_utils import tensor_tree_map, tree_map
+
+
+def _fetch_dims(tree):
+    shapes = []
+    tree_type = type(tree)
+    if tree_type is dict:
+        for v in tree.values():
+            shapes.extend(_fetch_dims(v))
+    elif tree_type is list or tree_type is tuple:
+        for t in tree:
+            shapes.extend(_fetch_dims(t))
+    elif tree_type is torch.Tensor:
+        shapes.append(tree.shape)
+    else:
+        raise ValueError("Not supported")
+
+    return shapes
+
+
+@torch.jit.ignore
+def _flat_idx_to_idx(
+    flat_idx: int,
+    dims: Tuple[int],
+) -> Tuple[int]:
+    idx = []
+    for d in reversed(dims):
+        idx.append(flat_idx % d)
+        flat_idx = flat_idx // d
+
+    return tuple(reversed(idx))
+
+
+@torch.jit.ignore
+def _get_minimal_slice_set(
+    start: Sequence[int],
+    end: Sequence[int],
+    dims: int,
+    start_edges: Optional[Sequence[bool]] = None,
+    end_edges: Optional[Sequence[bool]] = None,
+) -> Sequence[Tuple[int]]:
     """
-    This debug class helps detect and understand where the model starts getting very large or very small, and more
-    importantly `nan` or `inf` weight and activation elements.
-
-    There are 2 working modes:
-
-    1. Underflow/overflow detection (default)
-    2. Specific batch absolute min/max tracing without detection
-
-    Mode 1: Underflow/overflow detection
-
-    To activate the underflow/overflow detection, initialize the object with the model :
-
-    ```python
-    debug_overflow = DebugUnderflowOverflow(model)
-    ```
-
-    then run the training as normal and if `nan` or `inf` gets detected in at least one of the weight, input or output
-    elements this module will throw an exception and will print `max_frames_to_save` frames that lead to this event,
-    each frame reporting
-
-    1. the fully qualified module name plus the class name whose `forward` was run
-    2. the absolute min and max value of all elements for each module weights, and the inputs and output
-
-    For example, here is the header and the last few frames in detection report for `google/mt5-small` run in fp16
-    mixed precision :
-
-    ```
-    Detected inf/nan during batch_number=0
-    Last 21 forward frames:
-    abs min  abs max  metadata
-    [...]
-                      encoder.block.2.layer.1.DenseReluDense.wi_0 Linear
-    2.17e-07 4.50e+00 weight
-    1.79e-06 4.65e+00 input[0]
-    2.68e-06 3.70e+01 output
-                      encoder.block.2.layer.1.DenseReluDense.wi_1 Linear
-    8.08e-07 2.66e+01 weight
-    1.79e-06 4.65e+00 input[0]
-    1.27e-04 2.37e+02 output
-                      encoder.block.2.layer.1.DenseReluDense.wo Linear
-    1.01e-06 6.44e+00 weight
-    0.00e+00 9.74e+03 input[0]
-    3.18e-04 6.27e+04 output
-                      encoder.block.2.layer.1.DenseReluDense T5DenseGatedGeluDense
-    1.79e-06 4.65e+00 input[0]
-    3.18e-04 6.27e+04 output
-                      encoder.block.2.layer.1.dropout Dropout
-    3.18e-04 6.27e+04 input[0]
-    0.00e+00      inf output
-    ```
-
-    You can see here, that `T5DenseGatedGeluDense.forward` resulted in output activations, whose absolute max value was
-    around 62.7K, which is very close to fp16's top limit of 64K. In the next frame we have `Dropout` which
-    renormalizes the weights, after it zeroed some of the elements, which pushes the absolute max value to more than
-    64K, and we get an overlow.
-
-    As you can see it's the previous frames that we need to look into when the numbers start going into very large for
-    fp16 numbers.
+    Produces an ordered sequence of tensor slices that, when used in sequence on a tensor with shape dims, yields
+    tensors that contain every leaf in the contiguous range [start, end]. Care is taken to yield a short sequence of
+    slices, and perhaps even the shortest possible (I'm pretty sure it's the latter).
 
-    The tracking is done in a forward hook, which gets invoked immediately after `forward` has completed.
-
-    By default the last 21 frames are printed. You can change the default to adjust for your needs. For example :
-
-    ```python
-    debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=100)
-    ```
-
-        To validate that you have set up this debugging feature correctly, and you intend to use it in a training that
-        may take hours to complete, first run it with normal tracing enabled for one of a few batches as explained in
-        the next section.
-
-
-        Mode 2. Specific batch absolute min/max tracing without detection
-
-        The second work mode is per-batch tracing with the underflow/overflow detection feature turned off.
+    end is INCLUSIVE.
+    """
 
-        Let's say you want to watch the absolute min and max values for all the ingredients of each `forward` call of a
-    given batch, and only do that for batches 1 and 3. Then you instantiate this class as :
+    # start_edges and end_edges both indicate whether, starting from any given
+    # dimension, the start/end index is at the top/bottom edge of the
+    # corresponding tensor, modeled as a tree
+    def reduce_edge_list(l):
+        tally = 1
+        for i in range(len(l)):
+            reversed_idx = -1 * (i + 1)
+            l[reversed_idx] *= tally
+            tally = l[reversed_idx]
+
+    if start_edges is None:
+        start_edges = [s == 0 for s in start]
+        reduce_edge_list(start_edges)
+    if end_edges is None:
+        end_edges = [e == (d - 1) for e, d in zip(end, dims)]
+        reduce_edge_list(end_edges)
+
+    # Base cases. Either start/end are empty and we're done, or the final,
+    # one-dimensional tensor can be simply sliced
+    if len(start) == 0:
+        return [tuple()]
+    elif len(start) == 1:
+        return [(slice(start[0], end[0] + 1),)]
+
+    slices = []
+    path = []
+
+    # Dimensions common to start and end can be selected directly
+    for s, e in zip(start, end):
+        if s == e:
+            path.append(slice(s, s + 1))
+        else:
+            break
 
-    ```python
-    debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])
-    ```
+    path = tuple(path)
+    divergence_idx = len(path)
 
-    And now full batches 1 and 3 will be traced using the same format as explained above. Batches are 0-indexed.
+    # start == end, and we're done
+    if divergence_idx == len(dims):
+        return [tuple(path)]
+
+    def upper():
+        sdi = start[divergence_idx]
+        return [
+            path + (slice(sdi, sdi + 1),) + s
+            for s in _get_minimal_slice_set(
+                start[divergence_idx + 1 :],
+                [d - 1 for d in dims[divergence_idx + 1 :]],
+                dims[divergence_idx + 1 :],
+                start_edges=start_edges[divergence_idx + 1 :],
+                end_edges=[1 for _ in end_edges[divergence_idx + 1 :]],
+            )
+        ]
 
-    This is helpful if you know that the program starts misbehaving after a certain batch number, so you can
-    fast-forward right to that area.
+    def lower():
+        edi = end[divergence_idx]
+        return [
+            path + (slice(edi, edi + 1),) + s
+            for s in _get_minimal_slice_set(
+                [0 for _ in start[divergence_idx + 1 :]],
+                end[divergence_idx + 1 :],
+                dims[divergence_idx + 1 :],
+                start_edges=[1 for _ in start_edges[divergence_idx + 1 :]],
+                end_edges=end_edges[divergence_idx + 1 :],
+            )
+        ]
 
+    # If both start and end are at the edges of the subtree rooted at
+    # divergence_idx, we can just select the whole subtree at once
+    if start_edges[divergence_idx] and end_edges[divergence_idx]:
+        slices.append(path + (slice(start[divergence_idx], end[divergence_idx] + 1),))
+    # If just start is at the edge, we can grab almost all of the subtree,
+    # treating only the ragged bottom edge as an edge case
+    elif start_edges[divergence_idx]:
+        slices.append(path + (slice(start[divergence_idx], end[divergence_idx]),))
+        slices.extend(lower())
+    # Analogous to the previous case, but the top is ragged this time
+    elif end_edges[divergence_idx]:
+        slices.extend(upper())
+        slices.append(
+            path + (slice(start[divergence_idx] + 1, end[divergence_idx] + 1),)
+        )
+    # If both sides of the range are ragged, we need to handle both sides
+    # separately. If there's contiguous meat in between them, we can index it
+    # in one big chunk
+    else:
+        slices.extend(upper())
+        middle_ground = end[divergence_idx] - start[divergence_idx]
+        if middle_ground > 1:
+            slices.append(
+                path + (slice(start[divergence_idx] + 1, end[divergence_idx]),)
+            )
+        slices.extend(lower())
 
-    Early stopping:
+    return [tuple(s) for s in slices]
 
-    You can also specify the batch number after which to stop the training, with :
 
-    ```python
-    debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)
-    ```
+@torch.jit.ignore
+def _chunk_slice(
+    t: torch.Tensor,
+    flat_start: int,
+    flat_end: int,
+    no_batch_dims: int,
+) -> torch.Tensor:
+    """
+    Equivalent to
 
-    This feature is mainly useful in the tracing mode, but you can use it for any mode.
+        t.reshape((-1,) + t.shape[no_batch_dims:])[flat_start:flat_end]
 
+    but without the need for the initial reshape call, which can be memory-intensive in certain situations. The only
+    reshape operations in this function are performed on sub-tensors that scale with (flat_end - flat_start), the chunk
+    size.
+    """
 
-    **Performance**:
+    batch_dims = t.shape[:no_batch_dims]
+    start_idx = list(_flat_idx_to_idx(flat_start, batch_dims))
+    # _get_minimal_slice_set is inclusive
+    end_idx = list(_flat_idx_to_idx(flat_end - 1, batch_dims))
+
+    # Get an ordered list of slices to perform
+    slices = _get_minimal_slice_set(
+        start_idx,
+        end_idx,
+        batch_dims,
+    )
+
+    sliced_tensors = [t[s] for s in slices]
+
+    return torch.cat([s.view((-1,) + t.shape[no_batch_dims:]) for s in sliced_tensors])
+
+
+def chunk_layer(
+    layer: Callable,
+    inputs: Dict[str, Any],
+    chunk_size: int,
+    no_batch_dims: int,
+    low_mem: bool = False,
+    _out: Any = None,
+    _add_into_out: bool = False,
+) -> Any:
+    """
+    Implements the "chunking" procedure described in section 1.11.8.
 
-    As this module measures absolute `min`/``max` of each weight of the model on every forward it'll slow the training
-    down. Therefore remember to turn it off once the debugging needs have been met.
+    Layer outputs and inputs are assumed to be simple "pytrees," consisting only of (arbitrarily nested) lists, tuples,
+    and dicts with torch.Tensor leaves.
 
     Args:
-        model (`nn.Module`):
-            The model to debug.
-        max_frames_to_save (`int`, *optional*, defaults to 21):
-            How many frames back to record
-        trace_batch_nums(`List[int]`, *optional*, defaults to `[]`):
-            Which batch numbers to trace (turns detection off)
-        abort_after_batch_num  (`int``, *optional*):
-            Whether to abort after a certain batch number has finished
+        layer:
+            The layer to be applied chunk-wise
+        inputs:
+            A (non-nested) dictionary of keyworded inputs. All leaves must be tensors and must share the same batch
+            dimensions.
+        chunk_size:
+            The number of sub-batches per chunk. If multiple batch dimensions are specified, a "sub-batch" is defined
+            as a single indexing of all batch dimensions simultaneously (s.t. the number of sub-batches is the product
+            of the batch dimensions).
+        no_batch_dims:
+            How many of the initial dimensions of each input tensor can be considered batch dimensions.
+        low_mem:
+            Avoids flattening potentially large input tensors. Unnecessary in most cases, and is ever so slightly
+            slower than the default setting.
+    Returns:
+        The reassembled output of the layer on the inputs.
     """
+    if not (len(inputs) > 0):
+        raise ValueError("Must provide at least one input")
 
-    def __init__(
-        self,
-        model,
-        max_frames_to_save=21,
-        trace_batch_nums=[],
-        abort_after_batch_num=None,
-    ):
-        self.model = model
-        self.trace_batch_nums = trace_batch_nums
-        self.abort_after_batch_num = abort_after_batch_num
-
-        # keep a LIFO buffer of frames to dump as soon as inf/nan is encountered to give context to the problem emergence
-        self.frames = collections.deque([], max_frames_to_save)
-        self.frame = []
-        self.batch_number = 0
-        self.total_calls = 0
-        self.detected_overflow = False
-        self.prefix = "                 "
-
-        self.analyse_model()
-
-        self.register_forward_hook()
-
-    def save_frame(self, frame=None):
-        if frame is not None:
-            self.expand_frame(frame)
-        self.frames.append("\n".join(self.frame))
-        self.frame = []  # start a new frame
-
-    def expand_frame(self, line):
-        self.frame.append(line)
-
-    def trace_frames(self):
-        print("\n".join(self.frames))
-        self.frames = []
-
-    def reset_saved_frames(self):
-        self.frames = []
-
-    def dump_saved_frames(self):
-        print(f"\nDetected inf/nan during batch_number={self.batch_number}")
-        print(f"Last {len(self.frames)} forward frames:")
-        print(f"{'abs min':8} {'abs max':8} metadata")
-        print("\n".join(self.frames))
-        print("\n\n")
-        self.frames = []
-
-    def analyse_model(self):
-        # extract the fully qualified module names, to be able to report at run time. e.g.:
-        # encoder.block.2.layer.0.SelfAttention.o
-        #
-        # for shared weights only the first shared module name will be registered
-        self.module_names = {m: name for name, m in self.model.named_modules()}
-        # self.longest_module_name = max(len(v) for v in self.module_names.values())
-
-    def analyse_variable(self, var, ctx):
-        if torch.is_tensor(var):
-            self.expand_frame(get_abs_min_max(var, ctx))
-            if detect_overflow(var, ctx):
-                self.detected_overflow = True
-        elif var is None:
-            self.expand_frame(f"{'None':>17} {ctx}")
-        else:
-            self.expand_frame(f"{'not a tensor':>17} {ctx}")
-
-    def batch_start_frame(self):
-        self.expand_frame(
-            f"\n\n{self.prefix} *** Starting batch number={self.batch_number} ***"
-        )
-        self.expand_frame(f"{'abs min':8} {'abs max':8} metadata")
+    initial_dims = [shape[:no_batch_dims] for shape in _fetch_dims(inputs)]
+    orig_batch_dims = tuple([max(s) for s in zip(*initial_dims)])
 
-    def batch_end_frame(self):
-        self.expand_frame(
-            f"{self.prefix} *** Finished batch number={self.batch_number-1} ***\n\n"
-        )
+    def _prep_inputs(t):
+        if not low_mem:
+            if not sum(t.shape[:no_batch_dims]) == no_batch_dims:
+                t = t.expand(orig_batch_dims + t.shape[no_batch_dims:])
+            t = t.reshape(-1, *t.shape[no_batch_dims:])
+        else:
+            t = t.expand(orig_batch_dims + t.shape[no_batch_dims:])
+        return t
 
-    def create_frame(self, module, input, output):
-        self.expand_frame(
-            f"{self.prefix} {self.module_names[module]} {module.__class__.__name__}"
+    prepped_inputs = tensor_tree_map(_prep_inputs, inputs)
+    prepped_outputs = None
+    if _out is not None:
+        prepped_outputs = tensor_tree_map(
+            lambda t: t.view([-1] + list(t.shape[no_batch_dims:])), _out
         )
 
-        # params
-        for name, p in module.named_parameters(recurse=False):
-            self.analyse_variable(p, name)
-
-        # inputs
-        if isinstance(input, tuple):
-            for i, x in enumerate(input):
-                self.analyse_variable(x, f"input[{i}]")
-        else:
-            self.analyse_variable(input, "input")
-
-        # outputs
-        if isinstance(output, tuple):
-            for i, x in enumerate(output):
-                # possibly a tuple of tuples
-                if isinstance(x, tuple):
-                    for j, y in enumerate(x):
-                        self.analyse_variable(y, f"output[{i}][{j}]")
-                else:
-                    self.analyse_variable(x, f"output[{i}]")
+    flat_batch_dim = 1
+    for d in orig_batch_dims:
+        flat_batch_dim *= d
+
+    no_chunks = flat_batch_dim // chunk_size + (flat_batch_dim % chunk_size != 0)
+
+    def _select_chunk(t):
+        return t[i : i + chunk_size] if t.shape[0] != 1 else t
+
+    i = 0
+    out = prepped_outputs
+    for _ in range(no_chunks):
+        # Chunk the input
+        if not low_mem:
+            select_chunk = _select_chunk
         else:
-            self.analyse_variable(output, "output")
-
-        self.save_frame()
-
-    def register_forward_hook(self):
-        self.model.apply(self._register_forward_hook)
-
-    def _register_forward_hook(self, module):
-        module.register_forward_hook(self.forward_hook)
-
-    def forward_hook(self, module, input, output):
-        # - input is a tuple of packed inputs (could be non-Tensors)
-        # - output could be a Tensor or a tuple of Tensors and non-Tensors
-
-        last_frame_of_batch = False
-
-        trace_mode = True if self.batch_number in self.trace_batch_nums else False
-        if trace_mode:
-            self.reset_saved_frames()
-
-        if self.total_calls == 0:
-            self.batch_start_frame()
-        self.total_calls += 1
-
-        # count batch numbers - the very first forward hook of the batch will be called when the
-        # batch completes - i.e. it gets called very last - we know this batch has finished
-        if module == self.model:
-            self.batch_number += 1
-            last_frame_of_batch = True
-
-        self.create_frame(module, input, output)
-
-        # if last_frame_of_batch:
-        #     self.batch_end_frame()
-
-        if trace_mode:
-            self.trace_frames()
-
-        if last_frame_of_batch:
-            self.batch_start_frame()
+            select_chunk = partial(
+                _chunk_slice,
+                flat_start=i,
+                flat_end=min(flat_batch_dim, i + chunk_size),
+                no_batch_dims=len(orig_batch_dims),
+            )
 
-        if self.detected_overflow and not trace_mode:
-            self.dump_saved_frames()
+        chunks = tensor_tree_map(select_chunk, prepped_inputs)
 
-            # now we can abort, as it's pointless to continue running
-            raise ValueError(
-                "DebugUnderflowOverflow: inf/nan detected, aborting as there is no point running further. "
-                "Please scroll up above this traceback to see the activation values prior to this event."
-            )
+        # Run the layer on the chunk
+        output_chunk = layer(**chunks)
 
-        # abort after certain batch if requested to do so
-        if (
-            self.abort_after_batch_num is not None
-            and self.batch_number > self.abort_after_batch_num
-        ):
-            raise ValueError(
-                f"DebugUnderflowOverflow: aborting after {self.batch_number} batches due to"
-                f" `abort_after_batch_num={self.abort_after_batch_num}` arg"
+        # Allocate space for the output
+        if out is None:
+            out = tensor_tree_map(
+                lambda t: t.new_zeros((flat_batch_dim,) + t.shape[1:]), output_chunk
             )
 
+        # Put the chunk in its pre-allocated space
+        out_type = type(output_chunk)
+        if out_type is dict:
+
+            def assign(d1, d2):
+                for k, v in d1.items():
+                    if type(v) is dict:
+                        assign(v, d2[k])
+                    else:
+                        if _add_into_out:
+                            v[i : i + chunk_size] += d2[k]
+                        else:
+                            v[i : i + chunk_size] = d2[k]
+
+            assign(out, output_chunk)
+        elif out_type is tuple:
+            for x1, x2 in zip(out, output_chunk):
+                if _add_into_out:
+                    x1[i : i + chunk_size] += x2
+                else:
+                    x1[i : i + chunk_size] = x2
+        elif out_type is torch.Tensor:
+            if _add_into_out:
+                out[i : i + chunk_size] += output_chunk
+            else:
+                out[i : i + chunk_size] = output_chunk
+        else:
+            raise ValueError("Not supported")
 
-def get_abs_min_max(var, ctx):
-    abs_var = var.abs()
-    return f"{abs_var.min():8.2e} {abs_var.max():8.2e} {ctx}"
+        i += chunk_size
 
+    out = tensor_tree_map(lambda t: t.view(orig_batch_dims + t.shape[1:]), out)
 
-def detect_overflow(var, ctx):
-    """
-    Report whether the tensor contains any `nan` or `inf` entries.
+    return out
 
-    This is useful for detecting overflows/underflows and best to call right after the function that did some math that
-    modified the tensor in question.
 
-    This function contains a few other helper features that you can enable and tweak directly if you want to track
-    various other things.
+class ChunkSizeTuner:
+    def __init__(
+        self,
+        # Heuristically, runtimes for most of the modules in the network
+        # plateau earlier than this on all GPUs I've run the model on.
+        max_chunk_size=512,
+    ):
+        self.max_chunk_size = max_chunk_size
+        self.cached_chunk_size = None
+        self.cached_arg_data = None
+
+    def _determine_favorable_chunk_size(self, fn, args, min_chunk_size):
+        logging.info("Tuning chunk size...")
+
+        if min_chunk_size >= self.max_chunk_size:
+            return min_chunk_size
+
+        candidates = [2**l for l in range(int(math.log(self.max_chunk_size, 2)) + 1)]
+        candidates = [c for c in candidates if c > min_chunk_size]
+        candidates = [min_chunk_size] + candidates
+        candidates[-1] += 4
+
+        def test_chunk_size(chunk_size):
+            try:
+                with torch.no_grad():
+                    fn(*args, chunk_size=chunk_size)
+                return True
+            except RuntimeError:
+                return False
+
+        min_viable_chunk_size_index = 0
+        i = len(candidates) - 1
+        while i > min_viable_chunk_size_index:
+            viable = test_chunk_size(candidates[i])
+            if not viable:
+                i = (min_viable_chunk_size_index + i) // 2
+            else:
+                min_viable_chunk_size_index = i
+                i = (i + len(candidates) - 1) // 2
+
+        return candidates[min_viable_chunk_size_index]
+
+    def _compare_arg_caches(self, ac1, ac2):
+        consistent = True
+        for a1, a2 in zip(ac1, ac2):
+            assert type(ac1) == type(ac2)
+            if type(ac1) is list or type(ac1) is tuple:
+                consistent &= self._compare_arg_caches(a1, a2)
+            elif type(ac1) is dict:
+                a1_items = [v for _, v in sorted(a1.items(), key=lambda x: x[0])]
+                a2_items = [v for _, v in sorted(a2.items(), key=lambda x: x[0])]
+                consistent &= self._compare_arg_caches(a1_items, a2_items)
+            else:
+                consistent &= a1 == a2
 
-    Args:
-        var: the tensor variable to check
-        ctx: the message to print as a context
+        return consistent
 
-    Return:
-        `True` if `inf` or `nan` was detected, `False` otherwise
-    """
-    detected = False
-    if torch.isnan(var).any().item():
-        detected = True
-        print(f"{ctx} has nans")
-    if torch.isinf(var).any().item():
-        detected = True
-        print(f"{ctx} has infs")
-
-    # if needed to monitor large elements can enable the following
-    if 0:  # and detected:
-        n100 = var[torch.ge(var.abs(), 100)]
-        if n100.numel() > 0:
-            print(f"{ctx}:  n100={n100.numel()}")
-        n1000 = var[torch.ge(var.abs(), 1000)]
-        if n1000.numel() > 0:
-            print(f"{ctx}: n1000={n1000.numel()}")
-        n10000 = var[torch.ge(var.abs(), 10000)]
-        if n10000.numel() > 0:
-            print(f"{ctx}: n10000={n10000.numel()}")
-
-    if 0:
-        print(f"min={var.min():9.2e} max={var.max():9.2e}")
-
-    if 0:
-        print(
-            f"min={var.min():9.2e} max={var.max():9.2e} var={var.var():9.2e} mean={var.mean():9.2e} ({ctx})"
+    def tune_chunk_size(
+        self,
+        representative_fn: Callable,
+        args: Tuple[Any],
+        min_chunk_size: int,
+    ) -> int:
+        consistent = True
+        arg_data = tree_map(
+            lambda a: a.shape if type(a) is torch.Tensor else a, args, object
         )
+        if self.cached_arg_data is not None:
+            # If args have changed shape/value, we need to re-tune
+            assert len(self.cached_arg_data) == len(arg_data)
+            consistent = self._compare_arg_caches(self.cached_arg_data, arg_data)
+        else:
+            # Otherwise, we can reuse the precomputed value
+            consistent = False
 
-    return detected
-
+        if not consistent:
+            self.cached_chunk_size = self._determine_favorable_chunk_size(
+                representative_fn,
+                args,
+                min_chunk_size,
+            )
+            self.cached_arg_data = arg_data
 
-class DebugOption(ExplicitEnum):
-    UNDERFLOW_OVERFLOW = "underflow_overflow"
-    TPU_METRICS_DEBUG = "tpu_metrics_debug"
+        return self.cached_chunk_size
```

### Comparing `xs_transformers-1.0.0/xs_transformers/dynamic_module_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/mmbt/modeling_mmbt.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,461 +1,457 @@
 # coding=utf-8
-# Copyright 2021 The HuggingFace Inc. team.
+# Copyright (c) Facebook, Inc. and its affiliates.
+# Copyright (c) HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Utilities to dynamically load objects from the Hub."""
+"""PyTorch MMBT model."""
 
-import importlib
-import os
-import re
-import shutil
-import sys
-from pathlib import Path
-from typing import Dict, Optional, Union
-
-from huggingface_hub import HfFolder, model_info
-
-from .utils import (
-    HF_MODULES_CACHE,
-    TRANSFORMERS_DYNAMIC_MODULE_NAME,
-    cached_file,
-    is_offline_mode,
+
+import torch
+from torch import nn
+from torch.nn import CrossEntropyLoss, MSELoss
+
+from ...modeling_outputs import BaseModelOutputWithPooling, SequenceClassifierOutput
+from ...modeling_utils import ModuleUtilsMixin
+from ...utils import (
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
     logging,
+    replace_return_docstrings,
 )
 
-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name
-
-
-def init_hf_modules():
-    """
-    Creates the cache directory for modules with an init, and adds it to the Python path.
-    """
-    # This function has already been executed if HF_MODULES_CACHE already is in the Python path.
-    if HF_MODULES_CACHE in sys.path:
-        return
-
-    sys.path.append(HF_MODULES_CACHE)
-    os.makedirs(HF_MODULES_CACHE, exist_ok=True)
-    init_path = Path(HF_MODULES_CACHE) / "__init__.py"
-    if not init_path.exists():
-        init_path.touch()
+logger = logging.get_logger(__name__)
 
+_CONFIG_FOR_DOC = "MMBTConfig"
 
-def create_dynamic_module(name: Union[str, os.PathLike]):
-    """
-    Creates a dynamic module in the cache directory for modules.
-    """
-    init_hf_modules()
-    dynamic_module_path = Path(HF_MODULES_CACHE) / name
-    # If the parent module does not exist yet, recursively create it.
-    if not dynamic_module_path.parent.exists():
-        create_dynamic_module(dynamic_module_path.parent)
-    os.makedirs(dynamic_module_path, exist_ok=True)
-    init_path = dynamic_module_path / "__init__.py"
-    if not init_path.exists():
-        init_path.touch()
 
+class ModalEmbeddings(nn.Module):
+    """Generic Modal Embeddings which takes in an encoder, and a transformer embedding."""
 
-def get_relative_imports(module_file):
-    """
-    Get the list of modules that are relatively imported in a module file.
+    def __init__(self, config, encoder, embeddings):
+        super().__init__()
+        self.config = config
+        self.encoder = encoder
+        self.proj_embeddings = nn.Linear(config.modal_hidden_size, config.hidden_size)
+        self.position_embeddings = embeddings.position_embeddings
+        self.token_type_embeddings = embeddings.token_type_embeddings
+        self.word_embeddings = embeddings.word_embeddings
+        self.LayerNorm = embeddings.LayerNorm
+        self.dropout = nn.Dropout(p=config.hidden_dropout_prob)
+
+    def forward(
+        self,
+        input_modal,
+        start_token=None,
+        end_token=None,
+        position_ids=None,
+        token_type_ids=None,
+    ):
+        token_embeddings = self.proj_embeddings(self.encoder(input_modal))
+        seq_length = token_embeddings.size(1)
+
+        if start_token is not None:
+            start_token_embeds = self.word_embeddings(start_token)
+            seq_length += 1
+            token_embeddings = torch.cat(
+                [start_token_embeds.unsqueeze(1), token_embeddings], dim=1
+            )
 
-    Args:
-        module_file (`str` or `os.PathLike`): The module file to inspect.
-    """
-    with open(module_file, "r", encoding="utf-8") as f:
-        content = f.read()
+        if end_token is not None:
+            end_token_embeds = self.word_embeddings(end_token)
+            seq_length += 1
+            token_embeddings = torch.cat(
+                [token_embeddings, end_token_embeds.unsqueeze(1)], dim=1
+            )
 
-    # Imports of the form `import .xxx`
-    relative_imports = re.findall(
-        r"^\s*import\s+\.(\S+)\s*$", content, flags=re.MULTILINE
-    )
-    # Imports of the form `from .xxx import yyy`
-    relative_imports += re.findall(
-        r"^\s*from\s+\.(\S+)\s+import", content, flags=re.MULTILINE
-    )
-    # Unique-ify
-    return list(set(relative_imports))
+        if position_ids is None:
+            position_ids = torch.arange(
+                seq_length, dtype=torch.long, device=input_modal.device
+            )
+            position_ids = position_ids.unsqueeze(0).expand(
+                input_modal.size(0), seq_length
+            )
 
+        if token_type_ids is None:
+            token_type_ids = torch.zeros(
+                (input_modal.size(0), seq_length),
+                dtype=torch.long,
+                device=input_modal.device,
+            )
 
-def get_relative_import_files(module_file):
-    """
-    Get the list of all files that are needed for a given module. Note that this function recurses through the relative
-    imports (if a imports b and b imports c, it will return module files for b and c).
+        position_embeddings = self.position_embeddings(position_ids)
+        token_type_embeddings = self.token_type_embeddings(token_type_ids)
+        embeddings = token_embeddings + position_embeddings + token_type_embeddings
+        embeddings = self.LayerNorm(embeddings)
+        embeddings = self.dropout(embeddings)
+        return embeddings
+
+
+MMBT_START_DOCSTRING = r"""
+    MMBT model was proposed in [Supervised Multimodal Bitransformers for Classifying Images and
+    Text](https://github.com/facebookresearch/mmbt) by Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Davide Testuggine.
+    It's a supervised multimodal bitransformer model that fuses information from text and other image encoders, and
+    obtain state-of-the-art performance on various multimodal classification benchmark tasks.
+
+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
+    etc.)
+
+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
+    and behavior.
+
+    Parameters:
+        config ([`MMBTConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration.
+        transformer (`nn.Module`): A text transformer that is used by MMBT.
+            It should have embeddings, encoder, and pooler attributes.
+        encoder (`nn.Module`): Encoder for the second modality.
+            It should take in a batch of modal inputs and return k, n dimension embeddings.
+"""
 
+MMBT_INPUTS_DOCSTRING = r"""
     Args:
-        module_file (`str` or `os.PathLike`): The module file to inspect.
-    """
-    no_change = False
-    files_to_check = [module_file]
-    all_relative_imports = []
-
-    # Let's recurse through all relative imports
-    while not no_change:
-        new_imports = []
-        for f in files_to_check:
-            new_imports.extend(get_relative_imports(f))
-
-        module_path = Path(module_file).parent
-        new_import_files = [str(module_path / m) for m in new_imports]
-        new_import_files = [
-            f for f in new_import_files if f not in all_relative_imports
-        ]
-        files_to_check = [f"{f}.py" for f in new_import_files]
-
-        no_change = len(new_import_files) == 0
-        all_relative_imports.extend(files_to_check)
-
-    return all_relative_imports
-
-
-def check_imports(filename):
-    """
-    Check if the current Python environment contains all the libraries that are imported in a file.
-    """
-    with open(filename, "r", encoding="utf-8") as f:
-        content = f.read()
-
-    # Imports of the form `import xxx`
-    imports = re.findall(r"^\s*import\s+(\S+)\s*$", content, flags=re.MULTILINE)
-    # Imports of the form `from xxx import yyy`
-    imports += re.findall(r"^\s*from\s+(\S+)\s+import", content, flags=re.MULTILINE)
-    # Only keep the top-level module
-    imports = [imp.split(".")[0] for imp in imports if not imp.startswith(".")]
-
-    # Unique-ify and test we got them all
-    imports = list(set(imports))
-    missing_packages = []
-    for imp in imports:
-        try:
-            importlib.import_module(imp)
-        except ImportError:
-            missing_packages.append(imp)
-
-    if len(missing_packages) > 0:
-        raise ImportError(
-            "This modeling file requires the following packages that were not found in your environment: "
-            f"{', '.join(missing_packages)}. Run `pip install {' '.join(missing_packages)}`"
+        input_modal (`torch.FloatTensor` of shape `(batch_size, ***)`):
+            The other modality data. It will be the shape that the encoder for that type expects. e.g. With an Image
+            Encoder, the shape would be (batch_size, channels, height, width)
+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+            Indices of input sequence tokens in the vocabulary. It does not expect [CLS] token to be added as it's
+            appended to the end of other modality embeddings. Indices can be obtained using [`BertTokenizer`]. See
+            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        modal_start_tokens (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Optional start token to be added to Other Modality Embedding. [CLS] Most commonly used for classification
+            tasks.
+        modal_end_tokens (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Optional end token to be added to Other Modality Embedding. [SEP] Most commonly used.
+        attention_mask (*optional*) `torch.FloatTensor` of shape `(batch_size, sequence_length)`:
+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+            [What are attention masks?](../glossary#attention-mask)
+        token_type_ids (*optional*) `torch.LongTensor` of shape `(batch_size, sequence_length)`:
+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
+            1]`:
+
+            - 0 corresponds to a *sentence A* token,
+            - 1 corresponds to a *sentence B* token.
+
+            [What are token type IDs?](../glossary#token-type-ids)
+        modal_token_type_ids (*optional*) `torch.LongTensor` of shape `(batch_size, modal_sequence_length)`:
+            Segment token indices to indicate different portions of the non-text modality. The embeddings from these
+            tokens will be summed with the respective token embeddings for the non-text modality.
+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
+            config.max_position_embeddings - 1]`.
+
+            [What are position IDs?](../glossary#position-ids)
+        modal_position_ids (`torch.LongTensor` of shape `(batch_size, modal_sequence_length)`, *optional*):
+            Indices of positions of each input sequence tokens in the position embeddings for the non-text modality.
+            Selected in the range `[0, config.max_position_embeddings - 1]`.
+
+            [What are position IDs?](../glossary#position-ids)
+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
+
+            - 1 indicates the head is **not masked**,
+            - 0 indicates the head is **masked**.
+
+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, embedding_dim)`, *optional*):
+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
+            model's internal embedding lookup matrix.
+        encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
+            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
+            the model is configured as a decoder.
+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
+            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
+            tensors for more detail.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+"""
+
+
+@add_start_docstrings(
+    "The bare MMBT Model outputting raw hidden-states without any specific head on top.",
+    MMBT_START_DOCSTRING,
+)
+class MMBTModel(nn.Module, ModuleUtilsMixin):
+    def __init__(self, config, transformer, encoder):
+        super().__init__()
+        self.config = config
+        self.transformer = transformer
+        self.modal_encoder = ModalEmbeddings(config, encoder, transformer.embeddings)
+
+    @add_start_docstrings_to_model_forward(MMBT_INPUTS_DOCSTRING)
+    @replace_return_docstrings(
+        output_type=BaseModelOutputWithPooling, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_modal,
+        input_ids=None,
+        modal_start_tokens=None,
+        modal_end_tokens=None,
+        attention_mask=None,
+        token_type_ids=None,
+        modal_token_type_ids=None,
+        position_ids=None,
+        modal_position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        encoder_hidden_states=None,
+        encoder_attention_mask=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        r"""
+        Returns:
+
+        Examples:
+
+        ```python
+        # For example purposes. Not runnable.
+        transformer = BertModel.from_pretrained("bert-base-uncased")
+        encoder = ImageEncoder(args)
+        mmbt = MMBTModel(config, transformer, encoder)
+        ```"""
+        output_attentions = (
+            output_attentions
+            if output_attentions is not None
+            else self.config.output_attentions
+        )
+        output_hidden_states = (
+            output_hidden_states
+            if output_hidden_states is not None
+            else self.config.output_hidden_states
+        )
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
         )
 
-    return get_relative_imports(filename)
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError(
+                "You cannot specify both input_ids and inputs_embeds at the same time"
+            )
+        elif input_ids is not None:
+            input_txt_shape = input_ids.size()
+        elif inputs_embeds is not None:
+            input_txt_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
 
-def get_class_in_module(class_name, module_path):
-    """
-    Import a module on the cache directory for modules and extract a class from it.
-    """
-    module_path = module_path.replace(os.path.sep, ".")
-    module = importlib.import_module(module_path)
-    return getattr(module, class_name)
-
-
-def get_cached_module_file(
-    pretrained_model_name_or_path: Union[str, os.PathLike],
-    module_file: str,
-    cache_dir: Optional[Union[str, os.PathLike]] = None,
-    force_download: bool = False,
-    resume_download: bool = False,
-    proxies: Optional[Dict[str, str]] = None,
-    use_auth_token: Optional[Union[bool, str]] = None,
-    revision: Optional[str] = None,
-    local_files_only: bool = False,
-):
-    """
-    Prepares Downloads a module from a local folder or a distant repo and returns its path inside the cached
-    Transformers module.
+        modal_embeddings = self.modal_encoder(
+            input_modal,
+            start_token=modal_start_tokens,
+            end_token=modal_end_tokens,
+            position_ids=modal_position_ids,
+            token_type_ids=modal_token_type_ids,
+        )
 
-    Args:
-        pretrained_model_name_or_path (`str` or `os.PathLike`):
-            This can be either:
+        input_modal_shape = modal_embeddings.size()[:-1]
 
-            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on
-              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced
-              under a user or organization name, like `dbmdz/bert-base-german-cased`.
-            - a path to a *directory* containing a configuration file saved using the
-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.
-
-        module_file (`str`):
-            The name of the module file containing the class to look for.
-        cache_dir (`str` or `os.PathLike`, *optional*):
-            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
-            cache should not be used.
-        force_download (`bool`, *optional*, defaults to `False`):
-            Whether or not to force to (re-)download the configuration files and override the cached versions if they
-            exist.
-        resume_download (`bool`, *optional*, defaults to `False`):
-            Whether or not to delete incompletely received file. Attempts to resume the download if such a file exists.
-        proxies (`Dict[str, str]`, *optional*):
-            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
-            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
-        use_auth_token (`str` or *bool*, *optional*):
-            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
-            when running `huggingface-cli login` (stored in `~/.huggingface`).
-        revision (`str`, *optional*, defaults to `"main"`):
-            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
-            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
-            identifier allowed by git.
-        local_files_only (`bool`, *optional*, defaults to `False`):
-            If `True`, will only try to load the tokenizer configuration from local files.
+        if token_type_ids is None:
+            token_type_ids = torch.ones(
+                input_txt_shape, dtype=torch.long, device=device
+            )
 
-    <Tip>
+        txt_embeddings = self.transformer.embeddings(
+            input_ids=input_ids,
+            position_ids=position_ids,
+            token_type_ids=token_type_ids,
+            inputs_embeds=inputs_embeds,
+        )
 
-    Passing `use_auth_token=True` is required when you want to use a private model.
+        embedding_output = torch.cat([modal_embeddings, txt_embeddings], 1)
 
-    </Tip>
+        input_shape = embedding_output.size()[:-1]
 
-    Returns:
-        `str`: The path to the module inside the cache.
-    """
-    if is_offline_mode() and not local_files_only:
-        logger.info("Offline mode: forcing local_files_only=True")
-        local_files_only = True
-
-    # Download and cache module_file from the repo `pretrained_model_name_or_path` of grab it if it's a local file.
-    pretrained_model_name_or_path = str(pretrained_model_name_or_path)
-    if os.path.isdir(pretrained_model_name_or_path):
-        submodule = "local"
-    else:
-        submodule = pretrained_model_name_or_path.replace("/", os.path.sep)
-
-    try:
-        # Load from URL or cache if already cached
-        resolved_module_file = cached_file(
-            pretrained_model_name_or_path,
-            module_file,
-            cache_dir=cache_dir,
-            force_download=force_download,
-            proxies=proxies,
-            resume_download=resume_download,
-            local_files_only=local_files_only,
-            use_auth_token=use_auth_token,
-        )
-
-    except EnvironmentError:
-        logger.error(
-            f"Could not locate the {module_file} inside {pretrained_model_name_or_path}."
-        )
-        raise
-
-    # Check we have all the requirements in our environment
-    modules_needed = check_imports(resolved_module_file)
-
-    # Now we move the module inside our cached dynamic modules.
-    full_submodule = TRANSFORMERS_DYNAMIC_MODULE_NAME + os.path.sep + submodule
-    create_dynamic_module(full_submodule)
-    submodule_path = Path(HF_MODULES_CACHE) / full_submodule
-    if submodule == "local":
-        # We always copy local files (we could hash the file to see if there was a change, and give them the name of
-        # that hash, to only copy when there is a modification but it seems overkill for now).
-        # The only reason we do the copy is to avoid putting too many folders in sys.path.
-        shutil.copy(resolved_module_file, submodule_path / module_file)
-        for module_needed in modules_needed:
-            module_needed = f"{module_needed}.py"
-            shutil.copy(
-                os.path.join(pretrained_model_name_or_path, module_needed),
-                submodule_path / module_needed,
+        if attention_mask is None:
+            attention_mask = torch.ones(input_shape, device=device)
+        else:
+            attention_mask = torch.cat(
+                [
+                    torch.ones(input_modal_shape, device=device, dtype=torch.long),
+                    attention_mask,
+                ],
+                dim=1,
             )
-    else:
-        # Get the commit hash
-        # TODO: we will get this info in the etag soon, so retrieve it from there and not here.
-        if isinstance(use_auth_token, str):
-            token = use_auth_token
-        elif use_auth_token is True:
-            token = HfFolder.get_token()
+        if encoder_attention_mask is None:
+            encoder_attention_mask = torch.ones(input_shape, device=device)
         else:
-            token = None
-
-        commit_hash = model_info(
-            pretrained_model_name_or_path, revision=revision, token=token
-        ).sha
-
-        # The module file will end up being placed in a subfolder with the git hash of the repo. This way we get the
-        # benefit of versioning.
-        submodule_path = submodule_path / commit_hash
-        full_submodule = full_submodule + os.path.sep + commit_hash
-        create_dynamic_module(full_submodule)
-
-        if not (submodule_path / module_file).exists():
-            shutil.copy(resolved_module_file, submodule_path / module_file)
-        # Make sure we also have every file with relative
-        for module_needed in modules_needed:
-            if not (submodule_path / module_needed).exists():
-                get_cached_module_file(
-                    pretrained_model_name_or_path,
-                    f"{module_needed}.py",
-                    cache_dir=cache_dir,
-                    force_download=force_download,
-                    resume_download=resume_download,
-                    proxies=proxies,
-                    use_auth_token=use_auth_token,
-                    revision=revision,
-                    local_files_only=local_files_only,
-                )
-    return os.path.join(full_submodule, module_file)
-
-
-def get_class_from_dynamic_module(
-    pretrained_model_name_or_path: Union[str, os.PathLike],
-    module_file: str,
-    class_name: str,
-    cache_dir: Optional[Union[str, os.PathLike]] = None,
-    force_download: bool = False,
-    resume_download: bool = False,
-    proxies: Optional[Dict[str, str]] = None,
-    use_auth_token: Optional[Union[bool, str]] = None,
-    revision: Optional[str] = None,
-    local_files_only: bool = False,
-    **kwargs,
-):
-    """
-    Extracts a class from a module file, present in the local folder or repository of a model.
+            encoder_attention_mask = torch.cat(
+                [torch.ones(input_modal_shape, device=device), encoder_attention_mask],
+                dim=1,
+            )
 
-    <Tip warning={true}>
+        extended_attention_mask = self.get_extended_attention_mask(
+            attention_mask, input_shape
+        )
+        encoder_extended_attention_mask = self.invert_attention_mask(
+            encoder_attention_mask
+        )
+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
 
-    Calling this function will execute the code in the module file found locally or downloaded from the Hub. It should
-    therefore only be called on trusted repos.
+        encoder_outputs = self.transformer.encoder(
+            embedding_output,
+            attention_mask=extended_attention_mask,
+            head_mask=head_mask,
+            encoder_hidden_states=encoder_hidden_states,
+            encoder_attention_mask=encoder_extended_attention_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-    </Tip>
+        sequence_output = encoder_outputs[0]
+        pooled_output = self.transformer.pooler(sequence_output)
 
-    Args:
-        pretrained_model_name_or_path (`str` or `os.PathLike`):
-            This can be either:
+        if not return_dict:
+            return (sequence_output, pooled_output) + encoder_outputs[1:]
 
-            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on
-              huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced
-              under a user or organization name, like `dbmdz/bert-base-german-cased`.
-            - a path to a *directory* containing a configuration file saved using the
-              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.
-
-        module_file (`str`):
-            The name of the module file containing the class to look for.
-        class_name (`str`):
-            The name of the class to import in the module.
-        cache_dir (`str` or `os.PathLike`, *optional*):
-            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard
-            cache should not be used.
-        force_download (`bool`, *optional*, defaults to `False`):
-            Whether or not to force to (re-)download the configuration files and override the cached versions if they
-            exist.
-        resume_download (`bool`, *optional*, defaults to `False`):
-            Whether or not to delete incompletely received file. Attempts to resume the download if such a file exists.
-        proxies (`Dict[str, str]`, *optional*):
-            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',
-            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.
-        use_auth_token (`str` or `bool`, *optional*):
-            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated
-            when running `huggingface-cli login` (stored in `~/.huggingface`).
-        revision (`str`, *optional*, defaults to `"main"`):
-            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
-            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any
-            identifier allowed by git.
-        local_files_only (`bool`, *optional*, defaults to `False`):
-            If `True`, will only try to load the tokenizer configuration from local files.
+        return BaseModelOutputWithPooling(
+            last_hidden_state=sequence_output,
+            pooler_output=pooled_output,
+            hidden_states=encoder_outputs.hidden_states,
+            attentions=encoder_outputs.attentions,
+        )
 
-    <Tip>
+    def get_input_embeddings(self):
+        return self.embeddings.word_embeddings
 
-    Passing `use_auth_token=True` is required when you want to use a private model.
+    def set_input_embeddings(self, value):
+        self.embeddings.word_embeddings = value
 
-    </Tip>
 
-    Returns:
-        `type`: The class, dynamically imported from the module.
+@add_start_docstrings(
+    """
+    MMBT Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)
+    """,
+    MMBT_START_DOCSTRING,
+    MMBT_INPUTS_DOCSTRING,
+)
+class MMBTForClassification(nn.Module):
+    r"""
+    **labels**: (*optional*) `torch.LongTensor` of shape `(batch_size,)`:
+        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
+        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
+        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+
+    Returns: *Tuple* comprising various elements depending on the configuration (config) and inputs: **loss**:
+    (*optional*, returned when `labels` is provided) `torch.FloatTensor` of shape `(1,)`: Classification (or
+    regression if config.num_labels==1) loss. **logits**:
+        `torch.FloatTensor` of shape `(batch_size, config.num_labels)` Classification (or regression if
+        config.num_labels==1) scores (before SoftMax).
+    **hidden_states**: (*optional*, returned when `output_hidden_states=True`) list of `torch.FloatTensor` (one for
+    the output of each layer + the output of the embeddings) of shape `(batch_size, sequence_length, hidden_size)`:
+    Hidden-states of the model at the output of each layer plus the initial embedding outputs. **attentions**:
+    (*optional*, returned when `output_attentions=True`) list of `torch.FloatTensor` (one for each layer) of shape
+    `(batch_size, num_heads, sequence_length, sequence_length)`: Attentions weights after the attention softmax, used
+    to compute the weighted average in the self-attention heads.
 
     Examples:
 
     ```python
-    # Download module `modeling.py` from huggingface.co and cache then extract the class `MyBertModel` from this
-    # module.
-    cls = get_class_from_dynamic_module("sgugger/my-bert-model", "modeling.py", "MyBertModel")
+    # For example purposes. Not runnable.
+    transformer = BertModel.from_pretrained("bert-base-uncased")
+    encoder = ImageEncoder(args)
+    model = MMBTForClassification(config, transformer, encoder)
+    outputs = model(input_modal, input_ids, labels=labels)
+    loss, logits = outputs[:2]
     ```"""
-    # And lastly we get the class inside our newly created module
-    final_module = get_cached_module_file(
-        pretrained_model_name_or_path,
-        module_file,
-        cache_dir=cache_dir,
-        force_download=force_download,
-        resume_download=resume_download,
-        proxies=proxies,
-        use_auth_token=use_auth_token,
-        revision=revision,
-        local_files_only=local_files_only,
-    )
-    return get_class_in_module(class_name, final_module.replace(".py", ""))
 
+    def __init__(self, config, transformer, encoder):
+        super().__init__()
+        self.num_labels = config.num_labels
+
+        self.mmbt = MMBTModel(config, transformer, encoder)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
+
+    def forward(
+        self,
+        input_modal,
+        input_ids=None,
+        modal_start_tokens=None,
+        modal_end_tokens=None,
+        attention_mask=None,
+        token_type_ids=None,
+        modal_token_type_ids=None,
+        position_ids=None,
+        modal_position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        labels=None,
+        return_dict=None,
+    ):
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
 
-def custom_object_save(obj, folder, config=None):
-    """
-    Save the modeling files corresponding to a custom model/configuration/tokenizer etc. in a given folder. Optionally
-    adds the proper fields in a config.
+        outputs = self.mmbt(
+            input_modal=input_modal,
+            input_ids=input_ids,
+            modal_start_tokens=modal_start_tokens,
+            modal_end_tokens=modal_end_tokens,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+            modal_token_type_ids=modal_token_type_ids,
+            position_ids=position_ids,
+            modal_position_ids=modal_position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            return_dict=return_dict,
+        )
 
-    Args:
-        obj (`Any`): The object for which to save the module files.
-        folder (`str` or `os.PathLike`): The folder where to save.
-        config (`PretrainedConfig` or dictionary, `optional`):
-            A config in which to register the auto_map corresponding to this custom object.
-    """
-    if obj.__module__ == "__main__":
-        logger.warning(
-            f"We can't save the code defining {obj} in {folder} as it's been defined in __main__. You should put "
-            "this code in a separate module so we can include it in the saved folder and make it easier to share via "
-            "the Hub."
-        )
-
-    def _set_auto_map_in_config(_config):
-        module_name = obj.__class__.__module__
-        last_module = module_name.split(".")[-1]
-        full_name = f"{last_module}.{obj.__class__.__name__}"
-        # Special handling for tokenizers
-        if "Tokenizer" in full_name:
-            slow_tokenizer_class = None
-            fast_tokenizer_class = None
-            if obj.__class__.__name__.endswith("Fast"):
-                # Fast tokenizer: we have the fast tokenizer class and we may have the slow one has an attribute.
-                fast_tokenizer_class = f"{last_module}.{obj.__class__.__name__}"
-                if getattr(obj, "slow_tokenizer_class", None) is not None:
-                    slow_tokenizer = getattr(obj, "slow_tokenizer_class")
-                    slow_tok_module_name = slow_tokenizer.__module__
-                    last_slow_tok_module = slow_tok_module_name.split(".")[-1]
-                    slow_tokenizer_class = (
-                        f"{last_slow_tok_module}.{slow_tokenizer.__name__}"
-                    )
-            else:
-                # Slow tokenizer: no way to have the fast class
-                slow_tokenizer_class = f"{last_module}.{obj.__class__.__name__}"
+        pooled_output = outputs[1]
 
-            full_name = (slow_tokenizer_class, fast_tokenizer_class)
+        pooled_output = self.dropout(pooled_output)
+        logits = self.classifier(pooled_output)
 
-        if isinstance(_config, dict):
-            auto_map = _config.get("auto_map", {})
-            auto_map[obj._auto_class] = full_name
-            _config["auto_map"] = auto_map
-        elif getattr(_config, "auto_map", None) is not None:
-            _config.auto_map[obj._auto_class] = full_name
-        else:
-            _config.auto_map = {obj._auto_class: full_name}
+        loss = None
+        if labels is not None:
+            if self.num_labels == 1:
+                #  We are doing regression
+                loss_fct = MSELoss()
+                loss = loss_fct(logits.view(-1), labels.view(-1))
+            else:
+                loss_fct = CrossEntropyLoss()
+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
 
-    # Add object class to the config auto_map
-    if isinstance(config, (list, tuple)):
-        for cfg in config:
-            _set_auto_map_in_config(cfg)
-    elif config is not None:
-        _set_auto_map_in_config(config)
-
-    # Copy module file to the output folder.
-    object_file = sys.modules[obj.__module__].__file__
-    dest_file = Path(folder) / (Path(object_file).name)
-    shutil.copy(object_file, dest_file)
-
-    # Gather all relative imports recursively and make sure they are copied as well.
-    for needed_file in get_relative_import_files(object_file):
-        dest_file = Path(folder) / (Path(needed_file).name)
-        shutil.copy(needed_file, dest_file)
+        if not return_dict:
+            output = (logits,) + outputs[2:]
+            return ((loss,) + output) if loss is not None else output
+
+        return SequenceClassifierOutput(
+            loss=loss,
+            logits=logits,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
+        )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/generation_flax_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/fnet/modeling_fnet.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,1118 +1,1320 @@
 # coding=utf-8
-# Copyright 2021 The Google AI Flax Team Authors, and The HuggingFace Inc. team.
-# Copyright (c) 2020, NVIDIA CORPORATION.  All rights reserved.
+# Copyright 2021 Google Research and The HuggingFace Inc. team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+""" PyTorch FNet model."""
 
-
-import inspect
 import warnings
+from dataclasses import dataclass
 from functools import partial
-from typing import Any, Dict, Optional
+from typing import Optional, Tuple, Union
 
-import flax
-import jax
-import jax.numpy as jnp
-import numpy as np
-from jax import lax
-
-from .generation_flax_logits_process import (
-    FlaxForcedBOSTokenLogitsProcessor,
-    FlaxForcedEOSTokenLogitsProcessor,
-    FlaxLogitsProcessorList,
-    FlaxMinLengthLogitsProcessor,
-    FlaxTemperatureLogitsWarper,
-    FlaxTopKLogitsWarper,
-    FlaxTopPLogitsWarper,
+import torch
+import torch.utils.checkpoint
+from torch import nn
+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
+
+from ...utils import is_scipy_available
+
+if is_scipy_available():
+    from scipy import linalg
+
+from ...activations import ACT2FN
+from ...modeling_outputs import (
+    BaseModelOutput,
+    BaseModelOutputWithPooling,
+    MaskedLMOutput,
+    ModelOutput,
+    MultipleChoiceModelOutput,
+    NextSentencePredictorOutput,
+    QuestionAnsweringModelOutput,
+    SequenceClassifierOutput,
+    TokenClassifierOutput,
 )
-from .models.auto import (
-    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,
-    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,
-    FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING,
+from ...modeling_utils import PreTrainedModel
+from ...pytorch_utils import apply_chunking_to_forward
+from ...utils import (
+    add_code_sample_docstrings,
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
+    logging,
+    replace_return_docstrings,
 )
-from .utils import ModelOutput, logging
+from .configuration_fnet import FNetConfig
 
 logger = logging.get_logger(__name__)
 
+_CHECKPOINT_FOR_DOC = "google/fnet-base"
+_CONFIG_FOR_DOC = "FNetConfig"
+_TOKENIZER_FOR_DOC = "FNetTokenizer"
 
-@flax.struct.dataclass
-class FlaxGreedySearchOutput(ModelOutput):
-    """
-    Flax Base class for outputs of decoder-only generation models using greedy search.
+FNET_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "google/fnet-base",
+    "google/fnet-large"
+    # See all FNet models at https://huggingface.co/models?filter=fnet
+]
 
 
-    Args:
-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):
-            The generated sequences.
-    """
+# Adapted from https://github.com/google-research/google-research/blob/master/f_net/fourier.py
+def _two_dim_matmul(x, matrix_dim_one, matrix_dim_two):
+    """Applies 2D matrix multiplication to 3D input arrays."""
+    seq_length = x.shape[1]
+    matrix_dim_one = matrix_dim_one[:seq_length, :seq_length]
+    x = x.type(torch.complex64)
+    return torch.einsum("bij,jk,ni->bnk", x, matrix_dim_two, matrix_dim_one)
 
-    sequences: jnp.ndarray = None
 
+# # Adapted from https://github.com/google-research/google-research/blob/master/f_net/fourier.py
+def two_dim_matmul(x, matrix_dim_one, matrix_dim_two):
+    return _two_dim_matmul(x, matrix_dim_one, matrix_dim_two)
 
-@flax.struct.dataclass
-class FlaxSampleOutput(ModelOutput):
-    """
-    Flax Base class for outputs of decoder-only generation models using sampling.
 
+# Adapted from https://github.com/google-research/google-research/blob/master/f_net/fourier.py
+def fftn(x):
+    """
+    Applies n-dimensional Fast Fourier Transform (FFT) to input array.
 
     Args:
-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):
-            The generated sequences.
+        x: Input n-dimensional array.
+
+    Returns:
+        n-dimensional Fourier transform of input n-dimensional array.
     """
+    out = x
+    for axis in reversed(range(x.ndim)[1:]):  # We don't need to apply FFT to last axis
+        out = torch.fft.fft(out, axis=axis)
+    return out
 
-    sequences: jnp.ndarray = None
 
+class FNetEmbeddings(nn.Module):
+    """Construct the embeddings from word, position and token_type embeddings."""
 
-@flax.struct.dataclass
-class FlaxBeamSearchOutput(ModelOutput):
-    """
-    Flax Base class for outputs of decoder-only generation models using greedy search.
+    def __init__(self, config):
+        super().__init__()
+        self.word_embeddings = nn.Embedding(
+            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id
+        )
+        self.position_embeddings = nn.Embedding(
+            config.max_position_embeddings, config.hidden_size
+        )
+        self.token_type_embeddings = nn.Embedding(
+            config.type_vocab_size, config.hidden_size
+        )
 
+        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
+        # any TensorFlow checkpoint file
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        # NOTE: This is the project layer and will be needed. The original code allows for different embedding and different model dimensions.
+        self.projection = nn.Linear(config.hidden_size, config.hidden_size)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
-    Args:
-        sequences (`jnp.ndarray` of shape `(batch_size, max_length)`):
-            The generated sequences.
-        scores (`jnp.ndarray` of shape `(batch_size,)`):
-            The scores (log probabilities) of the generated sequences.
-    """
+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized
+        self.register_buffer(
+            "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1))
+        )
 
-    sequences: jnp.ndarray = None
-    scores: jnp.ndarray = None
+        self.register_buffer(
+            "token_type_ids",
+            torch.zeros(self.position_ids.size(), dtype=torch.long),
+            persistent=False,
+        )
 
+    def forward(
+        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None
+    ):
+        if input_ids is not None:
+            input_shape = input_ids.size()
+        else:
+            input_shape = inputs_embeds.size()[:-1]
 
-@flax.struct.dataclass
-class GreedyState:
-    cur_len: jnp.ndarray
-    sequences: jnp.ndarray
-    running_token: jnp.ndarray
-    is_sent_finished: jnp.ndarray
-    model_kwargs: Dict[str, jnp.ndarray]
-
-
-@flax.struct.dataclass
-class SampleState:
-    cur_len: jnp.ndarray
-    sequences: jnp.ndarray
-    running_token: jnp.ndarray
-    is_sent_finished: jnp.ndarray
-    prng_key: jnp.ndarray
-    model_kwargs: Dict[str, jnp.ndarray]
-
-
-@flax.struct.dataclass
-class BeamSearchState:
-    cur_len: jnp.ndarray
-    running_sequences: jnp.ndarray
-    running_scores: jnp.ndarray
-    sequences: jnp.ndarray
-    scores: jnp.ndarray
-    is_sent_finished: jnp.ndarray
-    model_kwargs: Dict[str, jnp.ndarray]
+        seq_length = input_shape[1]
 
+        if position_ids is None:
+            position_ids = self.position_ids[:, :seq_length]
 
-class FlaxGenerationMixin:
-    """
-    A class containing all functions for auto-regressive text generation, to be used as a mixin in
-    [`FlaxPreTrainedModel`].
+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs
+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves
+        # issue #5664
+        if token_type_ids is None:
+            if hasattr(self, "token_type_ids"):
+                buffered_token_type_ids = self.token_type_ids[:, :seq_length]
+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(
+                    input_shape[0], seq_length
+                )
+                token_type_ids = buffered_token_type_ids_expanded
+            else:
+                token_type_ids = torch.zeros(
+                    input_shape, dtype=torch.long, device=self.position_ids.device
+                )
 
-    The class exposes [`~generation_flax_utils.FlaxGenerationMixin.generate`], which can be used for:
-            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
-              `num_beams=1` and `do_sample=False`.
-            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
-              and `do_sample=True`.
-            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
-              and `do_sample=False`.
-    """
+        if inputs_embeds is None:
+            inputs_embeds = self.word_embeddings(input_ids)
+        token_type_embeddings = self.token_type_embeddings(token_type_ids)
+
+        embeddings = inputs_embeds + token_type_embeddings
+
+        position_embeddings = self.position_embeddings(position_ids)
+        embeddings += position_embeddings
+        embeddings = self.LayerNorm(embeddings)
+        embeddings = self.projection(embeddings)
+        embeddings = self.dropout(embeddings)
+        return embeddings
+
+
+class FNetBasicFourierTransform(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self._init_fourier_transform(config)
+
+    def _init_fourier_transform(self, config):
+        if not config.use_tpu_fourier_optimizations:
+            self.fourier_transform = partial(torch.fft.fftn, dim=(1, 2))
+        elif config.max_position_embeddings <= 4096:
+            if is_scipy_available():
+                self.register_buffer(
+                    "dft_mat_hidden",
+                    torch.tensor(linalg.dft(config.hidden_size), dtype=torch.complex64),
+                )
+                self.register_buffer(
+                    "dft_mat_seq",
+                    torch.tensor(
+                        linalg.dft(config.tpu_short_seq_length), dtype=torch.complex64
+                    ),
+                )
+                self.fourier_transform = partial(
+                    two_dim_matmul,
+                    matrix_dim_one=self.dft_mat_seq,
+                    matrix_dim_two=self.dft_mat_hidden,
+                )
+            else:
+                logging.warning(
+                    "SciPy is needed for DFT matrix calculation and is not found. Using TPU optimized fast fourier"
+                    " transform instead."
+                )
+                self.fourier_transform = fftn
+        else:
+            self.fourier_transform = fftn
 
-    @staticmethod
-    def _run_loop_in_debug(cond_fn, body_fn, init_state):
-        """
-        Run generation in untraced mode. This should only be used for debugging purposes.
-        """
-        state = init_state
-        while cond_fn(state):
-            state = body_fn(state)
-        return state
+    def forward(self, hidden_states):
+        # NOTE: We do not use torch.vmap as it is not integrated into PyTorch stable versions.
+        # Interested users can modify the code to use vmap from the nightly versions, getting the vmap from here:
+        # https://pytorch.org/docs/master/generated/torch.vmap.html. Note that fourier transform methods will need
+        # change accordingly.
+
+        outputs = self.fourier_transform(hidden_states).real
+        return (outputs,)
+
+
+class FNetBasicOutput(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+
+    def forward(self, hidden_states, input_tensor):
+        hidden_states = self.LayerNorm(input_tensor + hidden_states)
+        return hidden_states
+
+
+class FNetFourierTransform(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.self = FNetBasicFourierTransform(config)
+        self.output = FNetBasicOutput(config)
+
+    def forward(self, hidden_states):
+        self_outputs = self.self(hidden_states)
+        fourier_output = self.output(self_outputs[0], hidden_states)
+        outputs = (fourier_output,)
+        return outputs
+
+
+# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->FNet
+class FNetIntermediate(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
+        if isinstance(config.hidden_act, str):
+            self.intermediate_act_fn = ACT2FN[config.hidden_act]
+        else:
+            self.intermediate_act_fn = config.hidden_act
 
-    def _prepare_encoder_decoder_kwargs_for_generation(
-        self, input_ids, params, model_kwargs
-    ):
-        encoder_kwargs = {
-            argument: value
-            for argument, value in model_kwargs.items()
-            if not (
-                argument.startswith("decoder_") or argument.startswith("cross_attn")
-            )
-        }
-        model_kwargs["encoder_outputs"] = self.encode(
-            input_ids, params=params, return_dict=True, **encoder_kwargs
-        )
-        return model_kwargs
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.intermediate_act_fn(hidden_states)
+        return hidden_states
 
-    @staticmethod
-    def _expand_to_num_beams(tensor, num_beams):
-        return jnp.broadcast_to(
-            tensor[:, None], (tensor.shape[0], num_beams) + tensor.shape[1:]
-        )
 
-    def _adapt_logits_for_beam_search(self, logits):
-        """
-        This function can be overwritten in the specific modeling_flax_<model-name>.py classes to allow for custom beam
-        search behavior. Note that the only model that overwrites this method is [`~transformes.FlaxMarianMTModel`].
-        """
-        return logits
+# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->FNet
+class FNetOutput(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
 
-    def _validate_model_class(self):
-        """
-        Confirms that the model class is compatible with generation. If not, raises an exception that points to the
-        right class to use.
-        """
-        if not hasattr(self, "prepare_inputs_for_generation"):
-            generate_compatible_mappings = [
-                FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,
-                FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING,
-                FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,
-            ]
-            generate_compatible_classes = set()
-            for model_mapping in generate_compatible_mappings:
-                supported_models = model_mapping.get(type(self.config), default=None)
-                if supported_models is not None:
-                    generate_compatible_classes.add(supported_models.__name__)
-            exception_message = (
-                f"The current model class ({self.__class__.__name__}) is not compatible with `.generate()`, as "
-                "it doesn't have a language model head."
-            )
-            if generate_compatible_classes:
-                exception_message += f" Please use one of the following classes instead: {generate_compatible_classes}"
-            raise TypeError(exception_message)
-
-    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):
-        """Validates model kwargs for generation. Generate argument typos will also be caught here."""
-        unused_model_args = []
-        model_args = set(
-            inspect.signature(self.prepare_inputs_for_generation).parameters
-        )
-        # `kwargs` if often used to handle optional forward pass inputs like `attention_mask`. If
-        # `prepare_inputs_for_generation` doesn't accept `kwargs`, then a stricter check can be made ;)
-        if "kwargs" in model_args:
-            model_args |= set(inspect.signature(self.__call__).parameters)
-        for key, value in model_kwargs.items():
-            if value is not None and key not in model_args:
-                unused_model_args.append(key)
+    def forward(
+        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor
+    ) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.dropout(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        return hidden_states
 
-        if unused_model_args:
-            raise ValueError(
-                f"The following `model_kwargs` are not used by the model: {unused_model_args} (note: typos in the"
-                " generate arguments will also show up in this list)"
-            )
 
-    def generate(
-        self,
-        input_ids: jnp.ndarray,
-        max_length: Optional[int] = None,
-        max_new_tokens: Optional[int] = None,
-        pad_token_id: Optional[int] = None,
-        bos_token_id: Optional[int] = None,
-        eos_token_id: Optional[int] = None,
-        decoder_start_token_id: Optional[int] = None,
-        do_sample: Optional[bool] = None,
-        prng_key: Optional[jnp.ndarray] = None,
-        top_k: Optional[int] = None,
-        top_p: Optional[float] = None,
-        temperature: Optional[float] = None,
-        num_beams: Optional[int] = None,
-        no_repeat_ngram_size: Optional[int] = None,
-        min_length: Optional[int] = None,
-        forced_bos_token_id: Optional[int] = None,
-        forced_eos_token_id: Optional[int] = None,
-        length_penalty: Optional[float] = None,
-        early_stopping: Optional[bool] = None,
-        trace: bool = True,
-        params: Optional[Dict[str, jnp.ndarray]] = None,
-        **model_kwargs,
-    ):
-        r"""
-        Generates sequences of token ids for models with a language modeling head. The method supports the following
-        generation methods for text-decoder, text-to-text, speech-to-text, and vision-to-text models:
+class FNetLayer(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.chunk_size_feed_forward = config.chunk_size_feed_forward
+        self.seq_len_dim = 1  # The dimension which has the sequence length
+        self.fourier = FNetFourierTransform(config)
+        self.intermediate = FNetIntermediate(config)
+        self.output = FNetOutput(config)
 
-            - *greedy decoding* by calling [`~generation_flax_utils.FlaxGenerationMixin._greedy_search`] if
-              `num_beams=1` and `do_sample=False`.
-            - *multinomial sampling* by calling [`~generation_flax_utils.FlaxGenerationMixin._sample`] if `num_beams=1`
-              and `do_sample=True`.
-            - *beam-search decoding* by calling [`~generation_utils.FlaxGenerationMixin._beam_search`] if `num_beams>1`
-              and `do_sample=False`.
-
-        <Tip warning={true}>
-
-        Apart from `inputs`, all the arguments below will default to the value of the attribute of the same name as
-        defined in the model's config (`config.json`) which in turn defaults to the
-        [`~modeling_utils.PretrainedConfig`] of the model.
-
-        </Tip>
-
-        Most of these parameters are explained in more detail in [this blog
-        post](https://huggingface.co/blog/how-to-generate).
-
-        Parameters:
-            input_ids (`jnp.ndarray` of shape `(batch_size, sequence_length)`):
-                The sequence used as a prompt for the generation.
-            max_length (`int`, *optional*, defaults to `model.config.max_length`):
-                The maximum length the generated tokens can have. Corresponds to the length of the input prompt +
-                `max_new_tokens`. In general, prefer the use of `max_new_tokens`, which ignores the number of tokens in
-                the prompt.
-            max_new_tokens (`int`, *optional*):
-                The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.
-            do_sample (`bool`, *optional*, defaults to `False`):
-                Whether or not to use sampling ; use greedy decoding otherwise.
-            temperature (`float`, *optional*, defaults to 1.0):
-                The value used to module the next token probabilities.
-            top_k (`int`, *optional*, defaults to 50):
-                The number of highest probability vocabulary tokens to keep for top-k-filtering.
-            top_p (`float`, *optional*, defaults to 1.0):
-                If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher
-                are kept for generation.
-            pad_token_id (`int`, *optional*):
-                The id of the *padding* token.
-            bos_token_id (`int`, *optional*):
-                The id of the *beginning-of-sequence* token.
-            eos_token_id (`int`, *optional*):
-                The id of the *end-of-sequence* token.
-            num_beams (`int`, *optional*, defaults to 1):
-                Number of beams for beam search. 1 means no beam search.
-            decoder_start_token_id (`int`, *optional*):
-                If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token.
-            trace (`bool`, *optional*, defaults to `True`):
-                Whether to trace generation. Setting `trace=False` should only be used for debugging and will lead to a
-                considerably slower runtime.
-            params (`Dict[str, jnp.ndarray]`, *optional*):
-                Optionally the model parameters can be passed. Can be useful for parallelized generation.
-            model_kwargs:
-                Additional model specific kwargs will be forwarded to the `forward` function of the model. If the model
-                is an encoder-decoder model, encoder specific kwargs should not be prefixed and decoder specific kwargs
-                should be prefixed with *decoder_*. Also accepts `encoder_outputs` to skip encoder part.
+    def forward(self, hidden_states):
+        self_fourier_outputs = self.fourier(hidden_states)
+        fourier_output = self_fourier_outputs[0]
 
-        Return:
-            [`~utils.ModelOutput`].
+        layer_output = apply_chunking_to_forward(
+            self.feed_forward_chunk,
+            self.chunk_size_feed_forward,
+            self.seq_len_dim,
+            fourier_output,
+        )
 
-        Examples:
+        outputs = (layer_output,)
 
-        ```python
-        >>> from transformers import AutoTokenizer, FlaxAutoModelForCausalLM
+        return outputs
 
-        >>> tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
-        >>> model = FlaxAutoModelForCausalLM.from_pretrained("distilgpt2")
-        >>> input_context = "The dog"
-        >>> # encode input context
-        >>> input_ids = tokenizer(input_context, return_tensors="np").input_ids
-        >>> # generate candidates using sampling
-        >>> outputs = model.generate(input_ids=input_ids, max_length=20, top_k=30, do_sample=True)
-        >>> tokenizer.batch_decode(outputs, skip_special_tokens=True)
-        ```"""
-        # Validate the `.generate()` call
-        self._validate_model_class()
-        self._validate_model_kwargs(model_kwargs.copy())
+    def feed_forward_chunk(self, fourier_output):
+        intermediate_output = self.intermediate(fourier_output)
+        layer_output = self.output(intermediate_output, fourier_output)
+        return layer_output
 
-        # set init values
-        bos_token_id = (
-            bos_token_id if bos_token_id is not None else self.config.bos_token_id
-        )
-        pad_token_id = (
-            pad_token_id if pad_token_id is not None else self.config.pad_token_id
-        )
-        eos_token_id = (
-            eos_token_id if eos_token_id is not None else self.config.eos_token_id
-        )
-        decoder_start_token_id = (
-            decoder_start_token_id
-            if decoder_start_token_id is not None
-            else self.config.decoder_start_token_id
+
+class FNetEncoder(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.config = config
+        self.layer = nn.ModuleList(
+            [FNetLayer(config) for _ in range(config.num_hidden_layers)]
         )
-        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)
+        self.gradient_checkpointing = False
 
-        if decoder_start_token_id is None and self.config.is_encoder_decoder:
-            raise ValueError(
-                "`decoder_start_token_id` has to be defined for encoder-decoder generation."
-            )
+    def forward(self, hidden_states, output_hidden_states=False, return_dict=True):
+        all_hidden_states = () if output_hidden_states else None
 
-        # decoder-only models should use left-padding for generation (can't be checked with `trace=True`)
-        if not self.config.is_encoder_decoder and not trace:
-            if (
-                pad_token_id is not None
-                and jnp.sum(input_ids[:, -1] == pad_token_id) > 0
-            ):
-                logger.warning(
-                    "A decoder-only architecture is being used, but right-padding was detected! For correct "
-                    "generation results, please set `padding_side='left'` when initializing the tokenizer."
-                )
+        for i, layer_module in enumerate(self.layer):
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            if self.gradient_checkpointing and self.training:
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        return module(*inputs)
+
+                    return custom_forward
 
-        if self.config.is_encoder_decoder:
-            # add encoder_outputs to model_kwargs
-            if model_kwargs.get("encoder_outputs") is None:
-                model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(
-                    input_ids, params, model_kwargs
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(layer_module), hidden_states
                 )
-            # prepare decoder_input_ids for generation
-            input_ids = (
-                jnp.ones((input_ids.shape[0], 1), dtype="i4") * decoder_start_token_id
-            )
+            else:
+                layer_outputs = layer_module(hidden_states)
 
-        # Prepare `max_length` depending on other stopping criteria.
-        input_ids_seq_length = input_ids.shape[-1]
-        if max_length is None and max_new_tokens is None:
-            warnings.warn(
-                "Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to "
-                f"{self.config.max_length} (`self.config.max_length`). Controlling `max_length` via the config is "
-                "deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend "
-                "using `max_new_tokens` to control the maximum length of the generation.",
-                UserWarning,
-            )
-        elif max_length is None and max_new_tokens is not None:
-            max_length = max_new_tokens + input_ids_seq_length
-        elif max_length is not None and max_new_tokens is not None:
-            raise ValueError(
-                "Both `max_new_tokens` and `max_length` have been set but they serve the same purpose -- setting a"
-                " limit to the generated output length. Remove one of those arguments. Please refer to the"
-                " documentation for more information. "
-                "(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)"
-            )
-        # default to config if still None
-        max_length = max_length if max_length is not None else self.config.max_length
-        min_length = min_length if min_length is not None else self.config.min_length
+            hidden_states = layer_outputs[0]
 
-        if min_length is not None and min_length > max_length:
-            raise ValueError(
-                f"Unfeasable length constraints: the minimum length ({min_length}) is larger than the maximum "
-                f"length ({max_length})"
-            )
-        if input_ids_seq_length >= max_length:
-            input_ids_string = (
-                "decoder_input_ids" if self.config.is_encoder_decoder else "input_ids"
-            )
-            logger.warning(
-                f"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to"
-                f" {max_length}. This can lead to unexpected behavior. You should consider increasing"
-                "`max_new_tokens`."
-            )
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
 
-        do_sample = do_sample if do_sample is not None else self.config.do_sample
-        num_beams = num_beams if num_beams is not None else self.config.num_beams
+        if not return_dict:
+            return tuple(v for v in [hidden_states, all_hidden_states] if v is not None)
 
-        if not do_sample and num_beams == 1:
-            logits_processor = self._get_logits_processor(
-                no_repeat_ngram_size,
-                min_length,
-                max_length,
-                eos_token_id,
-                forced_bos_token_id,
-                forced_eos_token_id,
-            )
-            return self._greedy_search(
-                input_ids,
-                max_length,
-                pad_token_id,
-                eos_token_id,
-                logits_processor=logits_processor,
-                trace=trace,
-                params=params,
-                model_kwargs=model_kwargs,
-            )
-        elif do_sample and num_beams == 1:
-            logits_warper = self._get_logits_warper(
-                top_k=top_k, top_p=top_p, temperature=temperature
-            )
-            logits_processor = self._get_logits_processor(
-                no_repeat_ngram_size,
-                min_length,
-                max_length,
-                eos_token_id,
-                forced_bos_token_id,
-                forced_eos_token_id,
-            )
-            return self._sample(
-                input_ids,
-                max_length,
-                pad_token_id,
-                eos_token_id,
-                prng_key,
-                logits_warper=logits_warper,
-                logits_processor=logits_processor,
-                trace=trace,
-                params=params,
-                model_kwargs=model_kwargs,
-            )
-        elif not do_sample and num_beams > 1:
-            # broadcast input_ids & encoder_outputs
-            input_ids = self._expand_to_num_beams(input_ids, num_beams=num_beams)
-
-            if "encoder_outputs" in model_kwargs:
-                model_kwargs["encoder_outputs"][
-                    "last_hidden_state"
-                ] = self._expand_to_num_beams(
-                    model_kwargs["encoder_outputs"]["last_hidden_state"],
-                    num_beams=num_beams,
-                )
+        return BaseModelOutput(
+            last_hidden_state=hidden_states, hidden_states=all_hidden_states
+        )
 
-            if "attention_mask" in model_kwargs:
-                model_kwargs["attention_mask"] = self._expand_to_num_beams(
-                    model_kwargs["attention_mask"], num_beams=num_beams
-                )
 
-            logits_processor = self._get_logits_processor(
-                no_repeat_ngram_size,
-                min_length,
-                max_length,
-                eos_token_id,
-                forced_bos_token_id,
-                forced_eos_token_id,
-            )
+# Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->FNet
+class FNetPooler(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        self.activation = nn.Tanh()
 
-            return self._beam_search(
-                input_ids,
-                max_length,
-                pad_token_id,
-                eos_token_id,
-                length_penalty=length_penalty,
-                early_stopping=early_stopping,
-                logits_processor=logits_processor,
-                trace=trace,
-                params=params,
-                model_kwargs=model_kwargs,
-            )
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        # We "pool" the model by simply taking the hidden state corresponding
+        # to the first token.
+        first_token_tensor = hidden_states[:, 0]
+        pooled_output = self.dense(first_token_tensor)
+        pooled_output = self.activation(pooled_output)
+        return pooled_output
+
+
+# Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->FNet
+class FNetPredictionHeadTransform(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        if isinstance(config.hidden_act, str):
+            self.transform_act_fn = ACT2FN[config.hidden_act]
         else:
-            raise NotImplementedError("`Beam sampling is currently not implemented.")
+            self.transform_act_fn = config.hidden_act
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
 
-    def _get_logits_warper(
-        self,
-        top_k: Optional[int] = None,
-        top_p: Optional[float] = None,
-        temperature: Optional[float] = None,
-    ) -> FlaxLogitsProcessorList:
-        """
-        This class returns a [`FlaxLogitsProcessorList`] list object that contains all relevant [`FlaxLogitsWarper`]
-        instances used for multinomial sampling.
-        """
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.transform_act_fn(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states)
+        return hidden_states
 
-        # init warp parameters
-        top_k = top_k if top_k is not None else self.config.top_k
-        top_p = top_p if top_p is not None else self.config.top_p
-        temperature = (
-            temperature if temperature is not None else self.config.temperature
-        )
-        # instantiate warpers list
-        warpers = FlaxLogitsProcessorList()
-
-        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files
-        # all samplers can be found in `generation_utils_samplers.py`
-        if temperature is not None and temperature != 1.0:
-            warpers.append(FlaxTemperatureLogitsWarper(temperature))
-        if top_k is not None and top_k != 0:
-            warpers.append(FlaxTopKLogitsWarper(top_k=top_k, min_tokens_to_keep=1))
-        if top_p is not None and top_p < 1.0:
-            warpers.append(FlaxTopPLogitsWarper(top_p=top_p, min_tokens_to_keep=1))
 
-        return warpers
+class FNetLMPredictionHead(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.transform = FNetPredictionHeadTransform(config)
 
-    def _get_logits_processor(
-        self,
-        no_repeat_ngram_size: int,
-        min_length: int,
-        max_length: int,
-        eos_token_id: int,
-        forced_bos_token_id: int,
-        forced_eos_token_id: int,
-    ) -> FlaxLogitsProcessorList:
-        """
-        This class returns a [`FlaxLogitsProcessorList`] list object that contains all relevant [`FlaxLogitsProcessor`]
-        instances used to modify the scores of the language model head.
-        """
-        processors = FlaxLogitsProcessorList()
+        # The output weights are the same as the input embeddings, but there is
+        # an output-only bias for each token.
+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)
 
-        # init warp parameters
-        no_repeat_ngram_size = (
-            no_repeat_ngram_size
-            if no_repeat_ngram_size is not None
-            else self.config.no_repeat_ngram_size
-        )
-        eos_token_id = (
-            eos_token_id if eos_token_id is not None else self.config.eos_token_id
-        )
-        forced_bos_token_id = (
-            forced_bos_token_id
-            if forced_bos_token_id is not None
-            else self.config.forced_bos_token_id
-        )
-        forced_eos_token_id = (
-            forced_eos_token_id
-            if forced_eos_token_id is not None
-            else self.config.forced_eos_token_id
-        )
-
-        # the following idea is largely copied from this PR: https://github.com/huggingface/transformers/pull/5420/files
-        # all samplers can be found in `generation_utils_samplers.py`
-        if min_length is not None and eos_token_id is not None and min_length > -1:
-            processors.append(FlaxMinLengthLogitsProcessor(min_length, eos_token_id))
-        if forced_bos_token_id is not None:
-            processors.append(FlaxForcedBOSTokenLogitsProcessor(forced_bos_token_id))
-        if forced_eos_token_id is not None:
-            processors.append(
-                FlaxForcedEOSTokenLogitsProcessor(max_length, forced_eos_token_id)
-            )
-        return processors
+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
+        self.decoder.bias = self.bias
 
-    def _greedy_search(
-        self,
-        input_ids: None,
-        max_length: Optional[int] = None,
-        pad_token_id: Optional[int] = None,
-        eos_token_id: Optional[int] = None,
-        logits_processor: Optional[FlaxLogitsProcessorList] = None,
-        trace: bool = True,
-        params: Optional[Dict[str, jnp.ndarray]] = None,
-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,
-    ):
-        # init values
-        max_length = max_length if max_length is not None else self.config.max_length
-        pad_token_id = (
-            pad_token_id if pad_token_id is not None else self.config.pad_token_id
-        )
-        eos_token_id = (
-            eos_token_id if eos_token_id is not None else self.config.eos_token_id
-        )
-
-        batch_size, cur_len = input_ids.shape
-
-        eos_token_id = jnp.array(eos_token_id)
-        pad_token_id = jnp.array(pad_token_id)
-        cur_len = jnp.array(cur_len)
-
-        # per batch-item holding current token in loop.
-        sequences = jnp.full((batch_size, max_length), pad_token_id, dtype=jnp.int32)
-        sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0))
-
-        # per batch-item state bit indicating if sentence has finished.
-        is_sent_finished = jnp.zeros((batch_size,), dtype=jnp.bool_)
-
-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop
-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.
-        model = self.decode if self.config.is_encoder_decoder else self
-        # initialize model specific kwargs
-        model_kwargs = self.prepare_inputs_for_generation(
-            input_ids, max_length, **model_kwargs
-        )
-
-        # initialize state
-        state = GreedyState(
-            cur_len=cur_len,
-            sequences=sequences,
-            running_token=input_ids,
-            is_sent_finished=is_sent_finished,
-            model_kwargs=model_kwargs,
-        )
-
-        def greedy_search_cond_fn(state):
-            """state termination condition fn."""
-            has_reached_max_length = state.cur_len == max_length
-            all_sequence_finished = jnp.all(state.is_sent_finished)
-            finish_generation = jnp.logical_or(
-                has_reached_max_length, all_sequence_finished
-            )
-            return ~finish_generation
+    def forward(self, hidden_states):
+        hidden_states = self.transform(hidden_states)
+        hidden_states = self.decoder(hidden_states)
+        return hidden_states
 
-        def greedy_search_body_fn(state):
-            """state update fn."""
-            model_outputs = model(
-                state.running_token, params=params, **state.model_kwargs
-            )
-            logits = model_outputs.logits[:, -1]
+    def _tie_weights(self):
+        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)
+        self.bias = self.decoder.bias
 
-            # apply min_length, ...
-            logits = logits_processor(state.sequences, logits, state.cur_len)
 
-            next_token = jnp.argmax(logits, axis=-1)
+class FNetOnlyMLMHead(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.predictions = FNetLMPredictionHead(config)
 
-            next_token = (
-                next_token * ~state.is_sent_finished
-                + pad_token_id * state.is_sent_finished
-            )
-            next_is_sent_finished = state.is_sent_finished | (
-                next_token == eos_token_id
-            )
-            next_token = next_token[:, None]
+    def forward(self, sequence_output):
+        prediction_scores = self.predictions(sequence_output)
+        return prediction_scores
 
-            next_sequences = lax.dynamic_update_slice(
-                state.sequences, next_token, (0, state.cur_len)
-            )
-            next_model_kwargs = self.update_inputs_for_generation(
-                model_outputs, state.model_kwargs
-            )
-            return GreedyState(
-                cur_len=state.cur_len + 1,
-                sequences=next_sequences,
-                running_token=next_token,
-                is_sent_finished=next_is_sent_finished,
-                model_kwargs=next_model_kwargs,
-            )
 
-        # The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU
-        if input_ids.shape[1] > 1:
-            state = greedy_search_body_fn(state)
-
-        if not trace:
-            state = self._run_loop_in_debug(
-                greedy_search_cond_fn, greedy_search_body_fn, state
-            )
-        else:
-            state = lax.while_loop(greedy_search_cond_fn, greedy_search_body_fn, state)
+# Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->FNet
+class FNetOnlyNSPHead(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.seq_relationship = nn.Linear(config.hidden_size, 2)
 
-        return FlaxGreedySearchOutput(sequences=state.sequences)
+    def forward(self, pooled_output):
+        seq_relationship_score = self.seq_relationship(pooled_output)
+        return seq_relationship_score
 
-    def _sample(
-        self,
-        input_ids: None,
-        max_length: Optional[int] = None,
-        pad_token_id: Optional[int] = None,
-        eos_token_id: Optional[int] = None,
-        prng_key: Optional[jnp.ndarray] = None,
-        logits_processor: Optional[FlaxLogitsProcessorList] = None,
-        logits_warper: Optional[FlaxLogitsProcessorList] = None,
-        trace: bool = True,
-        params: Optional[Dict[str, jnp.ndarray]] = None,
-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,
-    ):
-        # init values
-        max_length = max_length if max_length is not None else self.config.max_length
-        pad_token_id = (
-            pad_token_id if pad_token_id is not None else self.config.pad_token_id
-        )
-        eos_token_id = (
-            eos_token_id if eos_token_id is not None else self.config.eos_token_id
-        )
-        prng_key = prng_key if prng_key is not None else jax.random.PRNGKey(0)
-
-        batch_size, cur_len = input_ids.shape
-
-        eos_token_id = jnp.array(eos_token_id)
-        pad_token_id = jnp.array(pad_token_id)
-        cur_len = jnp.array(cur_len)
-
-        # per batch-item holding current token in loop.
-        sequences = jnp.full((batch_size, max_length), pad_token_id, dtype=jnp.int32)
-        sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0))
-
-        # per batch-item state bit indicating if sentence has finished.
-        is_sent_finished = jnp.zeros((batch_size,), dtype=jnp.bool_)
-
-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop
-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.
-        model = self.decode if self.config.is_encoder_decoder else self
-
-        # initialize model specific kwargs
-        model_kwargs = self.prepare_inputs_for_generation(
-            input_ids, max_length, **model_kwargs
-        )
-
-        # initialize state
-        state = SampleState(
-            cur_len=cur_len,
-            sequences=sequences,
-            running_token=input_ids,
-            is_sent_finished=is_sent_finished,
-            prng_key=prng_key,
-            model_kwargs=model_kwargs,
-        )
-
-        def sample_search_cond_fn(state):
-            """state termination condition fn."""
-            has_reached_max_length = state.cur_len == max_length
-            all_sequence_finished = jnp.all(state.is_sent_finished)
-            finish_generation = jnp.logical_or(
-                has_reached_max_length, all_sequence_finished
-            )
-            return ~finish_generation
 
-        def sample_search_body_fn(state):
-            """state update fn."""
-            prng_key, prng_key_next = jax.random.split(state.prng_key)
-            model_outputs = model(
-                state.running_token, params=params, **state.model_kwargs
-            )
+# Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->FNet
+class FNetPreTrainingHeads(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.predictions = FNetLMPredictionHead(config)
+        self.seq_relationship = nn.Linear(config.hidden_size, 2)
 
-            logits = model_outputs.logits[:, -1]
+    def forward(self, sequence_output, pooled_output):
+        prediction_scores = self.predictions(sequence_output)
+        seq_relationship_score = self.seq_relationship(pooled_output)
+        return prediction_scores, seq_relationship_score
 
-            # apply min_length, ...
-            logits = logits_processor(state.sequences, logits, state.cur_len)
-            # apply top_p, top_k, temperature
-            logits = logits_warper(logits, logits, state.cur_len)
 
-            next_token = jax.random.categorical(prng_key, logits, axis=-1)
+class FNetPreTrainedModel(PreTrainedModel):
+    """
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
+    """
 
-            next_is_sent_finished = state.is_sent_finished | (
-                next_token == eos_token_id
-            )
-            next_token = (
-                next_token * ~next_is_sent_finished
-                + pad_token_id * next_is_sent_finished
-            )
-            next_token = next_token[:, None]
+    config_class = FNetConfig
+    base_model_prefix = "fnet"
+    supports_gradient_checkpointing = True
+    _keys_to_ignore_on_load_missing = [r"position_ids"]
+
+    def _init_weights(self, module):
+        """Initialize the weights"""
+        if isinstance(module, nn.Linear):
+            # Slightly different from the TF version which uses truncated_normal for initialization
+            # cf https://github.com/pytorch/pytorch/pull/5617
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            # NOTE: Original code uses same initialization as weights for biases as well.
+            if module.bias is not None:
+                module.bias.data.zero_()
+        elif isinstance(module, nn.Embedding):
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.padding_idx is not None:
+                module.weight.data[module.padding_idx].zero_()
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+
+    def _set_gradient_checkpointing(self, module, value=False):
+        if isinstance(module, FNetEncoder):
+            module.gradient_checkpointing = value
 
-            next_sequences = lax.dynamic_update_slice(
-                state.sequences, next_token, (0, state.cur_len)
-            )
-            next_model_kwargs = self.update_inputs_for_generation(
-                model_outputs, state.model_kwargs
-            )
 
-            return SampleState(
-                cur_len=state.cur_len + 1,
-                sequences=next_sequences,
-                running_token=next_token,
-                is_sent_finished=next_is_sent_finished,
-                model_kwargs=next_model_kwargs,
-                prng_key=prng_key_next,
-            )
+@dataclass
+class FNetForPreTrainingOutput(ModelOutput):
+    """
+    Output type of [`FNetForPreTraining`].
 
-        # The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU
-        if input_ids.shape[1] > 1:
-            state = sample_search_body_fn(state)
-
-        if not trace:
-            state = self._run_loop_in_debug(
-                sample_search_cond_fn, sample_search_body_fn, state
-            )
-        else:
-            state = lax.while_loop(sample_search_cond_fn, sample_search_body_fn, state)
+    Args:
+        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):
+            Total loss as the sum of the masked language modeling loss and the next sequence prediction
+            (classification) loss.
+        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
+        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):
+            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
+            before SoftMax).
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer
+            plus the initial embedding outputs.
+    """
 
-        return FlaxSampleOutput(sequences=state.sequences)
+    loss: Optional[torch.FloatTensor] = None
+    prediction_logits: torch.FloatTensor = None
+    seq_relationship_logits: torch.FloatTensor = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+
+
+FNET_START_DOCSTRING = r"""
+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
+    behavior.
+
+    Parameters:
+        config ([`FNetConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
+"""
 
-    def _beam_search(
+FNET_INPUTS_DOCSTRING = r"""
+    Args:
+        input_ids (`torch.LongTensor` of shape `({0})`):
+            Indices of input sequence tokens in the vocabulary.
+
+            Indices can be obtained using [`FNetTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+            [`PreTrainedTokenizer.__call__`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
+            1]`:
+
+            - 0 corresponds to a *sentence A* token,
+            - 1 corresponds to a *sentence B* token.
+
+            [What are token type IDs?](../glossary#token-type-ids)
+        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
+            config.max_position_embeddings - 1]`.
+
+            [What are position IDs?](../glossary#position-ids)
+
+        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):
+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the
+            model's internal embedding lookup matrix.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+"""
+
+
+@add_start_docstrings(
+    "The bare FNet Model transformer outputting raw hidden-states without any specific head on top.",
+    FNET_START_DOCSTRING,
+)
+class FNetModel(FNetPreTrainedModel):
+    """
+
+    The model can behave as an encoder, following the architecture described in [FNet: Mixing Tokens with Fourier
+    Transforms](https://arxiv.org/abs/2105.03824) by James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon.
+
+    """
+
+    def __init__(self, config, add_pooling_layer=True):
+        super().__init__(config)
+        self.config = config
+
+        self.embeddings = FNetEmbeddings(config)
+        self.encoder = FNetEncoder(config)
+
+        self.pooler = FNetPooler(config) if add_pooling_layer else None
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.embeddings.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.embeddings.word_embeddings = value
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=BaseModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
         self,
-        input_ids: None,
-        max_length: Optional[int] = None,
-        pad_token_id: Optional[int] = None,
-        eos_token_id: Optional[int] = None,
-        length_penalty: Optional[float] = None,
-        early_stopping: Optional[bool] = None,
-        logits_processor: Optional[FlaxLogitsProcessorList] = None,
-        trace: bool = True,
-        params: Optional[Dict[str, jnp.ndarray]] = None,
-        model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,
+        input_ids=None,
+        token_type_ids=None,
+        position_ids=None,
+        inputs_embeds=None,
+        output_hidden_states=None,
+        return_dict=None,
     ):
-        """
-        This beam search function is heavily inspired by Flax's official example:
-        https://github.com/google/flax/blob/master/examples/wmt/train.py#L254
-        """
+        output_hidden_states = (
+            output_hidden_states
+            if output_hidden_states is not None
+            else self.config.output_hidden_states
+        )
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
 
-        def flatten_beam_dim(tensor):
-            """Flattens the first two dimensions of a non-scalar array."""
-            # ignore scalars (e.g. cache index)
-            if tensor.ndim == 0:
-                return tensor
-            return tensor.reshape(
-                (tensor.shape[0] * tensor.shape[1],) + tensor.shape[2:]
+        if input_ids is not None and inputs_embeds is not None:
+            raise ValueError(
+                "You cannot specify both input_ids and inputs_embeds at the same time"
             )
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+            batch_size, seq_length = input_shape
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+            batch_size, seq_length = input_shape
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
-        def unflatten_beam_dim(tensor, batch_size, num_beams):
-            """Unflattens the first, flat batch*beam dimension of a non-scalar array."""
-            # ignore scalars (e.g. cache index)
-            if tensor.ndim == 0:
-                return tensor
-            return tensor.reshape((batch_size, num_beams) + tensor.shape[1:])
-
-        def gather_beams(nested, beam_indices, batch_size, new_num_beams):
-            """
-            Gathers the beam slices indexed by beam_indices into new beam array.
-            """
-            batch_indices = jnp.reshape(
-                jnp.arange(batch_size * new_num_beams) // new_num_beams,
-                (batch_size, new_num_beams),
+        if (
+            self.config.use_tpu_fourier_optimizations
+            and seq_length <= 4096
+            and self.config.tpu_short_seq_length != seq_length
+        ):
+            raise ValueError(
+                "The `tpu_short_seq_length` in FNetConfig should be set equal to the sequence length being passed to"
+                " the model when using TPU optimizations."
             )
 
-            def gather_fn(tensor):
-                # ignore scalars (e.g. cache index)
-                if tensor.ndim == 0:
-                    return tensor
-                else:
-                    return tensor[batch_indices, beam_indices]
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
 
-            return jax.tree_util.tree_map(gather_fn, nested)
+        if token_type_ids is None:
+            if hasattr(self.embeddings, "token_type_ids"):
+                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]
+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(
+                    batch_size, seq_length
+                )
+                token_type_ids = buffered_token_type_ids_expanded
+            else:
+                token_type_ids = torch.zeros(
+                    input_shape, dtype=torch.long, device=device
+                )
 
-        # init values
-        max_length = max_length if max_length is not None else self.config.max_length
-        pad_token_id = (
-            pad_token_id if pad_token_id is not None else self.config.pad_token_id
+        embedding_output = self.embeddings(
+            input_ids=input_ids,
+            position_ids=position_ids,
+            token_type_ids=token_type_ids,
+            inputs_embeds=inputs_embeds,
         )
-        eos_token_id = (
-            eos_token_id if eos_token_id is not None else self.config.eos_token_id
+        encoder_outputs = self.encoder(
+            embedding_output,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
         )
-        length_penalty = (
-            length_penalty if length_penalty is not None else self.config.length_penalty
+        sequence_output = encoder_outputs[0]
+
+        pooler_output = (
+            self.pooler(sequence_output) if self.pooler is not None else None
         )
-        early_stopping = (
-            early_stopping if early_stopping is not None else self.config.early_stopping
+
+        if not return_dict:
+            return (sequence_output, pooler_output) + encoder_outputs[1:]
+
+        return BaseModelOutputWithPooling(
+            last_hidden_state=sequence_output,
+            pooler_output=pooler_output,
+            hidden_states=encoder_outputs.hidden_states,
         )
 
-        batch_size, num_beams, cur_len = input_ids.shape
 
-        eos_token_id = jnp.array(eos_token_id)
-        pad_token_id = jnp.array(pad_token_id)
-        cur_len = jnp.array(cur_len)
+@add_start_docstrings(
+    """
+    FNet Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next
+    sentence prediction (classification)` head.
+    """,
+    FNET_START_DOCSTRING,
+)
+class FNetForPreTraining(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.fnet = FNetModel(config)
+        self.cls = FNetPreTrainingHeads(config)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_output_embeddings(self):
+        return self.cls.predictions.decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.cls.predictions.decoder = new_embeddings
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=FNetForPreTrainingOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        next_sentence_label: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, FNetForPreTrainingOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
+        next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair
+            (see `input_ids` docstring) Indices should be in `[0, 1]`:
+
+            - 0 indicates sequence B is a continuation of sequence A,
+            - 1 indicates sequence B is a random sequence.
+        kwargs (`Dict[str, any]`, optional, defaults to *{}*):
+            Used to hide legacy arguments that have been deprecated.
+
+        Returns:
 
-        # per batch,beam-item holding current token in loop.
-        sequences = jnp.full(
-            (batch_size, num_beams, max_length), pad_token_id, dtype=jnp.int32
-        )
-        running_sequences = jnp.full(
-            (batch_size, num_beams, max_length), pad_token_id, dtype=jnp.int32
-        )
-        running_sequences = lax.dynamic_update_slice(sequences, input_ids, (0, 0, 0))
+        Example:
 
-        # per batch,beam-item state bit indicating if sentence has finished.
-        is_sent_finished = jnp.zeros((batch_size, num_beams), dtype=jnp.bool_)
+        ```python
+        >>> from transformers import FNetTokenizer, FNetForPreTraining
+        >>> import torch
 
-        # per batch,beam-item score, logprobs
-        running_scores = jnp.tile(
-            jnp.array([0.0] + [np.array(-1.0e7)] * (num_beams - 1)), [batch_size, 1]
+        >>> tokenizer = FNetTokenizer.from_pretrained("google/fnet-base")
+        >>> model = FNetForPreTraining.from_pretrained("google/fnet-base")
+        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
+        >>> outputs = model(**inputs)
+        >>> prediction_logits = outputs.prediction_logits
+        >>> seq_relationship_logits = outputs.seq_relationship_logits
+        ```"""
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
         )
-        scores = jnp.ones((batch_size, num_beams)) * np.array(-1.0e7)
 
-        # For Seq2Seq generation, we only need to use the decoder instead of the whole model in generation loop
-        # and pass it the `encoder_outputs`, which are part of the `model_kwargs`.
-        model = self.decode if self.config.is_encoder_decoder else self
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-        # flatten beam dim
-        if "encoder_outputs" in model_kwargs:
-            model_kwargs["encoder_outputs"]["last_hidden_state"] = flatten_beam_dim(
-                model_kwargs["encoder_outputs"]["last_hidden_state"]
-            )
-        if "attention_mask" in model_kwargs:
-            model_kwargs["attention_mask"] = flatten_beam_dim(
-                model_kwargs["attention_mask"]
-            )
+        sequence_output, pooled_output = outputs[:2]
+        prediction_scores, seq_relationship_score = self.cls(
+            sequence_output, pooled_output
+        )
 
-        # initialize model specific kwargs
-        model_kwargs = self.prepare_inputs_for_generation(
-            flatten_beam_dim(input_ids), max_length, **model_kwargs
-        )
-
-        # initialize state
-        state = BeamSearchState(
-            cur_len=cur_len,
-            running_sequences=running_sequences,
-            running_scores=running_scores,
-            sequences=sequences,
-            scores=scores,
-            is_sent_finished=is_sent_finished,
-            model_kwargs=model_kwargs,
-        )
-
-        def beam_search_cond_fn(state):
-            """beam search state termination condition fn."""
-
-            # 1. is less than max length?
-            not_max_length_yet = state.cur_len < max_length
-
-            # 2. can the new beams still improve?
-            best_running_score = state.running_scores[:, -1:] / (
-                max_length**length_penalty
+        total_loss = None
+        if labels is not None and next_sentence_label is not None:
+            loss_fct = CrossEntropyLoss()
+            masked_lm_loss = loss_fct(
+                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)
             )
-            worst_finished_score = jnp.where(
-                state.is_sent_finished,
-                jnp.min(state.scores, axis=1, keepdims=True),
-                np.array(-1.0e7),
-            )
-            improvement_still_possible = jnp.all(
-                worst_finished_score < best_running_score
+            next_sentence_loss = loss_fct(
+                seq_relationship_score.view(-1, 2), next_sentence_label.view(-1)
             )
+            total_loss = masked_lm_loss + next_sentence_loss
 
-            # 3. is there still a beam that has not finished?
-            still_open_beam = ~(jnp.all(state.is_sent_finished) & early_stopping)
+        if not return_dict:
+            output = (prediction_scores, seq_relationship_score) + outputs[2:]
+            return ((total_loss,) + output) if total_loss is not None else output
 
-            return not_max_length_yet & still_open_beam & improvement_still_possible
+        return FNetForPreTrainingOutput(
+            loss=total_loss,
+            prediction_logits=prediction_scores,
+            seq_relationship_logits=seq_relationship_score,
+            hidden_states=outputs.hidden_states,
+        )
 
-        def beam_search_body_fn(state, input_ids_length=1):
-            """beam search state update fn."""
-            # 1. Forward current tokens
-            # Collect the current position slice along length to feed the fast
-            # autoregressive decoder model.  Flatten the beam dimension into batch
-            # dimension for feeding into the model.
-            # unflatten beam dimension
-            # Unflatten beam dimension in attention cache arrays
-            input_token = flatten_beam_dim(
-                lax.dynamic_slice(
-                    state.running_sequences,
-                    (0, 0, state.cur_len - input_ids_length),
-                    (batch_size, num_beams, input_ids_length),
-                )
-            )
-            model_outputs = model(input_token, params=params, **state.model_kwargs)
 
-            logits = unflatten_beam_dim(
-                model_outputs.logits[:, -1], batch_size, num_beams
-            )
-            cache = jax.tree_util.tree_map(
-                lambda tensor: unflatten_beam_dim(tensor, batch_size, num_beams),
-                model_outputs.past_key_values,
-            )
+@add_start_docstrings(
+    """FNet Model with a `language modeling` head on top.""", FNET_START_DOCSTRING
+)
+class FNetForMaskedLM(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.fnet = FNetModel(config)
+        self.cls = FNetOnlyMLMHead(config)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_output_embeddings(self):
+        return self.cls.predictions.decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.cls.predictions.decoder = new_embeddings
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=MaskedLMOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, MaskedLMOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
 
-            # adapt logits for FlaxMarianMTModel
-            logits = self._adapt_logits_for_beam_search(logits)
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-            # 2. Compute log probs
-            # get log probabilities from logits,
-            # process logits with processors (*e.g.* min_length, ...), and
-            # add new logprobs to existing running logprobs scores.
-            log_probs = jax.nn.log_softmax(logits)
-            log_probs = logits_processor(
-                flatten_beam_dim(running_sequences),
-                flatten_beam_dim(log_probs),
-                state.cur_len,
-            )
-            log_probs = unflatten_beam_dim(log_probs, batch_size, num_beams)
-            log_probs = log_probs + jnp.expand_dims(state.running_scores, axis=2)
-            vocab_size = log_probs.shape[2]
-            log_probs = log_probs.reshape((batch_size, num_beams * vocab_size))
-
-            # 3. Retrieve top-K
-            # Each item in batch has num_beams * vocab_size candidate sequences.
-            # For each item, get the top 2*k candidates with the highest log-
-            # probabilities. We gather the top 2*K beams here so that even if the best
-            # K sequences reach EOS simultaneously, we have another K sequences
-            # remaining to continue the live beam search.
-            # Gather the top 2*K scores from _all_ beams.
-            # Gather 2*k top beams.
-            # Recover the beam index by floor division.
-            # Recover token id by modulo division and expand Id array for broadcasting.
-            # Update sequences for the 2*K top-k new sequences.
-            beams_to_keep = 2 * num_beams
-            topk_log_probs, topk_indices = lax.top_k(log_probs, k=beams_to_keep)
-            topk_beam_indices = topk_indices // vocab_size
-            topk_running_sequences = gather_beams(
-                state.running_sequences, topk_beam_indices, batch_size, beams_to_keep
-            )
-            topk_ids = jnp.expand_dims(topk_indices % vocab_size, axis=2)
-            topk_sequences = lax.dynamic_update_slice(
-                topk_running_sequences, topk_ids, (0, 0, state.cur_len)
-            )
+        sequence_output = outputs[0]
+        prediction_scores = self.cls(sequence_output)
 
-            # 4. Check which sequences have ended
-            # Update current sequences:
-            # Did any of these sequences reach an end marker?
-            # To prevent these just finished sequences from being added to the current sequences
-            # set of active beam search sequences, set their log probs to a very large
-            # negative value.
-            did_topk_just_finished = topk_sequences[:, :, state.cur_len] == eos_token_id
-            running_topk_log_probs = topk_log_probs + did_topk_just_finished * np.array(
-                -1.0e7
-            )
-            # 5. Get running sequences scores for next
-            # Determine the top k beam indices (from top 2*k beams) from log probs
-            # and gather top k beams (from top 2*k beams).
-            next_topk_indices = jnp.flip(
-                lax.top_k(running_topk_log_probs, k=num_beams)[1], axis=1
-            )
-            next_running_sequences, next_running_scores = gather_beams(
-                [topk_sequences, running_topk_log_probs],
-                next_topk_indices,
-                batch_size,
-                num_beams,
+        masked_lm_loss = None
+        if labels is not None:
+            loss_fct = CrossEntropyLoss()  # -100 index = padding token
+            masked_lm_loss = loss_fct(
+                prediction_scores.view(-1, self.config.vocab_size), labels.view(-1)
             )
 
-            # 6. Process topk logits
-            # Further process log probs:
-            # - add length penalty
-            # - make sure no scores can be added anymore if beam is full
-            # - make sure still running sequences cannot be chosen as finalized beam
-            topk_log_probs = topk_log_probs / (state.cur_len**length_penalty)
-            beams_in_batch_are_full = (
-                jnp.broadcast_to(
-                    state.is_sent_finished.all(axis=-1, keepdims=True),
-                    did_topk_just_finished.shape,
-                )
-                & early_stopping
+        if not return_dict:
+            output = (prediction_scores,) + outputs[2:]
+            return (
+                ((masked_lm_loss,) + output) if masked_lm_loss is not None else output
             )
-            add_penalty = ~did_topk_just_finished | beams_in_batch_are_full
-            topk_log_probs += add_penalty * np.array(-1.0e7)
 
-            # 7. Get scores, sequences, is sentence finished for next.
-            # Combine sequences, scores, and flags along the beam dimension and compare
-            # new finished sequence scores to existing finished scores and select the
-            # best from the new set of beams
-            merged_sequences = jnp.concatenate(
-                [state.sequences, topk_sequences], axis=1
-            )
-            merged_scores = jnp.concatenate([state.scores, topk_log_probs], axis=1)
-            merged_is_sent_finished = jnp.concatenate(
-                [state.is_sent_finished, did_topk_just_finished], axis=1
-            )
-            topk_merged_indices = jnp.flip(
-                lax.top_k(merged_scores, k=num_beams)[1], axis=1
-            )
-            next_sequences, next_scores, next_is_sent_finished = gather_beams(
-                [merged_sequences, merged_scores, merged_is_sent_finished],
-                topk_merged_indices,
-                batch_size,
-                num_beams,
-            )
+        return MaskedLMOutput(
+            loss=masked_lm_loss,
+            logits=prediction_scores,
+            hidden_states=outputs.hidden_states,
+        )
 
-            # 8. Update model kwargs.
-            # Determine the top k beam indices from the original set of all beams.
-            # With these, gather the top k beam-associated caches.
-            next_running_indices = gather_beams(
-                topk_beam_indices, next_topk_indices, batch_size, num_beams
-            )
-            next_cache = gather_beams(
-                cache, next_running_indices, batch_size, num_beams
-            )
-            model_outputs["past_key_values"] = jax.tree_util.tree_map(
-                lambda x: flatten_beam_dim(x), next_cache
-            )
-            next_model_kwargs = self.update_inputs_for_generation(
-                model_outputs, state.model_kwargs
-            )
 
-            return BeamSearchState(
-                cur_len=state.cur_len + 1,
-                running_scores=next_running_scores,
-                running_sequences=next_running_sequences,
-                scores=next_scores,
-                sequences=next_sequences,
-                is_sent_finished=next_is_sent_finished,
-                model_kwargs=next_model_kwargs,
+@add_start_docstrings(
+    """FNet Model with a `next sentence prediction (classification)` head on top.""",
+    FNET_START_DOCSTRING,
+)
+class FNetForNextSentencePrediction(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.fnet = FNetModel(config)
+        self.cls = FNetOnlyNSPHead(config)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        **kwargs,
+    ) -> Union[Tuple, NextSentencePredictorOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair
+            (see `input_ids` docstring). Indices should be in `[0, 1]`:
+
+            - 0 indicates sequence B is a continuation of sequence A,
+            - 1 indicates sequence B is a random sequence.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> from transformers import FNetTokenizer, FNetForNextSentencePrediction
+        >>> import torch
+
+        >>> tokenizer = FNetTokenizer.from_pretrained("google/fnet-base")
+        >>> model = FNetForNextSentencePrediction.from_pretrained("google/fnet-base")
+        >>> prompt = "In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced."
+        >>> next_sentence = "The sky is blue due to the shorter wavelength of blue light."
+        >>> encoding = tokenizer(prompt, next_sentence, return_tensors="pt")
+        >>> outputs = model(**encoding, labels=torch.LongTensor([1]))
+        >>> logits = outputs.logits
+        >>> assert logits[0, 0] < logits[0, 1]  # next sentence was random
+        ```"""
+
+        if "next_sentence_label" in kwargs:
+            warnings.warn(
+                "The `next_sentence_label` argument is deprecated and will be removed in a future version, use"
+                " `labels` instead.",
+                FutureWarning,
             )
+            labels = kwargs.pop("next_sentence_label")
+
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-        # The very first prompt often has sequence length > 1, so run outside of `lax.while_loop` to comply with TPU
-        if input_ids.shape[-1] > 1:
-            state = partial(beam_search_body_fn, input_ids_length=input_ids.shape[-1])(
-                state
+        pooled_output = outputs[1]
+
+        seq_relationship_scores = self.cls(pooled_output)
+
+        next_sentence_loss = None
+        if labels is not None:
+            loss_fct = CrossEntropyLoss()
+            next_sentence_loss = loss_fct(
+                seq_relationship_scores.view(-1, 2), labels.view(-1)
             )
 
-        if not trace:
-            state = self._run_loop_in_debug(
-                beam_search_cond_fn, beam_search_body_fn, state
+        if not return_dict:
+            output = (seq_relationship_scores,) + outputs[2:]
+            return (
+                ((next_sentence_loss,) + output)
+                if next_sentence_loss is not None
+                else output
             )
-        else:
-            state = lax.while_loop(beam_search_cond_fn, beam_search_body_fn, state)
 
-        # Account for the edge-case where there are no finished sequences for a
-        # particular batch item. If so, return running sequences for that batch item.
-        none_finished = jnp.any(state.is_sent_finished, axis=1)
-        sequences = jnp.where(
-            none_finished[:, None, None], state.sequences, state.running_sequences
+        return NextSentencePredictorOutput(
+            loss=next_sentence_loss,
+            logits=seq_relationship_scores,
+            hidden_states=outputs.hidden_states,
+        )
+
+
+@add_start_docstrings(
+    """
+    FNet Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled
+    output) e.g. for GLUE tasks.
+    """,
+    FNET_START_DOCSTRING,
+)
+class FNetForSequenceClassification(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.num_labels = config.num_labels
+        self.fnet = FNetModel(config)
+
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=SequenceClassifierOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, SequenceClassifierOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        pooled_output = outputs[1]
+        pooled_output = self.dropout(pooled_output)
+        logits = self.classifier(pooled_output)
+
+        loss = None
+        if labels is not None:
+            if self.config.problem_type is None:
+                if self.num_labels == 1:
+                    self.config.problem_type = "regression"
+                elif self.num_labels > 1 and (
+                    labels.dtype == torch.long or labels.dtype == torch.int
+                ):
+                    self.config.problem_type = "single_label_classification"
+                else:
+                    self.config.problem_type = "multi_label_classification"
+
+            if self.config.problem_type == "regression":
+                loss_fct = MSELoss()
+                if self.num_labels == 1:
+                    loss = loss_fct(logits.squeeze(), labels.squeeze())
+                else:
+                    loss = loss_fct(logits, labels)
+            elif self.config.problem_type == "single_label_classification":
+                loss_fct = CrossEntropyLoss()
+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
+            elif self.config.problem_type == "multi_label_classification":
+                loss_fct = BCEWithLogitsLoss()
+                loss = loss_fct(logits, labels)
+        if not return_dict:
+            output = (logits,) + outputs[2:]
+            return ((loss,) + output) if loss is not None else output
+
+        return SequenceClassifierOutput(
+            loss=loss, logits=logits, hidden_states=outputs.hidden_states
+        )
+
+
+@add_start_docstrings(
+    """
+    FNet Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a
+    softmax) e.g. for RocStories/SWAG tasks.
+    """,
+    FNET_START_DOCSTRING,
+)
+class FNetForMultipleChoice(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.fnet = FNetModel(config)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        self.classifier = nn.Linear(config.hidden_size, 1)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, num_choices, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=MultipleChoiceModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, MultipleChoiceModelOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,
+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See
+            `input_ids` above)
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+        num_choices = (
+            input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]
+        )
+
+        input_ids = (
+            input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None
+        )
+        token_type_ids = (
+            token_type_ids.view(-1, token_type_ids.size(-1))
+            if token_type_ids is not None
+            else None
+        )
+        position_ids = (
+            position_ids.view(-1, position_ids.size(-1))
+            if position_ids is not None
+            else None
+        )
+        inputs_embeds = (
+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))
+            if inputs_embeds is not None
+            else None
         )
-        scores = jnp.where(none_finished[:, None], state.scores, state.running_scores)
 
-        # take best beam for each batch
-        sequences = sequences[:, -1]
-        scores = scores[:, -1]
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-        return FlaxBeamSearchOutput(sequences=sequences, scores=scores)
+        pooled_output = outputs[1]
+
+        pooled_output = self.dropout(pooled_output)
+        logits = self.classifier(pooled_output)
+        reshaped_logits = logits.view(-1, num_choices)
+
+        loss = None
+        if labels is not None:
+            loss_fct = CrossEntropyLoss()
+            loss = loss_fct(reshaped_logits, labels)
+
+        if not return_dict:
+            output = (reshaped_logits,) + outputs[2:]
+            return ((loss,) + output) if loss is not None else output
+
+        return MultipleChoiceModelOutput(
+            loss=loss, logits=reshaped_logits, hidden_states=outputs.hidden_states
+        )
+
+
+@add_start_docstrings(
+    """
+    FNet Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for
+    Named-Entity-Recognition (NER) tasks.
+    """,
+    FNET_START_DOCSTRING,
+)
+class FNetForTokenClassification(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.num_labels = config.num_labels
+
+        self.fnet = FNetModel(config)
+
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TokenClassifierOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        labels: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, TokenClassifierOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        sequence_output = outputs[0]
+
+        sequence_output = self.dropout(sequence_output)
+        logits = self.classifier(sequence_output)
+
+        loss = None
+        if labels is not None:
+            loss_fct = CrossEntropyLoss()
+            # Only keep active parts of the loss
+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
+
+        if not return_dict:
+            output = (logits,) + outputs[2:]
+            return ((loss,) + output) if loss is not None else output
+
+        return TokenClassifierOutput(
+            loss=loss, logits=logits, hidden_states=outputs.hidden_states
+        )
+
+
+@add_start_docstrings(
+    """
+    FNet Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear
+    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).
+    """,
+    FNET_START_DOCSTRING,
+)
+class FNetForQuestionAnswering(FNetPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.num_labels = config.num_labels
+
+        self.fnet = FNetModel(config)
+        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        FNET_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=QuestionAnsweringModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.Tensor] = None,
+        token_type_ids: Optional[torch.Tensor] = None,
+        position_ids: Optional[torch.Tensor] = None,
+        inputs_embeds: Optional[torch.Tensor] = None,
+        start_positions: Optional[torch.Tensor] = None,
+        end_positions: Optional[torch.Tensor] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, QuestionAnsweringModelOutput]:
+        r"""
+        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for position (index) of the start of the labelled span for computing the token classification loss.
+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
+            are not taken into account for computing the loss.
+        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for position (index) of the end of the labelled span for computing the token classification loss.
+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
+            are not taken into account for computing the loss.
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        outputs = self.fnet(
+            input_ids,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            inputs_embeds=inputs_embeds,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        sequence_output = outputs[0]
+
+        logits = self.qa_outputs(sequence_output)
+        start_logits, end_logits = logits.split(1, dim=-1)
+        start_logits = start_logits.squeeze(-1).contiguous()
+        end_logits = end_logits.squeeze(-1).contiguous()
+
+        total_loss = None
+        if start_positions is not None and end_positions is not None:
+            # If we are on multi-GPU, split add a dimension
+            if len(start_positions.size()) > 1:
+                start_positions = start_positions.squeeze(-1)
+            if len(end_positions.size()) > 1:
+                end_positions = end_positions.squeeze(-1)
+            # sometimes the start/end positions are outside our model inputs, we ignore these terms
+            ignored_index = start_logits.size(1)
+            start_positions = start_positions.clamp(0, ignored_index)
+            end_positions = end_positions.clamp(0, ignored_index)
+
+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)
+            start_loss = loss_fct(start_logits, start_positions)
+            end_loss = loss_fct(end_logits, end_positions)
+            total_loss = (start_loss + end_loss) / 2
+
+        if not return_dict:
+            output = (start_logits, end_logits) + outputs[2:]
+            return ((total_loss,) + output) if total_loss is not None else output
+
+        return QuestionAnsweringModelOutput(
+            loss=total_loss,
+            start_logits=start_logits,
+            end_logits=end_logits,
+            hidden_states=outputs.hidden_states,
+        )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/generation_logits_process.py` & `xs_transformers-1.0.1/xs_transformers/models/transfo_xl/tokenization_transfo_xl.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,876 +1,866 @@
 # coding=utf-8
-# Copyright 2020 The HuggingFace Inc. team
+# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.
+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+"""
+ Tokenization classes for Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl.
+"""
 
-import inspect
-import math
-from typing import Callable, Iterable, List, Optional, Tuple
-
-import numpy as np
-import torch
-
-from .utils import add_start_docstrings
-from .utils.logging import get_logger
 
-logger = get_logger(__name__)
+import glob
+import os
+import pickle
+import re
+from collections import Counter, OrderedDict
+from typing import List, Optional, Tuple
 
+import numpy as np
 
-LOGITS_PROCESSOR_INPUTS_DOCSTRING = r"""
-    Args:
-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
-            Indices of input sequence tokens in the vocabulary.
+from ...tokenization_utils import PreTrainedTokenizer
+from ...utils import (
+    cached_file,
+    is_sacremoses_available,
+    is_torch_available,
+    logging,
+    requires_backends,
+    torch_only_method,
+)
 
-            Indices can be obtained using [`BertTokenizer`]. See [`PreTrainedTokenizer.encode`] and
-            [`PreTrainedTokenizer.__call__`] for details.
+if is_sacremoses_available():
+    import sacremoses as sm
 
-            [What are input IDs?](../glossary#input-ids)
-        scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):
-            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam
-            search or log softmax for each vocabulary token when using beam search
-        kwargs:
-            Additional logits processor specific kwargs.
 
-    Return:
-        `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.
+if is_torch_available():
+    import torch
 
-"""
 
+logger = logging.get_logger(__name__)
 
-class LogitsProcessor:
-    """Abstract base class for all logit processors that can be applied during generation."""
+VOCAB_FILES_NAMES = {
+    "pretrained_vocab_file": "vocab.pkl",
+    "pretrained_vocab_file_torch": "vocab.bin",
+    "vocab_file": "vocab.txt",
+}
 
-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        """Torch method for processing logits."""
-        raise NotImplementedError(
-            f"{self.__class__} is an abstract class. Only classes inheriting this class can be called."
-        )
+PRETRAINED_VOCAB_FILES_MAP = {
+    "pretrained_vocab_file": {
+        "transfo-xl-wt103": "https://huggingface.co/transfo-xl-wt103/resolve/main/vocab.pkl",
+    }
+}
 
+PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
+    "transfo-xl-wt103": None,
+}
 
-class LogitsWarper:
-    """Abstract base class for all logit warpers that can be applied during generation with multinomial sampling."""
+PRETRAINED_CORPUS_ARCHIVE_MAP = {
+    "transfo-xl-wt103": "https://huggingface.co/transfo-xl-wt103/resolve/main/corpus.bin",
+}
+CORPUS_NAME = "corpus.bin"
 
-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        """Torch method for warping logits."""
-        raise NotImplementedError(
-            f"{self.__class__} is an abstract class. Only classes inheriting this class can be called."
-        )
+MATCH_NUMBERS = r"(?<=\d)[,.](?=\d)", r" @\g<0>@ "
+DETOKENIZE_NUMBERS = [(r" @\,@ ", r","), (r" @\.@ ", r".")]
 
 
-class LogitsProcessorList(list):
+def tokenize_numbers(text_array: List[str]) -> List[str]:
     """
-    This class can be used to create a list of [`LogitsProcessor`] or [`LogitsWarper`] to subsequently process a
-    `scores` input tensor. This class inherits from list and adds a specific *__call__* method to apply each
-    [`LogitsProcessor`] or [`LogitsWarper`] to the inputs.
-    """
-
-    @add_start_docstrings(LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs
-    ) -> torch.FloatTensor:
-        for processor in self:
-            function_args = inspect.signature(processor.__call__).parameters
-            if len(function_args) > 2:
-                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):
-                    raise ValueError(
-                        f"Make sure that all the required parameters: {list(function_args.keys())} for "
-                        f"{processor.__class__} are passed to the logits processor."
-                    )
-                scores = processor(input_ids, scores, **kwargs)
-            else:
-                scores = processor(input_ids, scores)
-        return scores
-
-
-class MinLengthLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] enforcing a min-length by setting EOS probability to 0.
+    Splits large comma-separated numbers and floating point values. This is done by replacing commas with ' @,@ ' and
+    dots with ' @.@ '.
 
     Args:
-        min_length (`int`):
-            The minimum length below which the score of `eos_token_id` is set to `-float("Inf")`.
-        eos_token_id (`int`):
-            The id of the *end-of-sequence* token.
-    """
-
-    def __init__(self, min_length: int, eos_token_id: int):
-        if not isinstance(min_length, int) or min_length < 0:
-            raise ValueError(
-                f"`min_length` has to be a positive integer, but is {min_length}"
-            )
+        text_array: An already tokenized text as list.
 
-        if not isinstance(eos_token_id, int) or eos_token_id < 0:
-            raise ValueError(
-                f"`eos_token_id` has to be a positive integer, but is {eos_token_id}"
-            )
+    Returns:
+        A list of strings with tokenized numbers.
 
-        self.min_length = min_length
-        self.eos_token_id = eos_token_id
+    Example:
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        cur_len = input_ids.shape[-1]
-        if cur_len < self.min_length:
-            scores[:, self.eos_token_id] = -float("inf")
-        return scores
+    ```python
+    >>> tokenize_numbers(["$", "5,000", "1.73", "m"])
+    ["$", "5", "@,@", "000", "1", "@.@", "73", "m"]
+    ```"""
+    tokenized = []
+    for i in range(len(text_array)):
+        reg, sub = MATCH_NUMBERS
+        replaced = re.sub(reg, sub, text_array[i]).split()
+        tokenized.extend(replaced)
 
+    return tokenized
 
-class TemperatureLogitsWarper(LogitsWarper):
-    r"""
-    [`LogitsWarper`] for temperature (exponential scaling output probability distribution).
 
-    Args:
-        temperature (`float`):
-            The value used to module the logits distribution.
+def detokenize_numbers(text: str) -> str:
     """
-
-    def __init__(self, temperature: float):
-        if not isinstance(temperature, float) or not (temperature > 0):
-            raise ValueError(
-                f"`temperature` has to be a strictly positive float, but is {temperature}"
-            )
-
-        self.temperature = temperature
-
-    def __call__(
-        self, input_ids: torch.Tensor, scores: torch.Tensor
-    ) -> torch.FloatTensor:
-        scores = scores / self.temperature
-        return scores
-
-
-class RepetitionPenaltyLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] enforcing an exponential penalty on repeated sequences.
+    Inverts the operation of *tokenize_numbers*. This is replacing ' @,@ ' and ' @.@' by ',' and '.'.
 
     Args:
-        repetition_penalty (`float`):
-            The parameter for repetition penalty. 1.0 means no penalty. See [this
-            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
-    """
-
-    def __init__(self, penalty: float):
-        if not isinstance(penalty, float) or not (penalty > 0):
-            raise ValueError(
-                f"`penalty` has to be a strictly positive float, but is {penalty}"
-            )
-
-        self.penalty = penalty
+        text: A string where the number should be detokenized.
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        score = torch.gather(scores, 1, input_ids)
+    Returns:
+        A detokenized string.
 
-        # if score < 0 then repetition penalty has to be multiplied to reduce the previous token probability
-        score = torch.where(score < 0, score * self.penalty, score / self.penalty)
+    Example:
 
-        scores.scatter_(1, input_ids, score)
-        return scores
+    ```python
+    >>> detokenize_numbers("$ 5 @,@ 000 1 @.@ 73 m")
+    "$ 5,000 1.73 m"
+    ```"""
+    for reg, sub in DETOKENIZE_NUMBERS:
+        text = re.sub(reg, sub, text)
+    return text
 
 
-class TopPLogitsWarper(LogitsWarper):
+class TransfoXLTokenizer(PreTrainedTokenizer):
     """
-    [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off.
+    Construct a Transformer-XL tokenizer adapted from Vocab class in [the original
+    code](https://github.com/kimiyoung/transformer-xl). The Transformer-XL tokenizer is a word-level tokenizer (no
+    sub-word tokenization).
 
-    Args:
-        top_p (`float`):
-            If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or
-            higher are kept for generation.
-        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
-            All filtered values will be set to this float value.
-        min_tokens_to_keep (`int`, *optional*, defaults to 1):
-            Minimum number of tokens that cannot be filtered.
-    """
-
-    def __init__(
-        self,
-        top_p: float,
-        filter_value: float = -float("Inf"),
-        min_tokens_to_keep: int = 1,
-    ):
-        top_p = float(top_p)
-        if top_p < 0 or top_p > 1.0:
-            raise ValueError(f"`top_p` has to be a float > 0 and < 1, but is {top_p}")
-
-        self.top_p = top_p
-        self.filter_value = filter_value
-        self.min_tokens_to_keep = min_tokens_to_keep
-
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        sorted_logits, sorted_indices = torch.sort(scores, descending=False)
-        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
-
-        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
-        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)
-        if self.min_tokens_to_keep > 1:
-            # Keep at least min_tokens_to_keep
-            sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0
-
-        # scatter sorted tensors to original indexing
-        indices_to_remove = sorted_indices_to_remove.scatter(
-            1, sorted_indices, sorted_indices_to_remove
-        )
-        scores = scores.masked_fill(indices_to_remove, self.filter_value)
-        return scores
-
-
-class TopKLogitsWarper(LogitsWarper):
-    r"""
-    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.
+    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to
+    this superclass for more information regarding those methods.
 
     Args:
-        top_k (`int`):
-            The number of highest probability vocabulary tokens to keep for top-k-filtering.
-        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
-            All filtered values will be set to this float value.
-        min_tokens_to_keep (`int`, *optional*, defaults to 1):
-            Minimum number of tokens that cannot be filtered.
+        special (`List[str]`, *optional*):
+            A list of special tokens (to be treated by the original implementation of this tokenizer).
+        min_freq (`int`, *optional*, defaults to 0):
+            The minimum number of times a token has to be present in order to be kept in the vocabulary (otherwise it
+            will be mapped to `unk_token`).
+        max_size (`int`, *optional*):
+            The maximum size of the vocabulary. If left unset, it will default to the size of the vocabulary found
+            after excluding the tokens according to the `min_freq` rule.
+        lower_case (`bool`, *optional*, defaults to `False`):
+            Whether or not to lowercase the input when tokenizing.
+        delimiter (`str`, *optional*):
+            The delimiter used between tokens.
+        vocab_file (`str`, *optional*):
+            File containing the vocabulary (from the original implementation).
+        pretrained_vocab_file (`str`, *optional*):
+            File containing the vocabulary as saved with the `save_pretrained()` method.
+        never_split (`List[str]`, *optional*):
+            List of tokens that should never be split. If no list is specified, will simply use the existing special
+            tokens.
+        unk_token (`str`, *optional*, defaults to `"<unk>"`):
+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
+            token instead.
+        eos_token (`str`, *optional*, defaults to `"<eos>"`):
+            The end of sequence token.
+        additional_special_tokens (`List[str]`, *optional*, defaults to `["<formula>"]`):
+            A list of additional special tokens (for the HuggingFace functionality).
+        language (`str`, *optional*, defaults to `"en"`):
+            The language of this tokenizer (used for mose preprocessing).
     """
 
-    def __init__(
-        self,
-        top_k: int,
-        filter_value: float = -float("Inf"),
-        min_tokens_to_keep: int = 1,
-    ):
-        if not isinstance(top_k, int) or top_k <= 0:
-            raise ValueError(
-                f"`top_k` has to be a strictly positive integer, but is {top_k}"
-            )
-
-        self.top_k = top_k
-        self.filter_value = filter_value
-        self.min_tokens_to_keep = min_tokens_to_keep
-
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        top_k = min(
-            max(self.top_k, self.min_tokens_to_keep), scores.size(-1)
-        )  # Safety check
-        # Remove all tokens with a probability less than the last token of the top-k
-        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
-        scores = scores.masked_fill(indices_to_remove, self.filter_value)
-        return scores
-
-
-class TypicalLogitsWarper(LogitsWarper):
-    r"""
-    [`LogitsWarper`] that performs typical decoding. See [Typical Decoding for Natural Language
-    Generation](https://arxiv.org/abs/2202.00666) for more information.
-
-    Args:
-        mass (`float`):
-            Value of typical_p between 0 and 1 inclusive, defaults to 0.9.
-        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
-            All filtered values will be set to this float value.
-        min_tokens_to_keep (`int`, *optional*, defaults to 1):
-            Minimum number of tokens that cannot be filtered.
-    """
+    vocab_files_names = VOCAB_FILES_NAMES
+    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
+    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
+    model_input_names = ["input_ids"]
 
     def __init__(
         self,
-        mass: float = 0.9,
-        filter_value: float = -float("Inf"),
-        min_tokens_to_keep: int = 1,
+        special=None,
+        min_freq=0,
+        max_size=None,
+        lower_case=False,
+        delimiter=None,
+        vocab_file=None,
+        pretrained_vocab_file: str = None,
+        never_split=None,
+        unk_token="<unk>",
+        eos_token="<eos>",
+        additional_special_tokens=["<formula>"],
+        language="en",
+        **kwargs,
     ):
-        mass = float(mass)
-        if not (mass > 0 and mass < 1):
-            raise ValueError(
-                f"`typical_p` has to be a float > 0 and < 1, but is {mass}"
-            )
-
-        self.filter_value = filter_value
-        self.mass = mass
-        self.min_tokens_to_keep = min_tokens_to_keep
-
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        # calculate entropy
-        normalized = torch.nn.functional.log_softmax(scores, dim=-1)
-        p = torch.exp(normalized)
-        ent = -(normalized * p).nansum(-1, keepdim=True)
-
-        # shift and sort
-        shifted_scores = torch.abs((-normalized) - ent)
-        sorted_scores, sorted_indices = torch.sort(shifted_scores, descending=False)
-        sorted_logits = scores.gather(-1, sorted_indices)
-        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
-
-        # Remove tokens with cumulative mass above the threshold
-        last_ind = (cumulative_probs < self.mass).sum(dim=1)
-        last_ind[last_ind < 0] = 0
-        sorted_indices_to_remove = sorted_scores > sorted_scores.gather(
-            1, last_ind.view(-1, 1)
-        )
-        if self.min_tokens_to_keep > 1:
-            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)
-            sorted_indices_to_remove[..., : self.min_tokens_to_keep] = 0
-        indices_to_remove = sorted_indices_to_remove.scatter(
-            1, sorted_indices, sorted_indices_to_remove
-        )
+        super().__init__(
+            special=special,
+            min_freq=min_freq,
+            max_size=max_size,
+            lower_case=lower_case,
+            delimiter=delimiter,
+            vocab_file=vocab_file,
+            pretrained_vocab_file=pretrained_vocab_file,
+            never_split=never_split,
+            unk_token=unk_token,
+            eos_token=eos_token,
+            additional_special_tokens=additional_special_tokens,
+            language=language,
+            **kwargs,
+        )
+        requires_backends(self, "sacremoses")
+
+        if never_split is None:
+            never_split = self.all_special_tokens
+        if special is None:
+            special = []
+        self.counter = Counter()
+        self.special = special
+        self.min_freq = min_freq
+        self.max_size = max_size
+        self.lower_case = lower_case
+        self.delimiter = delimiter
+        self.vocab_file = vocab_file
+        self.never_split = never_split
+        self.punctuation_symbols = '!"#$%&()*+,-./\\:;<=>?@[\\]^_`{|}~'
+        self.punction_without_space_before_pattern = re.compile(
+            rf"[^\s][{self.punctuation_symbols}]"
+        )
+        self.punctuation_with_space_around_pattern = (
+            self._compile_space_around_punctuation_pattern()
+        )
+        self.language = language
+        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)
+        self.moses_tokenizer = sm.MosesTokenizer(language)
+        self.moses_detokenizer = sm.MosesDetokenizer(language)
+
+        # This try... catch... is not beautiful but honestly this tokenizer was not made to be used
+        # in a library like ours, at all.
+        try:
+            vocab_dict = None
+            if pretrained_vocab_file is not None:
+                # Priority on pickle files (support PyTorch and TF)
+                with open(pretrained_vocab_file, "rb") as f:
+                    vocab_dict = pickle.load(f)
+
+                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer
+                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.
+                # We therefore load it with torch, if it's available.
+                if type(vocab_dict) == int:
+                    if not is_torch_available():
+                        raise ImportError(
+                            "Not trying to load dict with PyTorch as you need to install pytorch to load "
+                            "from a PyTorch pretrained vocabulary, "
+                            "or activate it with environment variables USE_TORCH=1 and USE_TF=0."
+                        )
+                    vocab_dict = torch.load(pretrained_vocab_file)
+
+            if vocab_dict is not None:
+                for key, value in vocab_dict.items():
+                    if key not in self.__dict__:
+                        self.__dict__[key] = value
+            elif vocab_file is not None:
+                self.build_vocab()
+
+        except Exception as e:
+            raise ValueError(
+                f"Unable to parse file {pretrained_vocab_file}. Unknown format. "
+                "If you tried to load a model saved through TransfoXLTokenizerFast, "
+                "please note they are not compatible."
+            ) from e
+
+        if vocab_file is not None:
+            self.build_vocab()
+
+    @property
+    def do_lower_case(self):
+        return self.lower_case
+
+    def _compile_space_around_punctuation_pattern(self):
+        look_ahead_for_special_token = f"(?=[{self.punctuation_symbols}])"
+        look_ahead_to_match_all_except_space = r"(?=[^\s])"
+        return re.compile(
+            r"" + look_ahead_for_special_token + look_ahead_to_match_all_except_space
+        )
+
+    def count_file(self, path, verbose=False, add_eos=False):
+        if verbose:
+            logger.info(f"counting file {path} ...")
+        assert os.path.exists(path), f"Input file {path} not found"
+
+        sents = []
+        with open(path, "r", encoding="utf-8") as f:
+            for idx, line in enumerate(f):
+                if verbose and idx > 0 and idx % 500000 == 0:
+                    logger.info(f"    line {idx}")
+                symbols = self.tokenize(line, add_eos=add_eos)
+                self.counter.update(symbols)
+                sents.append(symbols)
 
-        scores = scores.masked_fill(indices_to_remove, self.filter_value)
-        return scores
+        return sents
 
+    def count_sents(self, sents, verbose=False):
+        """
+        sents : a list of sentences, each a list of tokenized symbols
+        """
+        if verbose:
+            logger.info(f"counting {len(sents)} sents ...")
+        for idx, symbols in enumerate(sents):
+            if verbose and idx > 0 and idx % 500000 == 0:
+                logger.info(f"    line {idx}")
+            self.counter.update(symbols)
+
+    def _build_from_file(self, vocab_file):
+        self.idx2sym = []
+        self.sym2idx = OrderedDict()
+
+        with open(vocab_file, "r", encoding="utf-8") as f:
+            for line in f:
+                symb = line.strip().split()[0]
+                self.add_symbol(symb)
+        if "<UNK>" in self.sym2idx:
+            self.unk_idx = self.sym2idx["<UNK>"]
+        elif "<unk>" in self.sym2idx:
+            self.unk_idx = self.sym2idx["<unk>"]
+        else:
+            raise ValueError("No <unknown> token in vocabulary")
 
-def _get_ngrams(ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int):
-    generated_ngrams = [{} for _ in range(num_hypos)]
-    for idx in range(num_hypos):
-        gen_tokens = prev_input_ids[idx].tolist()
-        generated_ngram = generated_ngrams[idx]
-        for ngram in zip(*[gen_tokens[i:] for i in range(ngram_size)]):
-            prev_ngram_tuple = tuple(ngram[:-1])
-            generated_ngram[prev_ngram_tuple] = generated_ngram.get(
-                prev_ngram_tuple, []
-            ) + [ngram[-1]]
-    return generated_ngrams
-
-
-def _get_generated_ngrams(banned_ngrams, prev_input_ids, ngram_size, cur_len):
-    # Before decoding the next token, prevent decoding of ngrams that have already appeared
-    start_idx = cur_len + 1 - ngram_size
-    ngram_idx = tuple(prev_input_ids[start_idx:cur_len].tolist())
-    return banned_ngrams.get(ngram_idx, [])
-
-
-def _calc_banned_ngram_tokens(
-    ngram_size: int, prev_input_ids: torch.Tensor, num_hypos: int, cur_len: int
-) -> List[Iterable[int]]:
-    """Copied from fairseq for no_repeat_ngram in beam_search"""
-    if cur_len + 1 < ngram_size:
-        # return no banned tokens if we haven't generated no_repeat_ngram_size tokens yet
-        return [[] for _ in range(num_hypos)]
-
-    generated_ngrams = _get_ngrams(ngram_size, prev_input_ids, num_hypos)
-
-    banned_tokens = [
-        _get_generated_ngrams(
-            generated_ngrams[hypo_idx], prev_input_ids[hypo_idx], ngram_size, cur_len
-        )
-        for hypo_idx in range(num_hypos)
-    ]
-    return banned_tokens
-
+    def save_vocabulary(
+        self, save_directory: str, filename_prefix: Optional[str] = None
+    ) -> Tuple[str]:
+        if os.path.isdir(save_directory):
+            vocab_file = os.path.join(
+                save_directory,
+                (filename_prefix + "-" if filename_prefix else "")
+                + VOCAB_FILES_NAMES["pretrained_vocab_file"],
+            )
+        else:
+            vocab_file = (
+                filename_prefix + "-" if filename_prefix else ""
+            ) + save_directory
+        with open(vocab_file, "wb") as f:
+            pickle.dump(self.__dict__, f)
+        return (vocab_file,)
+
+    def build_vocab(self):
+        if self.vocab_file:
+            logger.info(f"building vocab from {self.vocab_file}")
+            self._build_from_file(self.vocab_file)
+            logger.info(f"final vocab size {len(self)}")
+        else:
+            logger.info(
+                f"building vocab with min_freq={self.min_freq}, max_size={self.max_size}"
+            )
+            self.idx2sym = []
+            self.sym2idx = OrderedDict()
 
-class NoRepeatNGramLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces no repetition of n-grams. See
-    [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).
+            for sym in self.special:
+                self.add_special(sym)
 
-    Args:
-        ngram_size (`int`):
-            All ngrams of size `ngram_size` can only occur once.
-    """
+            for sym, cnt in self.counter.most_common(self.max_size):
+                if cnt < self.min_freq:
+                    break
+                self.add_symbol(sym)
 
-    def __init__(self, ngram_size: int):
-        if not isinstance(ngram_size, int) or ngram_size <= 0:
-            raise ValueError(
-                f"`ngram_size` has to be a strictly positive integer, but is {ngram_size}"
+            logger.info(
+                f"final vocab size {len(self)} from {len(self.counter)} unique tokens"
             )
-        self.ngram_size = ngram_size
-
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        num_batch_hypotheses = scores.shape[0]
-        cur_len = input_ids.shape[-1]
-        banned_batch_tokens = _calc_banned_ngram_tokens(
-            self.ngram_size, input_ids, num_batch_hypotheses, cur_len
-        )
 
-        for i, banned_tokens in enumerate(banned_batch_tokens):
-            scores[i, banned_tokens] = -float("inf")
+    @torch_only_method
+    def encode_file(
+        self, path, ordered=False, verbose=False, add_eos=True, add_double_eos=False
+    ):
+        if verbose:
+            logger.info(f"encoding file {path} ...")
+        assert os.path.exists(path), f"Output file {path} not found"
+        encoded = []
+        with open(path, "r", encoding="utf-8") as f:
+            for idx, line in enumerate(f):
+                if verbose and idx > 0 and idx % 500000 == 0:
+                    logger.info(f"    line {idx}")
+                symbols = self.tokenize(
+                    line, add_eos=add_eos, add_double_eos=add_double_eos
+                )
+                encoded.append(self.convert_to_tensor(symbols))
 
-        return scores
+        if ordered:
+            encoded = torch.cat(encoded)
 
+        return encoded
 
-class EncoderNoRepeatNGramLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces no repetition of encoder input ids n-grams for the decoder ids. See
-    [ParlAI](https://github.com/facebookresearch/ParlAI/blob/master/parlai/core/torch_generator_agent.py#L1350).
+    @torch_only_method
+    def encode_sents(self, sents, ordered=False, verbose=False):
+        if verbose:
+            logger.info(f"encoding {len(sents)} sents ...")
+        encoded = []
+        for idx, symbols in enumerate(sents):
+            if verbose and idx > 0 and idx % 500000 == 0:
+                logger.info(f"    line {idx}")
+            encoded.append(self.convert_to_tensor(symbols))
+
+        if ordered:
+            encoded = torch.cat(encoded)
+
+        return encoded
+
+    def add_special(self, sym):
+        if sym not in self.sym2idx:
+            self.idx2sym.append(sym)
+            self.sym2idx[sym] = len(self.idx2sym) - 1
+            setattr(self, f"{sym.strip('<>')}_idx", self.sym2idx[sym])
+
+    def add_symbol(self, sym):
+        if sym not in self.sym2idx:
+            self.idx2sym.append(sym)
+            self.sym2idx[sym] = len(self.idx2sym) - 1
 
-    Args:
-        encoder_ngram_size (`int`):
-            All ngrams of size `ngram_size` can only occur within the encoder input ids.
-        encoder_input_ids (`int`):
-            The encoder_input_ids that should not be repeated within the decoder ids.
-    """
+    def move_added_token(self, token: str, target_idx: int):
+        """
+        Moves an added token to a specific position in the vocab. This method should be used when resizing an embedding
+        layer other than the last one in the `AdaptiveEmbedding` in order to move the token in the tokenizer from the
+        default position (at the very end) to the desired one.
 
-    def __init__(self, encoder_ngram_size: int, encoder_input_ids: torch.LongTensor):
-        if not isinstance(encoder_ngram_size, int) or encoder_ngram_size <= 0:
-            raise ValueError(
-                f"`encoder_ngram_size` has to be a strictly positive integer, but is {encoder_ngram_size}"
-            )
-        self.ngram_size = encoder_ngram_size
-        if len(encoder_input_ids.shape) == 1:
-            encoder_input_ids = encoder_input_ids.unsqueeze(0)
-        self.batch_size = encoder_input_ids.shape[0]
-        self.generated_ngrams = _get_ngrams(
-            encoder_ngram_size, encoder_input_ids, self.batch_size
+        Args:
+            token: The token to move to a specific position in the vocab.
+            target_idx: The position where the token should be moved to.
+        """
+        assert (
+            token in self.added_tokens_encoder
+        ), "Token which should be moved has to be an added token"
+        assert (
+            token not in self.idx2sym
+        ), "Token which should be moved is already in vocab"
+
+        # Insert sym into vocab
+        self.idx2sym.insert(target_idx, token)
+        self.sym2idx[token] = target_idx
+
+        # Shift following indices in sym2idx
+        for idx in range(target_idx + 1, len(self.idx2sym)):
+            current_sym = self.idx2sym[idx]
+            self.sym2idx[current_sym] = idx
+
+        # Delete token from added_tokens
+        old_index = self.added_tokens_encoder[token]
+        del self.added_tokens_decoder[old_index]
+        del self.added_tokens_encoder[token]
+
+    def moses_punct_norm(self, text):
+        return self.moses_punct_normalizer.normalize(text)
+
+    def moses_tokenize(self, text):
+        return self.moses_tokenizer.tokenize(
+            text,
+            aggressive_dash_splits=True,
+            return_str=False,
+            escape=False,
+            protected_patterns=self.never_split,
         )
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        # B x num_beams
-        num_hypos = scores.shape[0]
-        num_beams = num_hypos // self.batch_size
-        cur_len = input_ids.shape[-1]
-        banned_batch_tokens = [
-            _get_generated_ngrams(
-                self.generated_ngrams[hypo_idx // num_beams],
-                input_ids[hypo_idx],
-                self.ngram_size,
-                cur_len,
-            )
-            for hypo_idx in range(num_hypos)
-        ]
+    def moses_pipeline(self, text: str) -> List[str]:
+        """
+        Does basic tokenization using [`sacremoses.MosesPunctNormalizer`] and [`sacremoses.MosesTokenizer`] with
+        *aggressive_dash_splits=True* (see [`sacremoses.tokenize.MosesTokenizer.tokenize`]). Additionally, large
+        comma-separated numbers and floating point values are split. E.g. "23,000 people are 1.80m tall" -> "23 @,@ 000
+        people are 1 @.@ 80m tall"
 
-        for i, banned_tokens in enumerate(banned_batch_tokens):
-            scores[i, banned_tokens] = -float("inf")
+        Args:
+            text: Text to be tokenize
 
-        return scores
+        Returns:
+            A list of tokenized string
 
+        Example:
 
-class NoBadWordsLogitsProcessor(LogitsProcessor):
-    """
-    [`LogitsProcessor`] that enforces that specified sequences will never be sampled.
-
-    Args:
-        bad_words_ids (`List[List[int]]`):
-            List of list of token ids that are not allowed to be generated. In order to get the token ids of the words
-            that should not appear in the generated text, use `tokenizer(bad_words, add_prefix_space=True,
-            add_special_tokens=False).input_ids`.
-        eos_token_id (`int`):
-            The id of the *end-of-sequence* token.
-    """
-
-    def __init__(self, bad_words_ids: List[List[int]], eos_token_id: int):
-        if not isinstance(bad_words_ids, List) or len(bad_words_ids) == 0:
-            raise ValueError(
-                f"`bad_words_ids` has to be a non-empty list, but is {bad_words_ids}."
-            )
-        if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):
-            raise ValueError(
-                f"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}."
-            )
-        if any(
-            any(
-                (not isinstance(token_id, (int, np.integer)) or token_id < 0)
-                for token_id in bad_word_ids
-            )
-            for bad_word_ids in bad_words_ids
-        ):
-            raise ValueError(
-                f"Each list in `bad_words_ids` has to be a list of positive integers, but is {bad_words_ids}."
-            )
-
-        bad_words_ids = list(
-            filter(lambda bad_token_seq: bad_token_seq != [eos_token_id], bad_words_ids)
-        )
-        self.bad_words_id_length_1 = []
-        self.bad_words_id_length_greater_than_1 = []
-        for word in bad_words_ids:
-            if len(word) == 1:
-                self.bad_words_id_length_1.append(word[0])
+        ```python
+        >>> tokenizer = TransfoXLTokenizer.from_pretrained("transfo-xl-wt103")
+        >>> tokenizer.moses_pipeline("23,000 people are 1.80 m tall")
+        ['23', '@,@', '000', 'people', 'are', '1', '@.@', '80', 'm', 'tall']
+        ```"""
+        text = self.moses_punct_norm(text)
+        text = self.moses_tokenize(text)
+        text = tokenize_numbers(text)
+        return text
+
+    def _convert_id_to_token(self, idx):
+        """Converts an id in a token (BPE) using the vocab."""
+        assert 0 <= idx < len(self), f"Index {idx} out of vocabulary range"
+        return self.idx2sym[idx]
+
+    def _convert_token_to_id(self, sym):
+        """Converts a token (str) in an id using the vocab."""
+        if sym in self.sym2idx:
+            return self.sym2idx[sym]
+        else:
+            # logger.info(f'encounter unk {sym}')
+            # assert '<eos>' not in sym
+            if hasattr(self, "unk_idx"):
+                return self.sym2idx.get(sym, self.unk_idx)
+            # Backward compatibility with pre-trained models
+            elif "<unk>" in self.sym2idx:
+                return self.sym2idx["<unk>"]
+            elif "<UNK>" in self.sym2idx:
+                return self.sym2idx["<UNK>"]
             else:
-                self.bad_words_id_length_greater_than_1.append(word)
-
-        self.static_bad_words_mask: Optional[torch.LongTensor] = None
-
-        for banned_token_seq in self.bad_words_id_length_greater_than_1:
-            if len(banned_token_seq) == 0:
                 raise ValueError(
-                    f"Banned words token sequences {bad_words_ids} cannot have an empty list"
+                    "Token not in vocabulary and no <unk> token in vocabulary for replacement"
                 )
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        if self.static_bad_words_mask is None and len(self.bad_words_id_length_1) > 0:
-            self.static_bad_words_mask = self._calc_static_bad_word_mask(scores)
-
-        dynamic_banned_tokens = self._calc_banned_bad_words_ids(input_ids.tolist())
-        scores = self._set_scores_to_inf_for_banned_tokens(
-            scores, dynamic_banned_tokens
-        )
+    def convert_tokens_to_string(self, tokens):
+        """
+        Converts a sequence of tokens (string) in a single string. Additionally, the split numbers are converted back
+        into it's original form.
+        """
+        out_string = self.moses_detokenizer.detokenize(tokens)
+        return detokenize_numbers(out_string).strip()
 
-        return scores
+    @torch_only_method
+    def convert_to_tensor(self, symbols):
+        return torch.LongTensor(self.convert_tokens_to_ids(symbols))
+
+    @property
+    def vocab_size(self):
+        return len(self.idx2sym)
+
+    def get_vocab(self):
+        return dict(self.sym2idx, **self.added_tokens_encoder)
+
+    def _tokenize(self, line, add_eos=False, add_double_eos=False):
+        line = line.strip()
+        # convert to lower case
+        if self.lower_case:
+            line = line.lower()
+
+        # empty delimiter '' will evaluate False
+        if self.delimiter == "":
+            symbols = line
+        else:
+            symbols = self.moses_pipeline(line)
 
-    def _calc_static_bad_word_mask(self, scores: torch.FloatTensor) -> torch.BoolTensor:
-        static_bad_words_mask = torch.zeros(scores.shape[1])
-        static_bad_words_mask[self.bad_words_id_length_1] = 1
-        return static_bad_words_mask.unsqueeze(0).to(scores.device).bool()
-
-    def _tokens_match(self, prev_tokens: List[int], tokens: List[int]) -> bool:
-        if len(tokens) == 0:
-            # if bad word tokens is just one token always ban it
-            return True
-        elif len(tokens) > len(prev_tokens):
-            # if bad word tokens are longer then prev input_ids they can't be equal
-            return False
+        if add_double_eos:  # lm1b
+            return ["<S>"] + symbols + ["<S>"]
+        elif add_eos:
+            return symbols + ["<eos>"]
         else:
-            return prev_tokens[-len(tokens) :] == tokens
+            return symbols
 
-    def _calc_banned_bad_words_ids(
-        self, prev_input_ids: List[List[int]]
-    ) -> Iterable[int]:
-        banned_tokens = []
-        for prev_input_ids_slice in prev_input_ids:
-            banned_tokens_slice = []
-            for banned_token_seq in self.bad_words_id_length_greater_than_1:
-                if self._tokens_match(prev_input_ids_slice, banned_token_seq[:-1]):
-                    banned_tokens_slice.append(banned_token_seq[-1])
-
-            banned_tokens.append(banned_tokens_slice)
-
-        return banned_tokens
-
-    def _set_scores_to_inf_for_banned_tokens(
-        self, scores: torch.Tensor, banned_tokens: List[List[int]]
-    ) -> torch.Tensor:
-        """
-        Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a
-        list of list of banned tokens to ban in the format [[batch index, vocabulary position],...
 
-        Args:
-            scores: logits distribution of shape (batch size, vocabulary size)
-            banned_tokens: list of list of tokens to ban of length (batch_size)
+class LMOrderedIterator(object):
+    def __init__(self, data, bsz, bptt, device="cpu", ext_len=None):
+        """
+        data -- LongTensor -- the LongTensor is strictly ordered
         """
-        banned_mask_list = []
-        for idx, batch_banned_tokens in enumerate(banned_tokens):
-            for token in batch_banned_tokens:
-                # Eliminates invalid bad word IDs that are over the vocabulary size.
-                if token <= scores.shape[1]:
-                    banned_mask_list.append([idx, token])
-                else:
-                    logger.error(
-                        f"An invalid bad word ID is defined: {token}. This ID is not contained in the "
-                        "vocabulary, and is therefore ignored."
-                    )
-        if not banned_mask_list and self.static_bad_words_mask is None:
-            return scores
+        self.bsz = bsz
+        self.bptt = bptt
+        self.ext_len = ext_len if ext_len is not None else 0
 
-        else:
-            if banned_mask_list:
-                banned_mask = torch.LongTensor(banned_mask_list)
-                indices = torch.ones(len(banned_mask))
-                # A sparse tensor is generated from a list of coordinates: [[0, 1], [0, 2], [2, 0]]. A conversion to dense tensor generates:
-                # [ 0  1  1 ]
-                # [ 0  0  0 ]
-                # [ 1  0  0 ]
-
-                banned_mask = (
-                    torch.sparse.LongTensor(banned_mask.t(), indices, scores.size())
-                    .to(scores.device)
-                    .to_dense()
-                    .bool()
-                )
+        self.device = device
 
-                if self.static_bad_words_mask is not None:
-                    banned_mask = torch.bitwise_or(
-                        banned_mask, self.static_bad_words_mask
-                    )
-            else:
-                banned_mask = self.static_bad_words_mask
+        # Work out how cleanly we can divide the dataset into bsz parts.
+        self.n_step = data.size(0) // bsz
 
-            scores = scores.masked_fill(banned_mask, -float("inf"))
-            return scores
+        # Trim off any extra elements that wouldn't cleanly fit (remainders).
+        data = data.narrow(0, 0, self.n_step * bsz)
 
+        # Evenly divide the data across the bsz batches.
+        self.data = data.view(bsz, -1).t().contiguous().to(device)
 
-class PrefixConstrainedLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces constrained generation and is useful for prefix-conditioned constrained
-    generation. See [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904) for more information.
+        # Number of mini-batches
+        self.n_batch = (self.n_step + self.bptt - 1) // self.bptt
 
-    Args:
-        prefix_allowed_tokens_fn: (`Callable[[int, torch.Tensor], List[int]]`):
-            This function constraints the beam search to allowed tokens only at each step. This function takes 2
-            arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with the allowed tokens for the
-            next generation step conditioned on the previously generated tokens `inputs_ids` and the batch ID
-            `batch_id`.
-    """
+    def get_batch(self, i, bptt=None):
+        if bptt is None:
+            bptt = self.bptt
+        seq_len = min(bptt, self.data.size(0) - 1 - i)
 
-    def __init__(
-        self,
-        prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]],
-        num_beams: int,
-    ):
-        self._prefix_allowed_tokens_fn = prefix_allowed_tokens_fn
-        self._num_beams = num_beams
+        end_idx = i + seq_len
+        beg_idx = max(0, i - self.ext_len)
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        mask = torch.full_like(scores, -math.inf)
-        for batch_id, beam_sent in enumerate(
-            input_ids.view(-1, self._num_beams, input_ids.shape[-1])
-        ):
-            for beam_id, sent in enumerate(beam_sent):
-                mask[
-                    batch_id * self._num_beams + beam_id,
-                    self._prefix_allowed_tokens_fn(batch_id, sent),
-                ] = 0
-
-        return scores + mask
-
-
-class HammingDiversityLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces diverse beam search. Note that this logits processor is only effective for
-    [`PreTrainedModel.group_beam_search`]. See [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence
-    Models](https://arxiv.org/pdf/1610.02424.pdf) for more details.
+        data = self.data[beg_idx:end_idx]
+        target = self.data[i + 1 : i + 1 + seq_len]
 
-    Args:
-        diversity_penalty (`float`):
-            This value is subtracted from a beam's score if it generates a token same as any beam from other group at a
-            particular time. Note that `diversity_penalty` is only effective if `group beam search` is enabled.
-        num_beams (`int`):
-            Number of beams used for group beam search. See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more
-            details.
-        num_beam_groups (`int`):
-            Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.
-            See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.
-    """
+        data_out = data.transpose(0, 1).contiguous().to(self.device)
+        target_out = target.transpose(0, 1).contiguous().to(self.device)
 
-    def __init__(self, diversity_penalty: float, num_beams: int, num_beam_groups: int):
-        if not isinstance(diversity_penalty, float) or (not diversity_penalty > 0.0):
-            raise ValueError(
-                "`diversity_penalty` should be a float strictly larger than 0."
-            )
-        self._diversity_penalty = diversity_penalty
-        if not isinstance(num_beams, int) or num_beams < 2:
-            raise ValueError("`num_beams` should be an integer strictly larger than 1.")
-        self._num_beams = num_beams
-        if not isinstance(num_beam_groups, int) or num_beam_groups < 2:
-            raise ValueError(
-                "`num_beam_groups` should be an integer strictly larger than 1."
-            )
-        if num_beam_groups > num_beams:
-            raise ValueError("`beam_groups` has to be smaller or equal to `num_beams`.")
-        self._num_sub_beams = num_beams // num_beam_groups
+        return data_out, target_out, seq_len
 
-    def __call__(
-        self,
-        input_ids: torch.LongTensor,
-        scores: torch.FloatTensor,
-        current_tokens: torch.LongTensor,
-        beam_group_idx: int,
-    ) -> torch.FloatTensor:
-        # hamming diversity: penalise using same token in current group which was used in previous groups at
-        # the same time step
-        batch_size = current_tokens.shape[0] // self._num_beams
-        group_start_idx = beam_group_idx * self._num_sub_beams
-        group_end_idx = min(group_start_idx + self._num_sub_beams, self._num_beams)
-        group_size = group_end_idx - group_start_idx
-        vocab_size = scores.shape[-1]
-
-        if group_start_idx == 0:
-            return scores
-
-        for batch_idx in range(batch_size):
-            # predicted tokens of last time step of previous groups
-            previous_group_tokens = current_tokens[
-                batch_idx * self._num_beams : batch_idx * self._num_beams
-                + group_start_idx
-            ]
-            token_frequency = torch.bincount(
-                previous_group_tokens, minlength=vocab_size
-            ).to(scores.device)
-            scores[batch_idx * group_size : (batch_idx + 1) * group_size] -= (
-                self._diversity_penalty * token_frequency
-            )
-
-        return scores
-
-
-class ForcedBOSTokenLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces the specified token as the first generated token.
+    def get_fixlen_iter(self, start=0):
+        for i in range(start, self.data.size(0) - 1, self.bptt):
+            yield self.get_batch(i)
 
-    Args:
-        bos_token_id (`int`):
-            The id of the token to force as the first generated token.
-    """
+    def get_varlen_iter(self, start=0, std=5, min_len=5, max_deviation=3):
+        max_len = self.bptt + max_deviation * std
+        i = start
+        while True:
+            bptt = self.bptt if np.random.random() < 0.95 else self.bptt / 2.0
+            bptt = min(max_len, max(min_len, int(np.random.normal(bptt, std))))
+            data, target, seq_len = self.get_batch(i, bptt)
+            i += seq_len
+            yield data, target, seq_len
+            if i >= self.data.size(0) - 2:
+                break
 
-    def __init__(self, bos_token_id: int):
-        self.bos_token_id = bos_token_id
+    def __iter__(self):
+        return self.get_fixlen_iter()
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        cur_len = input_ids.shape[-1]
-        if cur_len == 1:
-            num_tokens = scores.shape[1]
-            scores[
-                :, [i for i in range(num_tokens) if i != self.bos_token_id]
-            ] = -float("inf")
-            scores[:, self.bos_token_id] = 0
-        return scores
-
-
-class ForcedEOSTokenLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.
 
-    Args:
-        max_length (`int`):
-            The maximum length of the sequence to be generated.
-        eos_token_id (`int`):
-            The id of the token to force as the last generated token when `max_length` is reached.
-    """
+class LMShuffledIterator(object):
+    def __init__(self, data, bsz, bptt, device="cpu", ext_len=None, shuffle=False):
+        """
+        data -- list[LongTensor] -- there is no order among the LongTensors
+        """
+        self.data = data
 
-    def __init__(self, max_length: int, eos_token_id: int):
-        self.max_length = max_length
-        self.eos_token_id = eos_token_id
-
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        cur_len = input_ids.shape[-1]
-        if cur_len == self.max_length - 1:
-            num_tokens = scores.shape[1]
-            scores[
-                :, [i for i in range(num_tokens) if i != self.eos_token_id]
-            ] = -float("inf")
-            scores[:, self.eos_token_id] = 0
-        return scores
-
-
-class InfNanRemoveLogitsProcessor(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that removes all `nan` and `inf` values to avoid the generation method to fail. Note that using
-    the logits processor should only be used if necessary since it can slow down the generation method. `max_length` is
-    reached.
-    """
+        self.bsz = bsz
+        self.bptt = bptt
+        self.ext_len = ext_len if ext_len is not None else 0
+
+        self.device = device
+        self.shuffle = shuffle
+
+    def get_sent_stream(self):
+        # index iterator
+        epoch_indices = (
+            np.random.permutation(len(self.data))
+            if self.shuffle
+            else np.array(range(len(self.data)))
+        )
+
+        # sentence iterator
+        for idx in epoch_indices:
+            yield self.data[idx]
+
+    @torch_only_method
+    def stream_iterator(self, sent_stream):
+        # streams for each data in the batch
+        streams = [None] * self.bsz
+
+        data = torch.LongTensor(self.bptt, self.bsz)
+        target = torch.LongTensor(self.bptt, self.bsz)
+
+        n_retain = 0
+
+        while True:
+            # data   : [n_retain+bptt x bsz]
+            # target : [bptt x bsz]
+            data[n_retain:].fill_(-1)
+            target.fill_(-1)
+
+            valid_batch = True
+
+            for i in range(self.bsz):
+                n_filled = 0
+                try:
+                    while n_filled < self.bptt:
+                        if streams[i] is None or len(streams[i]) <= 1:
+                            streams[i] = next(sent_stream)
+                        # number of new tokens to fill in
+                        n_new = min(len(streams[i]) - 1, self.bptt - n_filled)
+                        # first n_retain tokens are retained from last batch
+                        data[
+                            n_retain + n_filled : n_retain + n_filled + n_new, i
+                        ] = streams[i][:n_new]
+                        target[n_filled : n_filled + n_new, i] = streams[i][
+                            1 : n_new + 1
+                        ]
+                        streams[i] = streams[i][n_new:]
+                        n_filled += n_new
+                except StopIteration:
+                    valid_batch = False
+                    break
+
+            if not valid_batch:
+                return
+
+            data_out = data.transpose(0, 1).contiguous().to(self.device)
+            target_out = target.transpose(0, 1).contiguous().to(self.device)
+
+            yield data_out, target_out, self.bptt
+
+            n_retain = min(data.size(0), self.ext_len)
+            if n_retain > 0:
+                data[:n_retain] = data[-n_retain:]
+            data.resize_(n_retain + self.bptt, data.size(1))
+
+    def __iter__(self):
+        # sent_stream is an iterator
+        sent_stream = self.get_sent_stream()
 
-    def __call__(
-        self, input_ids: torch.LongTensor, scores: torch.FloatTensor
-    ) -> torch.FloatTensor:
-        # set all nan values to 0.0
-        scores[scores != scores] = 0.0
-
-        # set all inf values to max possible value
-        scores[scores == float("inf")] = torch.finfo(scores.dtype).max
-
-        return scores
-
-
-class ExponentialDecayLengthPenalty(LogitsProcessor):
-    r"""
-    [`LogitsProcessor`] that exponentially increases the score of the eos_token_id after regulation_start has been
-    reached.
+        for batch in self.stream_iterator(sent_stream):
+            yield batch
 
-    Args:
-        exponential_decay_length_penalty (`tuple(int, float)`, *optional*):
-            This tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty
-            starts and `decay_factor` represents the factor of exponential decay
-        eos_token_id (`int`):
-            The id of the *end-of-sequence* token.
-        input_ids_seq_length (`int`):
-            The length of the input sequence.
-    """
 
+class LMMultiFileIterator(LMShuffledIterator):
     def __init__(
-        self,
-        exponential_decay_length_penalty: Tuple,
-        eos_token_id: int,
-        input_ids_seq_length: int,
+        self, paths, vocab, bsz, bptt, device="cpu", ext_len=None, shuffle=False
     ):
-        self.regulation_start = (
-            exponential_decay_length_penalty[0] + input_ids_seq_length
-        )
-        self.regulation_factor = exponential_decay_length_penalty[1]
-        self.eos_token_id = eos_token_id
-
-    def __call__(
-        self, input_ids: torch.Tensor, scores: torch.Tensor
-    ) -> torch.FloatTensor:
-        cur_len = input_ids.shape[-1]
-        if cur_len > self.regulation_start:
-            scores[:, self.eos_token_id] = scores[:, self.eos_token_id] * pow(
-                self.regulation_factor, cur_len - self.regulation_start
-            )
-        return scores
-
-
-class LogitNormalization(LogitsProcessor, LogitsWarper):
-    r"""
-    [`LogitsWarper`] and [`LogitsProcessor`] for normalizing the scores using log-softmax. It's important to normalize
-    the scores during beam search, after applying the logits processors or warpers, since the search algorithm used in
-    this library doesn't do it (it only does it before, but they may need re-normalization) but it still supposes that
-    the scores are normalized when comparing the hypotheses.
-    """
-
-    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.Tensor:
-        scores = scores.log_softmax(dim=-1)
-        return scores
-
-
-class SuppressTokensAtBeginLogitsProcessor(LogitsProcessor):
-    r"""
-    [`SuppressTokensAtBeginLogitsProcessor`] supresses a list of tokens as soon as the `generate` function starts
-    generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` at not
-    sampled at the begining of the generation.
-    """
-
-    def __init__(self, begin_suppress_tokens, begin_index):
-        self.begin_suppress_tokens = list(begin_suppress_tokens)
-        self.begin_index = begin_index
-
-    def __call__(self, input_ids, scores):
-        if input_ids.shape[1] == self.begin_index:
-            scores[:, self.begin_suppress_tokens] = -float("inf")
+        self.paths = paths
+        self.vocab = vocab
 
-        return scores
-
-
-class SuppressTokensLogitsProcessor(LogitsProcessor):
-    r"""This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so that they
-    are not sampled."""
+        self.bsz = bsz
+        self.bptt = bptt
+        self.ext_len = ext_len if ext_len is not None else 0
+
+        self.device = device
+        self.shuffle = shuffle
+
+    def get_sent_stream(self, path):
+        sents = self.vocab.encode_file(path, add_double_eos=True)
+        if self.shuffle:
+            np.random.shuffle(sents)
+        sent_stream = iter(sents)
+
+        return sent_stream
+
+    def __iter__(self):
+        if self.shuffle:
+            np.random.shuffle(self.paths)
+
+        for path in self.paths:
+            # sent_stream is an iterator
+            sent_stream = self.get_sent_stream(path)
+            for batch in self.stream_iterator(sent_stream):
+                yield batch
+
+
+class TransfoXLCorpus(object):
+    @classmethod
+    @torch_only_method
+    def from_pretrained(
+        cls, pretrained_model_name_or_path, cache_dir=None, *inputs, **kwargs
+    ):
+        """
+        Instantiate a pre-processed corpus.
+        """
+        vocab = TransfoXLTokenizer.from_pretrained(
+            pretrained_model_name_or_path, *inputs, **kwargs
+        )
+        is_local = os.path.isdir(pretrained_model_name_or_path)
+        # redirect to the cache, if necessary
+        try:
+            resolved_corpus_file = cached_file(
+                pretrained_model_name_or_path, CORPUS_NAME, cache_dir=cache_dir
+            )
+        except EnvironmentError:
+            logger.error(
+                f"Corpus '{pretrained_model_name_or_path}' was not found in corpus list"
+                f" ({', '.join(PRETRAINED_CORPUS_ARCHIVE_MAP.keys())}. We assumed '{pretrained_model_name_or_path}'"
+                f" was a path or url but couldn't find files {CORPUS_NAME} at this path or url."
+            )
+            return None
+        if is_local:
+            logger.info(f"loading corpus file {resolved_corpus_file}")
+        else:
+            logger.info(
+                f"loading corpus file {CORPUS_NAME} from cache at {resolved_corpus_file}"
+            )
 
-    def __init__(self, suppress_tokens):
-        self.suppress_tokens = list(suppress_tokens)
+        # Instantiate tokenizer.
+        corpus = cls(*inputs, **kwargs)
+        corpus_dict = torch.load(resolved_corpus_file)
+        for key, value in corpus_dict.items():
+            corpus.__dict__[key] = value
+        corpus.vocab = vocab
+        if corpus.train is not None:
+            corpus.train = torch.tensor(corpus.train, dtype=torch.long)
+        if corpus.valid is not None:
+            corpus.valid = torch.tensor(corpus.valid, dtype=torch.long)
+        if corpus.test is not None:
+            corpus.test = torch.tensor(corpus.test, dtype=torch.long)
+        return corpus
+
+    def __init__(self, *args, **kwargs):
+        self.vocab = TransfoXLTokenizer(*args, **kwargs)
+        self.dataset = None
+        self.train = None
+        self.valid = None
+        self.test = None
+
+    def build_corpus(self, path, dataset):
+        self.dataset = dataset
+
+        if self.dataset in ["ptb", "wt2", "enwik8", "text8"]:
+            self.vocab.count_file(os.path.join(path, "train.txt"))
+            self.vocab.count_file(os.path.join(path, "valid.txt"))
+            self.vocab.count_file(os.path.join(path, "test.txt"))
+        elif self.dataset == "wt103":
+            self.vocab.count_file(os.path.join(path, "train.txt"))
+        elif self.dataset == "lm1b":
+            train_path_pattern = os.path.join(
+                path,
+                "1-billion-word-language-modeling-benchmark-r13output",
+                "training-monolingual.tokenized.shuffled",
+                "news.en-*",
+            )
+            train_paths = glob.glob(train_path_pattern)
+            # the vocab will load from file when build_vocab() is called
+
+        self.vocab.build_vocab()
+
+        if self.dataset in ["ptb", "wt2", "wt103"]:
+            self.train = self.vocab.encode_file(
+                os.path.join(path, "train.txt"), ordered=True
+            )
+            self.valid = self.vocab.encode_file(
+                os.path.join(path, "valid.txt"), ordered=True
+            )
+            self.test = self.vocab.encode_file(
+                os.path.join(path, "test.txt"), ordered=True
+            )
+        elif self.dataset in ["enwik8", "text8"]:
+            self.train = self.vocab.encode_file(
+                os.path.join(path, "train.txt"), ordered=True, add_eos=False
+            )
+            self.valid = self.vocab.encode_file(
+                os.path.join(path, "valid.txt"), ordered=True, add_eos=False
+            )
+            self.test = self.vocab.encode_file(
+                os.path.join(path, "test.txt"), ordered=True, add_eos=False
+            )
+        elif self.dataset == "lm1b":
+            self.train = train_paths
+            self.valid = self.vocab.encode_file(
+                os.path.join(path, "valid.txt"), ordered=False, add_double_eos=True
+            )
+            self.test = self.vocab.encode_file(
+                os.path.join(path, "test.txt"), ordered=False, add_double_eos=True
+            )
+
+    def get_iterator(self, split, *args, **kwargs):
+        if split == "train":
+            if self.dataset in ["ptb", "wt2", "wt103", "enwik8", "text8"]:
+                data_iter = LMOrderedIterator(self.train, *args, **kwargs)
+            elif self.dataset == "lm1b":
+                kwargs["shuffle"] = True
+                data_iter = LMMultiFileIterator(self.train, self.vocab, *args, **kwargs)
+        elif split in ["valid", "test"]:
+            data = self.valid if split == "valid" else self.test
+            if self.dataset in ["ptb", "wt2", "wt103", "enwik8", "text8"]:
+                data_iter = LMOrderedIterator(data, *args, **kwargs)
+            elif self.dataset == "lm1b":
+                data_iter = LMShuffledIterator(data, *args, **kwargs)
+        else:
+            data_iter = None
+            raise ValueError(f"Split not recognized: {split}")
 
-    def __call__(self, input_ids, scores):
-        scores[:, self.suppress_tokens] = -float("inf")
-        return scores
+        return data_iter
 
 
-class ForceTokensLogitsProcessor(LogitsProcessor):
-    r"""This processor takes a list of pairs of integers which indicates a mapping from generation indices to token
-    indices that will be forced before sampling. The processor will set their log probs to `inf` so that they are
-    sampled at their corresponding index."""
+@torch_only_method
+def get_lm_corpus(datadir, dataset):
+    fn = os.path.join(datadir, "cache.pt")
+    fn_pickle = os.path.join(datadir, "cache.pkl")
+    if os.path.exists(fn):
+        logger.info("Loading cached dataset...")
+        corpus = torch.load(fn_pickle)
+    elif os.path.exists(fn):
+        logger.info("Loading cached dataset from pickle...")
+        with open(fn, "rb") as fp:
+            corpus = pickle.load(fp)
+    else:
+        logger.info(f"Producing dataset {dataset}...")
+        kwargs = {}
+        if dataset in ["wt103", "wt2"]:
+            kwargs["special"] = ["<eos>"]
+            kwargs["lower_case"] = False
+        elif dataset == "ptb":
+            kwargs["special"] = ["<eos>"]
+            kwargs["lower_case"] = True
+        elif dataset == "lm1b":
+            kwargs["special"] = []
+            kwargs["lower_case"] = False
+            kwargs["vocab_file"] = os.path.join(datadir, "1b_word_vocab.txt")
+        elif dataset in ["enwik8", "text8"]:
+            pass
 
-    def __init__(self, force_token_map: List[List[int]]):
-        self.force_token_map = dict(force_token_map)
+        corpus = TransfoXLCorpus(datadir, dataset, **kwargs)
+        torch.save(corpus, fn)
 
-    def __call__(self, input_ids, scores):
-        generation_idx = input_ids.shape[-1]
-        current_token = self.force_token_map.get(generation_idx, None)
-        if current_token is not None:
-            scores[:, :] = -float("inf")
-            scores[:, current_token] = 0
-        return scores
+    return corpus
```

### Comparing `xs_transformers-1.0.0/xs_transformers/generation_tf_logits_process.py` & `xs_transformers-1.0.1/xs_transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,739 +1,634 @@
 # coding=utf-8
-# Copyright 2022 The HuggingFace Inc. team
+# Copyright 2021 The HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-import inspect
-from typing import List, Tuple
+"""
+Speech processor class for Wav2Vec2
+"""
+import os
+import warnings
+from contextlib import contextmanager, nullcontext
+from dataclasses import dataclass
+from multiprocessing import Pool, get_context, get_start_method
+from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Union
 
 import numpy as np
-import tensorflow as tf
-
-from .tf_utils import stable_softmax
-from .utils import add_start_docstrings
-from .utils.logging import get_logger
-
-logger = get_logger(__name__)
-
-
-TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING = r"""
-    Args:
-        input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):
-            Indices of input sequence tokens in the vocabulary.
 
-            Indices can be obtained using [`PreTrainedTokenizer`]. See [`PreTrainedTokenizer.encode`] and
-            [`PreTrainedTokenizer.__call__`] for details.
+from ...processing_utils import ProcessorMixin
+from ...utils import ModelOutput, logging, requires_backends
 
-            [What are input IDs?](../glossary#input-ids)
-        scores (`tf.Tensor` of shape `(batch_size, config.vocab_size)`):
-            Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam
-            search or log softmax for each vocabulary token when using beam search.
-        cur_len (`int`):
-            The current length of valid input sequence tokens. In the TF implementation, the input_ids' sequence length
-            is the maximum length generate can produce, and we need to know which of its tokens are valid.
-        kwargs:
-            Additional logits processor specific kwargs.
-
-    Return:
-        `tf.Tensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.
-"""
+logger = logging.get_logger(__name__)
 
 
-class TFLogitsProcessor:
-    """Abstract base class for all logit processors that can be applied during generation."""
-
-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        """TF method for processing logits."""
-        raise NotImplementedError(
-            f"{self.__class__} is an abstract class. Only classes inheriting this class can be called."
-        )
+if TYPE_CHECKING:
+    from pyctcdecode import BeamSearchDecoderCTC
 
+    from ...feature_extraction_utils import FeatureExtractionMixin
+    from ...tokenization_utils import PreTrainedTokenizerBase
 
-class TFLogitsWarper:
-    """Abstract base class for all logit warpers that can be applied during generation with multinomial sampling."""
 
-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        """TF method for warping logits."""
-        raise NotImplementedError(
-            f"{self.__class__} is an abstract class. Only classes inheriting this class can be called."
-        )
+ListOfDict = List[Dict[str, Union[int, str]]]
 
 
-class TFLogitsProcessorList(list):
-    """
-    This class can be used to create a list of [`TFLogitsProcessor`] to subsequently process a `scores` input tensor.
-    This class inherits from list and adds a specific *__call__* method to apply each [`TFLogitsProcessor`] to the
-    inputs.
+@dataclass
+class Wav2Vec2DecoderWithLMOutput(ModelOutput):
     """
-
-    @add_start_docstrings(TF_LOGITS_PROCESSOR_INPUTS_DOCSTRING)
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int, **kwargs
-    ) -> tf.Tensor:
-        for processor in self:
-            function_args = inspect.signature(processor.__call__).parameters
-            if len(function_args) > 3:
-                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):
-                    raise ValueError(
-                        f"Make sure that all the required parameters: {list(function_args.keys())} for "
-                        f"{processor.__class__} are passed to the logits processor."
-                    )
-                scores = processor(input_ids, scores, cur_len, **kwargs)
-            else:
-                scores = processor(input_ids, scores, cur_len)
-        return scores
-
-
-class TFTemperatureLogitsWarper(TFLogitsWarper):
-    r"""
-    [`TFLogitsWarper`] for temperature (exponential scaling output probability distribution).
+    Output type of [`Wav2Vec2DecoderWithLM`], with transcription.
 
     Args:
-        temperature (`float`):
-            The value used to module the logits distribution.
-    """
-
-    def __init__(self, temperature: float):
-        if not isinstance(temperature, float) or not (temperature > 0):
-            raise ValueError(
-                f"`temperature` has to be a strictly positive float, but is {temperature}"
-            )
-
-        self.temperature = temperature
+        text (list of `str` or `str`):
+            Decoded logits in text from. Usually the speech transcription.
+        logit_score (list of `float` or `float`):
+            Total logit score of the beam associated with produced text.
+        lm_score (list of `float`):
+            Fused lm_score of the beam associated with produced text.
+        word_offsets (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str, Union[int, str]]]`):
+            Offsets of the decoded words. In combination with sampling rate and model downsampling rate word offsets
+            can be used to compute time stamps for each word.
+    """
+
+    text: Union[List[str], str]
+    logit_score: Union[List[float], float] = None
+    lm_score: Union[List[float], float] = None
+    word_offsets: Union[List[ListOfDict], ListOfDict] = None
 
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        scores = scores / self.temperature
-        return scores
 
-
-class TFTopKLogitsWarper(TFLogitsWarper):
+class Wav2Vec2ProcessorWithLM(ProcessorMixin):
     r"""
-    [`TFLogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.
-
-    Args:
-        top_k (`int`):
-            The number of highest probability vocabulary tokens to keep for top-k-filtering.
-        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
-            All filtered values will be set to this float value.
-        min_tokens_to_keep (`int`, *optional*, defaults to 1):
-            Minimum number of tokens that cannot be filtered.
-    """
-
-    def __init__(
-        self,
-        top_k: int,
-        filter_value: float = -float("Inf"),
-        min_tokens_to_keep: int = 1,
-    ):
-        if not isinstance(top_k, int) or top_k <= 0:
-            raise ValueError(
-                f"`top_k` has to be a strictly positive integer, but is {top_k}"
-            )
-
-        self.top_k = top_k
-        self.filter_value = filter_value
-        self.min_tokens_to_keep = min_tokens_to_keep
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        top_k = min(
-            max(self.top_k, self.min_tokens_to_keep), scores.shape[-1]
-        )  # Safety check
-        # Boolean mask containing all tokens with a probability less than the last token of the top-k
-        indices_to_remove = scores < tf.math.top_k(scores, k=top_k)[0][..., -1:]
-        next_scores = tf.where(indices_to_remove, self.filter_value, scores)
-        return next_scores
-
-
-class TFTopPLogitsWarper(TFLogitsWarper):
-    """
-    [`TFLogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to <= prob_cut_off.
+    Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor, a Wav2Vec2 CTC tokenizer and a decoder
+    with language model support into a single processor for language model boosted speech recognition decoding.
 
     Args:
-        top_p (`float`):
-            If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or
-            higher are kept for generation.
-        filter_value (`float`, *optional*, defaults to `-float("Inf")`):
-            All filtered values will be set to this float value.
-        min_tokens_to_keep (`int`, *optional*, defaults to 1):
-            Minimum number of tokens that cannot be filtered.
+        feature_extractor ([`Wav2Vec2FeatureExtractor`]):
+            An instance of [`Wav2Vec2FeatureExtractor`]. The feature extractor is a required input.
+        tokenizer ([`Wav2Vec2CTCTokenizer`]):
+            An instance of [`Wav2Vec2CTCTokenizer`]. The tokenizer is a required input.
+        decoder (`pyctcdecode.BeamSearchDecoderCTC`):
+            An instance of [`pyctcdecode.BeamSearchDecoderCTC`]. The decoder is a required input.
     """
+    feature_extractor_class = "Wav2Vec2FeatureExtractor"
+    tokenizer_class = "Wav2Vec2CTCTokenizer"
 
     def __init__(
         self,
-        top_p: float,
-        filter_value: float = -float("Inf"),
-        min_tokens_to_keep: int = 1,
+        feature_extractor: "FeatureExtractionMixin",
+        tokenizer: "PreTrainedTokenizerBase",
+        decoder: "BeamSearchDecoderCTC",
     ):
-        if not isinstance(top_p, float) or (top_p < 0 or top_p > 1.0):
-            raise ValueError(f"`top_p` has to be a float > 0 and < 1, but is {top_p}")
-
-        self.top_p = top_p
-        self.filter_value = filter_value
-        self.min_tokens_to_keep = min_tokens_to_keep
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        topk_scores, topk_indices = tf.math.top_k(scores, scores.shape[-1])
-
-        mask_scores = tf.fill(scores.shape, self.filter_value)
-        cumulative_probs = tf.math.cumsum(stable_softmax(topk_scores, axis=-1), axis=-1)
-        score_mask = cumulative_probs < self.top_p
-
-        # Also include the token that is higher than top_p (the first false = shift and insert a True on the left)
-        score_mask = tf.concat(
-            (tf.ones([score_mask.shape[0], 1], dtype=tf.bool), score_mask[:, :-1]),
-            axis=-1,
-        )
-
-        # Ensure min tokens to keep
-        score_mask = tf.concat(
-            (
-                tf.ones([score_mask.shape[0], self.min_tokens_to_keep], dtype=tf.bool),
-                score_mask[:, self.min_tokens_to_keep :],
-            ),
-            axis=-1,
-        )
-
-        # Mask the values that do not fit the criteria
-        topk_next_scores = tf.where(score_mask, topk_scores, mask_scores)
-
-        # Undo the topk sorting: converts the 2D matrix of per-row original indices of shape (batch_size, vocab_size)
-        # to a 3D tensor of shape (batch_size, vocab_size, 2) containing the original score coordinate, from which we
-        # can scatter (i.e. `scatter_indices[row, col, :]` is a tensor containing `[row, topk_indices[row, col]]`)
-        scatter_rows = tf.tile(
-            tf.expand_dims(tf.range(topk_indices.shape[0]), axis=-1),
-            [1, topk_indices.shape[-1]],
-        )
-        scatter_indices = tf.stack((scatter_rows, topk_indices), axis=-1)
-        next_scores = tf.scatter_nd(
-            scatter_indices, topk_next_scores, shape=topk_next_scores.shape
-        )
-
-        return next_scores
-
-
-class TFMinLengthLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFLogitsProcessor`] enforcing a min-length by setting EOS probability to 0.
-
-    Args:
-        min_length (`int`):
-            The minimum length below which the score of `eos_token_id` is set to `-float("Inf")`.
-        eos_token_id (`int`):
-            The id of the *end-of-sequence* token.
-    """
+        from pyctcdecode import BeamSearchDecoderCTC
 
-    def __init__(self, min_length: int, eos_token_id: int):
-        if not isinstance(min_length, int) or min_length < 0:
+        super().__init__(feature_extractor, tokenizer)
+        if not isinstance(decoder, BeamSearchDecoderCTC):
             raise ValueError(
-                f"`min_length` has to be a positive integer, but is {min_length}"
+                f"`decoder` has to be of type {BeamSearchDecoderCTC.__class__}, but is {type(decoder)}"
             )
 
-        if not isinstance(eos_token_id, int) or eos_token_id < 0:
+        # make sure that decoder's alphabet and tokenizer's vocab match in content
+        missing_decoder_tokens = self.get_missing_alphabet_tokens(decoder, tokenizer)
+        if len(missing_decoder_tokens) > 0:
             raise ValueError(
-                f"`eos_token_id` has to be a positive integer, but is {eos_token_id}"
+                f"The tokens {missing_decoder_tokens} are defined in the tokenizer's "
+                "vocabulary, but not in the decoder's alphabet. "
+                f"Make sure to include {missing_decoder_tokens} in the decoder's alphabet."
             )
 
-        self.min_length = min_length
-        self.eos_token_id = eos_token_id
+        self.decoder = decoder
+        self.current_processor = self.feature_extractor
+        self._in_target_context_manager = False
 
-    def _apply_eos_token_mask(self, scores: tf.Tensor) -> tf.Tensor:
-        eos_token_id_mask = tf.range(scores.shape[-1]) == self.eos_token_id
-        scores = tf.where(eos_token_id_mask, float("-inf"), scores)
-        return scores
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        # applies eos token masking if the first argument is true
-        scores = tf.cond(
-            tf.less(cur_len, self.min_length),
-            lambda: self._apply_eos_token_mask(scores),
-            lambda: tf.identity(scores),
-        )
-        return scores
-
-
-class TFRepetitionPenaltyLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFLogitsProcessor`] enforcing an exponential penalty on repeated sequences.
+    def save_pretrained(self, save_directory):
+        super().save_pretrained(save_directory)
+        self.decoder.save_to_dir(save_directory)
 
-    Args:
-        repetition_penalty (`float`):
-            The parameter for repetition penalty. 1.0 means no penalty. See [this
-            paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.
-    """
-
-    def __init__(self, penalty: float):
-        if not isinstance(penalty, float) or not (penalty > 0):
-            raise ValueError(
-                f"`penalty` has to be a strictly positive float, but is {penalty}"
-            )
+    @classmethod
+    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
+        r"""
+        Instantiate a [`Wav2Vec2ProcessorWithLM`] from a pretrained Wav2Vec2 processor.
 
-        self.penalty = penalty
+        <Tip>
 
-    def _create_score_penalties(
-        self, input_ids: tf.Tensor, logits: tf.Tensor
-    ) -> tf.Tensor:
-        # We want to populate the penalties in the positions of `input_ids`. Since XLA can't handle shapes unknown
-        # before runtime, `tf.unique` can't be used. Therefore, we may have redundant updates, when a given row has
-        # the same token multiple times.
-
-        # Gathers the penalties to apply
-        logit_penalties = tf.gather(logits, input_ids, axis=1, batch_dims=1)
-        logit_penalties = tf.where(
-            logit_penalties > 0, 1 / self.penalty, logit_penalties
-        )
-        logit_penalties = tf.where(logit_penalties < 0, self.penalty, logit_penalties)
-
-        # Scatters the penalties
-        token_penalties = tf.ones(logits.shape)
-        batch_size = input_ids.shape[0]
-        seq_len = tf.shape(input_ids)[
-            1
-        ]  # the sequence length has dynamic size, hence the dynamic shape
-        indexable_prev_input_ids = tf.concat(
-            (
-                tf.expand_dims(tf.repeat(tf.range(batch_size), seq_len), axis=-1),
-                tf.expand_dims(tf.reshape(input_ids, [-1]), axis=-1),
-            ),
-            axis=1,
-        )
-        token_penalties = tf.tensor_scatter_nd_update(
-            token_penalties,
-            indices=indexable_prev_input_ids,
-            updates=tf.reshape(logit_penalties, [-1]),
-        )
-        return token_penalties
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        score_penalties = self._create_score_penalties(input_ids[:, :cur_len], scores)
+        This class method is simply calling Wav2Vec2FeatureExtractor's
+        [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`], Wav2Vec2CTCTokenizer's
+        [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], and
+        [`pyctcdecode.BeamSearchDecoderCTC.load_from_hf_hub`].
 
-        scores = tf.math.multiply(scores, score_penalties)
+        Please refer to the docstrings of the methods above for more information.
 
-        return scores
+        </Tip>
 
+        Args:
+            pretrained_model_name_or_path (`str` or `os.PathLike`):
+                This can be either:
 
-class TFNoBadWordsLogitsProcessor(TFLogitsProcessor):
-    """
-    [`TFLogitsProcessor`] that enforces that specified sequences will never be sampled.
+                - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on
+                  huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or
+                  namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
+                - a path to a *directory* containing a feature extractor file saved using the
+                  [`~SequenceFeatureExtractor.save_pretrained`] method, e.g., `./my_model_directory/`.
+                - a path or url to a saved feature extractor JSON *file*, e.g.,
+                  `./my_model_directory/preprocessor_config.json`.
+            **kwargs
+                Additional keyword arguments passed along to both [`SequenceFeatureExtractor`] and
+                [`PreTrainedTokenizer`]
+        """
+        requires_backends(cls, "pyctcdecode")
+        from pyctcdecode import BeamSearchDecoderCTC
 
-    Args:
-        bad_words_ids (`List[List[int]]`):
-            List of list of token ids that are not allowed to be generated. In order to get the tokens of the words
-            that should not appear in the generated text, use `tokenizer(bad_word, add_prefix_space=True).input_ids`.
-        eos_token_id (`int`):
-            The id of the *end-of-sequence* token.
-    """
+        feature_extractor, tokenizer = super()._get_arguments_from_pretrained(
+            pretrained_model_name_or_path, **kwargs
+        )
 
-    def __init__(self, bad_words_ids: List[List[int]], eos_token_id: int):
-        if not isinstance(bad_words_ids, List) or len(bad_words_ids) == 0:
-            raise ValueError(
-                f"`bad_words_ids` has to be a non-empty list, but is {bad_words_ids}."
-            )
-        if any(not isinstance(bad_word_ids, list) for bad_word_ids in bad_words_ids):
-            raise ValueError(
-                f"`bad_words_ids` has to be a list of lists, but is {bad_words_ids}."
-            )
-        if any(
-            any(
-                (not isinstance(token_id, (int, np.integer)) or token_id < 0)
-                for token_id in bad_word_ids
-            )
-            for bad_word_ids in bad_words_ids
+        if os.path.isdir(pretrained_model_name_or_path) or os.path.isfile(
+            pretrained_model_name_or_path
         ):
-            raise ValueError(
-                f"Each list in `bad_words_ids` has to be a list of positive integers, but is {bad_words_ids}."
-            )
+            decoder = BeamSearchDecoderCTC.load_from_dir(pretrained_model_name_or_path)
+        else:
+            # BeamSearchDecoderCTC has no auto class
+            kwargs.pop("_from_auto", None)
+            # snapshot_download has no `trust_remote_code` flag
+            kwargs.pop("trust_remote_code", None)
 
-        # stores the information about bad words in three tensors:
-        # 1. a rectangular tensor with the forbidden sequences (padded with `-1`), for full data comparisons
-        self.bad_word_seqs_ids = tf.ragged.constant(bad_words_ids).to_tensor(
-            default_value=-1
-        )
-        # 2. a tensor with the unpadded length of each forbidden sequence, for quick length comparisons
-        bad_word_seqs_len = [len(bad_words) for bad_words in bad_words_ids]
-        if any([word_len == 0 for word_len in bad_word_seqs_len]):
-            raise ValueError(
-                f"Banned words token sequences {bad_words_ids} cannot have an empty list"
+            # make sure that only relevant filenames are downloaded
+            language_model_filenames = os.path.join(
+                BeamSearchDecoderCTC._LANGUAGE_MODEL_SERIALIZED_DIRECTORY, "*"
             )
-        self.bad_word_seqs_len = tf.convert_to_tensor(bad_word_seqs_len, dtype=tf.int32)
-        # 3. a tensor containing the last token for each sequence, for easy access to the tokens that may be banned
-        self.seq_forbidden_tokens = tf.convert_to_tensor(
-            [bad_words[-1] for bad_words in bad_words_ids]
-        )
-
-    def _calc_row_banned_bad_tokens(self, row_input_ids: tf.Tensor) -> tf.Tensor:
-        def _tokens_match(bad_word_seq_number):
-            def _len_one():
-                # If the bad sequence only has one token, always mask it
-                return tf.cond(
-                    tf.math.equal(self.bad_word_seqs_len[bad_word_seq_number], 1),
-                    lambda: tf.ones((), dtype=tf.bool),
-                    _len_greater_than_cur_len,
-                )
+            alphabet_filename = BeamSearchDecoderCTC._ALPHABET_SERIALIZED_FILENAME
+            allow_regex = [language_model_filenames, alphabet_filename]
 
-            def _len_greater_than_cur_len():
-                # Otherwise, if the bad sequence is longer than the current length they can't ever match
-                return tf.cond(
-                    tf.math.greater(
-                        self.bad_word_seqs_len[bad_word_seq_number],
-                        tf.shape(row_input_ids)[0],
-                    ),
-                    lambda: tf.zeros((), dtype=tf.bool),
-                    _match_found,
-                )
-
-            def _match_found():
-                # Finaly, runs the actual comparison. Can only be called if the previous comparisons do not yield
-                # an answer (otherwise we get indexing exceptions)
-                compare_len = self.bad_word_seqs_len[bad_word_seq_number] - 1
-                return tf.cond(
-                    tf.math.reduce_all(
-                        tf.math.equal(
-                            row_input_ids[-compare_len:],
-                            self.bad_word_seqs_ids[bad_word_seq_number, :compare_len],
-                        )
-                    ),
-                    lambda: tf.ones((), dtype=tf.bool),
-                    lambda: tf.zeros((), dtype=tf.bool),
-                )
-
-            match = _len_one()
-            return match
-
-        # Compares the current row against all bad word sequences, obtaining a mask with the matches.
-        match_mask = tf.map_fn(
-            _tokens_match,
-            tf.range(self.bad_word_seqs_ids.shape[0]),
-            fn_output_signature=tf.bool,
-        )
-        row_banned_tokens = self.seq_forbidden_tokens[match_mask]
-        return row_banned_tokens
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        # We want to mask some banned tokens, at a score level. Since the banned tokens depend on the previous
-        # `input_ids`, they may have a different length for each row, and they may even be empty for some rows.
-        # To remain simple and XLA-compatible, we work on a per-row fashion.
-        # TODO (Joao): this function might trigger XLA retracing as `cur_len` increases. Fix it if it becomes
-        # a frequent choke point. (make `cur_len` a tensor?)
-        def _get_row_updated_score(row_inputs: Tuple[tf.Tensor]) -> tf.Tensor:
-            row_input_ids, row_score = row_inputs
-            banned_tokens = self._calc_row_banned_bad_tokens(row_input_ids[:cur_len])
-            banned_tokens_mask = tf.scatter_nd(
-                indices=tf.expand_dims(banned_tokens, axis=-1),
-                updates=tf.ones_like(banned_tokens, dtype=tf.bool),
-                shape=row_score.shape,
+            decoder = BeamSearchDecoderCTC.load_from_hf_hub(
+                pretrained_model_name_or_path, allow_regex=allow_regex, **kwargs
             )
-            row_score = tf.where(banned_tokens_mask, -float("inf"), row_score)
-            return row_score
-
-        scores = tf.map_fn(
-            _get_row_updated_score, (input_ids, scores), fn_output_signature=tf.float32
-        )
-        return scores
-
 
-class TFNoRepeatNGramLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFLogitsProcessor`] that enforces no repetition of n-grams. See
-    [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).
+        # set language model attributes
+        for attribute in ["alpha", "beta", "unk_score_offset", "score_boundary"]:
+            value = kwargs.pop(attribute, None)
 
-    Args:
-        ngram_size (`int`):
-            All ngrams of size `ngram_size` can only occur once.
-    """
+            if value is not None:
+                cls._set_language_model_attribute(decoder, attribute, value)
 
-    def __init__(self, ngram_size: int):
-        if not isinstance(ngram_size, int) or ngram_size <= 0:
+        # make sure that decoder's alphabet and tokenizer's vocab match in content
+        missing_decoder_tokens = cls.get_missing_alphabet_tokens(decoder, tokenizer)
+        if len(missing_decoder_tokens) > 0:
             raise ValueError(
-                f"`ngram_size` has to be a strictly positive integer, but is {ngram_size}"
-            )
-        self.ngram_size = ngram_size
-
-    def calc_banned_ngram_tokens(self, input_ids, num_hypos, cur_len):
-        # Copied from fairseq for no_repeat_ngram in beam_search
-        if cur_len + 1 < self.ngram_size:
-            # return no banned tokens if we haven't generated ngram_size tokens yet
-            return [[] for _ in range(num_hypos)]
-        generated_ngrams = [{} for _ in range(num_hypos)]
-        prev_input_ids = input_ids[:, :cur_len]
-        for idx in range(num_hypos):
-            gen_tokens = prev_input_ids[idx].numpy().tolist()
-            generated_ngram = generated_ngrams[idx]
-            for ngram in zip(*[gen_tokens[i:] for i in range(self.ngram_size)]):
-                prev_ngram_tuple = tuple(ngram[:-1])
-                generated_ngram[prev_ngram_tuple] = generated_ngram.get(
-                    prev_ngram_tuple, []
-                ) + [ngram[-1]]
-
-        def _get_generated_ngrams(hypo_idx):
-            # Before decoding the next token, prevent decoding of ngrams that have already appeared
-            start_idx = cur_len + 1 - self.ngram_size
-            ngram_idx = tuple(
-                prev_input_ids[hypo_idx, start_idx:cur_len].numpy().tolist()
-            )
-            return generated_ngrams[hypo_idx].get(ngram_idx, [])
-
-        banned_tokens = [
-            _get_generated_ngrams(hypo_idx) for hypo_idx in range(num_hypos)
-        ]
-
-        return banned_tokens
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        # TODO (joao): enable XLA on this logits processor. See discussion and attempts in
-        # https://github.com/huggingface/transformers/pull/16974
-        if not tf.executing_eagerly():
-            raise NotImplementedError(
-                "TFNoRepeatNGramLogitsProcessor is only implemented for eager execution."
-            )
-
-        batch_size, vocab_size = scores.shape
-        banned_tokens = self.calc_banned_ngram_tokens(input_ids, batch_size, cur_len)
-
-        # create banned_tokens boolean mask
-        banned_tokens_indices_mask = []
-        for banned_tokens_slice in banned_tokens:
-            banned_tokens_indices_mask.append(
-                [
-                    True if token in banned_tokens_slice else False
-                    for token in range(vocab_size)
-                ]
+                f"The tokens {missing_decoder_tokens} are defined in the tokenizer's "
+                "vocabulary, but not in the decoder's alphabet. "
+                f"Make sure to include {missing_decoder_tokens} in the decoder's alphabet."
             )
 
-        scores = tf.where(
-            tf.convert_to_tensor(banned_tokens_indices_mask, dtype=tf.bool),
-            -float("inf"),
-            scores,
+        return cls(
+            feature_extractor=feature_extractor, tokenizer=tokenizer, decoder=decoder
         )
 
-        return scores
+    @staticmethod
+    def _set_language_model_attribute(
+        decoder: "BeamSearchDecoderCTC", attribute: str, value: float
+    ):
+        setattr(decoder.model_container[decoder._model_key], attribute, value)
 
+    @property
+    def language_model(self):
+        return self.decoder.model_container[self.decoder._model_key]
+
+    @staticmethod
+    def get_missing_alphabet_tokens(decoder, tokenizer):
+        from pyctcdecode.alphabet import BLANK_TOKEN_PTN, UNK_TOKEN, UNK_TOKEN_PTN
+
+        # we need to make sure that all of the tokenizer's except the special tokens
+        # are present in the decoder's alphabet. Retrieve missing alphabet token
+        # from decoder
+        tokenizer_vocab_list = list(tokenizer.get_vocab().keys())
+
+        # replace special tokens
+        for i, token in enumerate(tokenizer_vocab_list):
+            if BLANK_TOKEN_PTN.match(token):
+                tokenizer_vocab_list[i] = ""
+            if token == tokenizer.word_delimiter_token:
+                tokenizer_vocab_list[i] = " "
+            if UNK_TOKEN_PTN.match(token):
+                tokenizer_vocab_list[i] = UNK_TOKEN
+
+        # are any of the extra tokens no special tokenizer tokens?
+        missing_tokens = set(tokenizer_vocab_list) - set(decoder._alphabet.labels)
+
+        return missing_tokens
+
+    def __call__(self, *args, **kwargs):
+        """
+        When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor's
+        [`~Wav2Vec2FeatureExtractor.__call__`] and returns its output. If used in the context
+        [`~Wav2Vec2ProcessorWithLM.as_target_processor`] this method forwards all its arguments to
+        Wav2Vec2CTCTokenizer's [`~Wav2Vec2CTCTokenizer.__call__`]. Please refer to the docstring of the above two
+        methods for more information.
+        """
+        # For backward compatibility
+        if self._in_target_context_manager:
+            return self.current_processor(*args, **kwargs)
+
+        if "raw_speech" in kwargs:
+            warnings.warn(
+                "Using `raw_speech` as a keyword argument is deprecated. Use `audio` instead."
+            )
+            audio = kwargs.pop("raw_speech")
+        else:
+            audio = kwargs.pop("audio", None)
+        text = kwargs.pop("text", None)
+        if len(args) > 0:
+            audio = args[0]
+            args = args[1:]
+
+        if audio is None and text is None:
+            raise ValueError(
+                "You need to specify either an `audio` or `text` input to process."
+            )
+
+        if audio is not None:
+            inputs = self.feature_extractor(audio, *args, **kwargs)
+        if text is not None:
+            encodings = self.tokenizer(text, **kwargs)
+
+        if text is None:
+            return inputs
+        elif audio is None:
+            return encodings
+        else:
+            inputs["labels"] = encodings["input_ids"]
+            return inputs
+
+    def pad(self, *args, **kwargs):
+        """
+        When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor's
+        [`~Wav2Vec2FeatureExtractor.pad`] and returns its output. If used in the context
+        [`~Wav2Vec2ProcessorWithLM.as_target_processor`] this method forwards all its arguments to
+        Wav2Vec2CTCTokenizer's [`~Wav2Vec2CTCTokenizer.pad`]. Please refer to the docstring of the above two methods
+        for more information.
+        """
+        # For backward compatibility
+        if self._in_target_context_manager:
+            return self.current_processor.pad(*args, **kwargs)
+
+        input_features = kwargs.pop("input_features", None)
+        labels = kwargs.pop("labels", None)
+        if len(args) > 0:
+            input_features = args[0]
+            args = args[1:]
+
+        if input_features is not None:
+            input_features = self.feature_extractor.pad(input_features, *args, **kwargs)
+        if labels is not None:
+            labels = self.tokenizer.pad(labels, **kwargs)
+
+        if labels is None:
+            return input_features
+        elif input_features is None:
+            return labels
+        else:
+            input_features["labels"] = labels["input_ids"]
+            return input_features
 
-class TFForcedBOSTokenLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFLogitsProcessor`] that enforces the specified token as the first generated token.
+    def batch_decode(
+        self,
+        logits: np.ndarray,
+        pool: Optional[Pool] = None,
+        num_processes: Optional[int] = None,
+        beam_width: Optional[int] = None,
+        beam_prune_logp: Optional[float] = None,
+        token_min_logp: Optional[float] = None,
+        hotwords: Optional[Iterable[str]] = None,
+        hotword_weight: Optional[float] = None,
+        alpha: Optional[float] = None,
+        beta: Optional[float] = None,
+        unk_score_offset: Optional[float] = None,
+        lm_score_boundary: Optional[bool] = None,
+        output_word_offsets: bool = False,
+    ):
+        """
+        Batch decode output logits to audio transcription with language model support.
 
-    Args:
-        bos_token_id (`int`):
-            The id of the token to force as the first generated token.
-    """
+        <Tip>
 
-    def __init__(self, bos_token_id: int):
-        if bos_token_id < 0:
-            raise ValueError(
-                f"The forced bos token id  must be a non-negative integer, got {bos_token_id}"
-            )
-        self.bos_token_id = bos_token_id
+        This function makes use of Python's multiprocessing. Currently, multiprocessing is available only on Unix
+        systems (see this [issue](https://github.com/kensho-technologies/pyctcdecode/issues/65)).
 
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        if cur_len == 1:
-            batch_size, num_tokens = scores.shape
-            # sets the score to 0 in the bos_token_id column
-            scores = tf.zeros((batch_size, 1))
-            # sets the score to -inf everywhere else
-            if self.bos_token_id > 0:
-                scores = tf.concat(
-                    (
-                        tf.broadcast_to(-float("inf"), (batch_size, self.bos_token_id)),
-                        scores,
-                    ),
-                    axis=-1,
+        If you are decoding multiple batches, consider creating a `Pool` and passing it to `batch_decode`. Otherwise,
+        `batch_decode` will be very slow since it will create a fresh `Pool` for each call. See usage example below.
+
+        </Tip>
+
+        Args:
+            logits (`np.ndarray`):
+                The logits output vector of the model representing the log probabilities for each token.
+            pool (`multiprocessing.Pool`, *optional*):
+                An optional user-managed pool. If not set, one will be automatically created and closed. The pool
+                should be instantiated *after* `Wav2Vec2ProcessorWithLM`. Otherwise, the LM won't be available to the
+                pool's sub-processes.
+
+                <Tip>
+
+                Currently, only pools created with a 'fork' context can be used. If a 'spawn' pool is passed, it will
+                be ignored and sequential decoding will be used instead.
+
+                </Tip>
+
+            num_processes (`int`, *optional*):
+                If `pool` is not set, number of processes on which the function should be parallelized over. Defaults
+                to the number of available CPUs.
+            beam_width (`int`, *optional*):
+                Maximum number of beams at each step in decoding. Defaults to pyctcdecode's DEFAULT_BEAM_WIDTH.
+            beam_prune_logp (`int`, *optional*):
+                Beams that are much worse than best beam will be pruned Defaults to pyctcdecode's DEFAULT_PRUNE_LOGP.
+            token_min_logp (`int`, *optional*):
+                Tokens below this logp are skipped unless they are argmax of frame Defaults to pyctcdecode's
+                DEFAULT_MIN_TOKEN_LOGP.
+            hotwords (`List[str]`, *optional*):
+                List of words with extra importance, can be OOV for LM
+            hotword_weight (`int`, *optional*):
+                Weight factor for hotword importance Defaults to pyctcdecode's DEFAULT_HOTWORD_WEIGHT.
+            alpha (`float`, *optional*):
+                Weight for language model during shallow fusion
+            beta (`float`, *optional*):
+                Weight for length score adjustment of during scoring
+            unk_score_offset (`float`, *optional*):
+                Amount of log score offset for unknown tokens
+            lm_score_boundary (`bool`, *optional*):
+                Whether to have kenlm respect boundaries when scoring
+            output_word_offsets (`bool`, *optional*, defaults to `False`):
+                Whether or not to output word offsets. Word offsets can be used in combination with the sampling rate
+                and model downsampling rate to compute the time-stamps of transcribed words.
+
+                <Tip>
+
+                Please take a look at the Example of [`~Wav2Vec2ProcessorWithLM.decode`] to better understand how to
+                make use of `output_word_offsets`. [`~Wav2Vec2ProcessorWithLM.batch_decode`] works the same way with
+                batched output.
+
+                </Tip>
+
+        Returns:
+            [`~models.wav2vec2.Wav2Vec2DecoderWithLMOutput`].
+
+        Example:
+            See [Decoding multiple audios](#decoding-multiple-audios).
+        """
+
+        from pyctcdecode.constants import (
+            DEFAULT_BEAM_WIDTH,
+            DEFAULT_HOTWORD_WEIGHT,
+            DEFAULT_MIN_TOKEN_LOGP,
+            DEFAULT_PRUNE_LOGP,
+        )
+
+        # set defaults
+        beam_width = beam_width if beam_width is not None else DEFAULT_BEAM_WIDTH
+        beam_prune_logp = (
+            beam_prune_logp if beam_prune_logp is not None else DEFAULT_PRUNE_LOGP
+        )
+        token_min_logp = (
+            token_min_logp if token_min_logp is not None else DEFAULT_MIN_TOKEN_LOGP
+        )
+        hotword_weight = (
+            hotword_weight if hotword_weight is not None else DEFAULT_HOTWORD_WEIGHT
+        )
+
+        # reset params at every forward call. It's just a `set` method in pyctcdecode
+        self.decoder.reset_params(
+            alpha=alpha,
+            beta=beta,
+            unk_score_offset=unk_score_offset,
+            lm_score_boundary=lm_score_boundary,
+        )
+
+        # create multiprocessing pool and list numpy arrays
+        # filter out logits padding
+        logits_list = [array[(array != -100.0).all(axis=-1)] for array in logits]
+
+        # create a pool if necessary while also using it as a context manager to close itself
+        if pool is None:
+            # fork is safe to use only on Unix, see "Contexts and start methods" section on
+            # multiprocessing's docs (https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods)
+            default_context = get_start_method()
+
+            if default_context == "fork":
+                cm = pool = get_context().Pool(num_processes)
+            else:
+                logger.warning(
+                    "Parallel batch decoding is not currently supported in this platform. "
+                    "Falling back to sequential decoding."
                 )
-            if self.bos_token_id < (num_tokens - 1):
-                scores = tf.concat(
-                    (
-                        scores,
-                        tf.broadcast_to(
-                            -float("inf"),
-                            (batch_size, (num_tokens - 1) - self.bos_token_id),
-                        ),
-                    ),
-                    axis=-1,
+                cm = nullcontext()
+        else:
+            # pool is managed by the user, so we don't need to close it
+            cm = nullcontext()
+
+            if num_processes is not None:
+                logger.warning(
+                    "Parameter `num_process` was passed, but it will be ignored since `pool` was also specified."
                 )
-        return scores
-
-
-class TFForcedEOSTokenLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFLogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.
-
-    Args:
-        max_length (`int`):
-            The maximum length of the sequence to be generated.
-        eos_token_id (`int`):
-            The id of the token to force as the last generated token when `max_length` is reached.
-    """
 
-    def __init__(self, max_length: int, eos_token_id: int):
-        self.max_length = max_length
-        if eos_token_id < 0:
-            raise ValueError(
-                f"The forced eos token id must be a non-negative integer, got {eos_token_id}"
+        # pyctcdecode
+        with cm:
+            decoded_beams = self.decoder.decode_beams_batch(
+                pool=pool,
+                logits_list=logits_list,
+                beam_width=beam_width,
+                beam_prune_logp=beam_prune_logp,
+                token_min_logp=token_min_logp,
+                hotwords=hotwords,
+                hotword_weight=hotword_weight,
+            )
+
+        # extract text and scores
+        batch_texts, logit_scores, lm_scores, word_offsets = [], [], [], []
+        for d in decoded_beams:
+            batch_texts.append(d[0][0])
+            logit_scores.append(d[0][-2])
+            lm_scores.append(d[0][-1])
+            word_offsets.append(
+                [
+                    {"word": t[0], "start_offset": t[1][0], "end_offset": t[1][1]}
+                    for t in d[0][1]
+                ]
             )
-        self.eos_token_id = eos_token_id
 
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        if cur_len == self.max_length - 1:
-            batch_size, num_tokens = scores.shape
-            # sets the score to 0 in the eos_token_id column
-            scores = tf.zeros((batch_size, 1))
-            # sets the score to -inf everywhere else
-            if self.eos_token_id > 0:
-                scores = tf.concat(
-                    (
-                        tf.broadcast_to(-float("inf"), (batch_size, self.eos_token_id)),
-                        scores,
-                    ),
-                    axis=-1,
-                )
-            if self.eos_token_id < (num_tokens - 1):
-                scores = tf.concat(
-                    (
-                        scores,
-                        tf.broadcast_to(
-                            -float("inf"),
-                            (batch_size, (num_tokens - 1) - self.eos_token_id),
-                        ),
-                    ),
-                    axis=-1,
-                )
-        return scores
+        word_offsets = word_offsets if output_word_offsets else None
 
+        return Wav2Vec2DecoderWithLMOutput(
+            text=batch_texts,
+            logit_score=logit_scores,
+            lm_score=lm_scores,
+            word_offsets=word_offsets,
+        )
 
-class TFSuppressTokensAtBeginLogitsProcessor(TFLogitsProcessor):
-    r"""
-    [`TFSuppressTokensAtBeginLogitsProcessor`] suppresses a list of tokens as soon as the `generate` function starts
-    generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` at not
-    sampled at the begining of the generation.
-    """
+    def decode(
+        self,
+        logits: np.ndarray,
+        beam_width: Optional[int] = None,
+        beam_prune_logp: Optional[float] = None,
+        token_min_logp: Optional[float] = None,
+        hotwords: Optional[Iterable[str]] = None,
+        hotword_weight: Optional[float] = None,
+        alpha: Optional[float] = None,
+        beta: Optional[float] = None,
+        unk_score_offset: Optional[float] = None,
+        lm_score_boundary: Optional[bool] = None,
+        output_word_offsets: bool = False,
+    ):
+        """
+        Decode output logits to audio transcription with language model support.
 
-    def __init__(self, begin_suppress_tokens, begin_index):
-        self.begin_suppress_tokens = list(begin_suppress_tokens)
-        self.begin_index = begin_index
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        scores = tf.cond(
-            tf.equal(cur_len, self.begin_index),
-            lambda: tf.tensor_scatter_nd_update(
-                scores,
-                indices=[
-                    [i, token]
-                    for i in range(scores.shape[0])
-                    for token in self.begin_suppress_tokens
-                ],
-                updates=[
-                    -float("inf")
-                    for _ in range(scores.shape[0] * len(self.begin_suppress_tokens))
-                ],
-            ),
-            lambda: scores,
-        )
-        return scores
-
-
-class TFSuppressTokensLogitsProcessor(TFLogitsProcessor):
-    r"""This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so that they
-    are not sampled."""
-
-    def __init__(self, suppress_tokens):
-        self.suppress_tokens = list(suppress_tokens)
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        scores = tf.tensor_scatter_nd_update(
-            scores,
-            indices=[
-                [i, token]
-                for i in range(scores.shape[0])
-                for token in self.suppress_tokens
-            ],
-            updates=[
-                -float("inf")
-                for _ in range(scores.shape[0] * len(self.suppress_tokens))
-            ],
-        )
-        return scores
-
-
-class TFForceTokensLogitsProcessor(TFLogitsProcessor):
-    r"""This processor takes a list of pairs of integers which indicates a mapping from generation indices to token
-    indices that will be forced before sampling. The processor will set their log probs to `0` and all other tokens to
-    `-inf` so that they are sampled at their corresponding index."""
-
-    def __init__(self, force_token_map: List[List[int]]):
-        force_token_map = dict(force_token_map)
-        # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the
-        # index of the array corresponds to the index of the token to be forced, for XLA compatibility.
-        # Indexes without forced tokens will have an negative value.
-        force_token_array = (
-            np.ones((max(force_token_map.keys()) + 1), dtype=np.int32) * -1
-        )
-        for index, token in force_token_map.items():
-            force_token_array[index] = token
-        self.force_token_array = tf.convert_to_tensor(force_token_array, dtype=tf.int32)
-
-    def __call__(
-        self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int
-    ) -> tf.Tensor:
-        def _force_token(generation_idx):
-            batch_size = scores.shape[0]
-            current_token = self.force_token_array[generation_idx]
-
-            new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float("inf")
-            indices = tf.stack(
-                (tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1
-            )
-            updates = tf.zeros((batch_size,), dtype=scores.dtype)
-            new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)
-            return new_scores
-
-        scores = tf.cond(
-            tf.greater_equal(cur_len, tf.shape(self.force_token_array)[0]),
-            # If the current length is geq than the length of force_token_array, the processor does nothing.
-            lambda: tf.identity(scores),
-            # Otherwise, it may force a certain token.
-            lambda: tf.cond(
-                tf.greater_equal(self.force_token_array[cur_len], 0),
-                # Only valid (positive) tokens are forced
-                lambda: _force_token(cur_len),
-                # Otherwise, the processor does nothing.
-                lambda: scores,
-            ),
-        )
-        return scores
+        Args:
+            logits (`np.ndarray`):
+                The logits output vector of the model representing the log probabilities for each token.
+            beam_width (`int`, *optional*):
+                Maximum number of beams at each step in decoding. Defaults to pyctcdecode's DEFAULT_BEAM_WIDTH.
+            beam_prune_logp (`int`, *optional*):
+                A threshold to prune beams with log-probs less than best_beam_logp + beam_prune_logp. The value should
+                be <= 0. Defaults to pyctcdecode's DEFAULT_PRUNE_LOGP.
+            token_min_logp (`int`, *optional*):
+                Tokens with log-probs below token_min_logp are skipped unless they are have the maximum log-prob for an
+                utterance. Defaults to pyctcdecode's DEFAULT_MIN_TOKEN_LOGP.
+            hotwords (`List[str]`, *optional*):
+                List of words with extra importance which can be missing from the LM's vocabulary, e.g. ["huggingface"]
+            hotword_weight (`int`, *optional*):
+                Weight multiplier that boosts hotword scores. Defaults to pyctcdecode's DEFAULT_HOTWORD_WEIGHT.
+            alpha (`float`, *optional*):
+                Weight for language model during shallow fusion
+            beta (`float`, *optional*):
+                Weight for length score adjustment of during scoring
+            unk_score_offset (`float`, *optional*):
+                Amount of log score offset for unknown tokens
+            lm_score_boundary (`bool`, *optional*):
+                Whether to have kenlm respect boundaries when scoring
+            output_word_offsets (`bool`, *optional*, defaults to `False`):
+                Whether or not to output word offsets. Word offsets can be used in combination with the sampling rate
+                and model downsampling rate to compute the time-stamps of transcribed words.
+
+                <Tip>
+
+                Please take a look at the example below to better understand how to make use of `output_word_offsets`.
+
+                </Tip>
+
+        Returns:
+            [`~models.wav2vec2.Wav2Vec2DecoderWithLMOutput`].
+
+        Example:
+
+        ```python
+        >>> # Let's see how to retrieve time steps for a model
+        >>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC
+        >>> from datasets import load_dataset
+        >>> import datasets
+        >>> import torch
+
+        >>> # import model, feature extractor, tokenizer
+        >>> model = AutoModelForCTC.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")
+        >>> processor = AutoProcessor.from_pretrained("patrickvonplaten/wav2vec2-base-100h-with-lm")
+
+        >>> # load first sample of English common_voice
+        >>> dataset = load_dataset("common_voice", "en", split="train", streaming=True)
+        >>> dataset = dataset.cast_column("audio", datasets.Audio(sampling_rate=16_000))
+        >>> dataset_iter = iter(dataset)
+        >>> sample = next(dataset_iter)
+
+        >>> # forward sample through model to get greedily predicted transcription ids
+        >>> input_values = processor(sample["audio"]["array"], return_tensors="pt").input_values
+        >>> with torch.no_grad():
+        ...     logits = model(input_values).logits[0].cpu().numpy()
+
+        >>> # retrieve word stamps (analogous commands for `output_char_offsets`)
+        >>> outputs = processor.decode(logits, output_word_offsets=True)
+        >>> # compute `time_offset` in seconds as product of downsampling ratio and sampling_rate
+        >>> time_offset = model.config.inputs_to_logits_ratio / processor.feature_extractor.sampling_rate
+
+        >>> word_offsets = [
+        ...     {
+        ...         "word": d["word"],
+        ...         "start_time": round(d["start_offset"] * time_offset, 2),
+        ...         "end_time": round(d["end_offset"] * time_offset, 2),
+        ...     }
+        ...     for d in outputs.word_offsets
+        ... ]
+        >>> # compare word offsets with audio `common_voice_en_100038.mp3` online on the dataset viewer:
+        >>> # https://huggingface.co/datasets/common_voice/viewer/en/train
+        >>> word_offsets[:4]
+        [{'word': 'WHY', 'start_time': 1.42, 'end_time': 1.54}, {'word': 'DOES', 'start_time': 1.64, 'end_time': 1.88}, {'word': 'A', 'start_time': 2.12, 'end_time': 2.14}, {'word': 'MILE', 'start_time': 2.26, 'end_time': 2.46}]
+        ```"""
+
+        from pyctcdecode.constants import (
+            DEFAULT_BEAM_WIDTH,
+            DEFAULT_HOTWORD_WEIGHT,
+            DEFAULT_MIN_TOKEN_LOGP,
+            DEFAULT_PRUNE_LOGP,
+        )
+
+        # set defaults
+        beam_width = beam_width if beam_width is not None else DEFAULT_BEAM_WIDTH
+        beam_prune_logp = (
+            beam_prune_logp if beam_prune_logp is not None else DEFAULT_PRUNE_LOGP
+        )
+        token_min_logp = (
+            token_min_logp if token_min_logp is not None else DEFAULT_MIN_TOKEN_LOGP
+        )
+        hotword_weight = (
+            hotword_weight if hotword_weight is not None else DEFAULT_HOTWORD_WEIGHT
+        )
+
+        # reset params at every forward call. It's just a `set` method in pyctcdecode
+        self.decoder.reset_params(
+            alpha=alpha,
+            beta=beta,
+            unk_score_offset=unk_score_offset,
+            lm_score_boundary=lm_score_boundary,
+        )
+
+        # pyctcdecode
+        decoded_beams = self.decoder.decode_beams(
+            logits,
+            beam_width=beam_width,
+            beam_prune_logp=beam_prune_logp,
+            token_min_logp=token_min_logp,
+            hotwords=hotwords,
+            hotword_weight=hotword_weight,
+        )
+
+        word_offsets = None
+        if output_word_offsets:
+            word_offsets = [
+                {"word": word, "start_offset": start_offset, "end_offset": end_offset}
+                for word, (start_offset, end_offset) in decoded_beams[0][2]
+            ]
+
+        # more output features will be added in the future
+        return Wav2Vec2DecoderWithLMOutput(
+            text=decoded_beams[0][0],
+            logit_score=decoded_beams[0][-2],
+            lm_score=decoded_beams[0][-1],
+            word_offsets=word_offsets,
+        )
+
+    @contextmanager
+    def as_target_processor(self):
+        """
+        Temporarily sets the processor for processing the target. Useful for encoding the labels when fine-tuning
+        Wav2Vec2.
+        """
+        warnings.warn(
+            "`as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your "
+            "labels by using the argument `text` of the regular `__call__` method (either in the same call as "
+            "your audio inputs, or in a separate call."
+        )
+        self._in_target_context_manager = True
+        self.current_processor = self.tokenizer
+        yield
+        self.current_processor = self.feature_extractor
+        self._in_target_context_manager = False
```

### Comparing `xs_transformers-1.0.0/xs_transformers/integrations.py` & `xs_transformers-1.0.1/xs_transformers/models/transfo_xl/modeling_transfo_xl.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1491 +1,1445 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# coding=utf-8
+# Copyright 2018 Google AI, Google Brain and Carnegie Mellon University Authors and the HuggingFace Inc. team.
+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
-Integrations with other Python libraries.
+ PyTorch Transformer XL model. Adapted from https://github.com/kimiyoung/transformer-xl. In particular
+ https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py
 """
-import functools
-import importlib.util
-import json
-import numbers
-import os
-import pickle
-import shutil
-import sys
-import tempfile
-from dataclasses import asdict
-from pathlib import Path
-from typing import TYPE_CHECKING, Dict, Optional
-
-import numpy as np
-
-from . import __version__ as version
-from .utils import flatten_dict, is_datasets_available, is_torch_available, logging
+import warnings
+from dataclasses import dataclass
+from typing import List, Optional, Tuple, Union
+
+import torch
+from torch import nn
+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss
+
+from ...modeling_utils import PreTrainedModel
+from ...utils import (
+    ModelOutput,
+    add_code_sample_docstrings,
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
+    logging,
+)
+from .configuration_transfo_xl import TransfoXLConfig
+from .modeling_transfo_xl_utilities import ProjectedAdaptiveLogSoftmax
 
 logger = logging.get_logger(__name__)
 
-if is_torch_available():
-    import torch
+_CHECKPOINT_FOR_DOC = "transfo-xl-wt103"
+_CONFIG_FOR_DOC = "TransfoXLConfig"
+_TOKENIZER_FOR_DOC = "TransfoXLTokenizer"
 
-# comet_ml requires to be imported before any ML frameworks
-_has_comet = (
-    importlib.util.find_spec("comet_ml") is not None
-    and os.getenv("COMET_MODE", "").upper() != "DISABLED"
-)
-if _has_comet:
-    try:
-        import comet_ml  # noqa: F401
+TRANSFO_XL_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "transfo-xl-wt103",
+    # See all Transformer XL models at https://huggingface.co/models?filter=transfo-xl
+]
 
-        if hasattr(comet_ml, "config") and comet_ml.config.get_config("comet.api_key"):
-            _has_comet = True
-        else:
-            if os.getenv("COMET_MODE", "").upper() != "DISABLED":
-                logger.warning("comet_ml is installed but `COMET_API_KEY` is not set.")
-            _has_comet = False
-    except (ImportError, ValueError):
-        _has_comet = False
-
-_has_neptune = importlib.util.find_spec("neptune") is not None
-if TYPE_CHECKING and _has_neptune:
-    from neptune.new.metadata_containers.run import Run
-
-from .trainer_callback import ProgressCallback, TrainerCallback  # noqa: E402
-from .trainer_utils import (  # noqa: E402
-    PREFIX_CHECKPOINT_DIR,
-    BestRun,
-    IntervalStrategy,
-)
-from .training_args import ParallelMode  # noqa: E402
-from .utils import ENV_VARS_TRUE_VALUES, is_torch_tpu_available  # noqa: E402
 
+def build_tf_to_pytorch_map(model, config):
+    """
+    A map of modules from TF to PyTorch. This time I use a map to keep the PyTorch model as identical to the original
+    PyTorch model as possible.
+    """
+    tf_to_pt_map = {}
 
-# Integration functions:
-def is_wandb_available():
-    # any value of WANDB_DISABLED disables wandb
-    if os.getenv("WANDB_DISABLED", "").upper() in ENV_VARS_TRUE_VALUES:
-        logger.warning(
-            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the "
-            "--report_to flag to control the integrations used for logging result (for instance --report_to none)."
+    if hasattr(model, "transformer"):
+        # We are loading in a TransfoXLLMHeadModel => we will load also the Adaptive Softmax
+        tf_to_pt_map.update(
+            {
+                "transformer/adaptive_softmax/cutoff_0/cluster_W": model.crit.cluster_weight,
+                "transformer/adaptive_softmax/cutoff_0/cluster_b": model.crit.cluster_bias,
+            }
+        )
+        for i, (out_l, proj_l, tie_proj) in enumerate(
+            zip(model.crit.out_layers, model.crit.out_projs, config.tie_projs)
+        ):
+            layer_str = f"transformer/adaptive_softmax/cutoff_{i}/"
+            if config.tie_word_embeddings:
+                tf_to_pt_map.update({layer_str + "b": out_l.bias})
+            else:
+                raise NotImplementedError
+                # I don't think this is implemented in the TF code
+                tf_to_pt_map.update(
+                    {
+                        layer_str + "lookup_table": out_l.weight,
+                        layer_str + "b": out_l.bias,
+                    }
+                )
+            if not tie_proj:
+                tf_to_pt_map.update({layer_str + "proj": proj_l})
+        # Now load the rest of the transformer
+        model = model.transformer
+
+    # Embeddings
+    for i, (embed_l, proj_l) in enumerate(
+        zip(model.word_emb.emb_layers, model.word_emb.emb_projs)
+    ):
+        layer_str = f"transformer/adaptive_embed/cutoff_{i}/"
+        tf_to_pt_map.update(
+            {layer_str + "lookup_table": embed_l.weight, layer_str + "proj_W": proj_l}
         )
-        return False
-    return importlib.util.find_spec("wandb") is not None
 
+    # Transformer blocks
+    for i, b in enumerate(model.layers):
+        layer_str = f"transformer/layer_{i}/"
+        tf_to_pt_map.update(
+            {
+                layer_str + "rel_attn/LayerNorm/gamma": b.dec_attn.layer_norm.weight,
+                layer_str + "rel_attn/LayerNorm/beta": b.dec_attn.layer_norm.bias,
+                layer_str + "rel_attn/o/kernel": b.dec_attn.o_net.weight,
+                layer_str + "rel_attn/qkv/kernel": b.dec_attn.qkv_net.weight,
+                layer_str + "rel_attn/r/kernel": b.dec_attn.r_net.weight,
+                layer_str + "ff/LayerNorm/gamma": b.pos_ff.layer_norm.weight,
+                layer_str + "ff/LayerNorm/beta": b.pos_ff.layer_norm.bias,
+                layer_str + "ff/layer_1/kernel": b.pos_ff.CoreNet[0].weight,
+                layer_str + "ff/layer_1/bias": b.pos_ff.CoreNet[0].bias,
+                layer_str + "ff/layer_2/kernel": b.pos_ff.CoreNet[3].weight,
+                layer_str + "ff/layer_2/bias": b.pos_ff.CoreNet[3].bias,
+            }
+        )
 
-def is_comet_available():
-    return _has_comet
+    # Relative positioning biases
+    if config.untie_r:
+        r_r_list = []
+        r_w_list = []
+        for b in model.layers:
+            r_r_list.append(b.dec_attn.r_r_bias)
+            r_w_list.append(b.dec_attn.r_w_bias)
+    else:
+        r_r_list = [model.r_r_bias]
+        r_w_list = [model.r_w_bias]
+    tf_to_pt_map.update(
+        {"transformer/r_r_bias": r_r_list, "transformer/r_w_bias": r_w_list}
+    )
+    return tf_to_pt_map
 
 
-def is_tensorboard_available():
-    return (
-        importlib.util.find_spec("tensorboard") is not None
-        or importlib.util.find_spec("tensorboardX") is not None
-    )
+def load_tf_weights_in_transfo_xl(model, config, tf_path):
+    """Load tf checkpoints in a pytorch model"""
+    try:
+        import numpy as np
+        import tensorflow as tf
+    except ImportError:
+        logger.error(
+            "Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see "
+            "https://www.tensorflow.org/install/ for installation instructions."
+        )
+        raise
+    # Build TF to PyTorch weights loading map
+    tf_to_pt_map = build_tf_to_pytorch_map(model, config)
+
+    # Load weights from TF model
+    init_vars = tf.train.list_variables(tf_path)
+    tf_weights = {}
+    for name, shape in init_vars:
+        logger.info(f"Loading TF weight {name} with shape {shape}")
+        array = tf.train.load_variable(tf_path, name)
+        tf_weights[name] = array
+
+    for name, pointer in tf_to_pt_map.items():
+        assert name in tf_weights
+        array = tf_weights[name]
+        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
+        # which are not required for using pretrained model
+        if "kernel" in name or "proj" in name:
+            array = np.transpose(array)
+        if ("r_r_bias" in name or "r_w_bias" in name) and len(pointer) > 1:
+            # Here we will split the TF weights
+            assert len(pointer) == array.shape[0]
+            for i, p_i in enumerate(pointer):
+                arr_i = array[i, ...]
+                try:
+                    assert p_i.shape == arr_i.shape
+                except AssertionError as e:
+                    e.args += (p_i.shape, arr_i.shape)
+                    raise
+                logger.info(f"Initialize PyTorch weight {name} for layer {i}")
+                p_i.data = torch.from_numpy(arr_i)
+        else:
+            try:
+                assert (
+                    pointer.shape == array.shape
+                ), f"Pointer shape {pointer.shape} and array shape {array.shape} mismatched"
+            except AssertionError as e:
+                e.args += (pointer.shape, array.shape)
+                raise
+            logger.info(f"Initialize PyTorch weight {name}")
+            pointer.data = torch.from_numpy(array)
+        tf_weights.pop(name, None)
+        tf_weights.pop(name + "/Adam", None)
+        tf_weights.pop(name + "/Adam_1", None)
+
+    logger.info(f"Weights not copied to PyTorch model: {', '.join(tf_weights.keys())}")
+    return model
+
+
+class PositionalEmbedding(nn.Module):
+    def __init__(self, demb):
+        super().__init__()
+
+        self.demb = demb
+
+        inv_freq = 1 / (10000 ** (torch.arange(0.0, demb, 2.0) / demb))
+        self.register_buffer("inv_freq", inv_freq)
+
+    def forward(self, pos_seq, bsz=None):
+        sinusoid_inp = torch.ger(pos_seq, self.inv_freq)
+        pos_emb = torch.cat([sinusoid_inp.sin(), sinusoid_inp.cos()], dim=-1)
 
+        if bsz is not None:
+            return pos_emb[:, None, :].expand(-1, bsz, -1)
+        else:
+            return pos_emb[:, None, :]
 
-def is_optuna_available():
-    return importlib.util.find_spec("optuna") is not None
 
+class PositionwiseFF(nn.Module):
+    def __init__(
+        self, d_model, d_inner, dropout, pre_lnorm=False, layer_norm_epsilon=1e-5
+    ):
+        super().__init__()
 
-def is_ray_available():
-    return importlib.util.find_spec("ray") is not None
+        self.d_model = d_model
+        self.d_inner = d_inner
+        self.dropout = dropout
+
+        self.CoreNet = nn.Sequential(
+            nn.Linear(d_model, d_inner),
+            nn.ReLU(inplace=True),
+            nn.Dropout(dropout),
+            nn.Linear(d_inner, d_model),
+            nn.Dropout(dropout),
+        )
 
+        self.layer_norm = nn.LayerNorm(d_model, eps=layer_norm_epsilon)
 
-def is_ray_tune_available():
-    if not is_ray_available():
-        return False
-    return importlib.util.find_spec("ray.tune") is not None
+        self.pre_lnorm = pre_lnorm
 
+    def forward(self, inp):
+        if self.pre_lnorm:
+            # layer normalization + positionwise feed-forward
+            core_out = self.CoreNet(self.layer_norm(inp))
 
-def is_sigopt_available():
-    return importlib.util.find_spec("sigopt") is not None
+            # residual connection
+            output = core_out + inp
+        else:
+            # positionwise feed-forward
+            core_out = self.CoreNet(inp)
 
+            # residual connection + layer normalization
+            output = self.layer_norm(inp + core_out)
 
-def is_azureml_available():
-    if importlib.util.find_spec("azureml") is None:
-        return False
-    if importlib.util.find_spec("azureml.core") is None:
-        return False
-    return importlib.util.find_spec("azureml.core.run") is not None
+        return output
 
 
-def is_mlflow_available():
-    if os.getenv("DISABLE_MLFLOW_INTEGRATION", "FALSE").upper() == "TRUE":
-        return False
-    return importlib.util.find_spec("mlflow") is not None
+class RelPartialLearnableMultiHeadAttn(nn.Module):
+    def __init__(
+        self,
+        n_head,
+        d_model,
+        d_head,
+        dropout,
+        dropatt=0,
+        pre_lnorm=False,
+        r_r_bias=None,
+        r_w_bias=None,
+        layer_norm_epsilon=1e-5,
+    ):
+        super().__init__()
 
+        self.n_head = n_head
+        self.d_model = d_model
+        self.d_head = d_head
+        self.dropout = dropout
 
-def is_fairscale_available():
-    return importlib.util.find_spec("fairscale") is not None
+        self.qkv_net = nn.Linear(d_model, 3 * n_head * d_head, bias=False)
 
+        self.drop = nn.Dropout(dropout)
+        self.dropatt = nn.Dropout(dropatt)
+        self.o_net = nn.Linear(n_head * d_head, d_model, bias=False)
 
-def is_neptune_available():
-    return _has_neptune
+        self.layer_norm = nn.LayerNorm(d_model, eps=layer_norm_epsilon)
 
+        self.scale = 1 / (d_head**0.5)
 
-def is_codecarbon_available():
-    return importlib.util.find_spec("codecarbon") is not None
+        self.pre_lnorm = pre_lnorm
 
+        if r_r_bias is None or r_w_bias is None:  # Biases are not shared
+            self.r_r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
+            self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
+        else:
+            self.r_r_bias = r_r_bias
+            self.r_w_bias = r_w_bias
 
-def hp_params(trial):
-    if is_optuna_available():
-        import optuna
+        self.r_net = nn.Linear(self.d_model, self.n_head * self.d_head, bias=False)
 
-        if isinstance(trial, optuna.Trial):
-            return trial.params
-    if is_ray_tune_available():
-        if isinstance(trial, dict):
-            return trial
+    def _rel_shift(self, x):
+        zero_pad_shape = (x.size(0), 1) + x.size()[2:]
+        zero_pad = torch.zeros(zero_pad_shape, device=x.device, dtype=x.dtype)
+        x_padded = torch.cat([zero_pad, x], dim=1)
 
-    if is_sigopt_available():
-        if isinstance(trial, dict):
-            return trial
+        x_padded_shape = (x.size(1) + 1, x.size(0)) + x.size()[2:]
+        x_padded = x_padded.view(*x_padded_shape)
 
-    if is_wandb_available():
-        if isinstance(trial, dict):
-            return trial
+        x = x_padded[1:].view_as(x)
 
-    raise RuntimeError(f"Unknown type for trial {trial.__class__}")
+        return x
 
+    def forward(
+        self, w, r, attn_mask=None, mems=None, head_mask=None, output_attentions=False
+    ):
+        qlen, rlen, bsz = w.size(0), r.size(0), w.size(1)
 
-def default_hp_search_backend():
-    if is_optuna_available():
-        return "optuna"
-    elif is_ray_tune_available():
-        return "ray"
-    elif is_sigopt_available():
-        return "sigopt"
+        if mems is not None:
+            cat = torch.cat([mems, w], 0)
+            if self.pre_lnorm:
+                w_heads = self.qkv_net(self.layer_norm(cat))
+            else:
+                w_heads = self.qkv_net(cat)
+            r_head_k = self.r_net(r)
 
+            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)
+            w_head_q = w_head_q[-qlen:]
+        else:
+            if self.pre_lnorm:
+                w_heads = self.qkv_net(self.layer_norm(w))
+            else:
+                w_heads = self.qkv_net(w)
+            r_head_k = self.r_net(r)
 
-def run_hp_search_optuna(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:
-    import optuna
+            w_head_q, w_head_k, w_head_v = torch.chunk(w_heads, 3, dim=-1)
 
-    if trainer.args.process_index == 0:
+        klen = w_head_k.size(0)
 
-        def _objective(trial, checkpoint_dir=None):
-            checkpoint = None
-            if checkpoint_dir:
-                for subdir in os.listdir(checkpoint_dir):
-                    if subdir.startswith(PREFIX_CHECKPOINT_DIR):
-                        checkpoint = os.path.join(checkpoint_dir, subdir)
-            trainer.objective = None
-            if trainer.args.world_size > 1:
-                if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
-                    raise RuntimeError(
-                        "only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently."
-                    )
-                trainer._hp_search_setup(trial)
-                torch.distributed.broadcast_object_list(
-                    pickle.dumps(trainer.args), src=0
-                )
-                trainer.train(resume_from_checkpoint=checkpoint)
-            else:
-                trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
-            # If there hasn't been any evaluation during the training loop.
-            if getattr(trainer, "objective", None) is None:
-                metrics = trainer.evaluate()
-                trainer.objective = trainer.compute_objective(metrics)
-            return trainer.objective
-
-        timeout = kwargs.pop("timeout", None)
-        n_jobs = kwargs.pop("n_jobs", 1)
-        study = optuna.create_study(direction=direction, **kwargs)
-        study.optimize(_objective, n_trials=n_trials, timeout=timeout, n_jobs=n_jobs)
-        best_trial = study.best_trial
-        return BestRun(str(best_trial.number), best_trial.value, best_trial.params)
-    else:
-        for i in range(n_trials):
-            trainer.objective = None
-            args_main_rank = list(pickle.dumps(trainer.args))
-            if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
-                raise RuntimeError(
-                    "only support DDP optuna HPO for ParallelMode.DISTRIBUTED currently."
-                )
-            torch.distributed.broadcast_object_list(args_main_rank, src=0)
-            args = pickle.loads(bytes(args_main_rank))
-            for key, value in asdict(args).items():
-                if key != "local_rank":
-                    setattr(trainer.args, key, value)
-            trainer.train(resume_from_checkpoint=None)
-            # If there hasn't been any evaluation during the training loop.
-            if getattr(trainer, "objective", None) is None:
-                metrics = trainer.evaluate()
-                trainer.objective = trainer.compute_objective(metrics)
-        return None
-
-
-def run_hp_search_ray(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:
-    import ray
-
-    def _objective(trial, local_trainer, checkpoint_dir=None):
-        try:
-            from transformers.utils.notebook import NotebookProgressCallback
-
-            if local_trainer.pop_callback(NotebookProgressCallback):
-                local_trainer.add_callback(ProgressCallback)
-        except ModuleNotFoundError:
-            pass
-
-        checkpoint = None
-        if checkpoint_dir:
-            for subdir in os.listdir(checkpoint_dir):
-                if subdir.startswith(PREFIX_CHECKPOINT_DIR):
-                    checkpoint = os.path.join(checkpoint_dir, subdir)
-        local_trainer.objective = None
-        local_trainer.train(resume_from_checkpoint=checkpoint, trial=trial)
-        # If there hasn't been any evaluation during the training loop.
-        if getattr(local_trainer, "objective", None) is None:
-            metrics = local_trainer.evaluate()
-            local_trainer.objective = local_trainer.compute_objective(metrics)
-            local_trainer._tune_save_checkpoint()
-            ray.tune.report(objective=local_trainer.objective, **metrics, done=True)
-
-    if not trainer._memory_tracker.skip_memory_metrics:
-        from .trainer_utils import TrainerMemoryTracker
-
-        logger.warning(
-            "Memory tracking for your Trainer is currently "
-            "enabled. Automatically disabling the memory tracker "
-            "since the memory tracker is not serializable."
-        )
-        trainer._memory_tracker = TrainerMemoryTracker(skip_memory_metrics=True)
-
-    # The model and TensorBoard writer do not pickle so we have to remove them (if they exists)
-    # while doing the ray hp search.
-    _tb_writer = trainer.pop_callback(TensorBoardCallback)
-    trainer.model = None
-
-    # Setup default `resources_per_trial`.
-    if "resources_per_trial" not in kwargs:
-        # Default to 1 CPU and 1 GPU (if applicable) per trial.
-        kwargs["resources_per_trial"] = {"cpu": 1}
-        if trainer.args.n_gpu > 0:
-            kwargs["resources_per_trial"]["gpu"] = 1
-        resource_msg = "1 CPU" + (" and 1 GPU" if trainer.args.n_gpu > 0 else "")
-        logger.info(
-            "No `resources_per_trial` arg was passed into "
-            "`hyperparameter_search`. Setting it to a default value "
-            f"of {resource_msg} for each trial."
-        )
-    # Make sure each trainer only uses GPUs that were allocated per trial.
-    gpus_per_trial = kwargs["resources_per_trial"].get("gpu", 0)
-    trainer.args._n_gpu = gpus_per_trial
-
-    # Setup default `progress_reporter`.
-    if "progress_reporter" not in kwargs:
-        from ray.tune import CLIReporter
-
-        kwargs["progress_reporter"] = CLIReporter(metric_columns=["objective"])
-    if "keep_checkpoints_num" in kwargs and kwargs["keep_checkpoints_num"] > 0:
-        # `keep_checkpoints_num=0` would disabled checkpointing
-        trainer.use_tune_checkpoints = True
-        if kwargs["keep_checkpoints_num"] > 1:
-            logger.warning(
-                f"Currently keeping {kwargs['keep_checkpoints_num']} checkpoints for each trial. "
-                "Checkpoints are usually huge, "
-                "consider setting `keep_checkpoints_num=1`."
-            )
-    if "scheduler" in kwargs:
-        from ray.tune.schedulers import (
-            ASHAScheduler,
-            HyperBandForBOHB,
-            MedianStoppingRule,
-            PopulationBasedTraining,
+        w_head_q = w_head_q.view(
+            qlen, bsz, self.n_head, self.d_head
+        )  # qlen x bsz x n_head x d_head
+        w_head_k = w_head_k.view(
+            klen, bsz, self.n_head, self.d_head
+        )  # qlen x bsz x n_head x d_head
+        w_head_v = w_head_v.view(
+            klen, bsz, self.n_head, self.d_head
+        )  # qlen x bsz x n_head x d_head
+
+        r_head_k = r_head_k.view(
+            rlen, self.n_head, self.d_head
+        )  # qlen x n_head x d_head
+
+        # compute attention score
+        rw_head_q = w_head_q + self.r_w_bias  # qlen x bsz x n_head x d_head
+        AC = torch.einsum(
+            "ibnd,jbnd->ijbn", (rw_head_q, w_head_k)
+        )  # qlen x klen x bsz x n_head
+
+        rr_head_q = w_head_q + self.r_r_bias
+        BD = torch.einsum(
+            "ibnd,jnd->ijbn", (rr_head_q, r_head_k)
+        )  # qlen x klen x bsz x n_head
+        BD = self._rel_shift(BD)
+
+        # [qlen x klen x bsz x n_head]
+        attn_score = AC + BD
+        attn_score.mul_(self.scale)
+
+        mask_value = torch.finfo(attn_score.dtype).min
+
+        # compute attention probability
+        if attn_mask is not None and torch.sum(attn_mask).item():
+            attn_mask = attn_mask == 1  # Switch to bool
+            if attn_mask.dim() == 2:
+                attn_score = (
+                    attn_score.float()
+                    .masked_fill(attn_mask[None, :, :, None], mask_value)
+                    .type_as(attn_score)
+                )
+            elif attn_mask.dim() == 3:
+                attn_score = (
+                    attn_score.float()
+                    .masked_fill(attn_mask[:, :, :, None], mask_value)
+                    .type_as(attn_score)
+                )
+
+        # [qlen x klen x bsz x n_head]
+        attn_prob = nn.functional.softmax(attn_score, dim=1)
+        attn_prob = self.dropatt(attn_prob)
+
+        # Mask heads if we want to
+        if head_mask is not None:
+            attn_prob = attn_prob * head_mask
+
+        # compute attention vector
+        attn_vec = torch.einsum("ijbn,jbnd->ibnd", (attn_prob, w_head_v))
+
+        # [qlen x bsz x n_head x d_head]
+        attn_vec = attn_vec.contiguous().view(
+            attn_vec.size(0), attn_vec.size(1), self.n_head * self.d_head
         )
 
-        # Check if checkpointing is enabled for PopulationBasedTraining
-        if isinstance(kwargs["scheduler"], PopulationBasedTraining):
-            if not trainer.use_tune_checkpoints:
-                logger.warning(
-                    "You are using PopulationBasedTraining but you haven't enabled checkpointing. "
-                    "This means your trials will train from scratch everytime they are exploiting "
-                    "new configurations. Consider enabling checkpointing by passing "
-                    "`keep_checkpoints_num=1` as an additional argument to `Trainer.hyperparameter_search`."
-                )
+        # linear projection
+        attn_out = self.o_net(attn_vec)
+        attn_out = self.drop(attn_out)
+
+        if self.pre_lnorm:
+            # residual connection
+            outputs = [w + attn_out]
+        else:
+            # residual connection + layer normalization
+            outputs = [self.layer_norm(w + attn_out)]
 
-        # Check for `do_eval` and `eval_during_training` for schedulers that require intermediate reporting.
-        if isinstance(
-            kwargs["scheduler"],
-            (
-                ASHAScheduler,
-                MedianStoppingRule,
-                HyperBandForBOHB,
-                PopulationBasedTraining,
-            ),
-        ) and (
-            not trainer.args.do_eval
-            or trainer.args.evaluation_strategy == IntervalStrategy.NO
-        ):
-            raise RuntimeError(
-                "You are using {cls} as a scheduler but you haven't enabled evaluation during training. "
-                "This means your trials will not report intermediate results to Ray Tune, and "
-                "can thus not be stopped early or used to exploit other trials parameters. "
-                "If this is what you want, do not use {cls}. If you would like to use {cls}, "
-                "make sure you pass `do_eval=True` and `evaluation_strategy='steps'` in the "
-                "Trainer `args`.".format(cls=type(kwargs["scheduler"]).__name__)
-            )
+        if output_attentions:
+            outputs.append(attn_prob)
 
-    trainable = ray.tune.with_parameters(_objective, local_trainer=trainer)
+        return outputs
 
-    @functools.wraps(trainable)
-    def dynamic_modules_import_trainable(*args, **kwargs):
-        """
-        Wrapper around `tune.with_parameters` to ensure datasets_modules are loaded on each Actor.
 
-        Without this, an ImportError will be thrown. See https://github.com/huggingface/transformers/issues/11565.
+class RelPartialLearnableDecoderLayer(nn.Module):
+    def __init__(
+        self,
+        n_head,
+        d_model,
+        d_head,
+        d_inner,
+        dropout,
+        layer_norm_epsilon=1e-5,
+        **kwargs,
+    ):
+        super().__init__()
 
-        Assumes that `_objective`, defined above, is a function.
-        """
-        if is_datasets_available():
-            import datasets.load
+        self.dec_attn = RelPartialLearnableMultiHeadAttn(
+            n_head,
+            d_model,
+            d_head,
+            dropout,
+            layer_norm_epsilon=layer_norm_epsilon,
+            **kwargs,
+        )
+        self.pos_ff = PositionwiseFF(
+            d_model,
+            d_inner,
+            dropout,
+            pre_lnorm=kwargs.get("pre_lnorm"),
+            layer_norm_epsilon=layer_norm_epsilon,
+        )
 
-            dynamic_modules_path = os.path.join(
-                datasets.load.init_dynamic_modules(), "__init__.py"
-            )
-            # load dynamic_modules from path
-            spec = importlib.util.spec_from_file_location(
-                "datasets_modules", dynamic_modules_path
-            )
-            datasets_modules = importlib.util.module_from_spec(spec)
-            sys.modules[spec.name] = datasets_modules
-            spec.loader.exec_module(datasets_modules)
-        return trainable(*args, **kwargs)
-
-    # special attr set by tune.with_parameters
-    if hasattr(trainable, "__mixins__"):
-        dynamic_modules_import_trainable.__mixins__ = trainable.__mixins__
-
-    analysis = ray.tune.run(
-        dynamic_modules_import_trainable,
-        config=trainer.hp_space(None),
-        num_samples=n_trials,
-        **kwargs,
-    )
-    best_trial = analysis.get_best_trial(
-        metric="objective", mode=direction[:3], scope=trainer.args.ray_scope
-    )
-    best_run = BestRun(
-        best_trial.trial_id, best_trial.last_result["objective"], best_trial.config
-    )
-    if _tb_writer is not None:
-        trainer.add_callback(_tb_writer)
-    return best_run
-
-
-def run_hp_search_sigopt(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:
-    import sigopt
-    from transformers.utils.versions import importlib_metadata
-
-    if trainer.args.process_index == 0:
-        if importlib_metadata.version("sigopt") >= "8.0.0":
-            sigopt.set_project("huggingface")
-
-            experiment = sigopt.create_experiment(
-                name="huggingface-tune",
-                type="offline",
-                parameters=trainer.hp_space(None),
-                metrics=[
-                    dict(name="objective", objective=direction, strategy="optimize")
-                ],
-                parallel_bandwidth=1,
-                budget=n_trials,
-            )
+    def forward(
+        self,
+        dec_inp,
+        r,
+        dec_attn_mask=None,
+        mems=None,
+        head_mask=None,
+        output_attentions=False,
+    ):
+        attn_outputs = self.dec_attn(
+            dec_inp,
+            r,
+            attn_mask=dec_attn_mask,
+            mems=mems,
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+        )
+        ff_output = self.pos_ff(attn_outputs[0])
 
-            logger.info(
-                f"created experiment: https://app.sigopt.com/experiment/{experiment.id}"
-            )
+        outputs = [ff_output] + attn_outputs[1:]
 
-            for run in experiment.loop():
-                with run:
-                    trainer.objective = None
-                    if trainer.args.world_size > 1:
-                        if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
-                            raise RuntimeError(
-                                "only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently."
-                            )
-                        trainer._hp_search_setup(run.run)
-                        torch.distributed.broadcast_object_list(
-                            pickle.dumps(trainer.args), src=0
-                        )
-                        trainer.train(resume_from_checkpoint=None)
-                    else:
-                        trainer.train(resume_from_checkpoint=None, trial=run.run)
-                    # If there hasn't been any evaluation during the training loop.
-                    if getattr(trainer, "objective", None) is None:
-                        metrics = trainer.evaluate()
-                        trainer.objective = trainer.compute_objective(metrics)
-                    run.log_metric("objective", trainer.objective)
-
-            best = list(experiment.get_best_runs())[0]
-            best_run = BestRun(
-                best.id, best.values["objective"].value, best.assignments
-            )
-        else:
-            from sigopt import Connection
+        return outputs
 
-            conn = Connection()
-            proxies = kwargs.pop("proxies", None)
-            if proxies is not None:
-                conn.set_proxies(proxies)
-
-            experiment = conn.experiments().create(
-                name="huggingface-tune",
-                parameters=trainer.hp_space(None),
-                metrics=[
-                    dict(name="objective", objective=direction, strategy="optimize")
-                ],
-                parallel_bandwidth=1,
-                observation_budget=n_trials,
-                project="huggingface",
-            )
-            logger.info(
-                f"created experiment: https://app.sigopt.com/experiment/{experiment.id}"
-            )
 
-            while experiment.progress.observation_count < experiment.observation_budget:
-                suggestion = conn.experiments(experiment.id).suggestions().create()
-                trainer.objective = None
-                if trainer.args.world_size > 1:
-                    if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
-                        raise RuntimeError(
-                            "only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently."
-                        )
-                    trainer._hp_search_setup(suggestion)
-                    torch.distributed.broadcast_object_list(
-                        pickle.dumps(trainer.args), src=0
-                    )
-                    trainer.train(resume_from_checkpoint=None)
-                else:
-                    trainer.train(resume_from_checkpoint=None, trial=suggestion)
-                # If there hasn't been any evaluation during the training loop.
-                if getattr(trainer, "objective", None) is None:
-                    metrics = trainer.evaluate()
-                    trainer.objective = trainer.compute_objective(metrics)
-
-                values = [dict(name="objective", value=trainer.objective)]
-                obs = (
-                    conn.experiments(experiment.id)
-                    .observations()
-                    .create(suggestion=suggestion.id, values=values)
-                )
-                logger.info(
-                    f"[suggestion_id, observation_id]: [{suggestion.id}, {obs.id}]"
-                )
-                experiment = conn.experiments(experiment.id).fetch()
+class AdaptiveEmbedding(nn.Module):
+    def __init__(
+        self, n_token, d_embed, d_proj, cutoffs, div_val=1, sample_softmax=False
+    ):
+        super().__init__()
 
-            best = list(
-                conn.experiments(experiment.id)
-                .best_assignments()
-                .fetch()
-                .iterate_pages()
-            )[0]
-            best_run = BestRun(best.id, best.value, best.assignments)
-        return best_run
-    else:
-        for i in range(n_trials):
-            trainer.objective = None
-            args_main_rank = list(pickle.dumps(trainer.args))
-            if trainer.args.parallel_mode != ParallelMode.DISTRIBUTED:
-                raise RuntimeError(
-                    "only support DDP Sigopt HPO for ParallelMode.DISTRIBUTED currently."
-                )
-            torch.distributed.broadcast_object_list(args_main_rank, src=0)
-            args = pickle.loads(bytes(args_main_rank))
-            for key, value in asdict(args).items():
-                if key != "local_rank":
-                    setattr(trainer.args, key, value)
-            trainer.train(resume_from_checkpoint=None)
-            # If there hasn't been any evaluation during the training loop.
-            if getattr(trainer, "objective", None) is None:
-                metrics = trainer.evaluate()
-                trainer.objective = trainer.compute_objective(metrics)
-        return None
-
-
-def run_hp_search_wandb(trainer, n_trials: int, direction: str, **kwargs) -> BestRun:
-    from .integrations import is_wandb_available
-
-    if not is_wandb_available():
-        raise ImportError("This function needs wandb installed: `pip install wandb`")
-    import wandb
-
-    # add WandbCallback if not already added in trainer callbacks
-    reporting_to_wandb = False
-    for callback in trainer.callback_handler.callbacks:
-        if isinstance(callback, WandbCallback):
-            reporting_to_wandb = True
-            break
-    if not reporting_to_wandb:
-        trainer.add_callback(WandbCallback())
-    trainer.args.report_to = "wandb"
-    best_trial = {"run_id": None, "objective": None, "hyperparameters": None}
-    sweep_id = kwargs.pop("sweep_id", None)
-    project = kwargs.pop("project", None)
-    name = kwargs.pop("name", None)
-    entity = kwargs.pop("entity", None)
-    metric = kwargs.pop("metric", "eval/loss")
-
-    sweep_config = trainer.hp_space(None)
-    sweep_config["metric"]["goal"] = direction
-    sweep_config["metric"]["name"] = metric
-    if name:
-        sweep_config["name"] = name
-
-    def _objective():
-        run = wandb.run if wandb.run else wandb.init()
-        trainer.state.trial_name = run.name
-        run.config.update({"assignments": {}, "metric": metric})
-        config = wandb.config
-
-        trainer.objective = None
-
-        trainer.train(resume_from_checkpoint=None, trial=vars(config)["_items"])
-        # If there hasn't been any evaluation during the training loop.
-        if getattr(trainer, "objective", None) is None:
-            metrics = trainer.evaluate()
-            trainer.objective = trainer.compute_objective(metrics)
-            format_metrics = rewrite_logs(metrics)
-            if metric not in format_metrics:
-                logger.warning(
-                    f"Provided metric {metric} not found. This might result in unexpected sweeps charts. The available"
-                    f" metrics are {format_metrics.keys()}"
-                )
-        best_score = False
-        if best_trial["run_id"] is not None:
-            if direction == "minimize":
-                best_score = trainer.objective < best_trial["objective"]
-            elif direction == "maximize":
-                best_score = trainer.objective > best_trial["objective"]
-
-        if best_score or best_trial["run_id"] is None:
-            best_trial["run_id"] = run.id
-            best_trial["objective"] = trainer.objective
-            best_trial["hyperparameters"] = dict(config)
-
-        return trainer.objective
-
-    sweep_id = (
-        wandb.sweep(sweep_config, project=project, entity=entity)
-        if not sweep_id
-        else sweep_id
-    )
-    logger.info(f"wandb sweep id - {sweep_id}")
-    wandb.agent(sweep_id, function=_objective, count=n_trials)
+        self.n_token = n_token
+        self.d_embed = d_embed
 
-    return BestRun(
-        best_trial["run_id"], best_trial["objective"], best_trial["hyperparameters"]
-    )
+        self.cutoffs = cutoffs + [n_token]
+        self.div_val = div_val
+        self.d_proj = d_proj
 
+        self.emb_scale = d_proj**0.5
 
-def get_available_reporting_integrations():
-    integrations = []
-    if is_azureml_available():
-        integrations.append("azure_ml")
-    if is_comet_available():
-        integrations.append("comet_ml")
-    if is_mlflow_available():
-        integrations.append("mlflow")
-    if is_neptune_available():
-        integrations.append("neptune")
-    if is_tensorboard_available():
-        integrations.append("tensorboard")
-    if is_wandb_available():
-        integrations.append("wandb")
-    if is_codecarbon_available():
-        integrations.append("codecarbon")
-    return integrations
-
-
-def rewrite_logs(d):
-    new_d = {}
-    eval_prefix = "eval_"
-    eval_prefix_len = len(eval_prefix)
-    test_prefix = "test_"
-    test_prefix_len = len(test_prefix)
-    for k, v in d.items():
-        if k.startswith(eval_prefix):
-            new_d["eval/" + k[eval_prefix_len:]] = v
-        elif k.startswith(test_prefix):
-            new_d["test/" + k[test_prefix_len:]] = v
+        self.cutoff_ends = [0] + self.cutoffs
+
+        self.emb_layers = nn.ModuleList()
+        self.emb_projs = nn.ParameterList()
+        if div_val == 1:
+            self.emb_layers.append(
+                nn.Embedding(n_token, d_embed, sparse=sample_softmax > 0)
+            )
+            if d_proj != d_embed:
+                self.emb_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_embed)))
         else:
-            new_d["train/" + k] = v
-    return new_d
+            for i in range(len(self.cutoffs)):
+                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
+                d_emb_i = d_embed // (div_val**i)
+                self.emb_layers.append(nn.Embedding(r_idx - l_idx, d_emb_i))
+                self.emb_projs.append(nn.Parameter(torch.FloatTensor(d_proj, d_emb_i)))
+
+    def forward(self, inp):
+        if self.div_val == 1:
+            embed = self.emb_layers[0](inp)
+            if self.d_proj != self.d_embed:
+                embed = nn.functional.linear(embed, self.emb_projs[0])
+        else:
+            param = next(self.parameters())
+            inp_flat = inp.view(-1)
+            emb_flat = torch.zeros(
+                [inp_flat.size(0), self.d_proj], dtype=param.dtype, device=param.device
+            )
+            for i in range(len(self.cutoffs)):
+                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]
 
+                mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)
+                indices_i = mask_i.nonzero().squeeze()
 
-class TensorBoardCallback(TrainerCallback):
-    """
-    A [`TrainerCallback`] that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).
+                if indices_i.numel() == 0:
+                    continue
 
-    Args:
-        tb_writer (`SummaryWriter`, *optional*):
-            The writer to use. Will instantiate one if not set.
-    """
+                inp_i = inp_flat.index_select(0, indices_i) - l_idx
+                emb_i = self.emb_layers[i](inp_i)
+                emb_i = nn.functional.linear(emb_i, self.emb_projs[i])
 
-    def __init__(self, tb_writer=None):
-        has_tensorboard = is_tensorboard_available()
-        if not has_tensorboard:
-            raise RuntimeError(
-                "TensorBoardCallback requires tensorboard to be installed. Either update your PyTorch version or"
-                " install tensorboardX."
-            )
-        if has_tensorboard:
-            try:
-                from torch.utils.tensorboard import SummaryWriter  # noqa: F401
+                emb_flat.index_copy_(0, indices_i, emb_i)
 
-                self._SummaryWriter = SummaryWriter
-            except ImportError:
-                try:
-                    from tensorboardX import SummaryWriter
+            embed_shape = inp.size() + (self.d_proj,)
+            embed = emb_flat.view(embed_shape)
 
-                    self._SummaryWriter = SummaryWriter
-                except ImportError:
-                    self._SummaryWriter = None
-        else:
-            self._SummaryWriter = None
-        self.tb_writer = tb_writer
-
-    def _init_summary_writer(self, args, log_dir=None):
-        log_dir = log_dir or args.logging_dir
-        if self._SummaryWriter is not None:
-            self.tb_writer = self._SummaryWriter(log_dir=log_dir)
-
-    def on_train_begin(self, args, state, control, **kwargs):
-        if not state.is_world_process_zero:
-            return
-
-        log_dir = None
-
-        if state.is_hyper_param_search:
-            trial_name = state.trial_name
-            if trial_name is not None:
-                log_dir = os.path.join(args.logging_dir, trial_name)
-
-        if self.tb_writer is None:
-            self._init_summary_writer(args, log_dir)
-
-        if self.tb_writer is not None:
-            self.tb_writer.add_text("args", args.to_json_string())
-            if "model" in kwargs:
-                model = kwargs["model"]
-                if hasattr(model, "config") and model.config is not None:
-                    model_config_json = model.config.to_json_string()
-                    self.tb_writer.add_text("model_config", model_config_json)
-            # Version of TensorBoard coming from tensorboardX does not have this method.
-            if hasattr(self.tb_writer, "add_hparams"):
-                self.tb_writer.add_hparams(args.to_sanitized_dict(), metric_dict={})
-
-    def on_log(self, args, state, control, logs=None, **kwargs):
-        if not state.is_world_process_zero:
-            return
-
-        if self.tb_writer is None:
-            self._init_summary_writer(args)
-
-        if self.tb_writer is not None:
-            logs = rewrite_logs(logs)
-            for k, v in logs.items():
-                if isinstance(v, (int, float)):
-                    self.tb_writer.add_scalar(k, v, state.global_step)
-                else:
-                    logger.warning(
-                        "Trainer is attempting to log a value of "
-                        f'"{v}" of type {type(v)} for key "{k}" as a scalar. '
-                        "This invocation of Tensorboard's writer.add_scalar() "
-                        "is incorrect so we dropped this attribute."
-                    )
-            self.tb_writer.flush()
+        embed.mul_(self.emb_scale)
 
-    def on_train_end(self, args, state, control, **kwargs):
-        if self.tb_writer:
-            self.tb_writer.close()
-            self.tb_writer = None
+        return embed
 
 
-class WandbCallback(TrainerCallback):
+class TransfoXLPreTrainedModel(PreTrainedModel):
     """
-    A [`TrainerCallback`] that sends the logs to [Weight and Biases](https://www.wandb.com/).
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
     """
 
-    def __init__(self):
-        has_wandb = is_wandb_available()
-        if not has_wandb:
-            raise RuntimeError(
-                "WandbCallback requires wandb to be installed. Run `pip install wandb`."
-            )
-        if has_wandb:
-            import wandb
-
-            self._wandb = wandb
-        self._initialized = False
-        # log outputs
-        self._log_model = os.getenv(
-            "WANDB_LOG_MODEL", "FALSE"
-        ).upper() in ENV_VARS_TRUE_VALUES.union({"TRUE"})
+    config_class = TransfoXLConfig
+    load_tf_weights = load_tf_weights_in_transfo_xl
+    base_model_prefix = "transformer"
+
+    def _init_weight(self, weight):
+        if self.config.init == "uniform":
+            nn.init.uniform_(weight, -self.config.init_range, self.config.init_range)
+        elif self.config.init == "normal":
+            nn.init.normal_(weight, 0.0, self.config.init_std)
+
+    def _init_bias(self, bias):
+        nn.init.constant_(bias, 0.0)
+
+    def _init_weights(self, m):
+        """Initialize the weights."""
+        classname = m.__class__.__name__
+        if classname.find("Linear") != -1:
+            if hasattr(m, "weight") and m.weight is not None:
+                self._init_weight(m.weight)
+            if hasattr(m, "bias") and m.bias is not None:
+                self._init_bias(m.bias)
+        elif classname.find("AdaptiveEmbedding") != -1:
+            if hasattr(m, "emb_projs"):
+                for i in range(len(m.emb_projs)):
+                    if m.emb_projs[i] is not None:
+                        nn.init.normal_(m.emb_projs[i], 0.0, self.config.proj_init_std)
+        elif classname.find("Embedding") != -1:
+            if hasattr(m, "weight"):
+                self._init_weight(m.weight)
+        elif classname.find("ProjectedAdaptiveLogSoftmax") != -1:
+            if hasattr(m, "cluster_weight") and m.cluster_weight is not None:
+                self._init_weight(m.cluster_weight)
+            if hasattr(m, "cluster_bias") and m.cluster_bias is not None:
+                self._init_bias(m.cluster_bias)
+            if hasattr(m, "out_projs"):
+                for i in range(len(m.out_projs)):
+                    if m.out_projs[i] is not None:
+                        nn.init.normal_(m.out_projs[i], 0.0, self.config.proj_init_std)
+        elif classname.find("LayerNorm") != -1:
+            if hasattr(m, "weight"):
+                nn.init.normal_(m.weight, 1.0, self.config.init_std)
+            if hasattr(m, "bias") and m.bias is not None:
+                self._init_bias(m.bias)
+        else:
+            if hasattr(m, "r_emb"):
+                self._init_weight(m.r_emb)
+            if hasattr(m, "r_w_bias"):
+                self._init_weight(m.r_w_bias)
+            if hasattr(m, "r_r_bias"):
+                self._init_weight(m.r_r_bias)
+            if hasattr(m, "r_bias"):
+                self._init_bias(m.r_bias)
 
-    def setup(self, args, state, model, **kwargs):
+    def resize_token_embeddings(
+        self, new_num_tokens: Optional[int] = None, layer: Optional[int] = -1
+    ):
         """
-        Setup the optional Weights & Biases (*wandb*) integration.
+        Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size. Take care of tying
+        weights embeddings afterwards if the model class has a *tie_weights()* method.
 
-        One can subclass and override this method to customize the setup if needed. Find more information
-        [here](https://docs.wandb.ai/integrations/huggingface). You can also override the following environment
-        variables:
-
-        Environment:
-            WANDB_LOG_MODEL (`bool`, *optional*, defaults to `False`):
-                Whether or not to log model as artifact at the end of training. Use along with
-                *TrainingArguments.load_best_model_at_end* to upload best model.
-            WANDB_WATCH (`str`, *optional* defaults to `"gradients"`):
-                Can be `"gradients"`, `"all"` or `"false"`. Set to `"false"` to disable gradient logging or `"all"` to
-                log gradients and parameters.
-            WANDB_PROJECT (`str`, *optional*, defaults to `"huggingface"`):
-                Set this to a custom string to store results in a different project.
-            WANDB_DISABLED (`bool`, *optional*, defaults to `False`):
-                Whether or not to disable wandb entirely. Set *WANDB_DISABLED=true* to disable.
+        Arguments:
+            new_num_tokens: (*optional*) int:
+                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at
+                the end. Reducing the size will remove vectors from the end. If not provided or None: does nothing and
+                just returns a pointer to the input tokens `torch.nn.Embeddings` Module of the model.
+            layer: (*optional*) int:
+                Layer of the *AdaptiveEmbedding* where the resizing should be done. Per default the last layer will be
+                resized. Be aware that when resizing other than the last layer, you have to ensure that the new
+                token(s) in the tokenizer are at the corresponding position.
+
+        Return: `torch.nn.Embeddings` Pointer to the input tokens Embeddings Module of the model
         """
-        if self._wandb is None:
-            return
-        self._initialized = True
-        if state.is_world_process_zero:
-            logger.info(
-                'Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"'
-            )
-            combined_dict = {**args.to_sanitized_dict()}
+        base_model = getattr(
+            self, self.base_model_prefix, self
+        )  # get the base model if needed
 
-            if hasattr(model, "config") and model.config is not None:
-                model_config = model.config.to_dict()
-                combined_dict = {**model_config, **combined_dict}
-            trial_name = state.trial_name
-            init_args = {}
-            if trial_name is not None:
-                run_name = trial_name
-                init_args["group"] = args.run_name
-            else:
-                run_name = args.run_name
+        if new_num_tokens is None:
+            return self.get_input_embeddings()
 
-            if self._wandb.run is None:
-                self._wandb.init(
-                    project=os.getenv("WANDB_PROJECT", "huggingface"),
-                    name=run_name,
-                    **init_args,
-                )
-            # add config parameters (run may have been created manually)
-            self._wandb.config.update(combined_dict, allow_val_change=True)
+        new_num_tokens_layer, layer = self._get_new_num_tokens_layer(
+            new_num_tokens, layer
+        )
+        assert (
+            new_num_tokens_layer > 0
+        ), "The size of the new embedding layer cannot be 0 or less"
+        model_embeds = base_model._resize_token_embeddings(new_num_tokens_layer, layer)
+
+        # Update base model and current model config
+        self.config.vocab_size = new_num_tokens
+        base_model.vocab_size = new_num_tokens
+        base_model.n_token = new_num_tokens
+
+        new_embedding_shapes = self._get_embedding_shapes()
+        self._resize_cutoffs(
+            new_num_tokens, new_num_tokens_layer, new_embedding_shapes, layer
+        )
 
-            # define default x-axis (for latest wandb versions)
-            if getattr(self._wandb, "define_metric", None):
-                self._wandb.define_metric("train/global_step")
-                self._wandb.define_metric(
-                    "*", step_metric="train/global_step", step_sync=True
-                )
+        # Tie weights again if needed
+        self.tie_weights()
 
-            # keep track of model topology and gradients, unsupported on TPU
-            if not is_torch_tpu_available() and os.getenv("WANDB_WATCH") != "false":
-                self._wandb.watch(
-                    model,
-                    log=os.getenv("WANDB_WATCH", "gradients"),
-                    log_freq=max(100, args.logging_steps),
-                )
+        return model_embeds
 
-    def on_train_begin(self, args, state, control, model=None, **kwargs):
-        if self._wandb is None:
-            return
-        hp_search = state.is_hyper_param_search
-        if hp_search:
-            self._wandb.finish()
-            self._initialized = False
-            args.run_name = None
-        if not self._initialized:
-            self.setup(args, state, model, **kwargs)
-
-    def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwargs):
-        if self._wandb is None:
-            return
-        if self._log_model and self._initialized and state.is_world_process_zero:
-            from .trainer import Trainer
-
-            fake_trainer = Trainer(args=args, model=model, tokenizer=tokenizer)
-            with tempfile.TemporaryDirectory() as temp_dir:
-                fake_trainer.save_model(temp_dir)
-                metadata = (
-                    {
-                        k: v
-                        for k, v in dict(self._wandb.summary).items()
-                        if isinstance(v, numbers.Number) and not k.startswith("_")
-                    }
-                    if not args.load_best_model_at_end
-                    else {
-                        f"eval/{args.metric_for_best_model}": state.best_metric,
-                        "train/total_floss": state.total_flos,
-                    }
-                )
-                artifact = self._wandb.Artifact(
-                    name=f"model-{self._wandb.run.id}", type="model", metadata=metadata
-                )
-                for f in Path(temp_dir).glob("*"):
-                    if f.is_file():
-                        with artifact.new_file(f.name, mode="wb") as fa:
-                            fa.write(f.read_bytes())
-                self._wandb.run.log_artifact(artifact)
-
-    def on_log(self, args, state, control, model=None, logs=None, **kwargs):
-        if self._wandb is None:
-            return
-        if not self._initialized:
-            self.setup(args, state, model)
-        if state.is_world_process_zero:
-            logs = rewrite_logs(logs)
-            self._wandb.log({**logs, "train/global_step": state.global_step})
+    def _get_new_num_tokens_layer(self, new_num_tokens, layer):
+        embeddings = self.get_input_embeddings()
+        if layer == -1:
+            layer = len(embeddings.emb_layers) - 1
+        assert 0 <= layer <= len(embeddings.emb_layers) - 1
+
+        new_num_tokens_layer = (
+            new_num_tokens
+            - sum([emb.weight.shape[0] for emb in embeddings.emb_layers[:layer]])
+            - sum([emb.weight.shape[0] for emb in embeddings.emb_layers[layer + 1 :]])
+        )
+        return new_num_tokens_layer, layer
 
+    def _get_embedding_shapes(self):
+        embeddings = self.get_input_embeddings()
+        return [emb.weight.shape[0] for emb in embeddings.emb_layers]
+
+    def _resize_token_embeddings(self, new_num_tokens, layer=-1):
+        embeddings = self.get_input_embeddings()
+        if new_num_tokens is None:
+            return embeddings
+        new_embeddings_layer = self._get_resized_embeddings(
+            embeddings.emb_layers[layer], new_num_tokens
+        )
+        embeddings.emb_layers[layer] = new_embeddings_layer
 
-class CometCallback(TrainerCallback):
-    """
-    A [`TrainerCallback`] that sends the logs to [Comet ML](https://www.comet.ml/site/).
-    """
+        self.set_input_embeddings(embeddings)
 
-    def __init__(self):
-        if not _has_comet:
-            raise RuntimeError(
-                "CometCallback requires comet-ml to be installed. Run `pip install comet-ml`."
-            )
-        self._initialized = False
-        self._log_assets = False
+        return self.get_input_embeddings()
 
-    def setup(self, args, state, model):
-        """
-        Setup the optional Comet.ml integration.
+    def _resize_cutoffs(
+        self, new_num_tokens, new_emb_size, new_embedding_shapes, layer
+    ):
+        embeddings = self.get_input_embeddings()
 
-        Environment:
-            COMET_MODE (`str`, *optional*):
-                Whether to create an online, offline experiment or disable Comet logging. Can be "OFFLINE", "ONLINE",
-                or "DISABLED". Defaults to "ONLINE".
-            COMET_PROJECT_NAME (`str`, *optional*):
-                Comet project name for experiments
-            COMET_OFFLINE_DIRECTORY (`str`, *optional*):
-                Folder to use for saving offline experiments when `COMET_MODE` is "OFFLINE"
-            COMET_LOG_ASSETS (`str`, *optional*):
-                Whether or not to log training assets (tf event logs, checkpoints, etc), to Comet. Can be "TRUE", or
-                "FALSE". Defaults to "TRUE".
+        for i in range(layer, len(embeddings.cutoffs)):
+            embeddings.cutoffs[i] = sum(new_embedding_shapes[: i + 1])
 
-        For a number of configurable items in the environment, see
-        [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).
-        """
-        self._initialized = True
-        log_assets = os.getenv("COMET_LOG_ASSETS", "FALSE").upper()
-        if log_assets in {"TRUE", "1"}:
-            self._log_assets = True
-        if state.is_world_process_zero:
-            comet_mode = os.getenv("COMET_MODE", "ONLINE").upper()
-            experiment = None
-            experiment_kwargs = {
-                "project_name": os.getenv("COMET_PROJECT_NAME", "huggingface")
-            }
-            if comet_mode == "ONLINE":
-                experiment = comet_ml.Experiment(**experiment_kwargs)
-                experiment.log_other("Created from", "transformers")
-                logger.info("Automatic Comet.ml online logging enabled")
-            elif comet_mode == "OFFLINE":
-                experiment_kwargs["offline_directory"] = os.getenv(
-                    "COMET_OFFLINE_DIRECTORY", "./"
-                )
-                experiment = comet_ml.OfflineExperiment(**experiment_kwargs)
-                experiment.log_other("Created from", "transformers")
-                logger.info(
-                    "Automatic Comet.ml offline logging enabled; use `comet upload` when finished"
-                )
-            if experiment is not None:
-                experiment._set_model_graph(model, framework="transformers")
-                experiment._log_parameters(
-                    args, prefix="args/", framework="transformers"
-                )
-                if hasattr(model, "config"):
-                    experiment._log_parameters(
-                        model.config, prefix="config/", framework="transformers"
-                    )
+        embeddings.cutoff_ends = [0] + embeddings.cutoffs
+        embeddings.n_token = new_num_tokens
 
-    def on_train_begin(self, args, state, control, model=None, **kwargs):
-        if not self._initialized:
-            self.setup(args, state, model)
-
-    def on_log(self, args, state, control, model=None, logs=None, **kwargs):
-        if not self._initialized:
-            self.setup(args, state, model)
-        if state.is_world_process_zero:
-            experiment = comet_ml.config.get_global_experiment()
-            if experiment is not None:
-                experiment._log_metrics(
-                    logs,
-                    step=state.global_step,
-                    epoch=state.epoch,
-                    framework="transformers",
-                )
+        self.config.cutoffs = embeddings.cutoffs[:-1]
 
-    def on_train_end(self, args, state, control, **kwargs):
-        if self._initialized and state.is_world_process_zero:
-            experiment = comet_ml.config.get_global_experiment()
-            if experiment is not None:
-                if self._log_assets is True:
-                    logger.info("Logging checkpoints. This may take time.")
-                    experiment.log_asset_folder(
-                        args.output_dir,
-                        recursive=True,
-                        log_file_name=True,
-                        step=state.global_step,
-                    )
-                experiment.end()
+        return embeddings.cutoffs
 
 
-class AzureMLCallback(TrainerCallback):
+@dataclass
+class TransfoXLModelOutput(ModelOutput):
     """
-    A [`TrainerCallback`] that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).
-    """
-
-    def __init__(self, azureml_run=None):
-        if not is_azureml_available():
-            raise RuntimeError(
-                "AzureMLCallback requires azureml to be installed. Run `pip install azureml-sdk`."
-            )
-        self.azureml_run = azureml_run
+    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).
 
-    def on_init_end(self, args, state, control, **kwargs):
-        from azureml.core.run import Run
+    Args:
+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):
+            Sequence of hidden-states at the output of the last layer of the model.
+        mems (`List[torch.FloatTensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
 
-        if self.azureml_run is None and state.is_world_process_zero:
-            self.azureml_run = Run.get_context()
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+    """
 
-    def on_log(self, args, state, control, logs=None, **kwargs):
-        if self.azureml_run and state.is_world_process_zero:
-            for k, v in logs.items():
-                if isinstance(v, (int, float)):
-                    self.azureml_run.log(k, v, description=k)
+    last_hidden_state: torch.FloatTensor
+    mems: List[torch.FloatTensor] = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
 
 
-class MLflowCallback(TrainerCallback):
+@dataclass
+class TransfoXLSequenceClassifierOutputWithPast(ModelOutput):
     """
-    A [`TrainerCallback`] that sends the logs to [MLflow](https://www.mlflow.org/). Can be disabled by setting
-    environment variable `DISABLE_MLFLOW_INTEGRATION = TRUE`.
+    Base class for outputs of sentence classification models.
+
+    Args:
+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):
+            Classification (or regression if config.num_labels==1) loss.
+        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):
+            Classification (or regression if config.num_labels==1) scores (before SoftMax).
+        mems (`List[torch.FloatTensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
     """
 
-    def __init__(self):
-        if not is_mlflow_available():
-            raise RuntimeError(
-                "MLflowCallback requires mlflow to be installed. Run `pip install mlflow`."
-            )
-        import mlflow
+    loss: Optional[torch.FloatTensor] = None
+    logits: torch.FloatTensor = None
+    mems: List[torch.FloatTensor] = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
 
-        self._MAX_PARAM_VAL_LENGTH = mlflow.utils.validation.MAX_PARAM_VAL_LENGTH
-        self._MAX_PARAMS_TAGS_PER_BATCH = (
-            mlflow.utils.validation.MAX_PARAMS_TAGS_PER_BATCH
-        )
 
-        self._initialized = False
-        self._auto_end_run = False
-        self._log_artifacts = False
-        self._ml_flow = mlflow
+@dataclass
+class TransfoXLLMHeadModelOutput(ModelOutput):
+    """
+    Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).
 
-    def setup(self, args, state, model):
-        """
-        Setup the optional MLflow integration.
+    Args:
+        losses (`torch.FloatTensor` of shape *(batch_size, sequence_length-1)*, *optional*, returned when `labels` is provided):
+            Language modeling losses (not reduced).
+        prediction_scores (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):
+            Prediction scores of the language modeling head (scores for each vocabulary token after SoftMax).
+        mems (`List[torch.FloatTensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks). Can be used (see `mems`
+            input) to speed up sequential decoding. The token ids which have their past given to this model should not
+            be passed as input ids as they have already been computed.
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+        loss (`torch.FloatTensor` of shape `()`, *optional*, returned when `labels` is provided)
+            Reduced language modeling loss.
+    """
 
-        Environment:
-            HF_MLFLOW_LOG_ARTIFACTS (`str`, *optional*):
-                Whether to use MLflow .log_artifact() facility to log artifacts. This only makes sense if logging to a
-                remote server, e.g. s3 or GCS. If set to `True` or *1*, will copy each saved checkpoint on each save in
-                [`TrainingArguments`]'s `output_dir` to the local or remote artifact storage. Using it without a remote
-                storage will just copy the files to your artifact location.
-            MLFLOW_EXPERIMENT_NAME (`str`, *optional*):
-                Whether to use an MLflow experiment_name under which to launch the run. Default to "None" which will
-                point to the "Default" experiment in MLflow. Otherwise, it is a case sensitive name of the experiment
-                to be activated. If an experiment with this name does not exist, a new experiment with this name is
-                created.
-            MLFLOW_TAGS (`str`, *optional*):
-                A string dump of a dictionary of key/value pair to be added to the MLflow run as tags. Example:
-                os.environ['MLFLOW_TAGS']='{"release.candidate": "RC1", "release.version": "2.2.0"}'
-            MLFLOW_NESTED_RUN (`str`, *optional*):
-                Whether to use MLflow nested runs. If set to `True` or *1*, will create a nested run inside the current
-                run.
-            MLFLOW_RUN_ID (`str`, *optional*):
-                Allow to reattach to an existing run which can be usefull when resuming training from a checkpoint.
-                When MLFLOW_RUN_ID environment variable is set, start_run attempts to resume a run with the specified
-                run ID and other parameters are ignored.
-            MLFLOW_FLATTEN_PARAMS (`str`, *optional*):
-                Whether to flatten the parameters dictionary before logging. Default to `False`.
-        """
-        self._log_artifacts = (
-            os.getenv("HF_MLFLOW_LOG_ARTIFACTS", "FALSE").upper()
-            in ENV_VARS_TRUE_VALUES
-        )
-        self._nested_run = (
-            os.getenv("MLFLOW_NESTED_RUN", "FALSE").upper() in ENV_VARS_TRUE_VALUES
-        )
-        self._experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", None)
-        self._flatten_params = (
-            os.getenv("MLFLOW_FLATTEN_PARAMS", "FALSE").upper() in ENV_VARS_TRUE_VALUES
-        )
-        self._run_id = os.getenv("MLFLOW_RUN_ID", None)
-        logger.debug(
-            f"MLflow experiment_name={self._experiment_name}, run_name={args.run_name}, nested={self._nested_run},"
-            f" tags={self._nested_run}"
-        )
-        if state.is_world_process_zero:
-            if self._ml_flow.active_run() is None or self._nested_run or self._run_id:
-                if self._experiment_name:
-                    # Use of set_experiment() ensure that Experiment is created if not exists
-                    self._ml_flow.set_experiment(self._experiment_name)
-                self._ml_flow.start_run(run_name=args.run_name, nested=self._nested_run)
-                logger.debug(
-                    f"MLflow run started with run_id={self._ml_flow.active_run().info.run_id}"
-                )
-                self._auto_end_run = True
-            combined_dict = args.to_dict()
-            if hasattr(model, "config") and model.config is not None:
-                model_config = model.config.to_dict()
-                combined_dict = {**model_config, **combined_dict}
-            combined_dict = (
-                flatten_dict(combined_dict) if self._flatten_params else combined_dict
-            )
-            # remove params that are too long for MLflow
-            for name, value in list(combined_dict.items()):
-                # internally, all values are converted to str in MLflow
-                if len(str(value)) > self._MAX_PARAM_VAL_LENGTH:
-                    logger.warning(
-                        f'Trainer is attempting to log a value of "{value}" for key "{name}" as a parameter. MLflow\'s'
-                        " log_param() only accepts values no longer than 250 characters so we dropped this attribute."
-                        " You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and"
-                        " avoid this message."
-                    )
-                    del combined_dict[name]
-            # MLflow cannot log more than 100 values in one go, so we have to split it
-            combined_dict_items = list(combined_dict.items())
-            for i in range(
-                0, len(combined_dict_items), self._MAX_PARAMS_TAGS_PER_BATCH
-            ):
-                self._ml_flow.log_params(
-                    dict(combined_dict_items[i : i + self._MAX_PARAMS_TAGS_PER_BATCH])
-                )
-            mlflow_tags = os.getenv("MLFLOW_TAGS", None)
-            if mlflow_tags:
-                mlflow_tags = json.loads(mlflow_tags)
-                self._ml_flow.set_tags(mlflow_tags)
-        self._initialized = True
-
-    def on_train_begin(self, args, state, control, model=None, **kwargs):
-        if not self._initialized:
-            self.setup(args, state, model)
-
-    def on_log(self, args, state, control, logs, model=None, **kwargs):
-        if not self._initialized:
-            self.setup(args, state, model)
-        if state.is_world_process_zero:
-            metrics = {}
-            for k, v in logs.items():
-                if isinstance(v, (int, float)):
-                    metrics[k] = v
-                else:
-                    logger.warning(
-                        f'Trainer is attempting to log a value of "{v}" of type {type(v)} for key "{k}" as a metric. '
-                        "MLflow's log_metric() only accepts float and int types so we dropped this attribute."
-                    )
-            self._ml_flow.log_metrics(metrics=metrics, step=state.global_step)
+    losses: Optional[torch.FloatTensor] = None
+    prediction_scores: torch.FloatTensor = None
+    mems: List[torch.FloatTensor] = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
+    loss: Optional[torch.FloatTensor] = None
 
-    def on_train_end(self, args, state, control, **kwargs):
-        if self._initialized and state.is_world_process_zero:
-            if self._auto_end_run and self._ml_flow.active_run():
-                self._ml_flow.end_run()
-
-    def on_save(self, args, state, control, **kwargs):
-        if self._initialized and state.is_world_process_zero and self._log_artifacts:
-            ckpt_dir = f"checkpoint-{state.global_step}"
-            artifact_path = os.path.join(args.output_dir, ckpt_dir)
-            logger.info(
-                f"Logging checkpoint artifacts in {ckpt_dir}. This may take time."
-            )
-            self._ml_flow.pyfunc.log_model(
-                ckpt_dir,
-                artifacts={"model_path": artifact_path},
-                python_model=self._ml_flow.pyfunc.PythonModel(),
-            )
+    @property
+    def logits(self):
+        # prediction scores are the output of the adaptive softmax, see
+        # the file `modeling_transfo_xl_utilities`. Since the adaptive
+        # softmax returns the log softmax value, `self.prediction_scores`
+        # are strictly speaking not exactly `logits`, but behave the same
+        # way logits do.
+        return self.prediction_scores
+
+
+TRANSFO_XL_START_DOCSTRING = r"""
+
+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
+    etc.)
+
+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
+    and behavior.
+
+    Parameters:
+        config ([`TransfoXLConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
+"""
 
-    def __del__(self):
-        # if the previous run is not terminated correctly, the fluent API will
-        # not let you start a new run before the previous one is killed
-        if (
-            self._auto_end_run
-            and callable(getattr(self._ml_flow, "active_run", None))
-            and self._ml_flow.active_run() is not None
-        ):
-            self._ml_flow.end_run()
+TRANSFO_XL_INPUTS_DOCSTRING = r"""
+    Args:
+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+            Indices of input sequence tokens in the vocabulary.
 
+            Indices can be obtained using [`TransfoXLTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+            [`PreTrainedTokenizer.__call__`] for details.
 
-class NeptuneMissingConfiguration(Exception):
-    def __init__(self):
-        super().__init__(
-            """
-        ------ Unsupported ---- We were not able to create new runs. You provided a custom Neptune run to
-        `NeptuneCallback` with the `run` argument. For the integration to work fully, provide your `api_token` and
-        `project` by saving them as environment variables or passing them to the callback.
-        """
+            [What are input IDs?](../glossary#input-ids)
+        mems (`List[torch.FloatTensor]` of length `config.n_layers`):
+            Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (see
+            `mems` output below). Can be used to speed up sequential decoding. The token ids which have their mems
+            given to this model should not be passed as `input_ids` as they have already been computed.
+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
+
+            - 1 indicates the head is **not masked**,
+            - 0 indicates the head is **masked**.
+
+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):
+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the
+            model's internal embedding lookup matrix.
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
+            tensors for more detail.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+"""
+
+
+@add_start_docstrings(
+    "The bare Bert Model transformer outputting raw hidden-states without any specific head on top.",
+    TRANSFO_XL_START_DOCSTRING,
+)
+class TransfoXLModel(TransfoXLPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.n_token = config.vocab_size
+
+        self.d_embed = config.d_embed
+        self.d_model = config.d_model
+        self.n_head = config.n_head
+        self.d_head = config.d_head
+
+        self.word_emb = AdaptiveEmbedding(
+            config.vocab_size,
+            config.d_embed,
+            config.d_model,
+            config.cutoffs,
+            div_val=config.div_val,
         )
 
+        self.drop = nn.Dropout(config.dropout)
 
-class NeptuneCallback(TrainerCallback):
-    """TrainerCallback that sends the logs to [Neptune](https://neptune.ai).
+        self.n_layer = config.n_layer
+        self.mem_len = config.mem_len
+        self.attn_type = config.attn_type
+
+        if not config.untie_r:
+            self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
+            self.r_r_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
+
+        self.layers = nn.ModuleList()
+        if config.attn_type == 0:  # the default attention
+            for i in range(config.n_layer):
+                self.layers.append(
+                    RelPartialLearnableDecoderLayer(
+                        config.n_head,
+                        config.d_model,
+                        config.d_head,
+                        config.d_inner,
+                        config.dropout,
+                        dropatt=config.dropatt,
+                        pre_lnorm=config.pre_lnorm,
+                        r_w_bias=None if config.untie_r else self.r_w_bias,
+                        r_r_bias=None if config.untie_r else self.r_r_bias,
+                        layer_norm_epsilon=config.layer_norm_epsilon,
+                    )
+                )
+        else:  # learnable embeddings and absolute embeddings are not used in our pretrained checkpoints
+            raise NotImplementedError  # Removed them to avoid maintaining dead code
 
-    Args:
-        api_token (`str`, optional):
-            Neptune API token obtained upon registration. You can leave this argument out if you have saved your token
-            to the `NEPTUNE_API_TOKEN` environment variable (strongly recommended). See full setup instructions in the
-            [docs](https://docs.neptune.ai/getting-started/installation).
-        project (`str`, optional):
-            Name of an existing Neptune project, in the form: "workspace-name/project-name". You can find and copy the
-            name from the project Settings -> Properties in Neptune. If None (default), the value of the
-            `NEPTUNE_PROJECT` environment variable will be used.
-        name (`str`, optional): Custom name for the run.
-        base_namespace (`str`, optional, defaults to "finetuning"): In the Neptune run, the root namespace
-            that will contain all of the logged metadata.
-        log_parameters (`bool`, optional, defaults to True):
-            If True, logs all Trainer arguments and model parameters provided by the Trainer.
-        log_checkpoints (`str`, optional, defaults to None):
-            If "same", uploads checkpoints whenever they are saved by the Trainer. If "last", uploads only the most
-            recently saved checkpoint. If "best", uploads the best checkpoint (among the ones saved by the Trainer). If
-            None, does not upload checkpoints.
-        run (`Run`, optional):
-            Pass a Neptune run object if you want to continue logging to an existing run. Read more about resuming runs
-            in the [docs](https://docs.neptune.ai/how-to-guides/neptune-api/resume-run).
-        **neptune_run_kwargs (optional):
-            Additional keyword arguments to be passed directly to the
-            [neptune.init_run()](https://docs.neptune.ai/api-reference/neptune#.init_run) function when a new run is
-            created.
-    """
+        self.same_length = config.same_length
+        self.clamp_len = config.clamp_len
 
-    integration_version_key = "source_code/integrations/transformers"
-    model_parameters_key = "model_parameters"
-    trial_name_key = "trial"
-    trial_params_key = "trial_params"
-    trainer_parameters_key = "trainer_parameters"
-    flat_metrics = {"train/epoch"}
+        if self.attn_type == 0:  # default attention
+            self.pos_emb = PositionalEmbedding(self.d_model)
+        else:  # learnable embeddings and absolute embeddings
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.word_emb
+
+    def set_input_embeddings(self, new_embeddings):
+        self.word_emb = new_embeddings
+
+    def backward_compatible(self):
+        self.sample_softmax = -1
+
+    def reset_memory_length(self, mem_len):
+        self.mem_len = mem_len
+
+    def _prune_heads(self, heads):
+        logger.info("Head pruning is not implemented for Transformer-XL model")
+        pass
+
+    def init_mems(self, bsz):
+        if self.mem_len > 0:
+            mems = []
+            param = next(self.parameters())
+            for i in range(self.n_layer):
+                empty = torch.zeros(
+                    self.mem_len,
+                    bsz,
+                    self.config.d_model,
+                    dtype=param.dtype,
+                    device=param.device,
+                )
+                mems.append(empty)
 
-    def __init__(
+            return mems
+        else:
+            return None
+
+    def _update_mems(self, hids, mems, mlen, qlen):
+        # does not deal with None
+        if mems is None:
+            return None
+
+        # mems is not None
+        assert len(hids) == len(mems), "len(hids) != len(mems)"
+
+        # There are `mlen + qlen` steps that can be cached into mems
+        with torch.no_grad():
+            new_mems = []
+            end_idx = mlen + max(0, qlen)
+            beg_idx = max(0, end_idx - self.mem_len)
+            for i in range(len(hids)):
+                cat = torch.cat([mems[i], hids[i]], dim=0)
+                new_mems.append(cat[beg_idx:end_idx].detach())
+
+        return new_mems
+
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TransfoXLModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
         self,
-        *,
-        api_token: Optional[str] = None,
-        project: Optional[str] = None,
-        name: Optional[str] = None,
-        base_namespace: str = "finetuning",
-        run: Optional["Run"] = None,
-        log_parameters: bool = True,
-        log_checkpoints: Optional[str] = None,
-        **neptune_run_kwargs,
-    ):
-        if not is_neptune_available():
+        input_ids: Optional[torch.LongTensor] = None,
+        mems: Optional[List[torch.FloatTensor]] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, TransfoXLModelOutput]:
+        output_attentions = (
+            output_attentions
+            if output_attentions is not None
+            else self.config.output_attentions
+        )
+        output_hidden_states = (
+            output_hidden_states
+            if output_hidden_states is not None
+            else self.config.output_hidden_states
+        )
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        # the original code for Transformer-XL used shapes [len, bsz] but we want a unified interface in the library
+        # so we transpose here from shape [bsz, len] to shape [len, bsz]
+        if input_ids is not None and inputs_embeds is not None:
             raise ValueError(
-                "NeptuneCallback requires the Neptune client library to be installed. "
-                "To install the library, run `pip install neptune-client`."
+                "You cannot specify both input_ids and inputs_embeds at the same time"
             )
+        elif input_ids is not None:
+            input_ids = input_ids.transpose(0, 1).contiguous()
+            qlen, bsz = input_ids.size()
+        elif inputs_embeds is not None:
+            inputs_embeds = inputs_embeds.transpose(0, 1).contiguous()
+            qlen, bsz = inputs_embeds.shape[0], inputs_embeds.shape[1]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
+
+        if mems is None:
+            mems = self.init_mems(bsz)
 
-        from neptune.new.metadata_containers.run import Run
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads] (a head_mask for each layer)
+        # and head_mask is converted to shape [num_hidden_layers x qlen x klen x bsz x n_head]
+        if head_mask is not None:
+            if head_mask.dim() == 1:
+                head_mask = (
+                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(0).unsqueeze(0)
+                )
+                head_mask = head_mask.expand(self.n_layer, -1, -1, -1, -1)
+            elif head_mask.dim() == 2:
+                head_mask = head_mask.unsqueeze(1).unsqueeze(1).unsqueeze(1)
+            head_mask = head_mask.to(
+                dtype=next(self.parameters()).dtype
+            )  # switch to float if need + fp16 compatibility
+        else:
+            head_mask = [None] * self.n_layer
+
+        if inputs_embeds is not None:
+            word_emb = inputs_embeds
+        else:
+            word_emb = self.word_emb(input_ids)
 
-        try:
-            from neptune.new.integrations.utils import verify_type
-        except ImportError:
-            from neptune.new.internal.utils import verify_type
-
-        verify_type("api_token", api_token, (str, type(None)))
-        verify_type("project", project, (str, type(None)))
-        verify_type("name", name, (str, type(None)))
-        verify_type("base_namespace", base_namespace, str)
-        verify_type("run", run, (Run, type(None)))
-        verify_type("log_parameters", log_parameters, bool)
-        verify_type("log_checkpoints", log_checkpoints, (str, type(None)))
-
-        self._base_namespace_path = base_namespace
-        self._log_parameters = log_parameters
-        self._log_checkpoints = log_checkpoints
-        self._initial_run: Optional[Run] = run
-
-        self._run = None
-        self._is_monitoring_run = False
-        self._run_id = None
-        self._force_reset_monitoring_run = False
-        self._init_run_kwargs = {
-            "api_token": api_token,
-            "project": project,
-            "name": name,
-            **neptune_run_kwargs,
-        }
-
-        self._volatile_checkpoints_dir = None
-        self._should_upload_checkpoint = self._log_checkpoints is not None
-        self._recent_checkpoint_path = None
-
-        if self._log_checkpoints in {"last", "best"}:
-            self._target_checkpoints_namespace = f"checkpoints/{self._log_checkpoints}"
-            self._should_clean_recently_uploaded_checkpoint = True
-        else:
-            self._target_checkpoints_namespace = "checkpoints"
-            self._should_clean_recently_uploaded_checkpoint = False
-
-    def _stop_run_if_exists(self):
-        if self._run:
-            self._run.stop()
-            del self._run
-            self._run = None
-
-    def _initialize_run(self, **additional_neptune_kwargs):
-        from neptune.new import init_run
-        from neptune.new.exceptions import (
-            NeptuneMissingApiTokenException,
-            NeptuneMissingProjectNameException,
-        )
-
-        self._stop_run_if_exists()
-
-        try:
-            self._run = init_run(**self._init_run_kwargs, **additional_neptune_kwargs)
-            self._run_id = self._run["sys/id"].fetch()
-        except (
-            NeptuneMissingProjectNameException,
-            NeptuneMissingApiTokenException,
-        ) as e:
-            raise NeptuneMissingConfiguration() from e
-
-    def _use_initial_run(self):
-        self._run = self._initial_run
-        self._is_monitoring_run = True
-        self._run_id = self._run["sys/id"].fetch()
-        self._initial_run = None
-
-    def _ensure_run_with_monitoring(self):
-        if self._initial_run is not None:
-            self._use_initial_run()
-        else:
-            if not self._force_reset_monitoring_run and self._is_monitoring_run:
-                return
-
-            if (
-                self._run
-                and not self._is_monitoring_run
-                and not self._force_reset_monitoring_run
-            ):
-                self._initialize_run(run=self._run_id)
-                self._is_monitoring_run = True
+        mlen = mems[0].size(0) if mems is not None else 0
+        klen = mlen + qlen
+        if self.same_length:
+            all_ones = word_emb.new_ones((qlen, klen), dtype=torch.uint8)
+            mask_len = klen - self.mem_len
+            if mask_len > 0:
+                mask_shift_len = qlen - mask_len
             else:
-                self._initialize_run()
-                self._force_reset_monitoring_run = False
+                mask_shift_len = qlen
+            dec_attn_mask = (
+                torch.triu(all_ones, 1 + mlen) + torch.tril(all_ones, -mask_shift_len)
+            )[
+                :, :, None
+            ]  # -1
+        else:
+            dec_attn_mask = torch.triu(
+                word_emb.new_ones((qlen, klen), dtype=torch.uint8), diagonal=1 + mlen
+            )[:, :, None]
+
+        hids = []
+        attentions = [] if output_attentions else None
+        if self.attn_type == 0:  # default
+            pos_seq = torch.arange(
+                klen - 1, -1, -1.0, device=word_emb.device, dtype=word_emb.dtype
+            )
+            if self.clamp_len > 0:
+                pos_seq.clamp_(max=self.clamp_len)
+            pos_emb = self.pos_emb(pos_seq)
+
+            core_out = self.drop(word_emb)
+            pos_emb = self.drop(pos_emb)
+
+            for i, layer in enumerate(self.layers):
+                hids.append(core_out)
+                mems_i = None if mems is None else mems[i]
+                layer_outputs = layer(
+                    core_out,
+                    pos_emb,
+                    dec_attn_mask=dec_attn_mask,
+                    mems=mems_i,
+                    head_mask=head_mask[i],
+                    output_attentions=output_attentions,
+                )
+                core_out = layer_outputs[0]
+                if output_attentions:
+                    attentions.append(layer_outputs[1])
+        else:  # learnable embeddings and absolute embeddings
+            raise NotImplementedError  # Removed these to avoid maintaining dead code - They are not used in our pretrained checkpoint
+
+        core_out = self.drop(core_out)
+
+        new_mems = self._update_mems(hids, mems, mlen, qlen)
+
+        if output_hidden_states:
+            # Add last layer and transpose to library standard shape [bsz, len, hidden_dim]
+            hids.append(core_out)
+            hids = tuple(t.transpose(0, 1).contiguous() for t in hids)
+        else:
+            hids = None
+        if output_attentions:
+            # Transpose to library standard shape [bsz, n_heads, query_seq_len, key_seq_len]
+            attentions = tuple(t.permute(2, 3, 0, 1).contiguous() for t in attentions)
+        # We transpose back here to shape [bsz, len, hidden_dim]
+        core_out = core_out.transpose(0, 1).contiguous()
+
+        if not return_dict:
+            return tuple(
+                v for v in [core_out, new_mems, hids, attentions] if v is not None
+            )
+
+        return TransfoXLModelOutput(
+            last_hidden_state=core_out,
+            mems=new_mems,
+            hidden_states=hids,
+            attentions=attentions,
+        )
 
-    def _ensure_at_least_run_without_monitoring(self):
-        if self._initial_run is not None:
-            self._use_initial_run()
-        else:
-            if not self._run:
-                self._initialize_run(
-                    run=self._run_id,
-                    capture_stdout=False,
-                    capture_stderr=False,
-                    capture_hardware_metrics=False,
-                    capture_traceback=False,
-                )
-                self._is_monitoring_run = False
 
-    @property
-    def run(self):
-        if self._run is None:
-            self._ensure_at_least_run_without_monitoring()
-        return self._run
+@add_start_docstrings(
+    """
+    The Transformer-XL Model with a language modeling head on top (adaptive softmax with weights tied to the adaptive
+    input embeddings)
+    """,
+    TRANSFO_XL_START_DOCSTRING,
+)
+class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.transformer = TransfoXLModel(config)
+        self.sample_softmax = config.sample_softmax
+        self.trainer_compatible = getattr(config, "trainer_compatible", False)
+
+        if not self.trainer_compatible:
+            warnings.warn(
+                "The output of TransfoXL will be updated in v5 to support a single loss as first argument. In order"
+                "to use that updated output, please specify `trainer_compatible=True` as your configuration"
+                " attribute.",
+                DeprecationWarning,
+            )
+
+        assert self.sample_softmax <= 0, (
+            "Sampling from the softmax is not implemented yet. Please look at issue: #3310:"
+            " https://github.com/huggingface/transformers/issues/3310"
+        )
 
-    @property
-    def _metadata_namespace(self):
-        return self.run[self._base_namespace_path]
+        self.crit = ProjectedAdaptiveLogSoftmax(
+            config.vocab_size,
+            config.d_embed,
+            config.d_model,
+            config.cutoffs,
+            div_val=config.div_val,
+        )
 
-    def _log_integration_version(self):
-        self.run[NeptuneCallback.integration_version_key] = version
+        # Initialize weights and apply final processing
+        self.post_init()
 
-    def _log_trainer_parameters(self, args):
-        self._metadata_namespace[
-            NeptuneCallback.trainer_parameters_key
-        ] = args.to_sanitized_dict()
-
-    def _log_model_parameters(self, model):
-        if model and hasattr(model, "config") and model.config is not None:
-            self._metadata_namespace[
-                NeptuneCallback.model_parameters_key
-            ] = model.config.to_dict()
-
-    def _log_hyper_param_search_parameters(self, state):
-        if state and hasattr(state, "trial_name"):
-            self._metadata_namespace[NeptuneCallback.trial_name_key] = state.trial_name
-
-        if state and hasattr(state, "trial_params") and state.trial_params is not None:
-            self._metadata_namespace[
-                NeptuneCallback.trial_params_key
-            ] = state.trial_params
-
-    def _log_model_checkpoint(self, source_directory: str, checkpoint: str):
-        target_path = relative_path = os.path.join(source_directory, checkpoint)
-
-        if self._volatile_checkpoints_dir is not None:
-            consistent_checkpoint_path = os.path.join(
-                self._volatile_checkpoints_dir, checkpoint
-            )
-            try:
-                shutil.copytree(
-                    relative_path,
-                    os.path.join(consistent_checkpoint_path, relative_path),
-                )
-                target_path = consistent_checkpoint_path
-            except IOError as e:
-                logger.warning(
-                    "NeptuneCallback was unable to made a copy of checkpoint due to I/O exception: '{}'."
-                    "Could fail trying to upload.".format(e)
-                )
+    def tie_weights(self):
+        """
+        Run this to be sure output and input (adaptive) softmax weights are tied
+        """
 
-        self._metadata_namespace[self._target_checkpoints_namespace].upload_files(
-            target_path
-        )
+        if self.config.tie_word_embeddings:
+            for i in range(len(self.crit.out_layers)):
+                self._tie_or_clone_weights(
+                    self.crit.out_layers[i], self.transformer.word_emb.emb_layers[i]
+                )
+        if self.config.tie_projs:
+            for i, tie_proj in enumerate(self.config.tie_projs):
+                if (
+                    tie_proj
+                    and self.config.div_val == 1
+                    and self.config.d_model != self.config.d_embed
+                ):
+                    if self.config.torchscript:
+                        self.crit.out_projs[i] = nn.Parameter(
+                            self.transformer.word_emb.emb_projs[0].clone()
+                        )
+                    else:
+                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]
+                elif tie_proj and self.config.div_val != 1:
+                    if self.config.torchscript:
+                        self.crit.out_projs[i] = nn.Parameter(
+                            self.transformer.word_emb.emb_projs[i].clone()
+                        )
+                    else:
+                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]
 
-        if (
-            self._should_clean_recently_uploaded_checkpoint
-            and self._recent_checkpoint_path is not None
-        ):
-            self._metadata_namespace[self._target_checkpoints_namespace].delete_files(
-                self._recent_checkpoint_path
-            )
+    def reset_memory_length(self, mem_len):
+        self.transformer.reset_memory_length(mem_len)
 
-        self._recent_checkpoint_path = relative_path
+    def init_mems(self, bsz):
+        return self.transformer.init_mems(bsz)
 
-    def on_init_end(self, args, state, control, **kwargs):
-        self._volatile_checkpoints_dir = None
-        if self._log_checkpoints and (
-            args.overwrite_output_dir or args.save_total_limit is not None
-        ):
-            self._volatile_checkpoints_dir = tempfile.TemporaryDirectory().name
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TransfoXLLMHeadModelOutput,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.LongTensor] = None,
+        mems: Optional[List[torch.FloatTensor]] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, TransfoXLLMHeadModelOutput]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set
+            `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`
+            are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+        if input_ids is not None:
+            bsz, tgt_len = input_ids.size(0), input_ids.size(1)
+        elif inputs_embeds is not None:
+            bsz, tgt_len = inputs_embeds.size(0), inputs_embeds.size(1)
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
-        if self._log_checkpoints == "best" and not args.load_best_model_at_end:
-            raise ValueError(
-                "To save the best model checkpoint, the load_best_model_at_end argument must be enabled."
-            )
+        transformer_outputs = self.transformer(
+            input_ids,
+            mems=mems,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-    def on_train_begin(self, args, state, control, model=None, **kwargs):
-        if not state.is_world_process_zero:
-            return
-
-        self._ensure_run_with_monitoring()
-        self._force_reset_monitoring_run = True
-
-        self._log_integration_version()
-        if self._log_parameters:
-            self._log_trainer_parameters(args)
-            self._log_model_parameters(model)
-
-        if state.is_hyper_param_search:
-            self._log_hyper_param_search_parameters(state)
-
-    def on_train_end(self, args, state, control, **kwargs):
-        self._stop_run_if_exists()
-
-    def __del__(self):
-        if self._volatile_checkpoints_dir is not None:
-            shutil.rmtree(self._volatile_checkpoints_dir, ignore_errors=True)
-
-        self._stop_run_if_exists()
-
-    def on_save(self, args, state, control, **kwargs):
-        if self._should_upload_checkpoint:
-            self._log_model_checkpoint(
-                args.output_dir, f"checkpoint-{state.global_step}"
-            )
+        last_hidden = transformer_outputs[0]
+        pred_hid = last_hidden[:, -tgt_len:]
 
-    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
-        if self._log_checkpoints == "best":
-            best_metric_name = args.metric_for_best_model
-            if not best_metric_name.startswith("eval_"):
-                best_metric_name = f"eval_{best_metric_name}"
+        if labels is not None:
+            # Prevents all labels being -100 and throwing an error
+            # when backwarding the loss
+            miss_valid_label = labels[0, 1:].sum() == (labels.size(1) - 1) * -100
+            if miss_valid_label:
+                # Sets an <EOS> token, just to prevent loss from being NaN
+                labels[0, 1] = self.config.eos_token_id
+
+        softmax_output = self.crit(pred_hid, labels)
+        prediction_scores = (
+            softmax_output.view(bsz, tgt_len, -1) if labels is None else ()
+        )
 
-            metric_value = metrics.get(best_metric_name)
+        if labels is not None:
+            losses = softmax_output.view(bsz, tgt_len - 1)
+            # Avoids from incorporating padding (-100) tokens into loss value
+            loss = losses[losses != 0].mean()
+        else:
+            losses, loss = None, None
 
-            operator = np.greater if args.greater_is_better else np.less
+        if not return_dict:
+            if self.trainer_compatible:
+                output = (
+                    (prediction_scores, losses)
+                    if losses is not None
+                    else (prediction_scores,)
+                )
+                output += transformer_outputs[1:]
+                return ((loss,) + output) if loss is not None else output
+            else:
+                output = (prediction_scores, *transformer_outputs[1:])
+                output = ((losses,) + output) if losses is not None else output
+                return (output + (loss,)) if loss is not None else output
+
+        return TransfoXLLMHeadModelOutput(
+            loss=loss,
+            prediction_scores=prediction_scores,
+            losses=losses,
+            mems=transformer_outputs.mems,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
 
-            self._should_upload_checkpoint = state.best_metric is None or operator(
-                metric_value, state.best_metric
-            )
+    def get_output_embeddings(self):
+        """Double-check if you are using adaptive softmax."""
+        if self.sample_softmax > 0:
+            return self.out_layer
+        else:
+            return self.crit.out_layers[-1]
 
-    @classmethod
-    def get_run(cls, trainer):
-        for callback in trainer.callback_handler.callbacks:
-            if isinstance(callback, cls):
-                return callback.run
+    def prepare_inputs_for_generation(self, input_ids, past=None, **model_kwargs):
+        inputs = {}
 
-        raise Exception("The trainer doesn't have a NeptuneCallback configured.")
+        # if past is defined in model kwargs then use it for faster decoding
+        if past:
+            inputs["mems"] = past
+            inputs["input_ids"] = input_ids[:, -1].unsqueeze(-1)
+        else:
+            inputs["input_ids"] = input_ids
+
+        return inputs
 
-    def on_log(
-        self, args, state, control, logs: Optional[Dict[str, float]] = None, **kwargs
+    def _resize_cutoffs(
+        self, new_num_tokens, new_emb_size, new_embedding_shapes, layer
     ):
-        if not state.is_world_process_zero:
-            return
+        new_cutoffs = super()._resize_cutoffs(
+            new_num_tokens, new_emb_size, new_embedding_shapes, layer
+        )
 
-        if logs is not None:
-            for name, value in rewrite_logs(logs).items():
-                if isinstance(value, (int, float)):
-                    if name in NeptuneCallback.flat_metrics:
-                        self._metadata_namespace[name] = value
-                    else:
-                        self._metadata_namespace[name].log(
-                            value, step=state.global_step
-                        )
+        self.crit.cutoffs = new_cutoffs
+        self.crit.cutoff_ends = [0] + new_cutoffs
+        self.crit.n_token = new_num_tokens
+
+    @staticmethod
+    def _reorder_cache(
+        mems: List[torch.Tensor], beam_idx: torch.Tensor
+    ) -> List[torch.Tensor]:
+        """
+        This function is used to re-order the `mems` cache if [`~PreTrainedModel.beam_search`] or
+        [`~PreTrainedModel.beam_sample`] is called. This is required to match `mems` with the correct beam_idx at every
+        generation step.
+        """
+        return [
+            layer_past.index_select(1, beam_idx.to(layer_past.device))
+            for layer_past in mems
+        ]
 
 
-class CodeCarbonCallback(TrainerCallback):
-    """
-    A [`TrainerCallback`] that tracks the CO2 emission of training.
+@add_start_docstrings(
     """
+    The Transformer-XL Model transformer with a sequence classification head on top (linear layer).
 
-    def __init__(self):
-        if not is_codecarbon_available():
-            raise RuntimeError(
-                "CodeCarbonCallback requires `codecarbon` to be installed. Run `pip install codecarbon`."
-            )
-        import codecarbon
+    [`TransfoXLForSequenceClassification`] uses the last token in order to do the classification, as other causal
+    models (e.g. GPT-1) do.
 
-        self._codecarbon = codecarbon
-        self.tracker = None
+    Since it does classification on the last token, it requires to know the position of the last token. If a
+    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If
+    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
+    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in
+    each row of the batch).
+    """,
+    TRANSFO_XL_START_DOCSTRING,
+)
+class TransfoXLForSequenceClassification(TransfoXLPreTrainedModel):
+    _keys_to_ignore_on_load_missing = [r"h\.\d+\.attn\.masked_bias", r"lm_head.weight"]
 
-    def on_init_end(self, args, state, control, **kwargs):
-        if self.tracker is None and state.is_local_process_zero:
-            # CodeCarbon will automatically handle environment variables for configuration
-            self.tracker = self._codecarbon.EmissionsTracker(output_dir=args.output_dir)
-
-    def on_train_begin(self, args, state, control, model=None, **kwargs):
-        if self.tracker and state.is_local_process_zero:
-            self.tracker.start()
-
-    def on_train_end(self, args, state, control, **kwargs):
-        if self.tracker and state.is_local_process_zero:
-            self.tracker.stop()
-
-
-INTEGRATION_TO_CALLBACK = {
-    "azure_ml": AzureMLCallback,
-    "comet_ml": CometCallback,
-    "mlflow": MLflowCallback,
-    "neptune": NeptuneCallback,
-    "tensorboard": TensorBoardCallback,
-    "wandb": WandbCallback,
-    "codecarbon": CodeCarbonCallback,
-}
-
-
-def get_reporting_integration_callbacks(report_to):
-    for integration in report_to:
-        if integration not in INTEGRATION_TO_CALLBACK:
-            raise ValueError(
-                f"{integration} is not supported, only {', '.join(INTEGRATION_TO_CALLBACK.keys())} are supported."
-            )
-    return [INTEGRATION_TO_CALLBACK[integration] for integration in report_to]
+    def __init__(self, config):
+        super().__init__(config)
+        self.num_labels = config.num_labels
+        self.transformer = TransfoXLModel(config)
+        self.score = nn.Linear(config.d_embed, self.num_labels, bias=False)
+        # Initialize weights and apply final processing
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(TRANSFO_XL_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_TOKENIZER_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TransfoXLSequenceClassifierOutputWithPast,
+        config_class=_CONFIG_FOR_DOC,
+    )
+    def forward(
+        self,
+        input_ids: Optional[torch.LongTensor] = None,
+        mems: Optional[List[torch.FloatTensor]] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        labels: Optional[torch.LongTensor] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+    ) -> Union[Tuple, TransfoXLSequenceClassifierOutputWithPast]:
+        r"""
+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):
+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,
+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+        """
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        transformer_outputs = self.transformer(
+            input_ids,
+            mems=mems,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+        hidden_states = transformer_outputs[0]
+        logits = self.score(hidden_states)
+
+        if input_ids is not None:
+            batch_size, sequence_length = input_ids.shape[:2]
+        else:
+            batch_size, sequence_length = inputs_embeds.shape[:2]
+
+        assert (
+            self.config.pad_token_id is not None or batch_size == 1
+        ), "Cannot handle batch sizes > 1 if no padding token is defined."
+        if self.config.pad_token_id is None:
+            sequence_lengths = -1
+        else:
+            if input_ids is not None:
+                sequence_lengths = (
+                    torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1
+                )
+            else:
+                sequence_lengths = -1
+                logger.warning(
+                    f"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be "
+                    "unexpected if using padding tokens in conjunction with `inputs_embeds.`"
+                )
+
+        pooled_logits = logits[range(batch_size), sequence_lengths]
+
+        loss = None
+        if labels is not None:
+            if self.config.problem_type is None:
+                if self.num_labels == 1:
+                    self.config.problem_type = "regression"
+                elif self.num_labels > 1 and (
+                    labels.dtype == torch.long or labels.dtype == torch.int
+                ):
+                    self.config.problem_type = "single_label_classification"
+                else:
+                    self.config.problem_type = "multi_label_classification"
+
+            if self.config.problem_type == "regression":
+                loss_fct = MSELoss()
+                if self.num_labels == 1:
+                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())
+                else:
+                    loss = loss_fct(pooled_logits, labels)
+            elif self.config.problem_type == "single_label_classification":
+                loss_fct = CrossEntropyLoss()
+                loss = loss_fct(
+                    pooled_logits.view(-1, self.num_labels), labels.view(-1)
+                )
+            elif self.config.problem_type == "multi_label_classification":
+                loss_fct = BCEWithLogitsLoss()
+                loss = loss_fct(pooled_logits, labels)
+        if not return_dict:
+            output = (pooled_logits,) + transformer_outputs[1:]
+            return ((loss,) + output) if loss is not None else output
+
+        return TransfoXLSequenceClassifierOutputWithPast(
+            loss=loss,
+            logits=pooled_logits,
+            mems=transformer_outputs.mems,
+            hidden_states=transformer_outputs.hidden_states,
+            attentions=transformer_outputs.attentions,
+        )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/keras_callbacks.py` & `xs_transformers-1.0.1/xs_transformers/pipelines/automatic_speech_recognition.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,455 +1,486 @@
-import logging
-import os
-from pathlib import Path
-from time import sleep
-from typing import Callable, List, Optional, Union
+# Copyright 2021 The HuggingFace Team. All rights reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+import inspect
+from collections import defaultdict
+from typing import TYPE_CHECKING, Dict, Optional, Union
 
 import numpy as np
-import tensorflow as tf
-from huggingface_hub import Repository
-from packaging.version import parse
-from tensorflow.keras.callbacks import Callback
 
-from . import IntervalStrategy, PreTrainedTokenizerBase
-from .modelcard import TrainingSummary
-from .utils import get_full_repo_name
+from ..utils import is_torch_available, logging
+from .audio_utils import ffmpeg_read
+from .base import ChunkPipeline
+
+if TYPE_CHECKING:
+    from ...feature_extraction_sequence_utils import SequenceFeatureExtractor
+
+logger = logging.get_logger(__name__)
+
+if is_torch_available():
+    from ..models.auto.modeling_auto import (
+        MODEL_FOR_CTC_MAPPING,
+        MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING,
+    )
 
-logger = logging.getLogger(__name__)
 
+def rescale_stride(tokens_or_logits, stride, ratio):
+    """
+    Rescales the stride values from audio space to tokens/logits space.
 
-class KerasMetricCallback(Callback):
+    (160_000, 16_000, 16_000) -> (2000, 200, 200) for instance.
     """
-    Callback to compute metrics at the end of every epoch. Unlike normal Keras metrics, these do not need to be
-    compilable by TF. It is particularly useful for common NLP metrics like BLEU and ROUGE that require string
-    operations or generation loops that cannot be compiled. Predictions (or generations) will be computed on the
-    `eval_dataset` before being passed to the `metric_fn` in `np.ndarray` format. The `metric_fn` should compute
-    metrics and return a dict mapping metric names to metric values.
-
-    We provide an example of a suitable metric_fn that computes ROUGE scores for a summarization model below. Note that
-    this example skips some post-processing for readability and simplicity, and should probably not be used as-is!
-
-    ```py
-    from datasets import load_metric
-
-    rouge_metric = load_metric("rouge")
-
-
-    def rouge_fn(predictions, labels):
-        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
-        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
-        result = rouge_metric.compute(predictions=decoded_predictions, references=decoded_labels)
-        return {key: value.mid.fmeasure * 100 for key, value in result.items()}
-    ```
-
-    The above function will return a dict containing values which will be logged like any other Keras metric:
-
-    ```
-    {'rouge1': 37.4199, 'rouge2': 13.9768, 'rougeL': 34.361, 'rougeLsum': 35.0781
-    ```
-
-    Args:
-        metric_fn (`Callable`):
-            Metric function provided by the user. It will be called with two arguments - `predictions` and `labels`.
-            These contain the model's outputs and matching labels from the dataset. It should return a dict mapping
-            metric names to numerical values.
-        eval_dataset (`tf.data.Dataset` or `dict` or `tuple` or `np.ndarray` or `tf.Tensor`):
-            Validation data to be used to generate predictions for the `metric_fn`.
-        output_cols (`List[str], *optional*):
-            A list of columns to be retained from the model output as the predictions. Defaults to all.
-        label_cols ('`List[str]`, *optional*'):
-            A list of columns to be retained from the input dataset as the labels. Will be autodetected if this is not
-            supplied.
-        batch_size (`int`, *optional*):
-            Batch size. Only used when the data is not a pre-batched `tf.data.Dataset`.
-        predict_with_generate (`bool`, *optional*, defaults to `False`):
-            Whether we should use `model.generate()` to get outputs for the model.
-        use_xla_generation (`bool`, *optional*, defaults to `False`):
-            If we're generating, whether to compile model generation with XLA. This can massively increase the speed of
-            generation (up to 100X speedup) but will require a new XLA compilation for each input shape. When using XLA
-            generation, it's a good idea to pad your inputs to the same size, or to use the `pad_to_multiple_of`
-            argument in your `tokenizer` or `DataCollator`, which will reduce the number of unique input shapes and
-            save a lot of compilation time. This option has no effect is `predict_with_generate` is `False`.
-        generate_kwargs (`dict`, *optional*):
-            Keyword arguments to pass to `model.generate()` when generating. Has no effect if `predict_with_generate`
-            is `False`.
+    # Shape is [B, SEQ] for tokens
+    # [B, SEQ, V] for logits
+
+    new_strides = []
+    for input_n, left, right in stride:
+        token_n = int(round(input_n * ratio))
+        left = int(round(left / input_n * token_n))
+        right = int(round(right / input_n * token_n))
+        new_stride = (token_n, left, right)
+        new_strides.append(new_stride)
+
+    return new_strides
+
+
+def chunk_iter(inputs, feature_extractor, chunk_len, stride_left, stride_right):
+    inputs_len = inputs.shape[0]
+    step = chunk_len - stride_left - stride_right
+    for i in range(0, inputs_len, step):
+        # add start and end paddings to the chunk
+        chunk = inputs[i : i + chunk_len]
+        processed = feature_extractor(
+            chunk, sampling_rate=feature_extractor.sampling_rate, return_tensors="pt"
+        )
+        _stride_left = 0 if i == 0 else stride_left
+        is_last = i + step + stride_left >= inputs_len
+        _stride_right = 0 if is_last else stride_right
+        if chunk.shape[0] > _stride_left:
+            yield {
+                "is_last": is_last,
+                "stride": (chunk.shape[0], _stride_left, _stride_right),
+                **processed,
+            }
+
 
+class AutomaticSpeechRecognitionPipeline(ChunkPipeline):
+    """
+    Pipeline that aims at extracting spoken text contained within some audio.
+
+    The input can be either a raw waveform or a audio file. In case of the audio file, ffmpeg should be installed for
+    to support multiple audio formats
+
+    Arguments:
+        model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
+            The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
+            [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.
+        tokenizer ([`PreTrainedTokenizer`]):
+            The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from
+            [`PreTrainedTokenizer`].
+        feature_extractor ([`SequenceFeatureExtractor`]):
+            The feature extractor that will be used by the pipeline to encode waveform for the model.
+        chunk_length_s (`float`, *optional*, defaults to 0):
+            The input length for in each chunk. If `chunk_length_s = 0` then chunking is disabled (default). Only
+            available for CTC models, e.g. [`Wav2Vec2ForCTC`].
+
+            <Tip>
+
+            For more information on how to effectively use `chunk_length_s`, please have a look at the [ASR chunking
+            blog post](https://huggingface.co/blog/asr-chunking).
+
+            </Tip>
+
+        stride_length_s (`float`, *optional*, defaults to `chunk_length_s / 6`):
+            The length of stride on the left and right of each chunk. Used only with `chunk_length_s > 0`. This enables
+            the model to *see* more context and infer letters better than without this context but the pipeline
+            discards the stride bits at the end to make the final reconstitution as perfect as possible.
+
+            <Tip>
+
+            For more information on how to effectively use `stride_length_s`, please have a look at the [ASR chunking
+            blog post](https://huggingface.co/blog/asr-chunking).
+
+            </Tip>
+
+        framework (`str`, *optional*):
+            The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. The specified framework must be
+            installed. If no framework is specified, will default to the one currently installed. If no framework is
+            specified and both frameworks are installed, will default to the framework of the `model`, or to PyTorch if
+            no model is provided.
+        device (`int`, *optional*, defaults to -1):
+            Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
+            the associated CUDA device id.
+        decoder (`pyctcdecode.BeamSearchDecoderCTC`, *optional*):
+            [PyCTCDecode's
+            BeamSearchDecoderCTC](https://github.com/kensho-technologies/pyctcdecode/blob/2fd33dc37c4111417e08d89ccd23d28e9b308d19/pyctcdecode/decoder.py#L180)
+            can be passed for language model boosted decoding. See [`Wav2Vec2ProcessorWithLM`] for more information.
     """
 
     def __init__(
-        self,
-        metric_fn: Callable,
-        eval_dataset: Union[tf.data.Dataset, np.ndarray, tf.Tensor, tuple, dict],
-        output_cols: Optional[List[str]] = None,
-        label_cols: Optional[List[str]] = None,
-        batch_size: Optional[int] = None,
-        predict_with_generate: bool = False,
-        use_xla_generation: bool = False,
-        generate_kwargs: Optional[dict] = None,
+        self, feature_extractor: Union["SequenceFeatureExtractor", str], *args, **kwargs
     ):
-        super().__init__()
-        self.metric_fn = metric_fn
-        self.batch_size = batch_size
-        if not isinstance(eval_dataset, tf.data.Dataset):
-            if batch_size is None:
-                raise ValueError(
-                    "When passing data to KerasMetricCallback that is not a pre-batched tf.data.Dataset "
-                    "the batch_size argument must be set."
-                )
-            # Wrap a tf.data.Dataset around it
-            eval_dataset = tf.data.Dataset.from_tensor_slices(eval_dataset).batch(
-                batch_size, drop_remainder=False
-            )
-        self.eval_dataset = eval_dataset
-        self.predict_with_generate = predict_with_generate
-        self.output_cols = output_cols
-
-        # This next block attempts to parse out which elements of the dataset should be appended to the labels list
-        # that is passed to the metric_fn
-        if (
-            isinstance(eval_dataset.element_spec, tuple)
-            and len(eval_dataset.element_spec) == 2
+        super().__init__(*args, **kwargs)
+        self.feature_extractor = feature_extractor
+
+        if self.model.__class__ in MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING.values():
+            self.type = "seq2seq"
+        elif (
+            feature_extractor._processor_class
+            and feature_extractor._processor_class.endswith("WithLM")
+            and kwargs.get("decoder", None) is not None
         ):
-            input_spec, label_spec = eval_dataset.element_spec
-        else:
-            input_spec = eval_dataset.element_spec
-            label_spec = None
-        if label_cols is not None:
-            for label in label_cols:
-                if label not in input_spec:
-                    raise ValueError(
-                        f"Label {label} is in label_cols but could not be found in the dataset inputs!"
-                    )
-            self.label_cols = label_cols
-            self.use_keras_label = False
-        elif label_spec is not None:
-            # If the dataset inputs are split into a 2-tuple of inputs and labels,
-            # assume the second element is the labels
-            self.label_cols = None
-            self.use_keras_label = True
-        elif "labels" in input_spec:
-            self.label_cols = ["labels"]
-            self.use_keras_label = False
-            logging.warning(
-                "No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key."
-            )
-        elif "start_positions" in input_spec and "end_positions" in input_spec:
-            self.label_cols = ["start_positions", "end_positions"]
-            self.use_keras_label = False
-            logging.warning(
-                "No label_cols specified for KerasMetricCallback, assuming you want the "
-                "start_positions and end_positions keys."
-            )
+            self.decoder = kwargs["decoder"]
+            self.type = "ctc_with_lm"
         else:
+            self.type = "ctc"
+
+        if self.framework == "tf":
             raise ValueError(
-                "Could not autodetect label_cols for KerasMetricCallback, please specify them!"
+                "The AutomaticSpeechRecognitionPipeline is only available in PyTorch."
             )
-        if parse(tf.__version__) < parse("2.7"):
-            logging.warning(
-                "TF versions less than 2.7 may encounter issues with KerasMetricCallback!"
+
+        self.check_model_type(
+            dict(
+                MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING.items()
+                + MODEL_FOR_CTC_MAPPING.items()
             )
+        )
+
+    def __call__(
+        self,
+        inputs: Union[np.ndarray, bytes, str],
+        **kwargs,
+    ):
+        """
+        Classify the sequence(s) given as inputs. See the [`AutomaticSpeechRecognitionPipeline`] documentation for more
+        information.
+
+        Args:
+            inputs (`np.ndarray` or `bytes` or `str` or `dict`):
+                The inputs is either :
+                    - `str` that is the filename of the audio file, the file will be read at the correct sampling rate
+                      to get the waveform using *ffmpeg*. This requires *ffmpeg* to be installed on the system.
+                    - `bytes` it is supposed to be the content of an audio file and is interpreted by *ffmpeg* in the
+                      same way.
+                    - (`np.ndarray` of shape (n, ) of type `np.float32` or `np.float64`)
+                        Raw audio at the correct sampling rate (no further check will be done)
+                    - `dict` form can be used to pass raw audio sampled at arbitrary `sampling_rate` and let this
+                      pipeline do the resampling. The dict must be in the format `{"sampling_rate": int, "raw":
+                      np.array}` with optionally a `"stride": (left: int, right: int)` than can ask the pipeline to
+                      treat the first `left` samples and last `right` samples to be ignored in decoding (but used at
+                      inference to provide more context to the model). Only use `stride` with CTC models.
+            return_timestamps (*optional*, `str`):
+                Only available for pure CTC models. If set to `"char"`, the pipeline will return `timestamps` along the
+                text for every character in the text. For instance if you get `[{"text": "h", "timestamps": (0.5,0.6),
+                {"text": "i", "timestamps": (0.7, .9)}]`, then it means the model predicts that the letter "h" was
+                pronounced after `0.5` and before `0.6` seconds. If set to `"word"`, the pipeline will return
+                `timestamps` along the text for every word in the text. For instance if you get `[{"text": "hi ",
+                "timestamps": (0.5,0.9), {"text": "there", "timestamps": (1.0, .1.5)}]`, then it means the model
+                predicts that the word "hi" was pronounced after `0.5` and before `0.9` seconds.
+
+        Return:
+            `Dict`: A dictionary with the following keys:
+                - **text** (`str` ) -- The recognized text.
+                - **chunks** (*optional(, `List[Dict]`)
+                        When using `return_timestamps`, the `chunks` will become a list containing all the various text
+                        chunks identified by the model, *e.g.* `[{"text": "hi ", "timestamps": (0.5,0.9), {"text":
+                        "there", "timestamps": (1.0, 1.5)}]`. The original full text can roughly be recovered by doing
+                        `"".join(chunk["text"] for chunk in output["chunks"])`.
+        """
+        return super().__call__(inputs, **kwargs)
+
+    def _sanitize_parameters(self, **kwargs):
+        # No parameters on this pipeline right now
+        preprocess_params = {}
+        if "chunk_length_s" in kwargs:
+            preprocess_params["chunk_length_s"] = kwargs["chunk_length_s"]
+        if "stride_length_s" in kwargs:
+            preprocess_params["stride_length_s"] = kwargs["stride_length_s"]
+
+        postprocess_params = {}
+        if "decoder_kwargs" in kwargs:
+            postprocess_params["decoder_kwargs"] = kwargs["decoder_kwargs"]
+        if "return_timestamps" in kwargs:
+            postprocess_params["return_timestamps"] = kwargs["return_timestamps"]
+
+        return preprocess_params, {}, postprocess_params
+
+    def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):
+        if isinstance(inputs, str):
+            with open(inputs, "rb") as f:
+                inputs = f.read()
+
+        if isinstance(inputs, bytes):
+            inputs = ffmpeg_read(inputs, self.feature_extractor.sampling_rate)
+
+        stride = None
+        extra = {}
+        if isinstance(inputs, dict):
+            stride = inputs.pop("stride", None)
+            # Accepting `"array"` which is the key defined in `datasets` for
+            # better integration
+            if not (
+                "sampling_rate" in inputs and ("raw" in inputs or "array" in inputs)
+            ):
+                raise ValueError(
+                    "When passing a dictionnary to AutomaticSpeechRecognitionPipeline, the dict needs to contain a "
+                    '"raw" key containing the numpy array representing the audio and a "sampling_rate" key, '
+                    "containing the sampling_rate associated with that array"
+                )
 
-        self.use_xla_generation = use_xla_generation
-        self.generate_kwargs = {} if generate_kwargs is None else generate_kwargs
+            _inputs = inputs.pop("raw", None)
+            if _inputs is None:
+                _inputs = inputs.pop("array", None)
+            in_sampling_rate = inputs.pop("sampling_rate")
+            extra = inputs
+            inputs = _inputs
+            if in_sampling_rate != self.feature_extractor.sampling_rate:
+                import torch
+                from torchaudio import functional as F
+
+                inputs = F.resample(
+                    torch.from_numpy(inputs),
+                    in_sampling_rate,
+                    self.feature_extractor.sampling_rate,
+                ).numpy()
+                ratio = self.feature_extractor.sampling_rate / in_sampling_rate
+            else:
+                ratio = 1
+            if stride is not None:
+                if stride[0] + stride[1] > inputs.shape[0]:
+                    raise ValueError("Stride is too large for input")
+
+                # Stride needs to get the chunk length here, it's going to get
+                # swallowed by the `feature_extractor` later, and then batching
+                # can add extra data in the inputs, so we need to keep track
+                # of the original length in the stride so we can cut properly.
+                stride = (
+                    inputs.shape[0],
+                    int(round(stride[0] * ratio)),
+                    int(round(stride[1] * ratio)),
+                )
+        if not isinstance(inputs, np.ndarray):
+            raise ValueError(
+                f"We expect a numpy ndarray as input, got `{type(inputs)}`"
+            )
+        if len(inputs.shape) != 1:
+            raise ValueError(
+                "We expect a single channel audio input for AutomaticSpeechRecognitionPipeline"
+            )
 
-        self.generation_function = None
+        if chunk_length_s:
+            if self.type not in {"ctc", "ctc_with_lm"}:
+                raise ValueError(
+                    "`chunk_length_s` is only valid for CTC models, use other chunking options for other models"
+                )
+            if stride_length_s is None:
+                stride_length_s = chunk_length_s / 6
 
-    @staticmethod
-    def _concatenate_batches(batches, padding_index=-100):
-        # If all batches are unidimensional or same length, do a simple concatenation
-        if batches[0].ndim == 1 or all(
-            [batch.shape[1] == batches[0].shape[1] for batch in batches]
-        ):
-            return np.concatenate(batches, axis=0)
+            if isinstance(stride_length_s, (int, float)):
+                stride_length_s = [stride_length_s, stride_length_s]
 
-        # Welp, they're not the same length. Let's do some padding
-        max_len = max([batch.shape[1] for batch in batches])
-        num_samples = sum([batch.shape[0] for batch in batches])
-        output = np.full_like(
-            batches[0],
-            fill_value=padding_index,
-            shape=[num_samples, max_len] + list(batches[0].shape[2:]),
-        )
-        # i keeps track of which part of the concatenated array we're writing the next batch to
-        i = 0
-        for batch in batches:
-            output[i : i + len(batch), : batch.shape[1]] = batch
-            i += len(batch)
-        return output
-
-    def _postprocess_predictions_or_labels(self, inputs):
-        if isinstance(inputs[0], dict):
-            outputs = dict()
-            for key in inputs[0].keys():
-                outputs[key] = self._concatenate_batches(
-                    [batch[key] for batch in inputs]
+            # XXX: Carefuly, this variable will not exist in `seq2seq` setting.
+            # Currently chunking is not possible at this level for `seq2seq` so
+            # it's ok.
+            align_to = self.model.config.inputs_to_logits_ratio
+            chunk_len = int(
+                round(chunk_length_s * self.feature_extractor.sampling_rate / align_to)
+                * align_to
+            )
+            stride_left = int(
+                round(
+                    stride_length_s[0] * self.feature_extractor.sampling_rate / align_to
                 )
-            # If it's a dict with only one key, just return the array
-            if len(outputs) == 1:
-                outputs = list(outputs.values())[0]
-        elif isinstance(inputs[0], list) or isinstance(inputs[0], tuple):
-            outputs = []
-            for input_list in zip(*inputs):
-                outputs.append(self._concatenate_batches(input_list))
-            if len(outputs) == 1:
-                outputs = outputs[
-                    0
-                ]  # If it's a list with only one element, just return the array
-        elif isinstance(inputs[0], np.ndarray):
-            outputs = self._concatenate_batches(inputs)
-        elif isinstance(inputs[0], tf.Tensor):
-            outputs = self._concatenate_batches([tensor.numpy() for tensor in inputs])
-        else:
-            raise TypeError(f"Couldn't handle batch of type {type(inputs[0])}!")
-        return outputs
+                * align_to
+            )
+            stride_right = int(
+                round(
+                    stride_length_s[1] * self.feature_extractor.sampling_rate / align_to
+                )
+                * align_to
+            )
 
-    def on_epoch_end(self, epoch, logs=None):
-        if hasattr(self.model, "config"):
-            ignore_keys = getattr(self.model.config, "keys_to_ignore_at_inference", [])
-        else:
-            ignore_keys = []
+            if chunk_len < stride_left + stride_right:
+                raise ValueError("Chunk length must be superior to stride length")
 
-        main_input_name = None
-        if self.predict_with_generate:
-            # This dense conditional recognizes the case where we have an encoder-decoder model, but
-            # avoids getting tangled up when we just have a model with a layer called 'encoder'
-            if hasattr(self.model, "encoder") and hasattr(
-                self.model.encoder, "main_input_name"
+            # make sure that
+            for item in chunk_iter(
+                inputs, self.feature_extractor, chunk_len, stride_left, stride_right
             ):
-                if self.model.encoder.main_input_name != self.model.main_input_name:
-                    main_input_name = self.model.encoder.main_input_name
-            else:
-                main_input_name = getattr(self.model, "main_input_name", "input_ids")
+                yield item
+        else:
+            processed = self.feature_extractor(
+                inputs,
+                sampling_rate=self.feature_extractor.sampling_rate,
+                return_tensors="pt",
+            )
+            if stride is not None:
+                if self.model.__class__ in MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING.values():
+                    raise ValueError(
+                        "Stride is only usable with CTC models, try removing it"
+                    )
 
-            if self.use_xla_generation and self.generation_function is None:
+                processed["stride"] = stride
+            yield {"is_last": True, **processed, **extra}
 
-                def generation_function(inputs, attention_mask):
-                    return self.model.generate(
-                        inputs, attention_mask=attention_mask, **self.generate_kwargs
-                    )
+    def _forward(self, model_inputs):
+        is_last = model_inputs.pop("is_last")
+        if self.type == "seq2seq":
+            encoder = self.model.get_encoder()
+            # we need to pass `processed.get("attention_mask")` here since audio encoder
+            # attention mask  length is different from expected text decoder `encoder_attention_mask` length
+            # `generate` magic to create the mask automatically won't work, we basically need to help
+            # it here.
+            # Consume values so we can let extra information flow freely through
+            # the pipeline (important for `partial` in microphone)
+            if "input_features" in model_inputs:
+                inputs = model_inputs.pop("input_features")
+            elif "input_values" in model_inputs:
+                inputs = model_inputs.pop("input_values")
+            else:
+                raise ValueError(
+                    "Seq2Seq speech recognition model requires either a "
+                    f"`input_features` or `input_values` key, but only has {model_inputs.keys()}"
+                )
 
-                self.generation_function = tf.function(
-                    generation_function, jit_compile=True
+            accepts_attention_mask = "attention_mask" in set(
+                inspect.signature(encoder.forward).parameters.keys()
+            )
+            if accepts_attention_mask:
+                attention_mask = model_inputs.pop("attention_mask", None)
+                tokens = self.model.generate(
+                    encoder_outputs=encoder(inputs, attention_mask=attention_mask),
+                    attention_mask=attention_mask,
                 )
+            else:
+                tokens = self.model.generate(inputs)
 
-        prediction_list = []
-        label_list = []
+            out = {"tokens": tokens}
 
-        # The whole predict/generate loop is handled inside this method
-        for batch in self.eval_dataset:
-            if isinstance(batch, tuple):
-                batch, labels = batch
-            else:
-                labels = None
-            if self.predict_with_generate:
-                if isinstance(batch, dict):
-                    generation_inputs = batch[main_input_name]
-                    attention_mask = batch.get("attention_mask", None)
-                else:
-                    generation_inputs = batch
-                    attention_mask = None
-                if self.use_xla_generation:
-                    predictions = self.generation_function(
-                        generation_inputs, attention_mask=attention_mask
-                    )
-                else:
-                    predictions = self.model.generate(
-                        generation_inputs, attention_mask=attention_mask
-                    )
+        else:
+            stride = model_inputs.pop("stride", None)
+            input_values = model_inputs.pop("input_values")
+            attention_mask = model_inputs.pop("attention_mask", None)
+            outputs = self.model(
+                input_values=input_values, attention_mask=attention_mask
+            )
+            logits = outputs.logits
+
+            if self.type == "ctc_with_lm":
+                out = {"logits": logits}
             else:
-                predictions = self.model.predict_on_batch(batch)
-                if isinstance(predictions, dict):
-                    # This converts any dict-subclass to a regular dict
-                    # Keras REALLY doesn't like it when we pass around a BatchEncoding or other derived class
-                    predictions = dict(predictions)
-                if self.output_cols is not None:
-                    predictions = {key: predictions[key] for key in self.output_cols}
+                out = {"tokens": logits.argmax(dim=-1)}
+            if stride is not None:
+                # Send stride to `postprocess`.
+                # it needs to be handled there where
+                # the pieces are to be concatenated.
+                ratio = 1 / self.model.config.inputs_to_logits_ratio
+                if isinstance(stride, tuple):
+                    out["stride"] = rescale_stride(logits, [stride], ratio)[0]
                 else:
-                    predictions = {
-                        key: val
-                        for key, val in predictions.items()
-                        if key not in ignore_keys + ["loss"]
-                    }
-            prediction_list.append(predictions)
-            if not self.use_keras_label:
-                labels = {key: batch[key].numpy() for key in self.label_cols}
-            elif isinstance(labels, dict):
-                labels = {key: array.numpy() for key, array in labels.items()}
-            elif isinstance(labels, list) or isinstance(labels, tuple):
-                labels = [array.numpy() for array in labels]
-            elif isinstance(labels, tf.Tensor):
-                labels = labels.numpy()
-            else:
-                raise TypeError(f"Confused by labels of type {type(labels)}")
-            label_list.append(labels)
+                    out["stride"] = rescale_stride(logits, stride, ratio)
+        # Leftover
+        extra = model_inputs
+        return {"is_last": is_last, **out, **extra}
 
-        all_preds = self._postprocess_predictions_or_labels(prediction_list)
-        all_labels = self._postprocess_predictions_or_labels(label_list)
+    def postprocess(
+        self,
+        model_outputs,
+        decoder_kwargs: Optional[Dict] = None,
+        return_timestamps=None,
+    ):
+        # Optional return types
+        optional = {}
 
-        metric_output = self.metric_fn((all_preds, all_labels))
-        if not isinstance(metric_output, dict):
-            raise TypeError(
-                f"metric_fn should return a dict mapping metric names to values but instead returned {metric_output}"
+        if return_timestamps and self.type == "seq2seq":
+            raise ValueError("We cannot return_timestamps yet on non-ctc models !")
+        if return_timestamps == "char" and self.type == "ctc_with_lm":
+            raise ValueError(
+                "CTC with LM cannot return `char` timestamps, only `words`"
             )
-        # This is the critical bit - Keras passes a dict containing the loss and standard metric values for this epoch
-        # in the logs argument. Ordinarily, this is so the callback can read them, but in this case we write a bunch of
-        # new keys in there, which will then get read by the History callback and treated like any other metric value.
-        # I promise that I have it in writing from Chollet that this is okay.
-        logs.update(metric_output)
 
+        final_items = []
+        key = "logits" if self.type == "ctc_with_lm" else "tokens"
+        for outputs in model_outputs:
+            items = outputs[key].numpy()
+            stride = outputs.pop("stride", None)
+            if stride is not None:
+                total_n, left, right = stride
+                # Total_n might be < logits.shape[1]
+                # because of padding, that's why
+                # we need to reconstruct this information
+                # This won't work with left padding (which doesn't exist right now)
+                right_n = total_n - right
+                items = items[:, left:right_n]
+            final_items.append(items)
+        items = np.concatenate(final_items, axis=1)
+        items = items.squeeze(0)
+        if self.type == "ctc_with_lm":
+            if decoder_kwargs is None:
+                decoder_kwargs = {}
+            beams = self.decoder.decode_beams(items, **decoder_kwargs)
+            text = beams[0][0]
+            if return_timestamps:
+                # Simply cast from pyctcdecode format to wav2vec2 format to leverage
+                # pre-existing code later
+                chunk_offset = beams[0][2]
+                word_offsets = []
+                for word, (start_offset, end_offset) in chunk_offset:
+                    word_offsets.append(
+                        {
+                            "word": word,
+                            "start_offset": start_offset,
+                            "end_offset": end_offset,
+                        }
+                    )
 
-class PushToHubCallback(Callback):
-    """
-    Callback that will save and push the model to the Hub regularly. By default, it pushes once per epoch, but this can
-    be changed with the `save_strategy` argument. Pushed models can be accessed like any other model on the hub, such
-    as with the `from_pretrained` method.
-
-    ```py
-    from transformers.keras_callbacks import PushToHubCallback
-
-    push_to_hub_callback = PushToHubCallback(
-        output_dir="./model_save",
-        tokenizer=tokenizer,
-        hub_model_id="gpt5-7xlarge",
-    )
+        else:
+            skip_special_tokens = self.type != "ctc"
+            text = self.tokenizer.decode(items, skip_special_tokens=skip_special_tokens)
+            if return_timestamps:
+                char_offsets = self.tokenizer.decode(
+                    items,
+                    skip_special_tokens=skip_special_tokens,
+                    output_char_offsets=True,
+                )["char_offsets"]
+                if return_timestamps == "word":
+                    word_offsets = self.tokenizer._get_word_offsets(
+                        char_offsets, self.tokenizer.replace_word_delimiter_char
+                    )
 
-    model.fit(train_dataset, callbacks=[push_to_hub_callback])
-    ```
+        if return_timestamps:
+            if return_timestamps == "word":
+                offsets = word_offsets
+            else:
+                offsets = char_offsets
+            chunks = []
+            for item in offsets:
+                start = item["start_offset"] * self.model.config.inputs_to_logits_ratio
+                start /= self.feature_extractor.sampling_rate
 
-    Args:
-        output_dir (`str`):
-            The output directory where the model predictions and checkpoints will be written and synced with the
-            repository on the Hub.
-        save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"epoch"`):
-            The checkpoint save strategy to adopt during training. Possible values are:
-
-                - `"no"`: Save is done at the end of training.
-                - `"epoch"`: Save is done at the end of each epoch.
-                - `"steps"`: Save is done every `save_steps`
-        save_steps (`int`, *optional*):
-            The number of steps between saves when using the "steps" `save_strategy`.
-        tokenizer (`PreTrainedTokenizerBase`, *optional*):
-            The tokenizer used by the model. If supplied, will be uploaded to the repo alongside the weights.
-        hub_model_id (`str`, *optional*):
-            The name of the repository to keep in sync with the local `output_dir`. It can be a simple model ID in
-            which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,
-            for instance `"user_name/model"`, which allows you to push to an organization you are a member of with
-            `"organization_name/model"`.
-
-            Will default to the name of `output_dir`.
-        hub_token (`str`, *optional*):
-            The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with
-            `huggingface-cli login`.
-        checkpoint (`bool`, *optional*, defaults to `False`):
-            Whether to save full training checkpoints (including epoch and optimizer state) to allow training to be
-            resumed. Only usable when `save_strategy` is `"epoch"`.
-    """
+                stop = item["end_offset"] * self.model.config.inputs_to_logits_ratio
+                stop /= self.feature_extractor.sampling_rate
 
-    def __init__(
-        self,
-        output_dir: Union[str, Path],
-        save_strategy: Union[str, IntervalStrategy] = "epoch",
-        save_steps: Optional[int] = None,
-        tokenizer: Optional[PreTrainedTokenizerBase] = None,
-        hub_model_id: Optional[str] = None,
-        hub_token: Optional[str] = None,
-        checkpoint: bool = False,
-        **model_card_args,
-    ):
-        super().__init__()
-        if checkpoint and save_strategy != "epoch":
-            raise ValueError(
-                "Cannot save checkpoints when save_strategy is not 'epoch'!"
-            )
-        if isinstance(save_strategy, str):
-            save_strategy = IntervalStrategy(save_strategy.lower())
-        self.save_strategy = save_strategy
-        if self.save_strategy == IntervalStrategy.STEPS and (
-            not isinstance(save_steps, int) or save_steps <= 0
-        ):
-            raise ValueError(
-                "Please supply a positive integer argument for save_steps when save_strategy == 'steps'!"
-            )
-        self.save_steps = save_steps
-        output_dir = Path(output_dir)
-        if hub_model_id is None:
-            hub_model_id = output_dir.absolute().name
-        if "/" not in hub_model_id:
-            hub_model_id = get_full_repo_name(hub_model_id, token=hub_token)
-
-        self.output_dir = output_dir
-        self.hub_model_id = hub_model_id
-        self.repo = Repository(
-            str(self.output_dir),
-            clone_from=self.hub_model_id,
-            use_auth_token=hub_token if hub_token else True,
-        )
-        self.tokenizer = tokenizer
-        self.last_job = None
-        self.checkpoint = checkpoint
-        self.training_history = None
-        self.model_card_args = model_card_args
-
-    def on_train_begin(self, logs=None):
-        # Although we can access model.history, we have no guarantees that the History callback will fire before this
-        # one, so we keep track of it here too
-        self.training_history = []
-
-    def on_train_batch_end(self, batch, logs=None):
-        if (
-            self.save_strategy == IntervalStrategy.STEPS
-            and (batch + 1) % self.save_steps == 0
-        ):
-            if self.last_job is not None and not self.last_job.is_done:
-                return  # The last upload is still running, don't start another
-            self.model.save_pretrained(self.output_dir)
-            if self.tokenizer is not None:
-                self.tokenizer.save_pretrained(self.output_dir)
-            _, self.last_job = self.repo.push_to_hub(
-                commit_message=f"Training in progress steps {batch}", blocking=False
-            )
-
-    def on_epoch_end(self, epoch, logs=None):
-        logs = logs.copy()  # Don't accidentally write things that Keras will read later
-        if "epoch" not in logs:
-            logs["epoch"] = epoch
-        self.training_history.append(logs)
-        if self.save_strategy == IntervalStrategy.EPOCH:
-            if self.last_job is not None and not self.last_job.is_done:
-                return  # The last upload is still running, don't start another
-            self.model.save_pretrained(self.output_dir)
-            if self.tokenizer is not None:
-                self.tokenizer.save_pretrained(self.output_dir)
-            if self.checkpoint:
-                checkpoint_dir = os.path.join(self.output_dir, "checkpoint")
-                self.model._save_checkpoint(checkpoint_dir, epoch)
-            train_summary = TrainingSummary.from_keras(
-                model=self.model,
-                model_name=self.hub_model_id,
-                keras_history=self.training_history,
-                **self.model_card_args,
-            )
-            model_card = train_summary.to_model_card()
-            with (self.output_dir / "README.md").open("w") as f:
-                f.write(model_card)
-            _, self.last_job = self.repo.push_to_hub(
-                commit_message=f"Training in progress epoch {epoch}", blocking=False
-            )
-
-    def on_train_end(self, logs=None):
-        if self.last_job is not None and not self.last_job.is_done:
-            self.last_job._process.terminate()  # Gotta go fast
-            while not self.last_job.is_done:
-                sleep(1)
-        self.model.save_pretrained(self.output_dir)
-        if self.tokenizer is not None:
-            self.tokenizer.save_pretrained(self.output_dir)
-        train_summary = TrainingSummary.from_keras(
-            model=self.model,
-            model_name=self.hub_model_id,
-            keras_history=self.training_history,
-            **self.model_card_args,
-        )
-        model_card = train_summary.to_model_card()
-        with (self.output_dir / "README.md").open("w") as f:
-            f.write(model_card)
-        self.repo.push_to_hub(commit_message="End of training", blocking=True)
+                chunks.append(
+                    {"text": item[return_timestamps], "timestamp": (start, stop)}
+                )
+            optional["chunks"] = chunks
+
+        extra = defaultdict(list)
+        for output in model_outputs:
+            output.pop("tokens", None)
+            output.pop("logits", None)
+            output.pop("is_last", None)
+            for k, v in output.items():
+                extra[k].append(v)
+        return {"text": text, **optional, **extra}
```

### Comparing `xs_transformers-1.0.0/xs_transformers/modelcard.py` & `xs_transformers-1.0.1/xs_transformers/onnx/config.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,974 +1,886 @@
-# coding=utf-8
-# Copyright 2018 The HuggingFace Inc. team.
+# Copyright 2021 The HuggingFace Team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-""" Configuration base class and utilities."""
-
-
 import copy
-import json
-import os
+import dataclasses
 import warnings
-from dataclasses import dataclass
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Union
-
-import requests
-import yaml
-from huggingface_hub import model_info
-from huggingface_hub.utils import HFValidationError
-
-from . import __version__
-from .models.auto.modeling_auto import (
-    MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,
-    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,
-    MODEL_FOR_CTC_MAPPING_NAMES,
-    MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,
-    MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES,
-    MODEL_FOR_MASKED_LM_MAPPING_NAMES,
-    MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES,
-    MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES,
-    MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,
-    MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,
-    MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES,
-    MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES,
-    MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,
-)
-from .training_args import ParallelMode
-from .utils import (
-    MODEL_CARD_NAME,
-    cached_file,
-    is_datasets_available,
-    is_offline_mode,
-    is_tf_available,
-    is_tokenizers_available,
-    is_torch_available,
-    logging,
+from abc import ABC, abstractmethod
+from collections import OrderedDict
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    List,
+    Mapping,
+    Optional,
+    Tuple,
+    Union,
 )
 
-TASK_MAPPING = {
-    "text-generation": MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,
-    "image-classification": MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,
-    "image-segmentation": MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES,
-    "fill-mask": MODEL_FOR_MASKED_LM_MAPPING_NAMES,
-    "object-detection": MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES,
-    "question-answering": MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES,
-    "text2text-generation": MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES,
-    "text-classification": MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,
-    "table-question-answering": MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING_NAMES,
-    "token-classification": MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,
-    "audio-classification": MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,
-    "automatic-speech-recognition": {
-        **MODEL_FOR_CTC_MAPPING_NAMES,
-        **MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES,
-    },
-}
+import numpy as np
+from packaging import version
 
-logger = logging.get_logger(__name__)
+from ..utils import TensorType, is_torch_available, is_vision_available, logging
+from .utils import (
+    ParameterFormat,
+    compute_effective_axis_dimension,
+    compute_serialized_parameters_size,
+)
 
+if TYPE_CHECKING:
+    from ..configuration_utils import PretrainedConfig
+    from ..feature_extraction_utils import FeatureExtractionMixin
+    from ..tokenization_utils_base import PreTrainedTokenizerBase
 
-class ModelCard:
-    r"""
-    Structured Model Card class. Store model card as well as methods for loading/downloading/saving model cards.
-
-    Please read the following paper for details and explanation on the sections: "Model Cards for Model Reporting" by
-    Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,
-    Inioluwa Deborah Raji and Timnit Gebru for the proposal behind model cards. Link: https://arxiv.org/abs/1810.03993
 
-    Note: A model card can be loaded and saved to disk.
-    """
+if is_vision_available():
+    from PIL import Image
 
-    def __init__(self, **kwargs):
-        warnings.warn(
-            "The class `ModelCard` is deprecated and will be removed in version 5 of Transformers",
-            FutureWarning,
-        )
-        # Recommended attributes from https://arxiv.org/abs/1810.03993 (see papers)
-        self.model_details = kwargs.pop("model_details", {})
-        self.intended_use = kwargs.pop("intended_use", {})
-        self.factors = kwargs.pop("factors", {})
-        self.metrics = kwargs.pop("metrics", {})
-        self.evaluation_data = kwargs.pop("evaluation_data", {})
-        self.training_data = kwargs.pop("training_data", {})
-        self.quantitative_analyses = kwargs.pop("quantitative_analyses", {})
-        self.ethical_considerations = kwargs.pop("ethical_considerations", {})
-        self.caveats_and_recommendations = kwargs.pop("caveats_and_recommendations", {})
-
-        # Open additional attributes
-        for key, value in kwargs.items():
-            try:
-                setattr(self, key, value)
-            except AttributeError as err:
-                logger.error(f"Can't set {key} with value {value} for {self}")
-                raise err
-
-    def save_pretrained(self, save_directory_or_file):
-        """Save a model card object to the directory or file `save_directory_or_file`."""
-        if os.path.isdir(save_directory_or_file):
-            # If we save using the predefined names, we can load using `from_pretrained`
-            output_model_card_file = os.path.join(
-                save_directory_or_file, MODEL_CARD_NAME
-            )
-        else:
-            output_model_card_file = save_directory_or_file
+logger = logging.get_logger(__name__)
 
-        self.to_json_file(output_model_card_file)
-        logger.info(f"Model card saved in {output_model_card_file}")
 
-    @classmethod
-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
-        r"""
-        Instantiate a [`ModelCard`] from a pre-trained model model card.
-
-        Parameters:
-            pretrained_model_name_or_path: either:
-
-                - a string, the *model id* of a pretrained model card hosted inside a model repo on huggingface.co.
-                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a
-                  user or organization name, like `dbmdz/bert-base-german-cased`.
-                - a path to a *directory* containing a model card file saved using the [`~ModelCard.save_pretrained`]
-                  method, e.g.: `./my_model_directory/`.
-                - a path or url to a saved model card JSON *file*, e.g.: `./my_model_directory/modelcard.json`.
-
-            cache_dir: (*optional*) string:
-                Path to a directory in which a downloaded pre-trained model card should be cached if the standard cache
-                should not be used.
-
-            kwargs: (*optional*) dict: key/value pairs with which to update the ModelCard object after loading.
-
-                - The values in kwargs of any keys which are model card attributes will be used to override the loaded
-                  values.
-                - Behavior concerning key/value pairs whose keys are *not* model card attributes is controlled by the
-                  *return_unused_kwargs* keyword parameter.
-
-            proxies: (*optional*) dict, default None:
-                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128',
-                'http://hostname': 'foo.bar:4012'}. The proxies are used on each request.
-
-            return_unused_kwargs: (*optional*) bool:
-
-                - If False, then this function returns just the final model card object.
-                - If True, then this functions returns a tuple *(model card, unused_kwargs)* where *unused_kwargs* is a
-                  dictionary consisting of the key/value pairs whose keys are not model card attributes: ie the part of
-                  kwargs which has not been used to update *ModelCard* and is otherwise ignored.
-
-        Examples:
-
-        ```python
-        # Download model card from huggingface.co and cache.
-        modelcard = ModelCard.from_pretrained("bert-base-uncased")
-        # Model card was saved using *save_pretrained('./test/saved_model/')*
-        modelcard = ModelCard.from_pretrained("./test/saved_model/")
-        modelcard = ModelCard.from_pretrained("./test/saved_model/modelcard.json")
-        modelcard = ModelCard.from_pretrained("bert-base-uncased", output_attentions=True, foo=False)
-        ```"""
-        cache_dir = kwargs.pop("cache_dir", None)
-        proxies = kwargs.pop("proxies", None)
-        return_unused_kwargs = kwargs.pop("return_unused_kwargs", False)
-        from_pipeline = kwargs.pop("_from_pipeline", None)
-
-        user_agent = {"file_type": "model_card"}
-        if from_pipeline is not None:
-            user_agent["using_pipeline"] = from_pipeline
-
-        is_local = os.path.isdir(pretrained_model_name_or_path)
-        if os.path.isfile(pretrained_model_name_or_path):
-            resolved_model_card_file = pretrained_model_name_or_path
-            is_local = True
-        else:
-            try:
-                # Load from URL or cache if already cached
-                resolved_model_card_file = cached_file(
-                    pretrained_model_name_or_path,
-                    filename=MODEL_CARD_NAME,
-                    cache_dir=cache_dir,
-                    proxies=proxies,
-                    user_agent=user_agent,
-                )
-                if is_local:
-                    logger.info(f"loading model card file {resolved_model_card_file}")
-                else:
-                    logger.info(
-                        f"loading model card file {MODEL_CARD_NAME} from cache at {resolved_model_card_file}"
-                    )
-                # Load model card
-                modelcard = cls.from_json_file(resolved_model_card_file)
+DEFAULT_ONNX_OPSET = 11
 
-            except (EnvironmentError, json.JSONDecodeError):
-                # We fall back on creating an empty model card
-                modelcard = cls()
-
-        # Update model card with kwargs if needed
-        to_remove = []
-        for key, value in kwargs.items():
-            if hasattr(modelcard, key):
-                setattr(modelcard, key, value)
-                to_remove.append(key)
-        for key in to_remove:
-            kwargs.pop(key, None)
-
-        logger.info(f"Model card: {modelcard}")
-        if return_unused_kwargs:
-            return modelcard, kwargs
-        else:
-            return modelcard
+# 2 Gb
+EXTERNAL_DATA_FORMAT_SIZE_LIMIT = 2 * 1024 * 1024 * 1024
 
-    @classmethod
-    def from_dict(cls, json_object):
-        """Constructs a `ModelCard` from a Python dictionary of parameters."""
-        return cls(**json_object)
 
-    @classmethod
-    def from_json_file(cls, json_file):
-        """Constructs a `ModelCard` from a json file of parameters."""
-        with open(json_file, "r", encoding="utf-8") as reader:
-            text = reader.read()
-        dict_obj = json.loads(text)
-        return cls(**dict_obj)
-
-    def __eq__(self, other):
-        return self.__dict__ == other.__dict__
-
-    def __repr__(self):
-        return str(self.to_json_string())
-
-    def to_dict(self):
-        """Serializes this instance to a Python dictionary."""
-        output = copy.deepcopy(self.__dict__)
-        return output
-
-    def to_json_string(self):
-        """Serializes this instance to a JSON string."""
-        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + "\n"
-
-    def to_json_file(self, json_file_path):
-        """Save this instance to a json file."""
-        with open(json_file_path, "w", encoding="utf-8") as writer:
-            writer.write(self.to_json_string())
-
-
-AUTOGENERATED_TRAINER_COMMENT = """
-<!-- This model card has been generated automatically according to the information the Trainer had access to. You
-should probably proofread and complete it, then remove this comment. -->
-"""
-
-AUTOGENERATED_KERAS_COMMENT = """
-<!-- This model card has been generated automatically according to the information Keras had access to. You should
-probably proofread and complete it, then remove this comment. -->
-"""
-
-
-TASK_TAG_TO_NAME_MAPPING = {
-    "fill-mask": "Masked Language Modeling",
-    "image-classification": "Image Classification",
-    "image-segmentation": "Image Segmentation",
-    "multiple-choice": "Multiple Choice",
-    "object-detection": "Object Detection",
-    "question-answering": "Question Answering",
-    "summarization": "Summarization",
-    "table-question-answering": "Table Question Answering",
-    "text-classification": "Text Classification",
-    "text-generation": "Causal Language Modeling",
-    "text2text-generation": "Sequence-to-sequence Language Modeling",
-    "token-classification": "Token Classification",
-    "translation": "Translation",
-    "zero-shot-classification": "Zero Shot Classification",
-    "automatic-speech-recognition": "Automatic Speech Recognition",
-}
-
-
-METRIC_TAGS = [
-    "accuracy",
-    "bleu",
-    "f1",
-    "matthews_correlation",
-    "pearsonr",
-    "precision",
-    "recall",
-    "rouge",
-    "sacrebleu",
-    "spearmanr",
-    "wer",
-]
-
-
-def _listify(obj):
-    if obj is None:
-        return []
-    elif isinstance(obj, str):
-        return [obj]
-    else:
-        return obj
-
-
-def _insert_values_as_list(metadata, name, values):
-    if values is None:
-        return metadata
-    if isinstance(values, str):
-        values = [values]
-    values = [v for v in values if v is not None]
-    if len(values) == 0:
-        return metadata
-    metadata[name] = values
-    return metadata
-
-
-def infer_metric_tags_from_eval_results(eval_results):
-    if eval_results is None:
-        return {}
-    result = {}
-    for key in eval_results.keys():
-        if key.lower().replace(" ", "_") in METRIC_TAGS:
-            result[key.lower().replace(" ", "_")] = key
-        elif key.lower() == "rouge1":
-            result["rouge"] = key
-    return result
-
-
-def _insert_value(metadata, name, value):
-    if value is None:
-        return metadata
-    metadata[name] = value
-    return metadata
-
-
-def is_hf_dataset(dataset):
-    if not is_datasets_available():
-        return False
-
-    from datasets import Dataset
-
-    return isinstance(dataset, Dataset)
-
-
-def _get_mapping_values(mapping):
-    result = []
-    for v in mapping.values():
-        if isinstance(v, (tuple, list)):
-            result += list(v)
-        else:
-            result.append(v)
-    return result
+@dataclasses.dataclass
+class PatchingSpec:
+    """
+    Data class that holds patching specifications.
 
+    Args:
+        o: Module / object where the op to patch is located
+        name: Name of the op to monkey patch
+        custom_op: Custom op that patches the original op
+        orig_op: Original op that is being patched
+        op_wrapper: Wrapper (optional) that wraps both the original and custom ops.
+            It is useful for ops that are class or static methods for instance.
+    """
 
-@dataclass
-class TrainingSummary:
-    model_name: str
-    language: Optional[Union[str, List[str]]] = None
-    license: Optional[str] = None
-    tags: Optional[Union[str, List[str]]] = None
-    finetuned_from: Optional[str] = None
-    tasks: Optional[Union[str, List[str]]] = None
-    dataset: Optional[Union[str, List[str]]] = None
-    dataset_tags: Optional[Union[str, List[str]]] = None
-    dataset_args: Optional[Union[str, List[str]]] = None
-    dataset_metadata: Optional[Dict[str, Any]] = None
-    eval_results: Optional[Dict[str, float]] = None
-    eval_lines: Optional[List[str]] = None
-    hyperparameters: Optional[Dict[str, Any]] = None
-    source: Optional[str] = "trainer"
-
-    def __post_init__(self):
-        # Infer default license from the checkpoint used, if possible.
-        if (
-            self.license is None
-            and not is_offline_mode()
-            and self.finetuned_from is not None
-            and len(self.finetuned_from) > 0
-        ):
-            try:
-                info = model_info(self.finetuned_from)
-                for tag in info.tags:
-                    if tag.startswith("license:"):
-                        self.license = tag[8:]
-            except (requests.exceptions.HTTPError, HFValidationError):
-                pass
-
-    def create_model_index(self, metric_mapping):
-        model_index = {"name": self.model_name}
-
-        # Dataset mapping tag -> name
-        dataset_names = _listify(self.dataset)
-        dataset_tags = _listify(self.dataset_tags)
-        dataset_args = _listify(self.dataset_args)
-        dataset_metadata = _listify(self.dataset_metadata)
-        if len(dataset_args) < len(dataset_tags):
-            dataset_args = dataset_args + [None] * (
-                len(dataset_tags) - len(dataset_args)
-            )
-        dataset_mapping = {tag: name for tag, name in zip(dataset_tags, dataset_names)}
-        dataset_arg_mapping = {tag: arg for tag, arg in zip(dataset_tags, dataset_args)}
-        dataset_metadata_mapping = {
-            tag: metadata for tag, metadata in zip(dataset_tags, dataset_metadata)
-        }
+    o: Any
+    name: str
+    custom_op: Callable
+    orig_op: Optional[Callable] = None
+    op_wrapper: Optional[Callable] = None
 
-        task_mapping = {
-            task: TASK_TAG_TO_NAME_MAPPING[task]
-            for task in _listify(self.tasks)
-            if task in TASK_TAG_TO_NAME_MAPPING
-        }
 
-        model_index["results"] = []
+class OnnxConfig(ABC):
+    """
+    Base class for ONNX exportable model describing metadata on how to export the model through the ONNX format.
+    """
 
-        if len(task_mapping) == 0 and len(dataset_mapping) == 0:
-            return [model_index]
-        if len(task_mapping) == 0:
-            task_mapping = {None: None}
-        if len(dataset_mapping) == 0:
-            dataset_mapping = {None: None}
-
-        # One entry per dataset and per task
-        all_possibilities = [
-            (task_tag, ds_tag)
-            for task_tag in task_mapping
-            for ds_tag in dataset_mapping
-        ]
-        for task_tag, ds_tag in all_possibilities:
-            result = {}
-            if task_tag is not None:
-                result["task"] = {"name": task_mapping[task_tag], "type": task_tag}
-
-            if ds_tag is not None:
-                metadata = dataset_metadata_mapping.get(ds_tag, {})
-                result["dataset"] = {
-                    "name": dataset_mapping[ds_tag],
-                    "type": ds_tag,
-                    **metadata,
-                }
-                if dataset_arg_mapping[ds_tag] is not None:
-                    result["dataset"]["args"] = dataset_arg_mapping[ds_tag]
+    default_fixed_batch = 2
+    default_fixed_sequence = 8
+    default_fixed_num_choices = 4
+    torch_onnx_minimum_version = version.parse("1.8")
+    _tasks_to_common_outputs = {
+        "causal-lm": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "default": OrderedDict({"last_hidden_state": {0: "batch", 1: "sequence"}}),
+        "image-classification": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "image-segmentation": OrderedDict(
+            {
+                "logits": {0: "batch", 1: "sequence"},
+                "pred_boxes": {0: "batch", 1: "sequence"},
+                "pred_masks": {0: "batch", 1: "sequence"},
+            }
+        ),
+        "masked-im": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "masked-lm": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "multiple-choice": OrderedDict({"logits": {0: "batch"}}),
+        "object-detection": OrderedDict(
+            {
+                "logits": {0: "batch", 1: "sequence"},
+                "pred_boxes": {0: "batch", 1: "sequence"},
+            }
+        ),
+        "question-answering": OrderedDict(
+            {
+                "start_logits": {0: "batch", 1: "sequence"},
+                "end_logits": {0: "batch", 1: "sequence"},
+            }
+        ),
+        "semantic-segmentation": OrderedDict(
+            {"logits": {0: "batch", 1: "num_labels", 2: "height", 3: "width"}}
+        ),
+        "seq2seq-lm": OrderedDict({"logits": {0: "batch", 1: "decoder_sequence"}}),
+        "sequence-classification": OrderedDict({"logits": {0: "batch"}}),
+        "token-classification": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "vision2seq-lm": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+        "speech2seq-lm": OrderedDict({"logits": {0: "batch", 1: "sequence"}}),
+    }
+
+    def __init__(
+        self,
+        config: "PretrainedConfig",
+        task: str = "default",
+        patching_specs: List[PatchingSpec] = None,
+    ):
+        self._config = config
 
-            if len(metric_mapping) > 0:
-                result["metrics"] = []
-                for metric_tag, metric_name in metric_mapping.items():
-                    result["metrics"].append(
-                        {
-                            "name": metric_name,
-                            "type": metric_tag,
-                            "value": self.eval_results[metric_name],
-                        }
-                    )
+        if task not in self._tasks_to_common_outputs:
+            raise ValueError(
+                f"{task} is not a supported task, supported tasks: {self._tasks_to_common_outputs.keys()}"
+            )
+        self.task = task
 
-            # Remove partial results to avoid the model card being rejected.
-            if "task" in result and "dataset" in result and "metrics" in result:
-                model_index["results"].append(result)
-            else:
-                logger.info(
-                    f"Dropping the following result as it does not have all the necessary fields:\n{result}"
+        self._patching_specs = []
+        for spec in patching_specs if patching_specs is not None else []:
+            final_spec = spec
+            if spec.orig_op is None:
+                final_spec = dataclasses.replace(
+                    spec, orig_op=getattr(spec.o, spec.name)
                 )
+            self._patching_specs.append(final_spec)
 
-        return [model_index]
+    @classmethod
+    def from_model_config(
+        cls, config: "PretrainedConfig", task: str = "default"
+    ) -> "OnnxConfig":
+        """
+        Instantiate a OnnxConfig for a specific model
+
+        Args:
+            config: The model's configuration to use when exporting to ONNX
+
+        Returns:
+            OnnxConfig for this model
+        """
+        return cls(config, task=task)
+
+    @property
+    @abstractmethod
+    def inputs(self) -> Mapping[str, Mapping[int, str]]:
+        """
+        Mapping containing the axis definition of the input tensors to provide to the model
+
+        Returns:
+            For each input: its name associated to the axes symbolic name and the axis position within the tensor
+        """
+        raise NotImplementedError()
+
+    @property
+    def outputs(self) -> Mapping[str, Mapping[int, str]]:
+        """
+        Mapping containing the axis definition of the output tensors to provide to the model
+
+        Returns:
+            For each output: its name associated to the axes symbolic name and the axis position within the tensor
+        """
+        common_outputs = self._tasks_to_common_outputs[self.task]
+        return copy.deepcopy(common_outputs)
+
+    @property
+    def values_override(self) -> Optional[Mapping[str, Any]]:
+        """
+        Dictionary of keys to override in the model's config before exporting
+
+        Returns:
+            Dictionary with the keys (and their corresponding values) to override
+        """
+        if hasattr(self._config, "use_cache"):
+            return {"use_cache": False}
+
+        return None
+
+    @property
+    def default_batch_size(self) -> int:
+        """
+        The default batch size to use if no other indication
+
+        Returns:
+            Integer > 0
+        """
+        # Using 2 avoid ONNX making assumption about single sample batch
+        return OnnxConfig.default_fixed_batch
+
+    @property
+    def default_sequence_length(self) -> int:
+        """
+        The default sequence length to use if no other indication
+
+        Returns:
+            Integer > 0
+        """
+        return OnnxConfig.default_fixed_sequence
+
+    @property
+    def default_num_choices(self) -> int:
+        """
+        The default number of choices to use if no other indication
+
+        Returns:
+            Integer > 0
+        """
+        return OnnxConfig.default_fixed_num_choices
+
+    @property
+    def default_onnx_opset(self) -> int:
+        """
+        Which onnx opset to use when exporting the model
+
+        Returns:
+            Integer ONNX Opset version
+        """
+        return DEFAULT_ONNX_OPSET
+
+    @property
+    def atol_for_validation(self) -> float:
+        """
+        What absolute tolerance value to use during model conversion validation.
+
+        Returns:
+            Float absolute tolerance value.
+        """
+        return 1e-5
+
+    @property
+    def is_torch_support_available(self) -> bool:
+        """
+        The minimum PyTorch version required to export the model.
+
+        Returns:
+            `bool`: Whether the installed version of PyTorch is compatible with the model.
+        """
+        if is_torch_available():
+            from transformers.utils import torch_version
 
-    def create_metadata(self):
-        metric_mapping = infer_metric_tags_from_eval_results(self.eval_results)
+            return torch_version >= self.torch_onnx_minimum_version
+        else:
+            return False
 
-        metadata = {}
-        metadata = _insert_values_as_list(metadata, "language", self.language)
-        metadata = _insert_value(metadata, "license", self.license)
-        metadata = _insert_values_as_list(metadata, "tags", self.tags)
-        metadata = _insert_values_as_list(metadata, "datasets", self.dataset_tags)
-        metadata = _insert_values_as_list(
-            metadata, "metrics", list(metric_mapping.keys())
+    @staticmethod
+    def use_external_data_format(num_parameters: int) -> bool:
+        """
+        Flag indicating if the model requires using external data format
+
+        Args:
+            num_parameters: Number of parameter on the model
+
+        Returns:
+            True if model.num_parameters() * size_of(float32) >= 2Gb False otherwise
+        """
+
+        return (
+            compute_serialized_parameters_size(num_parameters, ParameterFormat.Float)
+            >= EXTERNAL_DATA_FORMAT_SIZE_LIMIT
         )
-        metadata["model-index"] = self.create_model_index(metric_mapping)
 
-        return metadata
+    def _generate_dummy_images(
+        self,
+        batch_size: int = 2,
+        num_channels: int = 3,
+        image_height: int = 40,
+        image_width: int = 40,
+    ):
+        images = []
+        for _ in range(batch_size):
+            data = np.random.rand(image_height, image_width, num_channels) * 255
+            images.append(Image.fromarray(data.astype("uint8")).convert("RGB"))
+        return images
+
+    def _generate_dummy_audio(
+        self,
+        batch_size: int = 2,
+        sampling_rate: int = 22050,
+        time_duration: float = 5.0,
+        frequency: int = 220,
+    ):
+        audio_data = []
+        for _ in range(batch_size):
+            # time variable
+            t = np.linspace(
+                0, time_duration, int(time_duration * sampling_rate), endpoint=False
+            )
 
-    def to_model_card(self):
-        model_card = ""
+            # generate pure sine wave at `frequency` Hz
+            audio_data.append(0.5 * np.sin(2 * np.pi * frequency * t))
 
-        metadata = yaml.dump(self.create_metadata(), sort_keys=False)
-        if len(metadata) > 0:
-            model_card = f"---\n{metadata}---\n"
+        return audio_data
 
-        # Now the model card for realsies.
-        if self.source == "trainer":
-            model_card += AUTOGENERATED_TRAINER_COMMENT
+    def generate_dummy_inputs(
+        self,
+        preprocessor: Union["PreTrainedTokenizerBase", "FeatureExtractionMixin"],
+        batch_size: int = -1,
+        seq_length: int = -1,
+        num_choices: int = -1,
+        is_pair: bool = False,
+        framework: Optional[TensorType] = None,
+        num_channels: int = 3,
+        image_width: int = 40,
+        image_height: int = 40,
+        sampling_rate: int = 22050,
+        time_duration: float = 5.0,
+        frequency: int = 220,
+        tokenizer: "PreTrainedTokenizerBase" = None,
+    ) -> Mapping[str, Any]:
+        """
+        Generate inputs to provide to the ONNX exporter for the specific framework
+
+        Args:
+            preprocessor: ([`PreTrainedTokenizerBase`] or [`FeatureExtractionMixin`]):
+                The preprocessor associated with this model configuration.
+            batch_size (`int`, *optional*, defaults to -1):
+                The batch size to export the model for (-1 means dynamic axis).
+            num_choices (`int`, *optional*, defaults to -1):
+                The number of candidate answers provided for multiple choice task (-1 means dynamic axis).
+            seq_length (`int`, *optional*, defaults to -1):
+                The sequence length to export the model for (-1 means dynamic axis).
+            is_pair (`bool`, *optional*, defaults to `False`):
+                Indicate if the input is a pair (sentence 1, sentence 2)
+            framework (`TensorType`, *optional*, defaults to `None`):
+                The framework (PyTorch or TensorFlow) that the tokenizer will generate tensors for.
+            num_channels (`int`, *optional*, defaults to 3):
+                The number of channels of the generated images.
+            image_width (`int`, *optional*, defaults to 40):
+                The width of the generated images.
+            image_height (`int`, *optional*, defaults to 40):
+                The height of the generated images.
+            sampling_rate (`int`, *optional* defaults to 22050)
+                The sampling rate for audio data generation.
+            time_duration (`float`, *optional* defaults to 5.0)
+                Total seconds of sampling for audio data generation.
+            frequency (`int`, *optional* defaults to 220)
+                The desired natural frequency of generated audio.
+
+        Returns:
+            Mapping[str, Tensor] holding the kwargs to provide to the model's forward function
+        """
+        from ..feature_extraction_utils import FeatureExtractionMixin
+        from ..tokenization_utils_base import PreTrainedTokenizerBase
+
+        if isinstance(preprocessor, PreTrainedTokenizerBase) and tokenizer is not None:
+            raise ValueError(
+                "You cannot provide both a tokenizer and a preprocessor to generate dummy inputs."
+            )
+        if tokenizer is not None:
+            warnings.warn(
+                "The `tokenizer` argument is deprecated and will be removed in version 5 of Transformers. Use"
+                " `preprocessor` instead.",
+                FutureWarning,
+            )
+            logger.warning(
+                "Overwriting the `preprocessor` argument with `tokenizer` to generate dummmy inputs."
+            )
+            preprocessor = tokenizer
+        if isinstance(preprocessor, PreTrainedTokenizerBase):
+            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX
+            batch_size = compute_effective_axis_dimension(
+                batch_size,
+                fixed_dimension=OnnxConfig.default_fixed_batch,
+                num_token_to_add=0,
+            )
+            # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX
+            token_to_add = preprocessor.num_special_tokens_to_add(is_pair)
+            seq_length = compute_effective_axis_dimension(
+                seq_length,
+                fixed_dimension=OnnxConfig.default_fixed_sequence,
+                num_token_to_add=token_to_add,
+            )
+            # Generate dummy inputs according to compute batch and sequence
+            input_token = (
+                preprocessor.unk_token
+                if (
+                    preprocessor.unk_token is not None
+                    and len(preprocessor.unk_token) > 0
+                )
+                else "0"
+            )
+            dummy_input = [" ".join([input_token]) * seq_length] * batch_size
+            if self.task == "multiple-choice":
+                # If dynamic axis (-1) we forward with a fixed dimension of 4 candidate answers to avoid optimizations
+                # made by ONNX
+                num_choices = compute_effective_axis_dimension(
+                    num_choices,
+                    fixed_dimension=OnnxConfig.default_fixed_num_choices,
+                    num_token_to_add=0,
+                )
+                dummy_input = dummy_input * num_choices
+                # The shape of the tokenized inputs values is [batch_size * num_choices, seq_length]
+                tokenized_input = preprocessor(dummy_input, text_pair=dummy_input)
+                # Unflatten the tokenized inputs values expanding it to the shape [batch_size, num_choices, seq_length]
+                for k, v in tokenized_input.items():
+                    tokenized_input[k] = [
+                        v[i : i + num_choices] for i in range(0, len(v), num_choices)
+                    ]
+                return dict(tokenized_input.convert_to_tensors(tensor_type=framework))
+            return dict(preprocessor(dummy_input, return_tensors=framework))
+        elif (
+            isinstance(preprocessor, FeatureExtractionMixin)
+            and preprocessor.model_input_names[0] == "pixel_values"
+        ):
+            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX
+            batch_size = compute_effective_axis_dimension(
+                batch_size, fixed_dimension=OnnxConfig.default_fixed_batch
+            )
+            dummy_input = self._generate_dummy_images(
+                batch_size, num_channels, image_height, image_width
+            )
+            return dict(preprocessor(images=dummy_input, return_tensors=framework))
+        elif (
+            isinstance(preprocessor, FeatureExtractionMixin)
+            and preprocessor.model_input_names[0] == "input_features"
+        ):
+            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX
+            batch_size = compute_effective_axis_dimension(
+                batch_size, fixed_dimension=OnnxConfig.default_fixed_batch
+            )
+            dummy_input = self._generate_dummy_audio(
+                batch_size, sampling_rate, time_duration, frequency
+            )
+            return dict(preprocessor(dummy_input, return_tensors=framework))
         else:
-            model_card += AUTOGENERATED_KERAS_COMMENT
-
-        model_card += f"\n# {self.model_name}\n\n"
+            raise ValueError(
+                "Unable to generate dummy inputs for the model. Please provide a tokenizer or a preprocessor."
+            )
 
-        if self.finetuned_from is None:
-            model_card += "This model was trained from scratch on "
-        else:
-            model_card += (
-                "This model is a fine-tuned version of"
-                f" [{self.finetuned_from}](https://huggingface.co/{self.finetuned_from}) on "
+    def generate_dummy_inputs_onnxruntime(
+        self, reference_model_inputs: Mapping[str, Any]
+    ) -> Mapping[str, Any]:
+        """
+        Generate inputs for ONNX Runtime using the reference model inputs. Override this to run inference with seq2seq
+        models which have the encoder and decoder exported as separate ONNX files.
+
+        Args:
+            reference_model_inputs ([`Mapping[str, Tensor]`):
+                Reference inputs for the model.
+
+        Returns:
+            `Mapping[str, Tensor]`: The mapping holding the kwargs to provide to the model's forward function
+        """
+        return reference_model_inputs
+
+    def patch_ops(self):
+        for spec in self._patching_specs:
+            custom_op = (
+                spec.custom_op
+                if spec.op_wrapper is None
+                else spec.op_wrapper(spec.custom_op)
             )
+            setattr(spec.o, spec.name, custom_op)
 
-        if self.dataset is None:
-            model_card += "an unknown dataset."
-        else:
-            if isinstance(self.dataset, str):
-                model_card += f"the {self.dataset} dataset."
-            elif isinstance(self.dataset, (tuple, list)) and len(self.dataset) == 1:
-                model_card += f"the {self.dataset[0]} dataset."
-            else:
-                model_card += (
-                    ", ".join([f"the {ds}" for ds in self.dataset[:-1]])
-                    + f" and the {self.dataset[-1]} datasets."
-                )
-
-        if self.eval_results is not None:
-            model_card += "\nIt achieves the following results on the evaluation set:\n"
-            model_card += "\n".join(
-                [
-                    f"- {name}: {_maybe_round(value)}"
-                    for name, value in self.eval_results.items()
-                ]
-            )
-        model_card += "\n"
-
-        model_card += "\n## Model description\n\nMore information needed\n"
-        model_card += "\n## Intended uses & limitations\n\nMore information needed\n"
-        model_card += "\n## Training and evaluation data\n\nMore information needed\n"
-
-        model_card += "\n## Training procedure\n"
-        model_card += "\n### Training hyperparameters\n"
-        if self.hyperparameters is not None:
-            model_card += "\nThe following hyperparameters were used during training:\n"
-            model_card += "\n".join(
-                [f"- {name}: {value}" for name, value in self.hyperparameters.items()]
+    def restore_ops(self):
+        for spec in self._patching_specs:
+            orig_op = (
+                spec.orig_op
+                if spec.op_wrapper is None
+                else spec.op_wrapper(spec.orig_op)
             )
-            model_card += "\n"
-        else:
-            model_card += "\nMore information needed\n"
+            setattr(spec.o, spec.name, orig_op)
 
-        if self.eval_lines is not None:
-            model_card += "\n### Training results\n\n"
-            model_card += make_markdown_table(self.eval_lines)
-            model_card += "\n"
+    @classmethod
+    def flatten_output_collection_property(
+        cls, name: str, field: Iterable[Any]
+    ) -> Dict[str, Any]:
+        """
+        Flatten any potential nested structure expanding the name of the field with the index of the element within the
+        structure.
+
+        Args:
+            name: The name of the nested structure
+            field: The structure to, potentially, be flattened
 
-        model_card += "\n### Framework versions\n\n"
-        model_card += f"- Transformers {__version__}\n"
+        Returns:
+            (Dict[str, Any]): Outputs with flattened structure and key mapping this new structure.
 
-        if self.source == "trainer" and is_torch_available():
-            import torch
+        """
+        from itertools import chain
 
-            model_card += f"- Pytorch {torch.__version__}\n"
-        elif self.source == "keras" and is_tf_available():
-            import tensorflow as tf
+        return {
+            f"{name}.{idx}": item for idx, item in enumerate(chain.from_iterable(field))
+        }
 
-            model_card += f"- TensorFlow {tf.__version__}\n"
-        if is_datasets_available():
-            import datasets
 
-            model_card += f"- Datasets {datasets.__version__}\n"
-        if is_tokenizers_available():
-            import tokenizers
+class OnnxConfigWithPast(OnnxConfig, ABC):
+    def __init__(
+        self,
+        config: "PretrainedConfig",
+        task: str = "default",
+        patching_specs: List[PatchingSpec] = None,
+        use_past: bool = False,
+    ):
+        super().__init__(config, task=task, patching_specs=patching_specs)
+        self.use_past = use_past
 
-            model_card += f"- Tokenizers {tokenizers.__version__}\n"
+    @classmethod
+    def with_past(
+        cls, config: "PretrainedConfig", task: str = "default"
+    ) -> "OnnxConfigWithPast":
+        """
+        Instantiate a OnnxConfig with `use_past` attribute set to True
+
+        Args:
+            config: The underlying model's config to use when exporting to ONNX
+
+        Returns:
+            OnnxConfig with `.use_past = True`
+        """
+        return cls(config, task=task, use_past=True)
+
+    @property
+    def outputs(self) -> Mapping[str, Mapping[int, str]]:
+        common_outputs = super().outputs
+        if self.use_past:
+            self.fill_with_past_key_values_(common_outputs, direction="outputs")
+
+        return common_outputs
+
+    @property
+    def values_override(self) -> Optional[Mapping[str, Any]]:
+        if hasattr(self._config, "use_cache"):
+            return {"use_cache": self.use_past}
+
+        return None
+
+    @property
+    def num_layers(self) -> int:
+        """
+        The number of layers attribute retrieved from the model config. Override this for model configs where the
+        number of layers attribute is not called `num_layers`.
+        """
+        if not hasattr(self._config, "num_layers"):
+            raise AttributeError(
+                "could not find the number of layers attribute in the model configuration, override the num_layers"
+                " property of the model OnnxConfig to solve this"
+            )
+        return self._config.num_layers
 
-        return model_card
+    @property
+    def num_attention_heads(self) -> int:
+        """
+        The number of attention heads attribute retrieved from the model config. Override this for model configs where
+        the number of attention heads attribute is not called `num_attention_heads`.
+        """
+        if not hasattr(self._config, "num_attention_heads"):
+            raise AttributeError(
+                "could not find the number of attention heads attribute in the model configuration, override the"
+                " num_attention_heads property of the model OnnxConfig to solve this"
+            )
+        return self._config.num_attention_heads
 
-    @classmethod
-    def from_trainer(
-        cls,
-        trainer,
-        language=None,
-        license=None,
-        tags=None,
-        model_name=None,
-        finetuned_from=None,
-        tasks=None,
-        dataset_tags=None,
-        dataset_metadata=None,
-        dataset=None,
-        dataset_args=None,
-    ):
-        # Infer default from dataset
-        one_dataset = (
-            trainer.train_dataset
-            if trainer.train_dataset is not None
-            else trainer.eval_dataset
+    def generate_dummy_inputs(
+        self,
+        tokenizer: "PreTrainedTokenizerBase",
+        batch_size: int = -1,
+        seq_length: int = -1,
+        is_pair: bool = False,
+        framework: Optional[TensorType] = None,
+    ) -> Mapping[str, Any]:
+        # TODO: should we set seq_length = 1 when self.use_past = True?
+        common_inputs = super().generate_dummy_inputs(
+            tokenizer,
+            batch_size=batch_size,
+            seq_length=seq_length,
+            is_pair=is_pair,
+            framework=framework,
         )
-        if is_hf_dataset(one_dataset) and (
-            dataset_tags is None or dataset_args is None
-        ):
-            default_tag = one_dataset.builder_name
-            # Those are not real datasets from the Hub so we exclude them.
-            if default_tag not in ["csv", "json", "pandas", "parquet", "text"]:
-                if dataset_metadata is None:
-                    dataset_metadata = [
-                        {
-                            "config": one_dataset.config_name,
-                            "split": str(one_dataset.split),
-                        }
-                    ]
-                if dataset_tags is None:
-                    dataset_tags = [default_tag]
-                if dataset_args is None:
-                    dataset_args = [one_dataset.config_name]
-
-        if dataset is None and dataset_tags is not None:
-            dataset = dataset_tags
-
-        # Infer default finetuned_from
-        if (
-            finetuned_from is None
-            and hasattr(trainer.model.config, "_name_or_path")
-            and not os.path.isdir(trainer.model.config._name_or_path)
-        ):
-            finetuned_from = trainer.model.config._name_or_path
 
-        # Infer default task tag:
-        if tasks is None:
-            model_class_name = trainer.model.__class__.__name__
-            for task, mapping in TASK_MAPPING.items():
-                if model_class_name in _get_mapping_values(mapping):
-                    tasks = task
-
-        if model_name is None:
-            model_name = Path(trainer.args.output_dir).name
-
-        # Add `generated_from_trainer` to the tags
-        if tags is None:
-            tags = ["generated_from_trainer"]
-        elif isinstance(tags, str) and tags != "generated_from_trainer":
-            tags = [tags, "generated_from_trainer"]
-        elif "generated_from_trainer" not in tags:
-            tags.append("generated_from_trainer")
-
-        _, eval_lines, eval_results = parse_log_history(trainer.state.log_history)
-        hyperparameters = extract_hyperparameters_from_trainer(trainer)
-
-        return cls(
-            language=language,
-            license=license,
-            tags=tags,
-            model_name=model_name,
-            finetuned_from=finetuned_from,
-            tasks=tasks,
-            dataset=dataset,
-            dataset_tags=dataset_tags,
-            dataset_args=dataset_args,
-            dataset_metadata=dataset_metadata,
-            eval_results=eval_results,
-            eval_lines=eval_lines,
-            hyperparameters=hyperparameters,
-        )
+        if self.use_past:
+            if not is_torch_available():
+                raise ValueError(
+                    "Cannot generate dummy past_keys inputs without PyTorch installed."
+                )
+            else:
+                import torch
 
-    @classmethod
-    def from_keras(
-        cls,
-        model,
-        model_name,
-        keras_history=None,
-        language=None,
-        license=None,
-        tags=None,
-        finetuned_from=None,
-        tasks=None,
-        dataset_tags=None,
-        dataset=None,
-        dataset_args=None,
+            batch, seqlen = common_inputs["input_ids"].shape
+            # Not using the same length for past_key_values
+            past_key_values_length = seqlen + 2
+            shape = (
+                batch,
+                self.num_attention_heads,
+                past_key_values_length,
+                self._config.hidden_size // self.num_attention_heads,
+            )
+
+            if "attention_mask" in common_inputs:
+                mask_dtype = common_inputs["attention_mask"].dtype
+                common_inputs["attention_mask"] = torch.cat(
+                    [
+                        common_inputs["attention_mask"],
+                        torch.ones(batch, past_key_values_length, dtype=mask_dtype),
+                    ],
+                    dim=1,
+                )
+
+            common_inputs["past_key_values"] = []
+            for _ in range(self.num_layers):
+                common_inputs["past_key_values"].append(
+                    (torch.zeros(shape), torch.zeros(shape))
+                )
+
+        return common_inputs
+
+    def fill_with_past_key_values_(
+        self,
+        inputs_or_outputs: Mapping[str, Mapping[int, str]],
+        direction: str,
+        inverted_values_shape: bool = False,
     ):
-        # Infer default from dataset
-        if dataset is not None:
-            if is_hf_dataset(dataset) and (
-                dataset_tags is None or dataset_args is None
-            ):
-                default_tag = dataset.builder_name
-                # Those are not real datasets from the Hub so we exclude them.
-                if default_tag not in ["csv", "json", "pandas", "parquet", "text"]:
-                    if dataset_tags is None:
-                        dataset_tags = [default_tag]
-                    if dataset_args is None:
-                        dataset_args = [dataset.config_name]
-
-        if dataset is None and dataset_tags is not None:
-            dataset = dataset_tags
-
-        # Infer default finetuned_from
-        if (
-            finetuned_from is None
-            and hasattr(model.config, "_name_or_path")
-            and not os.path.isdir(model.config._name_or_path)
-        ):
-            finetuned_from = model.config._name_or_path
+        """
+        Fill the input_or_outputs mapping with past_key_values dynamic axes considering.
+
+        Args:
+            inputs_or_outputs: The mapping to fill.
+            direction: either "inputs" or "outputs", it specifies whether input_or_outputs is the input mapping or the
+                output mapping, this is important for axes naming.
+            inverted_values_shape:
+                If `True`, store values on dynamic axis 1, else on axis 2.
+
+        """
+        if direction not in ["inputs", "outputs"]:
+            raise ValueError(
+                f'direction must either be "inputs" or "outputs", but {direction} was given'
+            )
 
-        # Infer default task tag:
-        if tasks is None:
-            model_class_name = model.__class__.__name__
-            for task, mapping in TASK_MAPPING.items():
-                if model_class_name in _get_mapping_values(mapping):
-                    tasks = task
-
-        # Add `generated_from_keras_callback` to the tags
-        if tags is None:
-            tags = ["generated_from_keras_callback"]
-        elif isinstance(tags, str) and tags != "generated_from_keras_callback":
-            tags = [tags, "generated_from_keras_callback"]
-        elif "generated_from_keras_callback" not in tags:
-            tags.append("generated_from_keras_callback")
+        name = "past_key_values" if direction == "inputs" else "present"
+        for i in range(self.num_layers):
+            inputs_or_outputs[f"{name}.{i}.key"] = {
+                0: "batch",
+                2: "past_sequence + sequence",
+            }
+            if inverted_values_shape:
+                inputs_or_outputs[f"{name}.{i}.value"] = {
+                    0: "batch",
+                    1: "past_sequence + sequence",
+                }
+            else:
+                inputs_or_outputs[f"{name}.{i}.value"] = {
+                    0: "batch",
+                    2: "past_sequence + sequence",
+                }
 
-        if keras_history is not None:
-            _, eval_lines, eval_results = parse_keras_history(keras_history)
+    def _flatten_past_key_values_(self, flattened_output, name, idx, t):
+        flattened_output[f"{name}.{idx}.key"] = t[0]
+        flattened_output[f"{name}.{idx}.value"] = t[1]
+
+    def flatten_output_collection_property(
+        self, name: str, field: Iterable[Any]
+    ) -> Dict[str, Any]:
+        flattened_output = {}
+        if name in ["present", "past_key_values"]:
+            for idx, t in enumerate(field):
+                self._flatten_past_key_values_(flattened_output, name, idx, t)
         else:
-            eval_lines = []
-            eval_results = dict()
-        hyperparameters = extract_hyperparameters_from_keras(model)
-
-        return cls(
-            language=language,
-            license=license,
-            tags=tags,
-            model_name=model_name,
-            finetuned_from=finetuned_from,
-            tasks=tasks,
-            dataset_tags=dataset_tags,
-            dataset=dataset,
-            dataset_args=dataset_args,
-            eval_results=eval_results,
-            eval_lines=eval_lines,
-            hyperparameters=hyperparameters,
-            source="keras",
-        )
+            flattened_output = super().flatten_output_collection_property(name, field)
 
+        return flattened_output
 
-def parse_keras_history(logs):
-    """
-    Parse the `logs` of either a `tf.keras.History` object returned by `model.fit()` or an accumulated logs `dict`
-    passed to the `PushToHubCallback`. Returns lines and logs compatible with those returned by `parse_log_history`.
-    """
-    if hasattr(logs, "history"):
-        # This looks like a `History` object
-        if not hasattr(logs, "epoch"):
-            # This history looks empty, return empty results
-            return None, [], dict()
-        logs.history["epoch"] = logs.epoch
-        logs = logs.history
-    else:
-        # Training logs is a list of dicts, let's invert it to a dict of lists to match a History object
-        logs = {
-            log_key: [single_dict[log_key] for single_dict in logs]
-            for log_key in logs[0]
-        }
 
-    lines = []
-    for i in range(len(logs["epoch"])):
-        epoch_dict = {
-            log_key: log_value_list[i] for log_key, log_value_list in logs.items()
-        }
-        values = dict()
-        for k, v in epoch_dict.items():
-            if k.startswith("val_"):
-                k = "validation_" + k[4:]
-            elif k != "epoch":
-                k = "train_" + k
-            splits = k.split("_")
-            name = " ".join([part.capitalize() for part in splits])
-            values[name] = v
-        lines.append(values)
+class OnnxSeq2SeqConfigWithPast(OnnxConfigWithPast):
+    @property
+    def outputs(self) -> Mapping[str, Mapping[int, str]]:
+        common_outputs = super(OnnxConfigWithPast, self).outputs
+        # Renaming the outputs axes properly.
+        for name, axes_names in common_outputs.items():
+            sequence_name = (
+                "encoder_sequence" if "encoder" in name else "decoder_sequence"
+            )
+            for axis_idx, name in axes_names.items():
+                if "sequence" in name:
+                    axes_names[axis_idx] = sequence_name
+                # We reset the value as the order in common_outputs (OrderedDict) is lost otherwise
+                else:
+                    axes_names[axis_idx] = name
+        if self.use_past:
+            self.fill_with_past_key_values_(common_outputs, direction="outputs")
+
+        return common_outputs
+
+    @property
+    def num_layers(self) -> Tuple[int]:
+        try:
+            num_layers = super().num_layers
+            num_layers = (num_layers, num_layers)
+        except AttributeError:
+            if hasattr(self._config, "encoder_layers") and hasattr(
+                self._config, "decoder_layers"
+            ):
+                num_layers = (self._config.encoder_layers, self._config.decoder_layers)
+            else:
+                raise AttributeError(
+                    "could not find the number of encoder and decoder layers attributes in the model configuration,"
+                    " override the num_layers property of the model OnnxConfig to solve this"
+                )
 
-    eval_results = lines[-1]
+        return num_layers
 
-    return logs, lines, eval_results
+    @property
+    def num_attention_heads(self) -> Tuple[int]:
+        try:
+            num_attention_heads = super().num_attention_heads
+            num_attention_heads = (num_attention_heads, num_attention_heads)
+        except AttributeError:
+            if hasattr(self._config, "encoder_attention_heads") and hasattr(
+                self._config, "decoder_attention_heads"
+            ):
+                num_attention_heads = (
+                    self._config.encoder_attention_heads,
+                    self._config.decoder_attention_heads,
+                )
+            else:
+                raise AttributeError(
+                    "could not find the number of attention heads for the encoder and the decoder attributes in the"
+                    " model configuration, override the num_attention_heads property of the model OnnxConfig to solve"
+                    " this"
+                )
+        return num_attention_heads
 
+    def generate_dummy_inputs(
+        self,
+        tokenizer: "PreTrainedTokenizerBase",
+        batch_size: int = -1,
+        seq_length: int = -1,
+        is_pair: bool = False,
+        framework: Optional[TensorType] = None,
+    ) -> Mapping[str, Any]:
+        encoder_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(
+            tokenizer,
+            batch_size=batch_size,
+            seq_length=seq_length,
+            is_pair=is_pair,
+            framework=framework,
+        )
 
-def parse_log_history(log_history):
-    """
-    Parse the `log_history` of a Trainer to get the intermediate and final evaluation results.
-    """
-    idx = 0
-    while idx < len(log_history) and "train_runtime" not in log_history[idx]:
-        idx += 1
-
-    # If there are no training logs
-    if idx == len(log_history):
-        idx -= 1
-        while idx >= 0 and "eval_loss" not in log_history[idx]:
-            idx -= 1
+        # Generate decoder inputs
+        decoder_seq_length = seq_length if not self.use_past else 1
+        decoder_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(
+            tokenizer,
+            batch_size=batch_size,
+            seq_length=decoder_seq_length,
+            is_pair=is_pair,
+            framework=framework,
+        )
+        decoder_inputs = {
+            f"decoder_{name}": tensor for name, tensor in decoder_inputs.items()
+        }
+        common_inputs = dict(**encoder_inputs, **decoder_inputs)
 
-        if idx >= 0:
-            return None, None, log_history[idx]
-        else:
-            return None, None, None
+        if self.use_past:
+            if not is_torch_available():
+                raise ValueError(
+                    "Cannot generate dummy past_keys inputs without PyTorch installed."
+                )
+            else:
+                import torch
+            batch = common_inputs["input_ids"].shape[0]
+            encoder_seq_length = common_inputs["input_ids"].shape[1]
+            decoder_seq_length = common_inputs["decoder_input_ids"].shape[1]
+            (
+                num_encoder_attention_heads,
+                num_decoder_attention_heads,
+            ) = self.num_attention_heads
+            encoder_shape = (
+                batch,
+                num_encoder_attention_heads,
+                encoder_seq_length,
+                self._config.hidden_size // num_encoder_attention_heads,
+            )
+            decoder_shape = (
+                batch,
+                num_decoder_attention_heads,
+                # Not using the same length for past_key_values
+                decoder_seq_length + 3,
+                self._config.hidden_size // num_decoder_attention_heads,
+            )
 
-    # From now one we can assume we have training logs:
-    train_log = log_history[idx]
-    lines = []
-    training_loss = "No log"
-    for i in range(idx):
-        if "loss" in log_history[i]:
-            training_loss = log_history[i]["loss"]
-        if "eval_loss" in log_history[i]:
-            metrics = log_history[i].copy()
-            _ = metrics.pop("total_flos", None)
-            epoch = metrics.pop("epoch", None)
-            step = metrics.pop("step", None)
-            _ = metrics.pop("eval_runtime", None)
-            _ = metrics.pop("eval_samples_per_second", None)
-            _ = metrics.pop("eval_steps_per_second", None)
-            values = {"Training Loss": training_loss, "Epoch": epoch, "Step": step}
-            for k, v in metrics.items():
-                if k == "eval_loss":
-                    values["Validation Loss"] = v
-                else:
-                    splits = k.split("_")
-                    name = " ".join([part.capitalize() for part in splits[1:]])
-                    values[name] = v
-            lines.append(values)
-
-    idx = len(log_history) - 1
-    while idx >= 0 and "eval_loss" not in log_history[idx]:
-        idx -= 1
-
-    if idx > 0:
-        eval_results = {}
-        for key, value in log_history[idx].items():
-            if key.startswith("eval_"):
-                key = key[5:]
-            if key not in [
-                "runtime",
-                "samples_per_second",
-                "steps_per_second",
-                "epoch",
-                "step",
-            ]:
-                camel_cased_key = " ".join(
-                    [part.capitalize() for part in key.split("_")]
-                )
-                eval_results[camel_cased_key] = value
-        return train_log, lines, eval_results
-    else:
-        return train_log, lines, None
-
-
-def extract_hyperparameters_from_keras(model):
-    import tensorflow as tf
-
-    hyperparameters = dict()
-    if hasattr(model, "optimizer") and model.optimizer is not None:
-        hyperparameters["optimizer"] = model.optimizer.get_config()
-    else:
-        hyperparameters["optimizer"] = None
-    hyperparameters[
-        "training_precision"
-    ] = tf.keras.mixed_precision.global_policy().name
-
-    return hyperparameters
-
-
-def _maybe_round(v, decimals=4):
-    if (
-        isinstance(v, float)
-        and len(str(v).split(".")) > 1
-        and len(str(v).split(".")[1]) > decimals
-    ):
-        return f"{v:.{decimals}f}"
-    return str(v)
+            common_inputs["past_key_values"] = []
+            # If the number of encoder and decoder layers are present in the model configuration, both are considered
+            num_encoder_layers, num_decoder_layers = self.num_layers
+            min_num_layers = min(num_encoder_layers, num_decoder_layers)
+            max_num_layers = (
+                max(num_encoder_layers, num_decoder_layers) - min_num_layers
+            )
+            remaining_side_name = (
+                "encoder" if num_encoder_layers > num_decoder_layers else "decoder"
+            )
 
+            for _ in range(min_num_layers):
+                # For encoder-decoder models, past_key_values contains pre-computed values for both the encoder and the
+                # decoder layers, hence a tuple of 4 tensors instead of 2
+                common_inputs["past_key_values"].append(
+                    (
+                        torch.zeros(decoder_shape),
+                        torch.zeros(decoder_shape),
+                        torch.zeros(encoder_shape),
+                        torch.zeros(encoder_shape),
+                    )
+                )
 
-def _regular_table_line(values, col_widths):
-    values_with_space = [
-        f"| {v}" + " " * (w - len(v) + 1) for v, w in zip(values, col_widths)
-    ]
-    return "".join(values_with_space) + "|\n"
+            # TODO: test this.
+            shape = encoder_shape if remaining_side_name == "encoder" else decoder_shape
+            for _ in range(min_num_layers, max_num_layers):
+                common_inputs["past_key_values"].append(
+                    (torch.zeros(shape), torch.zeros(shape))
+                )
 
+        return common_inputs
 
-def _second_table_line(col_widths):
-    values = ["|:" + "-" * w + ":" for w in col_widths]
-    return "".join(values) + "|\n"
+    def fill_with_past_key_values_(
+        self, inputs_or_outputs: Mapping[str, Mapping[int, str]], direction: str
+    ):
+        if direction not in ["inputs", "outputs"]:
+            raise ValueError(
+                f'direction must either be "inputs" or "outputs", but {direction} was given'
+            )
 
+        name = "past_key_values" if direction == "inputs" else "present"
 
-def make_markdown_table(lines):
-    """
-    Create a nice Markdown table from the results in `lines`.
-    """
-    if lines is None or len(lines) == 0:
-        return ""
-    col_widths = {key: len(str(key)) for key in lines[0].keys()}
-    for line in lines:
-        for key, value in line.items():
-            if col_widths[key] < len(_maybe_round(value)):
-                col_widths[key] = len(_maybe_round(value))
-
-    table = _regular_table_line(list(lines[0].keys()), list(col_widths.values()))
-    table += _second_table_line(list(col_widths.values()))
-    for line in lines:
-        table += _regular_table_line(
-            [_maybe_round(v) for v in line.values()], list(col_widths.values())
+        # If the number of encoder and decoder layers are present in the model configuration, both are considered
+        num_encoder_layers, num_decoder_layers = self.num_layers
+        min_num_layers = min(num_encoder_layers, num_decoder_layers)
+        max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers
+        remaining_side_name = (
+            "encoder" if num_encoder_layers > num_decoder_layers else "decoder"
         )
-    return table
-
 
-_TRAINING_ARGS_KEYS = [
-    "learning_rate",
-    "train_batch_size",
-    "eval_batch_size",
-    "seed",
-]
-
-
-def extract_hyperparameters_from_trainer(trainer):
-    hyperparameters = {k: getattr(trainer.args, k) for k in _TRAINING_ARGS_KEYS}
-
-    if trainer.args.parallel_mode not in [
-        ParallelMode.NOT_PARALLEL,
-        ParallelMode.NOT_DISTRIBUTED,
-    ]:
-        hyperparameters["distributed_type"] = (
-            "multi-GPU"
-            if trainer.args.parallel_mode == ParallelMode.DISTRIBUTED
-            else trainer.args.parallel_mode.value
+        encoder_sequence = "past_encoder_sequence"
+        decoder_sequence = (
+            "past_decoder_sequence"
+            if direction == "inputs"
+            else "past_decoder_sequence + sequence"
         )
-    if trainer.args.world_size > 1:
-        hyperparameters["num_devices"] = trainer.args.world_size
-    if trainer.args.gradient_accumulation_steps > 1:
-        hyperparameters[
-            "gradient_accumulation_steps"
-        ] = trainer.args.gradient_accumulation_steps
-
-    total_train_batch_size = (
-        trainer.args.train_batch_size
-        * trainer.args.world_size
-        * trainer.args.gradient_accumulation_steps
-    )
-    if total_train_batch_size != hyperparameters["train_batch_size"]:
-        hyperparameters["total_train_batch_size"] = total_train_batch_size
-    total_eval_batch_size = trainer.args.eval_batch_size * trainer.args.world_size
-    if total_eval_batch_size != hyperparameters["eval_batch_size"]:
-        hyperparameters["total_eval_batch_size"] = total_eval_batch_size
-
-    if trainer.args.adafactor:
-        hyperparameters["optimizer"] = "Adafactor"
-    else:
-        hyperparameters["optimizer"] = (
-            f"Adam with betas=({trainer.args.adam_beta1},{trainer.args.adam_beta2}) and"
-            f" epsilon={trainer.args.adam_epsilon}"
-        )
-
-    hyperparameters["lr_scheduler_type"] = trainer.args.lr_scheduler_type.value
-    if trainer.args.warmup_ratio != 0.0:
-        hyperparameters["lr_scheduler_warmup_ratio"] = trainer.args.warmup_ratio
-    if trainer.args.warmup_steps != 0.0:
-        hyperparameters["lr_scheduler_warmup_steps"] = trainer.args.warmup_steps
-    if trainer.args.max_steps != -1:
-        hyperparameters["training_steps"] = trainer.args.max_steps
-    else:
-        hyperparameters["num_epochs"] = trainer.args.num_train_epochs
-
-    if trainer.args.fp16:
-        if trainer.use_cuda_amp:
-            hyperparameters["mixed_precision_training"] = "Native AMP"
-        elif trainer.use_apex:
-            hyperparameters[
-                "mixed_precision_training"
-            ] = f"Apex, opt level {trainer.args.fp16_opt_level}"
 
-    if trainer.args.label_smoothing_factor != 0.0:
-        hyperparameters["label_smoothing_factor"] = trainer.args.label_smoothing_factor
+        for i in range(min_num_layers):
+            inputs_or_outputs[f"{name}.{i}.decoder.key"] = {
+                0: "batch",
+                2: decoder_sequence,
+            }
+            inputs_or_outputs[f"{name}.{i}.decoder.value"] = {
+                0: "batch",
+                2: decoder_sequence,
+            }
+            inputs_or_outputs[f"{name}.{i}.encoder.key"] = {
+                0: "batch",
+                2: encoder_sequence,
+            }
+            inputs_or_outputs[f"{name}.{i}.encoder.value"] = {
+                0: "batch",
+                2: encoder_sequence,
+            }
+
+        for i in range(min_num_layers, max_num_layers):
+            if remaining_side_name == "encoder":
+                axes_info = {0: "batch", 2: encoder_sequence}
+            else:
+                axes_info = {0: "batch", 2: decoder_sequence}
+            inputs_or_outputs[f"{name}.{i}.{remaining_side_name}.key"] = axes_info
 
-    return hyperparameters
+    def _flatten_past_key_values_(self, flattened_output, name, idx, t):
+        flattened_output[f"{name}.{idx}.decoder.key"] = t[0]
+        flattened_output[f"{name}.{idx}.decoder.value"] = t[1]
+        flattened_output[f"{name}.{idx}.encoder.key"] = t[2]
+        flattened_output[f"{name}.{idx}.encoder.value"] = t[3]
```

### Comparing `xs_transformers-1.0.0/xs_transformers/processing_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/retribert/modeling_retribert.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,254 +1,240 @@
 # coding=utf-8
-# Copyright 2022 The HuggingFace Inc. team.
+# Copyright 2019-present, the HuggingFace Inc. team, The Google AI Language Team and Facebook, Inc.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
- Processing saving/loading class for common processors.
+RetriBERT model
 """
 
-import importlib.util
-import os
-from pathlib import Path
-
-from .dynamic_module_utils import custom_object_save
-from .tokenization_utils_base import PreTrainedTokenizerBase
-from .utils import PushToHubMixin, copy_func, logging
 
-logger = logging.get_logger(__name__)
+import math
+from typing import Optional
 
-# Dynamically import the Transformers module to grab the attribute classes of the processor form their names.
-spec = importlib.util.spec_from_file_location(
-    "transformers",
-    Path(__file__).parent / "__init__.py",
-    submodule_search_locations=[Path(__file__).parent],
-)
-transformers_module = spec.loader.load_module()
+import torch
+import torch.utils.checkpoint as checkpoint
+from torch import nn
+
+from ...modeling_utils import PreTrainedModel
+from ...utils import add_start_docstrings, logging
+from ..bert.modeling_bert import BertModel
+from .configuration_retribert import RetriBertConfig
 
+logger = logging.get_logger(__name__)
 
-AUTO_TO_BASE_CLASS_MAPPING = {
-    "AutoTokenizer": "PreTrainedTokenizerBase",
-    "AutoFeatureExtractor": "FeatureExtractionMixin",
-}
+RETRIBERT_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "yjernite/retribert-base-uncased",
+    # See all RetriBert models at https://huggingface.co/models?filter=retribert
+]
 
 
-class ProcessorMixin(PushToHubMixin):
+# INTERFACE FOR ENCODER AND TASK SPECIFIC MODEL #
+class RetriBertPreTrainedModel(PreTrainedModel):
     """
-    This is a mixin used to provide saving/loading functionality for all processor classes.
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
     """
 
-    attributes = ["feature_extractor", "tokenizer"]
-    # Names need to be attr_class for attr in attributes
-    feature_extractor_class = None
-    tokenizer_class = None
-    _auto_class = None
-
-    # args have to match the attributes class attribute
-    def __init__(self, *args, **kwargs):
-        # Sanitize args and kwargs
-        for key in kwargs:
-            if key not in self.attributes:
-                raise TypeError(f"Unexepcted keyword argument {key}.")
-        for arg, attribute_name in zip(args, self.attributes):
-            if attribute_name in kwargs:
-                raise TypeError(f"Got multiple values for argument {attribute_name}.")
-            else:
-                kwargs[attribute_name] = arg
-
-        if len(kwargs) != len(self.attributes):
-            raise ValueError(
-                f"This processor requires {len(self.attributes)} arguments: {', '.join(self.attributes)}. Got "
-                f"{len(args)} arguments instead."
-            )
-
-        # Check each arg is of the proper class (this will also catch a user initializing in the wrong order)
-        for attribute_name, arg in kwargs.items():
-            class_name = getattr(self, f"{attribute_name}_class")
-            # Nothing is ever going to be an instance of "AutoXxx", in that case we check the base class.
-            class_name = AUTO_TO_BASE_CLASS_MAPPING.get(class_name, class_name)
-            if isinstance(class_name, tuple):
-                proper_class = tuple(
-                    getattr(transformers_module, n) for n in class_name if n is not None
-                )
-            else:
-                proper_class = getattr(transformers_module, class_name)
-
-            if not isinstance(arg, proper_class):
-                raise ValueError(
-                    f"Received a {type(arg).__name__} for argument {attribute_name}, but a {class_name} was expected."
-                )
-
-            setattr(self, attribute_name, arg)
-
-    def __repr__(self):
-        attributes_repr = [
-            f"- {name}: {repr(getattr(self, name))}" for name in self.attributes
-        ]
-        attributes_repr = "\n".join(attributes_repr)
-        return f"{self.__class__.__name__}:\n{attributes_repr}"
-
-    def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):
-        """
-        Saves the attributes of this processor (feature extractor, tokenizer...) in the specified directory so that it
-        can be reloaded using the [`~ProcessorMixin.from_pretrained`] method.
+    config_class = RetriBertConfig
+    load_tf_weights = None
+    base_model_prefix = "retribert"
+
+    def _init_weights(self, module):
+        """Initialize the weights"""
+        if isinstance(module, nn.Linear):
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.bias is not None:
+                module.bias.data.zero_()
+        elif isinstance(module, nn.Embedding):
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.padding_idx is not None:
+                module.weight.data[module.padding_idx].zero_()
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+
+
+RETRIBERT_START_DOCSTRING = r"""
+
+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the
+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
+    etc.)
+
+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.
+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
+    and behavior.
+
+    Parameters:
+        config ([`RetriBertConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
+"""
 
-        <Tip>
 
-        This class method is simply calling [`~feature_extraction_utils.FeatureExtractionMixin.save_pretrained`] and
-        [`~tokenization_utils_base.PreTrainedTokenizerBase.save_pretrained`]. Please refer to the docstrings of the
-        methods above for more information.
+@add_start_docstrings(
+    """Bert Based model to embed queries or document for document retrieval.""",
+    RETRIBERT_START_DOCSTRING,
+)
+class RetriBertModel(RetriBertPreTrainedModel):
+    def __init__(self, config: RetriBertConfig) -> None:
+        super().__init__(config)
+        self.projection_dim = config.projection_dim
+
+        self.bert_query = BertModel(config)
+        self.bert_doc = None if config.share_encoders else BertModel(config)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        self.project_query = nn.Linear(
+            config.hidden_size, config.projection_dim, bias=False
+        )
+        self.project_doc = nn.Linear(
+            config.hidden_size, config.projection_dim, bias=False
+        )
 
-        </Tip>
+        self.ce_loss = nn.CrossEntropyLoss(reduction="mean")
 
-        Args:
-            save_directory (`str` or `os.PathLike`):
-                Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
-                be created if it does not exist).
-            push_to_hub (`bool`, *optional*, defaults to `False`):
-                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
-                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your
-                namespace).
-            kwargs:
-                Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.
-        """
-        os.makedirs(save_directory, exist_ok=True)
+        # Initialize weights and apply final processing
+        self.post_init()
 
-        if push_to_hub:
-            commit_message = kwargs.pop("commit_message", None)
-            repo_id = kwargs.pop("repo_id", save_directory.split(os.path.sep)[-1])
-            repo_id, token = self._create_repo(repo_id, **kwargs)
-            files_timestamps = self._get_files_timestamps(save_directory)
-        # If we have a custom config, we copy the file defining it in the folder and set the attributes so it can be
-        # loaded from the Hub.
-        if self._auto_class is not None:
-            attrs = [
-                getattr(self, attribute_name) for attribute_name in self.attributes
-            ]
-            configs = [
-                (a.init_kwargs if isinstance(a, PreTrainedTokenizerBase) else a)
-                for a in attrs
-            ]
-            custom_object_save(self, save_directory, config=configs)
-
-        for attribute_name in self.attributes:
-            attribute = getattr(self, attribute_name)
-            # Include the processor class in the attribute config so this processor can then be reloaded with the
-            # `AutoProcessor` API.
-            if hasattr(attribute, "_set_processor_class"):
-                attribute._set_processor_class(self.__class__.__name__)
-            attribute.save_pretrained(save_directory)
-
-        if self._auto_class is not None:
-            # We added an attribute to the init_kwargs of the tokenizers, which needs to be cleaned up.
-            for attribute_name in self.attributes:
-                attribute = getattr(self, attribute_name)
-                if isinstance(attribute, PreTrainedTokenizerBase):
-                    del attribute.init_kwargs["auto_map"]
-
-        if push_to_hub:
-            self._upload_modified_files(
-                save_directory,
-                repo_id,
-                files_timestamps,
-                commit_message=commit_message,
-                token=token,
+    def embed_sentences_checkpointed(
+        self,
+        input_ids,
+        attention_mask,
+        sent_encoder,
+        checkpoint_batch_size=-1,
+    ):
+        # reproduces BERT forward pass with checkpointing
+        if checkpoint_batch_size < 0 or input_ids.shape[0] < checkpoint_batch_size:
+            return sent_encoder(input_ids, attention_mask=attention_mask)[1]
+        else:
+            # prepare implicit variables
+            device = input_ids.device
+            input_shape = input_ids.size()
+            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)
+            head_mask = [None] * sent_encoder.config.num_hidden_layers
+            extended_attention_mask: torch.Tensor = (
+                sent_encoder.get_extended_attention_mask(attention_mask, input_shape)
             )
 
-    @classmethod
-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
-        r"""
-        Instantiate a processor associated with a pretrained model.
-
-        <Tip>
-
-        This class method is simply calling the feature extractor
-        [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] and the tokenizer
-        [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`] methods. Please refer to the docstrings of the
-        methods above for more information.
-
-        </Tip>
-
-        Args:
-            pretrained_model_name_or_path (`str` or `os.PathLike`):
-                This can be either:
+            # define function for checkpointing
+            def partial_encode(*inputs):
+                encoder_outputs = sent_encoder.encoder(
+                    inputs[0],
+                    attention_mask=inputs[1],
+                    head_mask=head_mask,
+                )
+                sequence_output = encoder_outputs[0]
+                pooled_output = sent_encoder.pooler(sequence_output)
+                return pooled_output
+
+            # run embedding layer on everything at once
+            embedding_output = sent_encoder.embeddings(
+                input_ids=input_ids,
+                position_ids=None,
+                token_type_ids=token_type_ids,
+                inputs_embeds=None,
+            )
+            # run encoding and pooling on one mini-batch at a time
+            pooled_output_list = []
+            for b in range(math.ceil(input_ids.shape[0] / checkpoint_batch_size)):
+                b_embedding_output = embedding_output[
+                    b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size
+                ]
+                b_attention_mask = extended_attention_mask[
+                    b * checkpoint_batch_size : (b + 1) * checkpoint_batch_size
+                ]
+                pooled_output = checkpoint.checkpoint(
+                    partial_encode, b_embedding_output, b_attention_mask
+                )
+                pooled_output_list.append(pooled_output)
+            return torch.cat(pooled_output_list, dim=0)
 
-                - a string, the *model id* of a pretrained feature_extractor hosted inside a model repo on
-                  huggingface.co. Valid model ids can be located at the root-level, like `bert-base-uncased`, or
-                  namespaced under a user or organization name, like `dbmdz/bert-base-german-cased`.
-                - a path to a *directory* containing a feature extractor file saved using the
-                  [`~SequenceFeatureExtractor.save_pretrained`] method, e.g., `./my_model_directory/`.
-                - a path or url to a saved feature extractor JSON *file*, e.g.,
-                  `./my_model_directory/preprocessor_config.json`.
-            **kwargs
-                Additional keyword arguments passed along to both
-                [`~feature_extraction_utils.FeatureExtractionMixin.from_pretrained`] and
-                [`~tokenization_utils_base.PreTrainedTokenizer.from_pretrained`].
-        """
-        args = cls._get_arguments_from_pretrained(
-            pretrained_model_name_or_path, **kwargs
+    def embed_questions(
+        self,
+        input_ids,
+        attention_mask=None,
+        checkpoint_batch_size=-1,
+    ):
+        q_reps = self.embed_sentences_checkpointed(
+            input_ids,
+            attention_mask,
+            self.bert_query,
+            checkpoint_batch_size,
         )
-        return cls(*args)
-
-    @classmethod
-    def register_for_auto_class(cls, auto_class="AutoProcessor"):
-        """
-        Register this class with a given auto class. This should only be used for custom feature extractors as the ones
-        in the library are already mapped with `AutoProcessor`.
+        return self.project_query(q_reps)
 
-        <Tip warning={true}>
-
-        This API is experimental and may have some slight breaking changes in the next releases.
-
-        </Tip>
+    def embed_answers(
+        self,
+        input_ids,
+        attention_mask=None,
+        checkpoint_batch_size=-1,
+    ):
+        a_reps = self.embed_sentences_checkpointed(
+            input_ids,
+            attention_mask,
+            self.bert_query if self.bert_doc is None else self.bert_doc,
+            checkpoint_batch_size,
+        )
+        return self.project_doc(a_reps)
 
+    def forward(
+        self,
+        input_ids_query: torch.LongTensor,
+        attention_mask_query: Optional[torch.FloatTensor],
+        input_ids_doc: torch.LongTensor,
+        attention_mask_doc: Optional[torch.FloatTensor],
+        checkpoint_batch_size: int = -1,
+    ) -> torch.FloatTensor:
+        r"""
         Args:
-            auto_class (`str` or `type`, *optional*, defaults to `"AutoProcessor"`):
-                The auto class to register this new feature extractor with.
-        """
-        if not isinstance(auto_class, str):
-            auto_class = auto_class.__name__
-
-        import transformers.models.auto as auto_module
+            input_ids_query (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+                Indices of input sequence tokens in the vocabulary for the queries in a batch.
 
-        if not hasattr(auto_module, auto_class):
-            raise ValueError(f"{auto_class} is not a valid auto class.")
+                Indices can be obtained using [`RetriBertTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+                [`PreTrainedTokenizer.__call__`] for details.
 
-        cls._auto_class = auto_class
-
-    @classmethod
-    def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs):
-        args = []
-        for attribute_name in cls.attributes:
-            class_name = getattr(cls, f"{attribute_name}_class")
-            if isinstance(class_name, tuple):
-                classes = tuple(
-                    getattr(transformers_module, n) if n is not None else None
-                    for n in class_name
-                )
-                use_fast = kwargs.get("use_fast", True)
-                if use_fast and classes[1] is not None:
-                    attribute_class = classes[1]
-                else:
-                    attribute_class = classes[0]
-            else:
-                attribute_class = getattr(transformers_module, class_name)
-
-            args.append(
-                attribute_class.from_pretrained(pretrained_model_name_or_path, **kwargs)
-            )
-        return args
-
-
-ProcessorMixin.push_to_hub = copy_func(ProcessorMixin.push_to_hub)
-ProcessorMixin.push_to_hub.__doc__ = ProcessorMixin.push_to_hub.__doc__.format(
-    object="processor", object_class="AutoProcessor", object_files="processor files"
-)
+                [What are input IDs?](../glossary#input-ids)
+            attention_mask_query (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+                - 1 for tokens that are **not masked**,
+                - 0 for tokens that are **masked**.
+
+                [What are attention masks?](../glossary#attention-mask)
+            input_ids_doc (`torch.LongTensor` of shape `(batch_size, sequence_length)`):
+                Indices of input sequence tokens in the vocabulary for the documents in a batch.
+            attention_mask_doc (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):
+                Mask to avoid performing attention on documents padding token indices.
+            checkpoint_batch_size (`int`, *optional*, defaults to `-1`):
+                If greater than 0, uses gradient checkpointing to only compute sequence representation on
+                `checkpoint_batch_size` examples at a time on the GPU. All query representations are still compared to
+                all document representations in the batch.
+
+        Return:
+            `torch.FloatTensor``: The bidirectional cross-entropy loss obtained while trying to match each query to its
+            corresponding document and each document to its corresponding query in the batch
+        """
+        device = input_ids_query.device
+        q_reps = self.embed_questions(
+            input_ids_query, attention_mask_query, checkpoint_batch_size
+        )
+        a_reps = self.embed_answers(
+            input_ids_doc, attention_mask_doc, checkpoint_batch_size
+        )
+        compare_scores = torch.mm(q_reps, a_reps.t())
+        loss_qa = self.ce_loss(
+            compare_scores, torch.arange(compare_scores.shape[1]).to(device)
+        )
+        loss_aq = self.ce_loss(
+            compare_scores.t(), torch.arange(compare_scores.shape[0]).to(device)
+        )
+        loss = (loss_qa + loss_aq) / 2
+        return loss
```

### Comparing `xs_transformers-1.0.0/xs_transformers/tokenization_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,1047 +1,947 @@
 # coding=utf-8
-# Copyright 2020 The HuggingFace Inc. team.
+# Copyright 2022 The HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 """
- Tokenization classes for python tokenizers. For fast tokenizers (provided by HuggingFace's tokenizers library) see
- tokenization_utils_fast.py
+Fast tokenization class for LayoutLMv3. It overwrites 2 methods of the slow tokenizer class, namely _batch_encode_plus
+and _encode_plus, in which the Rust tokenizer is used.
 """
-import bisect
-import itertools
-import re
-import unicodedata
-from collections import OrderedDict
-from typing import Any, Dict, List, Optional, Tuple, Union, overload
-
-from .tokenization_utils_base import (
-    ENCODE_KWARGS_DOCSTRING,
-    ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,
-    INIT_TOKENIZER_DOCSTRING,
-    AddedToken,
+
+import json
+from typing import Dict, List, Optional, Tuple, Union
+
+from tokenizers import pre_tokenizers, processors
+
+from ...tokenization_utils_base import (
     BatchEncoding,
     EncodedInput,
-    EncodedInputPair,
+    PaddingStrategy,
     PreTokenizedInput,
-    PreTokenizedInputPair,
-    PreTrainedTokenizerBase,
+    TensorType,
     TextInput,
     TextInputPair,
     TruncationStrategy,
 )
-from .utils import PaddingStrategy, TensorType, add_end_docstrings, logging
+from ...tokenization_utils_fast import PreTrainedTokenizerFast
+from ...utils import add_end_docstrings, logging
+from .tokenization_layoutlmv3 import (
+    LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING,
+    LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,
+    LayoutLMv3Tokenizer,
+)
 
 logger = logging.get_logger(__name__)
 
-# Slow tokenizers are saved in a vocabulary plus three separated files
-SPECIAL_TOKENS_MAP_FILE = "special_tokens_map.json"
-ADDED_TOKENS_FILE = "added_tokens.json"
-TOKENIZER_CONFIG_FILE = "tokenizer_config.json"
-
-
-class Trie:
+VOCAB_FILES_NAMES = {
+    "vocab_file": "vocab.json",
+    "merges_file": "merges.txt",
+    "tokenizer_file": "tokenizer.json",
+}
+
+PRETRAINED_VOCAB_FILES_MAP = {
+    "vocab_file": {
+        "microsoft/layoutlmv3-base": "https://huggingface.co/microsoft/layoutlmv3-base/raw/main/vocab.json",
+        "microsoft/layoutlmv3-large": "https://huggingface.co/microsoft/layoutlmv3-large/raw/main/vocab.json",
+    },
+    "merges_file": {
+        "microsoft/layoutlmv3-base": "https://huggingface.co/microsoft/layoutlmv3-base/raw/main/merges.txt",
+        "microsoft/layoutlmv3-large": "https://huggingface.co/microsoft/layoutlmv3-large/raw/main/merges.txt",
+    },
+}
+
+PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
+    "microsoft/layoutlmv3-base": 512,
+    "microsoft/layoutlmv3-large": 512,
+}
+
+
+class LayoutLMv3TokenizerFast(PreTrainedTokenizerFast):
+    r"""
+    Construct a "fast" LayoutLMv3 tokenizer (backed by HuggingFace's *tokenizers* library). Based on BPE.
+
+    This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should
+    refer to this superclass for more information regarding those methods.
+
+    Args:
+        vocab_file (`str`):
+            Path to the vocabulary file.
+        merges_file (`str`):
+            Path to the merges file.
+        errors (`str`, *optional*, defaults to `"replace"`):
+            Paradigm to follow when decoding bytes to UTF-8. See
+            [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.
+        bos_token (`str`, *optional*, defaults to `"<s>"`):
+            The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.
+
+            <Tip>
+
+            When building a sequence using special tokens, this is not the token that is used for the beginning of
+            sequence. The token used is the `cls_token`.
+
+            </Tip>
+
+        eos_token (`str`, *optional*, defaults to `"</s>"`):
+            The end of sequence token.
+
+            <Tip>
+
+            When building a sequence using special tokens, this is not the token that is used for the end of sequence.
+            The token used is the `sep_token`.
+
+            </Tip>
+
+        sep_token (`str`, *optional*, defaults to `"</s>"`):
+            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
+            sequence classification or for a text and a question for question answering. It is also used as the last
+            token of a sequence built with special tokens.
+        cls_token (`str`, *optional*, defaults to `"<s>"`):
+            The classifier token which is used when doing sequence classification (classification of the whole sequence
+            instead of per-token classification). It is the first token of the sequence when built with special tokens.
+        unk_token (`str`, *optional*, defaults to `"<unk>"`):
+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
+            token instead.
+        pad_token (`str`, *optional*, defaults to `"<pad>"`):
+            The token used for padding, for example when batching sequences of different lengths.
+        mask_token (`str`, *optional*, defaults to `"<mask>"`):
+            The token used for masking values. This is the token used when training this model with masked language
+            modeling. This is the token which the model will try to predict.
+        add_prefix_space (`bool`, *optional*, defaults to `False`):
+            Whether or not to add an initial space to the input. This allows to treat the leading word just as any
+            other word. (RoBERTa tokenizer detect beginning of words by the preceding space).
+        trim_offsets (`bool`, *optional*, defaults to `True`):
+            Whether the post processing step should trim offsets to avoid including whitespaces.
+        cls_token_box (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`):
+            The bounding box to use for the special [CLS] token.
+        sep_token_box (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`):
+            The bounding box to use for the special [SEP] token.
+        pad_token_box (`List[int]`, *optional*, defaults to `[0, 0, 0, 0]`):
+            The bounding box to use for the special [PAD] token.
+        pad_token_label (`int`, *optional*, defaults to -100):
+            The label to use for padding tokens. Defaults to -100, which is the `ignore_index` of PyTorch's
+            CrossEntropyLoss.
+        only_label_first_subword (`bool`, *optional*, defaults to `True`):
+            Whether or not to only label the first subword, in case word labels are provided.
     """
-    Trie in Python. Creates a Trie out of a list of words. The trie is used to split on `added_tokens` in one pass
-    Loose reference https://en.wikipedia.org/wiki/Trie
-    """
-
-    def __init__(self):
-        self.data = {}
-
-    def add(self, word: str):
-        """
-        Passes over every char (utf-8 char) on word and recursively adds it to the internal `data` trie representation.
-        The special key `""` is used to represent termination.
-
-        This function is idempotent, adding twice the same word will leave the trie unchanged
 
-        Example:
-
-        ```python
-        >>> trie = Trie()
-        >>> trie.add("Hello 友達")
-        >>> trie.data
-        {"H": {"e": {"l": {"l": {"o": {" ": {"友": {"達": {"": 1}}}}}}}}}
-
-        >>> trie.add("Hello")
-        >>> trie.data
-        {"H": {"e": {"l": {"l": {"o": {"": 1, " ": {"友": {"達": {"": 1}}}}}}}}}
-        ```
-        """
-        if not word:
-            # Prevent empty string
-            return
-        ref = self.data
-        for char in word:
-            ref[char] = char in ref and ref[char] or {}
-            ref = ref[char]
-        ref[""] = 1
-
-    def split(self, text: str) -> List[str]:
-        """
-        Will look for the words added to the trie within `text`. Output is the original string splitted along the
-        boundaries of the words found.
-
-        This trie will match the longest possible word first !
-
-        Example:
-
-        ```python
-        >>> trie = Trie()
-        >>> trie.split("[CLS] This is a extra_id_100")
-        ["[CLS] This is a extra_id_100"]
-
-        >>> trie.add("[CLS]")
-        >>> trie.add("extra_id_1")
-        >>> trie.add("extra_id_100")
-        >>> trie.split("[CLS] This is a extra_id_100")
-        ["[CLS]", " This is a ", "extra_id_100"]
-        ```
-        """
-        # indexes are counted left of the chars index.
-        # "hello", index 0, is left of h, index 1 is between h and e.
-        # index 5 is right of the "o".
-
-        # States are going to capture every possible start (indexes as above)
-        # as keys, and have as values, a pointer to the position in the trie
-        # where we're at. This is a partial match for now.
-        # This enables to keep track of multiple matches while we're iterating
-        # the string
-        # If the trie contains, "blowing", and "lower" and we encounter the
-        # string "blower", we need to split into ["b", "lower"].
-        # This is where we need to keep track of multiple possible starts.
-        states = OrderedDict()
-
-        # This will contain every indices where we need
-        # to cut.
-        # We force to cut at offset 0 and len(text) (added later)
-        offsets = [0]
-
-        # This is used by the lookahead which needs to skip over
-        # some text where the full match exceeded the place in the initial
-        # for loop
-        skip = 0
-        # Main loop, Giving this algorithm O(n) complexity
-        for current, current_char in enumerate(text):
-            if skip and current < skip:
-                # Prevents the lookahead for matching twice
-                # like extra_id_100 and id_100
-                continue
-
-            # This will track every state
-            # that stop matching, we need to stop tracking them.
-            # If we look at "lowball", we're going to match "l" (add it to states), "o", "w", then
-            # fail on "b", we need to remove 0 from the valid states.
-            to_remove = set()
-            # Whenever we found a match, we need to drop everything
-            # this is a greedy algorithm, it will match on the first found token
-            reset = False
-
-            # In this case, we already have partial matches (But unfinished)
-            for start, trie_pointer in states.items():
-                if "" in trie_pointer:
-                    # This is a final match, we need to reset and
-                    # store the results in `offsets`.
-
-                    # Lookahead to match longest first
-                    # Important in case of extra_id_1 vs extra_id_100
-                    # Here we are also actively looking for other earlier partial
-                    # matches
-                    # "[CLS]", "L", we need to match CLS even if L is special
-                    for lookstart, looktrie_pointer in states.items():
-                        if lookstart > start:
-                            # This partial match is later, we can stop looking
-                            break
-                        elif lookstart < start:
-                            # This partial match is earlier, the trie pointer
-                            # was already updated, so index is + 1
-                            lookahead_index = current + 1
-                            end = current + 1
-                        else:
-                            # Here lookstart == start and
-                            #      looktrie_pointer == trie_pointer
-                            # It wasn't updated yet so indices are current ones
-                            lookahead_index = current
-                            end = current
-                        next_char = (
-                            text[lookahead_index]
-                            if lookahead_index < len(text)
-                            else None
-                        )
-                        if "" in looktrie_pointer:
-                            start = lookstart
-                            end = lookahead_index
-                            skip = lookahead_index
-
-                        while next_char in looktrie_pointer:
-                            looktrie_pointer = looktrie_pointer[next_char]
-                            lookahead_index += 1
-                            if "" in looktrie_pointer:
-                                start = lookstart
-                                end = lookahead_index
-                                skip = lookahead_index
-
-                            if lookahead_index == len(text):
-                                # End of string
-                                break
-                            next_char = text[lookahead_index]
-                        # End lookahead
-
-                    # Storing and resetting
-                    offsets.append(start)
-                    offsets.append(end)
-                    reset = True
-                    break
-                elif current_char in trie_pointer:
-                    # The current character being looked at has a match within the trie
-                    # update the pointer (it will be stored back into states later).
-                    trie_pointer = trie_pointer[current_char]
-
-                    # Storing back the new pointer into the states.
-                    # Partial matches got longer by one.
-                    states[start] = trie_pointer
-                else:
-                    # The new character has not match in the trie, we need
-                    # to stop keeping track of this partial match.
-                    # We can't do it directly within the loop because of how
-                    # python iteration works
-                    to_remove.add(start)
-
-            # Either clearing the full start (we found a real match)
-            # Or clearing only the partial matches that didn't work.
-            if reset:
-                states = {}
-            else:
-                for start in to_remove:
-                    del states[start]
+    vocab_files_names = VOCAB_FILES_NAMES
+    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
+    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
+    model_input_names = ["input_ids", "attention_mask"]
+    slow_tokenizer_class = LayoutLMv3Tokenizer
 
-            # If this character is a starting character within the trie
-            # start keeping track of this partial match.
-            if current >= skip and current_char in self.data:
-                states[current] = self.data[current_char]
-
-        # We have a cut at the end with states.
-        for start, trie_pointer in states.items():
-            if "" in trie_pointer:
-                # This is a final match, we need to reset and
-                # store the results in `offsets`.
-                end = len(text)
-                offsets.append(start)
-                offsets.append(end)
-                # Longest cut is always the one with lower start so the first
-                # item so we need to break.
-                break
-
-        return self.cut_text(text, offsets)
-
-    def cut_text(self, text, offsets):
-        # We have all the offsets now, we just need to do the actual splitting.
-        # We need to eventually add the first part of the string and the eventual
-        # last part.
-        offsets.append(len(text))
-        tokens = []
-        start = 0
-        for end in offsets:
-            if start > end:
-                logger.error(
-                    "There was a bug in Trie algorithm in tokenization. Attempting to recover. Please report it"
-                    " anyway."
-                )
-                continue
-            elif start == end:
-                # This might happen if there's a match at index 0
-                # we're also preventing zero-width cuts in case of two
-                # consecutive matches
-                continue
-            tokens.append(text[start:end])
-            start = end
-
-        return tokens
-
-
-def _is_whitespace(char):
-    """Checks whether `char` is a whitespace character."""
-    # \t, \n, and \r are technically control characters but we treat them
-    # as whitespace since they are generally considered as such.
-    if char == " " or char == "\t" or char == "\n" or char == "\r":
-        return True
-    cat = unicodedata.category(char)
-    if cat == "Zs":
-        return True
-    return False
-
-
-def _is_control(char):
-    """Checks whether `char` is a control character."""
-    # These are technically control characters but we count them as whitespace
-    # characters.
-    if char == "\t" or char == "\n" or char == "\r":
-        return False
-    cat = unicodedata.category(char)
-    if cat.startswith("C"):
-        return True
-    return False
-
-
-def _is_punctuation(char):
-    """Checks whether `char` is a punctuation character."""
-    cp = ord(char)
-    # We treat all non-letter/number ASCII as punctuation.
-    # Characters such as "^", "$", and "`" are not in the Unicode
-    # Punctuation class but we treat them as punctuation anyways, for
-    # consistency.
-    if (
-        (cp >= 33 and cp <= 47)
-        or (cp >= 58 and cp <= 64)
-        or (cp >= 91 and cp <= 96)
-        or (cp >= 123 and cp <= 126)
+    def __init__(
+        self,
+        vocab_file=None,
+        merges_file=None,
+        tokenizer_file=None,
+        errors="replace",
+        bos_token="<s>",
+        eos_token="</s>",
+        sep_token="</s>",
+        cls_token="<s>",
+        unk_token="<unk>",
+        pad_token="<pad>",
+        mask_token="<mask>",
+        add_prefix_space=True,
+        trim_offsets=True,
+        cls_token_box=[0, 0, 0, 0],
+        sep_token_box=[0, 0, 0, 0],
+        pad_token_box=[0, 0, 0, 0],
+        pad_token_label=-100,
+        only_label_first_subword=True,
+        **kwargs,
     ):
-        return True
-    cat = unicodedata.category(char)
-    if cat.startswith("P"):
-        return True
-    return False
-
+        super().__init__(
+            vocab_file,
+            merges_file,
+            tokenizer_file=tokenizer_file,
+            errors=errors,
+            bos_token=bos_token,
+            eos_token=eos_token,
+            sep_token=sep_token,
+            cls_token=cls_token,
+            unk_token=unk_token,
+            pad_token=pad_token,
+            mask_token=mask_token,
+            add_prefix_space=add_prefix_space,
+            trim_offsets=trim_offsets,
+            cls_token_box=cls_token_box,
+            sep_token_box=sep_token_box,
+            pad_token_box=pad_token_box,
+            pad_token_label=pad_token_label,
+            only_label_first_subword=only_label_first_subword,
+            **kwargs,
+        )
 
-def _is_end_of_word(text):
-    """Checks whether the last character in text is one of a punctuation, control or whitespace character."""
-    last_char = text[-1]
-    return bool(
-        _is_control(last_char) | _is_punctuation(last_char) | _is_whitespace(last_char)
-    )
+        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())
+        if pre_tok_state.get("add_prefix_space", add_prefix_space) != add_prefix_space:
+            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop("type"))
+            pre_tok_state["add_prefix_space"] = add_prefix_space
+            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)
+
+        self.add_prefix_space = add_prefix_space
+
+        tokenizer_component = "post_processor"
+        tokenizer_component_instance = getattr(
+            self.backend_tokenizer, tokenizer_component, None
+        )
+        if tokenizer_component_instance:
+            state = json.loads(tokenizer_component_instance.__getstate__())
 
+            # The lists 'sep' and 'cls' must be cased in tuples for the object `post_processor_class`
+            if "sep" in state:
+                state["sep"] = tuple(state["sep"])
+            if "cls" in state:
+                state["cls"] = tuple(state["cls"])
+
+            changes_to_apply = False
+
+            if state.get("add_prefix_space", add_prefix_space) != add_prefix_space:
+                state["add_prefix_space"] = add_prefix_space
+                changes_to_apply = True
+
+            if state.get("trim_offsets", trim_offsets) != trim_offsets:
+                state["trim_offsets"] = trim_offsets
+                changes_to_apply = True
+
+            if changes_to_apply:
+                component_class = getattr(processors, state.pop("type"))
+                new_value = component_class(**state)
+                setattr(self.backend_tokenizer, tokenizer_component, new_value)
+
+        # additional properties
+        self.cls_token_box = cls_token_box
+        self.sep_token_box = sep_token_box
+        self.pad_token_box = pad_token_box
+        self.pad_token_label = pad_token_label
+        self.only_label_first_subword = only_label_first_subword
 
-def _is_start_of_word(text):
-    """Checks whether the first character in text is one of a punctuation, control or whitespace character."""
-    first_char = text[0]
-    return bool(
-        _is_control(first_char)
-        | _is_punctuation(first_char)
-        | _is_whitespace(first_char)
+    @add_end_docstrings(
+        LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING,
+        LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,
     )
-
-
-def _insert_one_token_to_ordered_list(token_list: List[str], new_token: str):
-    """
-    Inserts one token to an ordered list if it does not already exist. Note: token_list must be sorted.
-    """
-    insertion_idx = bisect.bisect_left(token_list, new_token)
-    # Checks if new_token is already in the ordered token_list
-    if insertion_idx < len(token_list) and token_list[insertion_idx] == new_token:
-        # new_token is in token_list, don't add
-        return
-    else:
-        token_list.insert(insertion_idx, new_token)
-
-
-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
-class PreTrainedTokenizer(PreTrainedTokenizerBase):
-    """
-    Base class for all slow tokenizers.
-
-    Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].
-
-    Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading
-    pretrained tokenizers as well as adding tokens to the vocabulary.
-
-    This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the
-    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).
-    """
-
-    def __init__(self, **kwargs):
-        super().__init__(**kwargs)
-
-        # Added tokens - We store this for both slow and fast tokenizers
-        # until the serialization of Fast tokenizers is updated
-        self.added_tokens_encoder: Dict[str, int] = {}
-        self.added_tokens_decoder: Dict[int, str] = {}
-        self.unique_no_split_tokens: List[str] = []
-        self.tokens_trie = Trie()
-
-        self._decode_use_source_tokenizer = False
-
-    @property
-    def is_fast(self) -> bool:
-        return False
-
-    @property
-    def vocab_size(self) -> int:
-        """
-        `int`: Size of the base vocabulary (without the added tokens).
-        """
-        raise NotImplementedError
-
-    def get_added_vocab(self) -> Dict[str, int]:
-        """
-        Returns the added tokens in the vocabulary as a dictionary of token to index.
-
-        Returns:
-            `Dict[str, int]`: The added tokens.
-        """
-        return self.added_tokens_encoder
-
-    def __len__(self):
-        """
-        Size of the full vocabulary with the added tokens.
-        """
-        return self.vocab_size + len(self.added_tokens_encoder)
-
-    def _add_tokens(
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast.__call__
+    def __call__(
         self,
-        new_tokens: Union[List[str], List[AddedToken]],
-        special_tokens: bool = False,
-    ) -> int:
+        text: Union[
+            TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]
+        ],
+        text_pair: Optional[Union[PreTokenizedInput, List[PreTokenizedInput]]] = None,
+        boxes: Union[List[List[int]], List[List[List[int]]]] = None,
+        word_labels: Optional[Union[List[int], List[List[int]]]] = None,
+        add_special_tokens: bool = True,
+        padding: Union[bool, str, PaddingStrategy] = False,
+        truncation: Union[bool, str, TruncationStrategy] = None,
+        max_length: Optional[int] = None,
+        stride: int = 0,
+        pad_to_multiple_of: Optional[int] = None,
+        return_tensors: Optional[Union[str, TensorType]] = None,
+        return_token_type_ids: Optional[bool] = None,
+        return_attention_mask: Optional[bool] = None,
+        return_overflowing_tokens: bool = False,
+        return_special_tokens_mask: bool = False,
+        return_offsets_mapping: bool = False,
+        return_length: bool = False,
+        verbose: bool = True,
+        **kwargs,
+    ) -> BatchEncoding:
         """
-        Add a list of new tokens to the tokenizer class. If the new tokens are not in the vocabulary, they are added to
-        it with indices starting from length of the current vocabulary.
+        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of
+        sequences with word-level normalized bounding boxes and optional labels.
 
         Args:
-            new_tokens (`List[str]`or `List[tokenizers.AddedToken]`):
-                Token(s) to add in vocabulary. A token is only added if it's not already in the vocabulary (tested by
-                checking if the tokenizer assign the index of the `unk_token` to them).
-            special_tokens (`bool`, *optional*, defaults to `False`):
-                Whether or not the tokens should be added as special tokens.
-
-        Returns:
-            `int`: The number of tokens actually added to the vocabulary.
-
-        Examples:
-
-        ```python
-        # Let's see how to increase the vocabulary of Bert model and tokenizer
-        tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
-        model = BertModel.from_pretrained("bert-base-uncased")
-
-        num_added_toks = tokenizer.add_tokens(["new_tok1", "my_new-tok2"])
-        print("We have added", num_added_toks, "tokens")
-        # Note: resize_token_embeddings expects to receive the full size of the new vocabulary, i.e. the length of the tokenizer.
-        model.resize_token_embeddings(len(tokenizer))
-        ```"""
-        new_tokens = [str(tok) for tok in new_tokens]
-
-        tokens_to_add = []
-        for token in new_tokens:
-            if not isinstance(token, str):
-                raise TypeError(f"Token {token} is not a string but a {type(token)}.")
-            if (
-                not special_tokens
-                and hasattr(self, "do_lower_case")
-                and self.do_lower_case
-            ):
-                token = token.lower()
-            if (
-                token != self.unk_token
-                and self.convert_tokens_to_ids(token)
-                == self.convert_tokens_to_ids(self.unk_token)
-                and token not in tokens_to_add
-            ):
-                tokens_to_add.append(token)
-                if self.verbose:
-                    logger.info(f"Adding {token} to the vocabulary")
+            text (`str`, `List[str]`, `List[List[str]]`):
+                The sequence or batch of sequences to be encoded. Each sequence can be a string, a list of strings
+                (words of a single example or questions of a batch of examples) or a list of list of strings (batch of
+                words).
+            text_pair (`List[str]`, `List[List[str]]`):
+                The sequence or batch of sequences to be encoded. Each sequence should be a list of strings
+                (pretokenized string).
+            boxes (`List[List[int]]`, `List[List[List[int]]]`):
+                Word-level bounding boxes. Each bounding box should be normalized to be on a 0-1000 scale.
+            word_labels (`List[int]`, `List[List[int]]`, *optional*):
+                Word-level integer labels (for token classification tasks such as FUNSD, CORD).
+        """
+
+        # Input type checking for clearer error
+        def _is_valid_text_input(t):
+            if isinstance(t, str):
+                # Strings are fine
+                return True
+            elif isinstance(t, (list, tuple)):
+                # List are fine as long as they are...
+                if len(t) == 0:
+                    # ... empty
+                    return True
+                elif isinstance(t[0], str):
+                    # ... list of strings
+                    return True
+                elif isinstance(t[0], (list, tuple)):
+                    # ... list with an empty list or with a list of strings
+                    return len(t[0]) == 0 or isinstance(t[0][0], str)
+                else:
+                    return False
+            else:
+                return False
 
-        added_tok_encoder = dict(
-            (tok, len(self) + i) for i, tok in enumerate(tokens_to_add)
-        )
-        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}
-        self.added_tokens_encoder.update(added_tok_encoder)
-        self.added_tokens_decoder.update(added_tok_decoder)
-
-        # Make sure we don't split on any special tokens (even they were already in the vocab before e.g. for Albert)
-        if special_tokens:
-            if len(new_tokens) == 1:
-                _insert_one_token_to_ordered_list(
-                    self.unique_no_split_tokens, new_tokens[0]
+        if text_pair is not None:
+            # in case text + text_pair are provided, text = questions, text_pair = words
+            if not _is_valid_text_input(text):
+                raise ValueError(
+                    "text input must of type `str` (single example) or `List[str]` (batch of examples). "
                 )
-            else:
-                self.unique_no_split_tokens = sorted(
-                    set(self.unique_no_split_tokens).union(set(new_tokens))
+            if not isinstance(text_pair, (list, tuple)):
+                raise ValueError(
+                    "Words must be of type `List[str]` (single pretokenized example), "
+                    "or `List[List[str]]` (batch of pretokenized examples)."
                 )
         else:
-            # Or on the newly added tokens
-            if len(tokens_to_add) == 1:
-                _insert_one_token_to_ordered_list(
-                    self.unique_no_split_tokens, tokens_to_add[0]
-                )
-            else:
-                self.unique_no_split_tokens = sorted(
-                    set(self.unique_no_split_tokens).union(set(tokens_to_add))
+            # in case only text is provided => must be words
+            if not isinstance(text, (list, tuple)):
+                raise ValueError(
+                    "Words must be of type `List[str]` (single pretokenized example), "
+                    "or `List[List[str]]` (batch of pretokenized examples)."
                 )
-        self._create_trie(self.unique_no_split_tokens)
 
-        return len(tokens_to_add)
-
-    def _create_trie(self, unique_no_split_tokens):
-        trie = Trie()
-        for token in unique_no_split_tokens:
-            if (
-                hasattr(self, "do_lower_case")
-                and self.do_lower_case
-                and token not in self.all_special_tokens
-            ):
-                trie.add(token.lower())
-            else:
-                trie.add(token)
-        self.tokens_trie = trie
-
-    def num_special_tokens_to_add(self, pair: bool = False) -> int:
-        """
-        Returns the number of added tokens when encoding a sequence with special tokens.
-
-        <Tip>
-
-        This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put
-        this inside your training loop.
-
-        </Tip>
-
-        Args:
-            pair (`bool`, *optional*, defaults to `False`):
-                Whether the number of added tokens should be computed in the case of a sequence pair or a single
-                sequence.
-
-        Returns:
-            `int`: Number of special tokens added to sequences.
-        """
-        token_ids_0 = []
-        token_ids_1 = []
-        return len(
-            self.build_inputs_with_special_tokens(
-                token_ids_0, token_ids_1 if pair else None
+        if text_pair is not None:
+            is_batched = isinstance(text, (list, tuple))
+        else:
+            is_batched = (
+                isinstance(text, (list, tuple))
+                and text
+                and isinstance(text[0], (list, tuple))
             )
-        )
 
-    def tokenize(self, text: TextInput, **kwargs) -> List[str]:
-        """
-        Converts a string in a sequence of tokens, using the tokenizer.
-
-        Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies
-        (BPE/SentencePieces/WordPieces). Takes care of added tokens.
-
-        Args:
-            text (`str`):
-                The sequence to be encoded.
-            **kwargs (additional keyword arguments):
-                Passed along to the model-specific `prepare_for_tokenization` preprocessing method.
-
-        Returns:
-            `List[str]`: The list of tokens.
-        """
-        # Simple mapping string => AddedToken for special tokens with specific tokenization behaviors
-        all_special_tokens_extended = dict(
-            (str(t), t)
-            for t in self.all_special_tokens_extended
-            if isinstance(t, AddedToken)
-        )
-
-        text, kwargs = self.prepare_for_tokenization(text, **kwargs)
-
-        if kwargs:
-            logger.warning(f"Keyword arguments {kwargs} not recognized.")
+        words = text if text_pair is None else text_pair
+        if boxes is None:
+            raise ValueError("You must provide corresponding bounding boxes")
+        if is_batched:
+            if len(words) != len(boxes):
+                raise ValueError(
+                    "You must provide words and boxes for an equal amount of examples"
+                )
+            for words_example, boxes_example in zip(words, boxes):
+                if len(words_example) != len(boxes_example):
+                    raise ValueError(
+                        "You must provide as many words as there are bounding boxes"
+                    )
+        else:
+            if len(words) != len(boxes):
+                raise ValueError(
+                    "You must provide as many words as there are bounding boxes"
+                )
 
-        # TODO: should this be in the base class?
-        if hasattr(self, "do_lower_case") and self.do_lower_case:
-            # convert non-special tokens to lowercase
-            escaped_special_toks = [
-                re.escape(s_tok)
-                for s_tok in (self.unique_no_split_tokens + self.all_special_tokens)
-            ]
-            pattern = r"(" + r"|".join(escaped_special_toks) + r")|" + r"(.+?)"
-            text = re.sub(
-                pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), text
+        if is_batched:
+            if text_pair is not None and len(text) != len(text_pair):
+                raise ValueError(
+                    f"batch length of `text`: {len(text)} does not match batch length of `text_pair`:"
+                    f" {len(text_pair)}."
+                )
+            batch_text_or_text_pairs = (
+                list(zip(text, text_pair)) if text_pair is not None else text
+            )
+            is_pair = bool(text_pair is not None)
+            return self.batch_encode_plus(
+                batch_text_or_text_pairs=batch_text_or_text_pairs,
+                is_pair=is_pair,
+                boxes=boxes,
+                word_labels=word_labels,
+                add_special_tokens=add_special_tokens,
+                padding=padding,
+                truncation=truncation,
+                max_length=max_length,
+                stride=stride,
+                pad_to_multiple_of=pad_to_multiple_of,
+                return_tensors=return_tensors,
+                return_token_type_ids=return_token_type_ids,
+                return_attention_mask=return_attention_mask,
+                return_overflowing_tokens=return_overflowing_tokens,
+                return_special_tokens_mask=return_special_tokens_mask,
+                return_offsets_mapping=return_offsets_mapping,
+                return_length=return_length,
+                verbose=verbose,
+                **kwargs,
+            )
+        else:
+            return self.encode_plus(
+                text=text,
+                text_pair=text_pair,
+                boxes=boxes,
+                word_labels=word_labels,
+                add_special_tokens=add_special_tokens,
+                padding=padding,
+                truncation=truncation,
+                max_length=max_length,
+                stride=stride,
+                pad_to_multiple_of=pad_to_multiple_of,
+                return_tensors=return_tensors,
+                return_token_type_ids=return_token_type_ids,
+                return_attention_mask=return_attention_mask,
+                return_overflowing_tokens=return_overflowing_tokens,
+                return_special_tokens_mask=return_special_tokens_mask,
+                return_offsets_mapping=return_offsets_mapping,
+                return_length=return_length,
+                verbose=verbose,
+                **kwargs,
             )
 
-        no_split_token = set(self.unique_no_split_tokens)
-        tokens = self.tokens_trie.split(text)
-        # ["This is something", "<special_token_1>", "  else"]
-        for i, token in enumerate(tokens):
-            if token in no_split_token:
-                tok_extended = all_special_tokens_extended.get(token, None)
-                left = tokens[i - 1] if i > 0 else None
-                right = tokens[i + 1] if i < len(tokens) - 1 else None
-                if isinstance(tok_extended, AddedToken):
-                    if tok_extended.rstrip and right:
-                        # A bit counter-intuitive but we strip the left of the string
-                        # since tok_extended.rstrip means the special token is eating all white spaces on its right
-                        tokens[i + 1] = right.lstrip()
-                    # Strip white spaces on the left
-                    if tok_extended.lstrip and left:
-                        tokens[i - 1] = left.rstrip()  # Opposite here
-                else:
-                    # We strip left and right by default
-                    if right:
-                        tokens[i + 1] = right.lstrip()
-                    if left:
-                        tokens[i - 1] = left.rstrip()
-        # ["This is something", "<special_token_1>", "else"]
-        tokenized_text = []
-        for token in tokens:
-            # Need to skip eventual empty (fully stripped) tokens
-            if not token:
-                continue
-            if token in no_split_token:
-                tokenized_text.append(token)
-            else:
-                tokenized_text.extend(self._tokenize(token))
-        # ["This", " is", " something", "<special_token_1>", "else"]
-        return tokenized_text
-
-    def _tokenize(self, text, **kwargs):
-        """
-        Converts a string in a sequence of tokens (string), using the tokenizer. Split in words for word-based
-        vocabulary or sub-words for sub-word-based vocabularies (BPE/SentencePieces/WordPieces).
-
-        Do NOT take care of added tokens.
-        """
-        raise NotImplementedError
-
-    def convert_tokens_to_ids(
-        self, tokens: Union[str, List[str]]
-    ) -> Union[int, List[int]]:
-        """
-        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
-        vocabulary.
-
-        Args:
-            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).
-
-        Returns:
-            `int` or `List[int]`: The token id or list of token ids.
-        """
-        if tokens is None:
-            return None
-
-        if isinstance(tokens, str):
-            return self._convert_token_to_id_with_added_voc(tokens)
-
-        ids = []
-        for token in tokens:
-            ids.append(self._convert_token_to_id_with_added_voc(token))
-        return ids
-
-    def _convert_token_to_id_with_added_voc(self, token):
-        if token is None:
-            return None
-
-        if token in self.added_tokens_encoder:
-            return self.added_tokens_encoder[token]
-        return self._convert_token_to_id(token)
-
-    def _convert_token_to_id(self, token):
-        raise NotImplementedError
-
-    def _encode_plus(
+    @add_end_docstrings(
+        LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING,
+        LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,
+    )
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast.batch_encode_plus
+    def batch_encode_plus(
         self,
-        text: Union[TextInput, PreTokenizedInput, EncodedInput],
-        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,
+        batch_text_or_text_pairs: Union[
+            List[TextInput],
+            List[TextInputPair],
+            List[PreTokenizedInput],
+        ],
+        is_pair: bool = None,
+        boxes: Optional[List[List[List[int]]]] = None,
+        word_labels: Optional[Union[List[int], List[List[int]]]] = None,
         add_special_tokens: bool = True,
-        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
-        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
+        padding: Union[bool, str, PaddingStrategy] = False,
+        truncation: Union[bool, str, TruncationStrategy] = None,
         max_length: Optional[int] = None,
         stride: int = 0,
-        is_split_into_words: bool = False,
         pad_to_multiple_of: Optional[int] = None,
         return_tensors: Optional[Union[str, TensorType]] = None,
         return_token_type_ids: Optional[bool] = None,
         return_attention_mask: Optional[bool] = None,
         return_overflowing_tokens: bool = False,
         return_special_tokens_mask: bool = False,
         return_offsets_mapping: bool = False,
         return_length: bool = False,
         verbose: bool = True,
         **kwargs,
     ) -> BatchEncoding:
-        def get_input_ids(text):
-            if isinstance(text, str):
-                tokens = self.tokenize(text, **kwargs)
-                return self.convert_tokens_to_ids(tokens)
-            elif (
-                isinstance(text, (list, tuple))
-                and len(text) > 0
-                and isinstance(text[0], str)
-            ):
-                if is_split_into_words:
-                    tokens = list(
-                        itertools.chain(
-                            *(
-                                self.tokenize(t, is_split_into_words=True, **kwargs)
-                                for t in text
-                            )
-                        )
-                    )
-                    return self.convert_tokens_to_ids(tokens)
-                else:
-                    return self.convert_tokens_to_ids(text)
-            elif (
-                isinstance(text, (list, tuple))
-                and len(text) > 0
-                and isinstance(text[0], int)
-            ):
-                return text
-            else:
-                if is_split_into_words:
-                    raise ValueError(
-                        f"Input {text} is not valid. Should be a string or a list/tuple of strings when"
-                        " `is_split_into_words=True`."
-                    )
-                else:
-                    raise ValueError(
-                        f"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of"
-                        " integers."
-                    )
-
-        if return_offsets_mapping:
-            raise NotImplementedError(
-                "return_offset_mapping is not available when using Python tokenizers. "
-                "To use this feature, change your tokenizer to one deriving from "
-                "transformers.PreTrainedTokenizerFast. "
-                "More information on available tokenizers at "
-                "https://github.com/huggingface/transformers/pull/2674"
-            )
-
-        first_ids = get_input_ids(text)
-        second_ids = get_input_ids(text_pair) if text_pair is not None else None
+        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'
+        (
+            padding_strategy,
+            truncation_strategy,
+            max_length,
+            kwargs,
+        ) = self._get_padding_truncation_strategies(
+            padding=padding,
+            truncation=truncation,
+            max_length=max_length,
+            pad_to_multiple_of=pad_to_multiple_of,
+            verbose=verbose,
+            **kwargs,
+        )
 
-        return self.prepare_for_model(
-            first_ids,
-            pair_ids=second_ids,
+        return self._batch_encode_plus(
+            batch_text_or_text_pairs=batch_text_or_text_pairs,
+            is_pair=is_pair,
+            boxes=boxes,
+            word_labels=word_labels,
             add_special_tokens=add_special_tokens,
-            padding=padding_strategy.value,
-            truncation=truncation_strategy.value,
+            padding_strategy=padding_strategy,
+            truncation_strategy=truncation_strategy,
             max_length=max_length,
             stride=stride,
             pad_to_multiple_of=pad_to_multiple_of,
             return_tensors=return_tensors,
-            prepend_batch_axis=True,
-            return_attention_mask=return_attention_mask,
             return_token_type_ids=return_token_type_ids,
+            return_attention_mask=return_attention_mask,
             return_overflowing_tokens=return_overflowing_tokens,
             return_special_tokens_mask=return_special_tokens_mask,
+            return_offsets_mapping=return_offsets_mapping,
             return_length=return_length,
             verbose=verbose,
+            **kwargs,
         )
 
-    def _batch_encode_plus(
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast.tokenize
+    def tokenize(
         self,
-        batch_text_or_text_pairs: Union[
-            List[TextInput],
-            List[TextInputPair],
-            List[PreTokenizedInput],
-            List[PreTokenizedInputPair],
-            List[EncodedInput],
-            List[EncodedInputPair],
-        ],
+        text: str,
+        pair: Optional[str] = None,
+        add_special_tokens: bool = False,
+        **kwargs,
+    ) -> List[str]:
+        batched_input = [(text, pair)] if pair else [text]
+        encodings = self._tokenizer.encode_batch(
+            batched_input,
+            add_special_tokens=add_special_tokens,
+            is_pretokenized=False,
+            **kwargs,
+        )
+
+        return encodings[0].tokens
+
+    @add_end_docstrings(
+        LAYOUTLMV3_ENCODE_KWARGS_DOCSTRING,
+        LAYOUTLMV3_ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING,
+    )
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast.encode_plus
+    def encode_plus(
+        self,
+        text: Union[TextInput, PreTokenizedInput],
+        text_pair: Optional[PreTokenizedInput] = None,
+        boxes: Optional[List[List[int]]] = None,
+        word_labels: Optional[List[int]] = None,
         add_special_tokens: bool = True,
-        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
-        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
+        padding: Union[bool, str, PaddingStrategy] = False,
+        truncation: Union[bool, str, TruncationStrategy] = None,
         max_length: Optional[int] = None,
         stride: int = 0,
-        is_split_into_words: bool = False,
         pad_to_multiple_of: Optional[int] = None,
         return_tensors: Optional[Union[str, TensorType]] = None,
         return_token_type_ids: Optional[bool] = None,
         return_attention_mask: Optional[bool] = None,
         return_overflowing_tokens: bool = False,
         return_special_tokens_mask: bool = False,
         return_offsets_mapping: bool = False,
         return_length: bool = False,
         verbose: bool = True,
         **kwargs,
     ) -> BatchEncoding:
-        def get_input_ids(text):
-            if isinstance(text, str):
-                tokens = self.tokenize(text, **kwargs)
-                return self.convert_tokens_to_ids(tokens)
-            elif (
-                isinstance(text, (list, tuple))
-                and len(text) > 0
-                and isinstance(text[0], str)
-            ):
-                if is_split_into_words:
-                    tokens = list(
-                        itertools.chain(
-                            *(
-                                self.tokenize(t, is_split_into_words=True, **kwargs)
-                                for t in text
-                            )
-                        )
-                    )
-                    return self.convert_tokens_to_ids(tokens)
-                else:
-                    return self.convert_tokens_to_ids(text)
-            elif (
-                isinstance(text, (list, tuple))
-                and len(text) > 0
-                and isinstance(text[0], int)
-            ):
-                return text
-            else:
-                raise ValueError(
-                    "Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
-                )
-
-        if return_offsets_mapping:
-            raise NotImplementedError(
-                "return_offset_mapping is not available when using Python tokenizers. "
-                "To use this feature, change your tokenizer to one deriving from "
-                "transformers.PreTrainedTokenizerFast."
-            )
-
-        input_ids = []
-        for ids_or_pair_ids in batch_text_or_text_pairs:
-            if not isinstance(ids_or_pair_ids, (list, tuple)):
-                ids, pair_ids = ids_or_pair_ids, None
-            elif is_split_into_words and not isinstance(
-                ids_or_pair_ids[0], (list, tuple)
-            ):
-                ids, pair_ids = ids_or_pair_ids, None
-            else:
-                ids, pair_ids = ids_or_pair_ids
+        """
+        Tokenize and prepare for the model a sequence or a pair of sequences. .. warning:: This method is deprecated,
+        `__call__` should be used instead.
 
-            first_ids = get_input_ids(ids)
-            second_ids = get_input_ids(pair_ids) if pair_ids is not None else None
-            input_ids.append((first_ids, second_ids))
+        Args:
+            text (`str`, `List[str]`, `List[List[str]]`):
+                The first sequence to be encoded. This can be a string, a list of strings or a list of list of strings.
+            text_pair (`List[str]` or `List[int]`, *optional*):
+                Optional second sequence to be encoded. This can be a list of strings (words of a single example) or a
+                list of list of strings (words of a batch of examples).
+        """
+
+        # Backward compatibility for 'truncation_strategy', 'pad_to_max_length'
+        (
+            padding_strategy,
+            truncation_strategy,
+            max_length,
+            kwargs,
+        ) = self._get_padding_truncation_strategies(
+            padding=padding,
+            truncation=truncation,
+            max_length=max_length,
+            pad_to_multiple_of=pad_to_multiple_of,
+            verbose=verbose,
+            **kwargs,
+        )
 
-        batch_outputs = self._batch_prepare_for_model(
-            input_ids,
+        return self._encode_plus(
+            text=text,
+            boxes=boxes,
+            text_pair=text_pair,
+            word_labels=word_labels,
             add_special_tokens=add_special_tokens,
             padding_strategy=padding_strategy,
             truncation_strategy=truncation_strategy,
             max_length=max_length,
             stride=stride,
             pad_to_multiple_of=pad_to_multiple_of,
-            return_attention_mask=return_attention_mask,
+            return_tensors=return_tensors,
             return_token_type_ids=return_token_type_ids,
+            return_attention_mask=return_attention_mask,
             return_overflowing_tokens=return_overflowing_tokens,
             return_special_tokens_mask=return_special_tokens_mask,
+            return_offsets_mapping=return_offsets_mapping,
             return_length=return_length,
-            return_tensors=return_tensors,
             verbose=verbose,
+            **kwargs,
         )
 
-        return BatchEncoding(batch_outputs)
-
-    @add_end_docstrings(
-        ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING
-    )
-    def _batch_prepare_for_model(
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast._batch_encode_plus with LayoutLMv2->LayoutLMv3
+    def _batch_encode_plus(
         self,
-        batch_ids_pairs: List[Union[PreTokenizedInputPair, Tuple[List[int], None]]],
+        batch_text_or_text_pairs: Union[
+            List[TextInput],
+            List[TextInputPair],
+            List[PreTokenizedInput],
+        ],
+        is_pair: bool = None,
+        boxes: Optional[List[List[List[int]]]] = None,
+        word_labels: Optional[List[List[int]]] = None,
         add_special_tokens: bool = True,
         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
         max_length: Optional[int] = None,
         stride: int = 0,
         pad_to_multiple_of: Optional[int] = None,
         return_tensors: Optional[str] = None,
         return_token_type_ids: Optional[bool] = None,
         return_attention_mask: Optional[bool] = None,
         return_overflowing_tokens: bool = False,
         return_special_tokens_mask: bool = False,
+        return_offsets_mapping: bool = False,
         return_length: bool = False,
         verbose: bool = True,
     ) -> BatchEncoding:
-        """
-        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model. It
-        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and
-        manages a moving window (with user defined stride) for overflowing tokens
+        if not isinstance(batch_text_or_text_pairs, list):
+            raise TypeError(
+                f"batch_text_or_text_pairs has to be a list (got {type(batch_text_or_text_pairs)})"
+            )
 
-        Args:
-            batch_ids_pairs: list of tokenized input ids or input ids pairs
-        """
+        # Set the truncation and padding strategy and restore the initial configuration
+        self.set_truncation_and_padding(
+            padding_strategy=padding_strategy,
+            truncation_strategy=truncation_strategy,
+            max_length=max_length,
+            stride=stride,
+            pad_to_multiple_of=pad_to_multiple_of,
+        )
 
-        batch_outputs = {}
-        for first_ids, second_ids in batch_ids_pairs:
-            outputs = self.prepare_for_model(
-                first_ids,
-                second_ids,
-                add_special_tokens=add_special_tokens,
-                padding=PaddingStrategy.DO_NOT_PAD.value,  # we pad in batch afterward
-                truncation=truncation_strategy.value,
-                max_length=max_length,
-                stride=stride,
-                pad_to_multiple_of=None,  # we pad in batch afterward
-                return_attention_mask=False,  # we pad in batch afterward
+        if is_pair:
+            batch_text_or_text_pairs = [
+                (text.split(), text_pair)
+                for text, text_pair in batch_text_or_text_pairs
+            ]
+
+        encodings = self._tokenizer.encode_batch(
+            batch_text_or_text_pairs,
+            add_special_tokens=add_special_tokens,
+            is_pretokenized=True,  # we set this to True as LayoutLMv3 always expects pretokenized inputs
+        )
+
+        # Convert encoding to dict
+        # `Tokens` has type: Tuple[
+        #                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],
+        #                       List[EncodingFast]
+        #                    ]
+        # with nested dimensions corresponding to batch, overflows, sequence length
+        tokens_and_encodings = [
+            self._convert_encoding(
+                encoding=encoding,
                 return_token_type_ids=return_token_type_ids,
+                return_attention_mask=return_attention_mask,
                 return_overflowing_tokens=return_overflowing_tokens,
                 return_special_tokens_mask=return_special_tokens_mask,
+                return_offsets_mapping=True
+                if word_labels is not None
+                else return_offsets_mapping,  # we use offsets to create the labels
                 return_length=return_length,
-                return_tensors=None,  # We convert the whole batch to tensors at the end
-                prepend_batch_axis=False,
                 verbose=verbose,
             )
+            for encoding in encodings
+        ]
+
+        # Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension
+        # From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)
+        # (we say ~ because the number of overflow varies with the example in the batch)
+        #
+        # To match each overflowing sample with the original sample in the batch
+        # we add an overflow_to_sample_mapping array (see below)
+        sanitized_tokens = {}
+        for key in tokens_and_encodings[0][0].keys():
+            stack = [e for item, _ in tokens_and_encodings for e in item[key]]
+            sanitized_tokens[key] = stack
+        sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]
+
+        # If returning overflowing tokens, we need to return a mapping
+        # from the batch idx to the original sample
+        if return_overflowing_tokens:
+            overflow_to_sample_mapping = []
+            for i, (toks, _) in enumerate(tokens_and_encodings):
+                overflow_to_sample_mapping += [i] * len(toks["input_ids"])
+            sanitized_tokens["overflow_to_sample_mapping"] = overflow_to_sample_mapping
+
+        for input_ids in sanitized_tokens["input_ids"]:
+            self._eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)
+
+        # create the token boxes
+        token_boxes = []
+        for batch_index in range(len(sanitized_tokens["input_ids"])):
+            if return_overflowing_tokens:
+                original_index = sanitized_tokens["overflow_to_sample_mapping"][
+                    batch_index
+                ]
+            else:
+                original_index = batch_index
+            token_boxes_example = []
+            for id, sequence_id, word_id in zip(
+                sanitized_tokens["input_ids"][batch_index],
+                sanitized_encodings[batch_index].sequence_ids,
+                sanitized_encodings[batch_index].word_ids,
+            ):
+                if word_id is not None:
+                    if is_pair and sequence_id == 0:
+                        token_boxes_example.append(self.pad_token_box)
+                    else:
+                        token_boxes_example.append(boxes[original_index][word_id])
+                else:
+                    if id == self.cls_token_id:
+                        token_boxes_example.append(self.cls_token_box)
+                    elif id == self.sep_token_id:
+                        token_boxes_example.append(self.sep_token_box)
+                    elif id == self.pad_token_id:
+                        token_boxes_example.append(self.pad_token_box)
+                    else:
+                        raise ValueError("Id not recognized")
+            token_boxes.append(token_boxes_example)
+
+        sanitized_tokens["bbox"] = token_boxes
+
+        # optionally, create the labels
+        if word_labels is not None:
+            labels = []
+            for batch_index in range(len(sanitized_tokens["input_ids"])):
+                if return_overflowing_tokens:
+                    original_index = sanitized_tokens["overflow_to_sample_mapping"][
+                        batch_index
+                    ]
+                else:
+                    original_index = batch_index
+                labels_example = []
+                for id, offset, word_id in zip(
+                    sanitized_tokens["input_ids"][batch_index],
+                    sanitized_tokens["offset_mapping"][batch_index],
+                    sanitized_encodings[batch_index].word_ids,
+                ):
+                    if word_id is not None:
+                        if self.only_label_first_subword:
+                            if offset[0] == 0:
+                                # Use the real label id for the first token of the word, and padding ids for the remaining tokens
+                                labels_example.append(
+                                    word_labels[original_index][word_id]
+                                )
+                            else:
+                                labels_example.append(self.pad_token_label)
+                        else:
+                            labels_example.append(word_labels[original_index][word_id])
+                    else:
+                        labels_example.append(self.pad_token_label)
+                labels.append(labels_example)
+
+            sanitized_tokens["labels"] = labels
+            # finally, remove offsets if the user didn't want them
+            if not return_offsets_mapping:
+                del sanitized_tokens["offset_mapping"]
+
+        return BatchEncoding(
+            sanitized_tokens, sanitized_encodings, tensor_type=return_tensors
+        )
 
-            for key, value in outputs.items():
-                if key not in batch_outputs:
-                    batch_outputs[key] = []
-                batch_outputs[key].append(value)
-
-        batch_outputs = self.pad(
-            batch_outputs,
-            padding=padding_strategy.value,
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast._encode_plus
+    def _encode_plus(
+        self,
+        text: Union[TextInput, PreTokenizedInput],
+        text_pair: Optional[PreTokenizedInput] = None,
+        boxes: Optional[List[List[int]]] = None,
+        word_labels: Optional[List[int]] = None,
+        add_special_tokens: bool = True,
+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
+        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
+        max_length: Optional[int] = None,
+        stride: int = 0,
+        pad_to_multiple_of: Optional[int] = None,
+        return_tensors: Optional[bool] = None,
+        return_token_type_ids: Optional[bool] = None,
+        return_attention_mask: Optional[bool] = None,
+        return_overflowing_tokens: bool = False,
+        return_special_tokens_mask: bool = False,
+        return_offsets_mapping: bool = False,
+        return_length: bool = False,
+        verbose: bool = True,
+        **kwargs,
+    ) -> BatchEncoding:
+        # make it a batched input
+        # 2 options:
+        # 1) only text, in case text must be a list of str
+        # 2) text + text_pair, in which case text = str and text_pair a list of str
+        batched_input = [(text, text_pair)] if text_pair else [text]
+        batched_boxes = [boxes]
+        batched_word_labels = [word_labels] if word_labels is not None else None
+        batched_output = self._batch_encode_plus(
+            batched_input,
+            is_pair=bool(text_pair is not None),
+            boxes=batched_boxes,
+            word_labels=batched_word_labels,
+            add_special_tokens=add_special_tokens,
+            padding_strategy=padding_strategy,
+            truncation_strategy=truncation_strategy,
             max_length=max_length,
+            stride=stride,
             pad_to_multiple_of=pad_to_multiple_of,
+            return_tensors=return_tensors,
+            return_token_type_ids=return_token_type_ids,
             return_attention_mask=return_attention_mask,
+            return_overflowing_tokens=return_overflowing_tokens,
+            return_special_tokens_mask=return_special_tokens_mask,
+            return_offsets_mapping=return_offsets_mapping,
+            return_length=return_length,
+            verbose=verbose,
+            **kwargs,
         )
 
-        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)
-
-        return batch_outputs
-
-    def prepare_for_tokenization(
-        self, text: str, is_split_into_words: bool = False, **kwargs
-    ) -> Tuple[str, Dict[str, Any]]:
-        """
-        Performs any necessary transformations before tokenization.
-
-        This method should pop the arguments from kwargs and return the remaining `kwargs` as well. We test the
-        `kwargs` at the end of the encoding process to be sure all the arguments have been used.
+        # Return tensor is None, then we can remove the leading batch axis
+        # Overflowing tokens are returned as a batch of output so we keep them in this case
+        if return_tensors is None and not return_overflowing_tokens:
+            batched_output = BatchEncoding(
+                {
+                    key: value[0]
+                    if len(value) > 0 and isinstance(value[0], list)
+                    else value
+                    for key, value in batched_output.items()
+                },
+                batched_output.encodings,
+            )
 
-        Args:
-            text (`str`):
-                The text to prepare.
-            is_split_into_words (`bool`, *optional*, defaults to `False`):
-                Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the
-                tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)
-                which it will tokenize. This is useful for NER or token classification.
-            kwargs:
-                Keyword arguments to use for the tokenization.
+        self._eventual_warn_about_too_long_sequence(
+            batched_output["input_ids"], max_length, verbose
+        )
 
-        Returns:
-            `Tuple[str, Dict[str, Any]]`: The prepared text and the unused kwargs.
-        """
-        return (text, kwargs)
+        return batched_output
 
-    def get_special_tokens_mask(
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast._pad
+    def _pad(
         self,
-        token_ids_0: List,
-        token_ids_1: Optional[List] = None,
-        already_has_special_tokens: bool = False,
-    ) -> List[int]:
+        encoded_inputs: Union[Dict[str, EncodedInput], BatchEncoding],
+        max_length: Optional[int] = None,
+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
+        pad_to_multiple_of: Optional[int] = None,
+        return_attention_mask: Optional[bool] = None,
+    ) -> dict:
         """
-        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
-        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.
+        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)
 
         Args:
-            token_ids_0 (`List[int]`):
-                List of ids of the first sequence.
-            token_ids_1 (`List[int]`, *optional*):
-                List of ids of the second sequence.
-            already_has_special_tokens (`bool`, *optional*, defaults to `False`):
-                Whether or not the token list is already formatted with special tokens for the model.
+            encoded_inputs:
+                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).
+            max_length: maximum length of the returned list and optionally padding length (see below).
+                Will truncate by taking into account the special tokens.
+            padding_strategy: PaddingStrategy to use for padding.
+
+                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch
+                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)
+                - PaddingStrategy.DO_NOT_PAD: Do not pad
+                The tokenizer padding sides are defined in self.padding_side:
+
+                    - 'left': pads on the left of the sequences
+                    - 'right': pads on the right of the sequences
+            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.
+                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability
+                >= 7.5 (Volta).
+            return_attention_mask:
+                (optional) Set to False to avoid returning attention mask (default: set to model specifics)
+        """
+        # Load from model defaults
+        if return_attention_mask is None:
+            return_attention_mask = "attention_mask" in self.model_input_names
+
+        required_input = encoded_inputs[self.model_input_names[0]]
+
+        if padding_strategy == PaddingStrategy.LONGEST:
+            max_length = len(required_input)
+
+        if (
+            max_length is not None
+            and pad_to_multiple_of is not None
+            and (max_length % pad_to_multiple_of != 0)
+        ):
+            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of
+
+        needs_to_be_padded = (
+            padding_strategy != PaddingStrategy.DO_NOT_PAD
+            and len(required_input) != max_length
+        )
 
-        Returns:
-            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
-        """
-        if already_has_special_tokens:
-            if token_ids_1 is not None:
-                raise ValueError(
-                    "You should not supply a second sequence if the provided sequence of "
-                    "ids is already formatted with special tokens for the model."
+        # Initialize attention mask if not present.
+        if return_attention_mask and "attention_mask" not in encoded_inputs:
+            encoded_inputs["attention_mask"] = [1] * len(required_input)
+
+        if needs_to_be_padded:
+            difference = max_length - len(required_input)
+            if self.padding_side == "right":
+                if return_attention_mask:
+                    encoded_inputs["attention_mask"] = (
+                        encoded_inputs["attention_mask"] + [0] * difference
+                    )
+                if "token_type_ids" in encoded_inputs:
+                    encoded_inputs["token_type_ids"] = (
+                        encoded_inputs["token_type_ids"]
+                        + [self.pad_token_type_id] * difference
+                    )
+                if "bbox" in encoded_inputs:
+                    encoded_inputs["bbox"] = (
+                        encoded_inputs["bbox"] + [self.pad_token_box] * difference
+                    )
+                if "labels" in encoded_inputs:
+                    encoded_inputs["labels"] = (
+                        encoded_inputs["labels"] + [self.pad_token_label] * difference
+                    )
+                if "special_tokens_mask" in encoded_inputs:
+                    encoded_inputs["special_tokens_mask"] = (
+                        encoded_inputs["special_tokens_mask"] + [1] * difference
+                    )
+                encoded_inputs[self.model_input_names[0]] = (
+                    required_input + [self.pad_token_id] * difference
                 )
+            elif self.padding_side == "left":
+                if return_attention_mask:
+                    encoded_inputs["attention_mask"] = [
+                        0
+                    ] * difference + encoded_inputs["attention_mask"]
+                if "token_type_ids" in encoded_inputs:
+                    encoded_inputs["token_type_ids"] = [
+                        self.pad_token_type_id
+                    ] * difference + encoded_inputs["token_type_ids"]
+                if "bbox" in encoded_inputs:
+                    encoded_inputs["bbox"] = [
+                        self.pad_token_box
+                    ] * difference + encoded_inputs["bbox"]
+                if "labels" in encoded_inputs:
+                    encoded_inputs["labels"] = [
+                        self.pad_token_label
+                    ] * difference + encoded_inputs["labels"]
+                if "special_tokens_mask" in encoded_inputs:
+                    encoded_inputs["special_tokens_mask"] = [
+                        1
+                    ] * difference + encoded_inputs["special_tokens_mask"]
+                encoded_inputs[self.model_input_names[0]] = [
+                    self.pad_token_id
+                ] * difference + required_input
+            else:
+                raise ValueError("Invalid padding strategy:" + str(self.padding_side))
+
+        return encoded_inputs
+
+    # Copied from transformers.models.layoutlmv2.tokenization_layoutlmv2_fast.LayoutLMv2TokenizerFast.save_vocabulary
+    def save_vocabulary(
+        self, save_directory: str, filename_prefix: Optional[str] = None
+    ) -> Tuple[str]:
+        files = self._tokenizer.model.save(save_directory, name=filename_prefix)
+        return tuple(files)
+
+    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):
+        output = [self.bos_token_id] + token_ids_0 + [self.eos_token_id]
+        if token_ids_1 is None:
+            return output
 
-            return super().get_special_tokens_mask(
-                token_ids_0=token_ids_0,
-                token_ids_1=token_ids_1,
-                already_has_special_tokens=True,
-            )
-        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))
-
-    @overload
-    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = False) -> str:
-        ...
-
-    @overload
-    def convert_ids_to_tokens(
-        self, ids: List[int], skip_special_tokens: bool = False
-    ) -> List[str]:
-        ...
+        return output + [self.eos_token_id] + token_ids_1 + [self.eos_token_id]
 
-    def convert_ids_to_tokens(
-        self, ids: Union[int, List[int]], skip_special_tokens: bool = False
-    ) -> Union[str, List[str]]:
+    def create_token_type_ids_from_sequences(
+        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
+    ) -> List[int]:
         """
-        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
-        added tokens.
-
         Args:
-            ids (`int` or `List[int]`):
-                The token id (or token ids) to convert to tokens.
-            skip_special_tokens (`bool`, *optional*, defaults to `False`):
-                Whether or not to remove special tokens in the decoding.
-
+        Create a mask from the two sequences passed to be used in a sequence-pair classification task. RoBERTa does not:
+        make use of token type ids, therefore a list of zeros is returned.
+            token_ids_0 (`List[int]`):
+                List of IDs.
+            token_ids_1 (`List[int]`, *optional*):
+                Optional second list of IDs for sequence pairs.
         Returns:
-            `str` or `List[str]`: The decoded token(s).
+            `List[int]`: List of zeros.
         """
-        if isinstance(ids, int):
-            if ids in self.added_tokens_decoder:
-                return self.added_tokens_decoder[ids]
-            else:
-                return self._convert_id_to_token(ids)
-        tokens = []
-        for index in ids:
-            index = int(index)
-            if skip_special_tokens and index in self.all_special_ids:
-                continue
-            if index in self.added_tokens_decoder:
-                tokens.append(self.added_tokens_decoder[index])
-            else:
-                tokens.append(self._convert_id_to_token(index))
-        return tokens
-
-    def _convert_id_to_token(self, index: int) -> str:
-        raise NotImplementedError
-
-    def convert_tokens_to_string(self, tokens: List[str]) -> str:
-        return " ".join(tokens)
+        sep = [self.sep_token_id]
+        cls = [self.cls_token_id]
 
-    def _decode(
-        self,
-        token_ids: List[int],
-        skip_special_tokens: bool = False,
-        clean_up_tokenization_spaces: bool = True,
-        spaces_between_special_tokens: bool = True,
-        **kwargs,
-    ) -> str:
-        self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)
-
-        filtered_tokens = self.convert_ids_to_tokens(
-            token_ids, skip_special_tokens=skip_special_tokens
-        )
-
-        # To avoid mixing byte-level and unicode for byte-level BPT
-        # we need to build string separately for added tokens and byte-level tokens
-        # cf. https://github.com/huggingface/transformers/issues/1133
-        sub_texts = []
-        current_sub_text = []
-        for token in filtered_tokens:
-            if skip_special_tokens and token in self.all_special_ids:
-                continue
-            if token in self.added_tokens_encoder:
-                if current_sub_text:
-                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))
-                    current_sub_text = []
-                sub_texts.append(token)
-            else:
-                current_sub_text.append(token)
-        if current_sub_text:
-            sub_texts.append(self.convert_tokens_to_string(current_sub_text))
-
-        if spaces_between_special_tokens:
-            text = " ".join(sub_texts)
-        else:
-            text = "".join(sub_texts)
-
-        if clean_up_tokenization_spaces:
-            clean_text = self.clean_up_tokenization(text)
-            return clean_text
-        else:
-            return text
+        if token_ids_1 is None:
+            return len(cls + token_ids_0 + sep) * [0]
+        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `xs_transformers-1.0.0/xs_transformers/tokenization_utils_fast.py` & `xs_transformers-1.0.1/xs_transformers/data/processors/squad.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,819 +1,939 @@
-# coding=utf-8
-# Copyright 2020 The HuggingFace Inc. team.
+# Copyright 2020 The HuggingFace Team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
- Tokenization classes for fast tokenizers (provided by HuggingFace's tokenizers library). For slow (python) tokenizers
- see tokenization_utils.py
-"""
-import copy
+
 import json
 import os
-from collections import defaultdict
-from typing import Any, Dict, List, Optional, Tuple, Union
+from functools import partial
+from multiprocessing import Pool, cpu_count
 
-import tokenizers.pre_tokenizers as pre_tokenizers_fast
-from tokenizers import Encoding as EncodingFast
-from tokenizers import Tokenizer as TokenizerFast
-from tokenizers.decoders import Decoder as DecoderFast
-from tokenizers.trainers import (
-    BpeTrainer,
-    UnigramTrainer,
-    WordLevelTrainer,
-    WordPieceTrainer,
-)
+import numpy as np
+from tqdm import tqdm
 
-from .convert_slow_tokenizer import convert_slow_tokenizer
-from .tokenization_utils import PreTrainedTokenizer
-from .tokenization_utils_base import (
-    INIT_TOKENIZER_DOCSTRING,
-    AddedToken,
+from ...models.bert.tokenization_bert import whitespace_tokenize
+from ...tokenization_utils_base import (
     BatchEncoding,
-    PreTokenizedInput,
-    PreTokenizedInputPair,
     PreTrainedTokenizerBase,
-    SpecialTokensMixin,
-    TextInput,
-    TextInputPair,
     TruncationStrategy,
 )
-from .utils import PaddingStrategy, add_end_docstrings, logging
+from ...utils import is_tf_available, is_torch_available, logging
+from .utils import DataProcessor
 
-logger = logging.get_logger(__name__)
+# Store the tokenizers which insert 2 separators tokens
+MULTI_SEP_TOKENS_TOKENIZERS_SET = {"roberta", "camembert", "bart", "mpnet"}
 
-# Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file
-TOKENIZER_FILE = "tokenizer.json"
-SPECIAL_TOKENS_MAP_FILE = "special_tokens_map.json"
-TOKENIZER_CONFIG_FILE = "tokenizer_config.json"
-
-# Slow tokenizers have an additional added tokens files
-ADDED_TOKENS_FILE = "added_tokens.json"
-
-INIT_TOKENIZER_DOCSTRING += """
-        tokenizer_object ([`tokenizers.Tokenizer`]):
-            A [`tokenizers.Tokenizer`] object from 🤗 tokenizers to instantiate from. See [Using tokenizers from 🤗
-            tokenizers](../fast_tokenizers) for more information.
-        tokenizer_file ([`str`]):
-            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from 🤗
-            tokenizers.
-"""
-
-MODEL_TO_TRAINER_MAPPING = {
-    "BPE": BpeTrainer,
-    "Unigram": UnigramTrainer,
-    "WordLevel": WordLevelTrainer,
-    "WordPiece": WordPieceTrainer,
-}
 
-VOCAB_FILES_NAMES = {"tokenizer_file": TOKENIZER_FILE}
+if is_torch_available():
+    import torch
+    from torch.utils.data import TensorDataset
 
+if is_tf_available():
+    import tensorflow as tf
 
-@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
-class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
-    """
-    Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).
-
-    Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].
+logger = logging.get_logger(__name__)
 
-    Handles all the shared methods for tokenization and special tokens, as well as methods for
-    downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.
 
-    This class also contains the added tokens in a unified way on top of all tokenizers so we don't have to handle the
-    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).
-    """
+def _improve_answer_span(
+    doc_tokens, input_start, input_end, tokenizer, orig_answer_text
+):
+    """Returns tokenized answer spans that better match the annotated answer."""
+    tok_answer_text = " ".join(tokenizer.tokenize(orig_answer_text))
+
+    for new_start in range(input_start, input_end + 1):
+        for new_end in range(input_end, new_start - 1, -1):
+            text_span = " ".join(doc_tokens[new_start : (new_end + 1)])
+            if text_span == tok_answer_text:
+                return (new_start, new_end)
+
+    return (input_start, input_end)
+
+
+def _check_is_max_context(doc_spans, cur_span_index, position):
+    """Check if this is the 'max context' doc span for the token."""
+    best_score = None
+    best_span_index = None
+    for span_index, doc_span in enumerate(doc_spans):
+        end = doc_span.start + doc_span.length - 1
+        if position < doc_span.start:
+            continue
+        if position > end:
+            continue
+        num_left_context = position - doc_span.start
+        num_right_context = end - position
+        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length
+        if best_score is None or score > best_score:
+            best_score = score
+            best_span_index = span_index
+
+    return cur_span_index == best_span_index
+
+
+def _new_check_is_max_context(doc_spans, cur_span_index, position):
+    """Check if this is the 'max context' doc span for the token."""
+    # if len(doc_spans) == 1:
+    # return True
+    best_score = None
+    best_span_index = None
+    for span_index, doc_span in enumerate(doc_spans):
+        end = doc_span["start"] + doc_span["length"] - 1
+        if position < doc_span["start"]:
+            continue
+        if position > end:
+            continue
+        num_left_context = position - doc_span["start"]
+        num_right_context = end - position
+        score = min(num_left_context, num_right_context) + 0.01 * doc_span["length"]
+        if best_score is None or score > best_score:
+            best_score = score
+            best_span_index = span_index
 
-    vocab_files_names = VOCAB_FILES_NAMES
-    slow_tokenizer_class: PreTrainedTokenizer = None
-    can_save_slow_tokenizer: bool = True
-
-    def __init__(self, *args, **kwargs):
-        tokenizer_object = kwargs.pop("tokenizer_object", None)
-        slow_tokenizer = kwargs.pop("__slow_tokenizer", None)
-        fast_tokenizer_file = kwargs.pop("tokenizer_file", None)
-        from_slow = kwargs.pop("from_slow", False)
+    return cur_span_index == best_span_index
 
-        if from_slow and slow_tokenizer is None and self.slow_tokenizer_class is None:
-            raise ValueError(
-                "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you "
-                "have sentencepiece installed."
-            )
 
-        if tokenizer_object is not None:
-            fast_tokenizer = copy.deepcopy(tokenizer_object)
-        elif fast_tokenizer_file is not None and not from_slow:
-            # We have a serialization from tokenizers which let us directly build the backend
-            fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
-        elif slow_tokenizer is not None:
-            # We need to convert a slow tokenizer to build the backend
-            fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
-        elif self.slow_tokenizer_class is not None:
-            # We need to create and convert a slow tokenizer to build the backend
-            slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)
-            fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
-        else:
-            raise ValueError(
-                "Couldn't instantiate the backend tokenizer from one of: \n"
-                "(1) a `tokenizers` library serialization file, \n"
-                "(2) a slow tokenizer instance to convert or \n"
-                "(3) an equivalent slow tokenizer class to instantiate and convert. \n"
-                "You need to have sentencepiece installed to convert a slow tokenizer to a fast one."
-            )
+def _is_whitespace(c):
+    if c == " " or c == "\t" or c == "\r" or c == "\n" or ord(c) == 0x202F:
+        return True
+    return False
 
-        self._tokenizer = fast_tokenizer
 
-        if slow_tokenizer is not None:
-            kwargs.update(slow_tokenizer.init_kwargs)
+def squad_convert_example_to_features(
+    example, max_seq_length, doc_stride, max_query_length, padding_strategy, is_training
+):
+    features = []
+    if is_training and not example.is_impossible:
+        # Get start and end position
+        start_position = example.start_position
+        end_position = example.end_position
+
+        # If the answer cannot be found in the text, then skip this example.
+        actual_text = " ".join(example.doc_tokens[start_position : (end_position + 1)])
+        cleaned_answer_text = " ".join(whitespace_tokenize(example.answer_text))
+        if actual_text.find(cleaned_answer_text) == -1:
+            logger.warning(
+                f"Could not find answer: '{actual_text}' vs. '{cleaned_answer_text}'"
+            )
+            return []
+
+    tok_to_orig_index = []
+    orig_to_tok_index = []
+    all_doc_tokens = []
+    for i, token in enumerate(example.doc_tokens):
+        orig_to_tok_index.append(len(all_doc_tokens))
+        if tokenizer.__class__.__name__ in [
+            "RobertaTokenizer",
+            "LongformerTokenizer",
+            "BartTokenizer",
+            "RobertaTokenizerFast",
+            "LongformerTokenizerFast",
+            "BartTokenizerFast",
+        ]:
+            sub_tokens = tokenizer.tokenize(token, add_prefix_space=True)
+        else:
+            sub_tokens = tokenizer.tokenize(token)
+        for sub_token in sub_tokens:
+            tok_to_orig_index.append(i)
+            all_doc_tokens.append(sub_token)
+
+    if is_training and not example.is_impossible:
+        tok_start_position = orig_to_tok_index[example.start_position]
+        if example.end_position < len(example.doc_tokens) - 1:
+            tok_end_position = orig_to_tok_index[example.end_position + 1] - 1
+        else:
+            tok_end_position = len(all_doc_tokens) - 1
 
-        self._decode_use_source_tokenizer = False
+        (tok_start_position, tok_end_position) = _improve_answer_span(
+            all_doc_tokens,
+            tok_start_position,
+            tok_end_position,
+            tokenizer,
+            example.answer_text,
+        )
 
-        # We call this after having initialized the backend tokenizer because we update it.
-        super().__init__(**kwargs)
+    spans = []
 
-    @property
-    def is_fast(self) -> bool:
-        return True
+    truncated_query = tokenizer.encode(
+        example.question_text,
+        add_special_tokens=False,
+        truncation=True,
+        max_length=max_query_length,
+    )
+
+    # Tokenizers who insert 2 SEP tokens in-between <context> & <question> need to have special handling
+    # in the way they compute mask of added tokens.
+    tokenizer_type = type(tokenizer).__name__.replace("Tokenizer", "").lower()
+    sequence_added_tokens = (
+        tokenizer.model_max_length - tokenizer.max_len_single_sentence + 1
+        if tokenizer_type in MULTI_SEP_TOKENS_TOKENIZERS_SET
+        else tokenizer.model_max_length - tokenizer.max_len_single_sentence
+    )
+    sequence_pair_added_tokens = (
+        tokenizer.model_max_length - tokenizer.max_len_sentences_pair
+    )
+
+    span_doc_tokens = all_doc_tokens
+    while len(spans) * doc_stride < len(all_doc_tokens):
+        # Define the side we want to truncate / pad and the text/pair sorting
+        if tokenizer.padding_side == "right":
+            texts = truncated_query
+            pairs = span_doc_tokens
+            truncation = TruncationStrategy.ONLY_SECOND.value
+        else:
+            texts = span_doc_tokens
+            pairs = truncated_query
+            truncation = TruncationStrategy.ONLY_FIRST.value
+
+        encoded_dict = tokenizer.encode_plus(  # TODO(thom) update this logic
+            texts,
+            pairs,
+            truncation=truncation,
+            padding=padding_strategy,
+            max_length=max_seq_length,
+            return_overflowing_tokens=True,
+            stride=max_seq_length
+            - doc_stride
+            - len(truncated_query)
+            - sequence_pair_added_tokens,
+            return_token_type_ids=True,
+        )
 
-    @property
-    def vocab_size(self) -> int:
-        """
-        `int`: Size of the base vocabulary (without the added tokens).
-        """
-        return self._tokenizer.get_vocab_size(with_added_tokens=False)
+        paragraph_len = min(
+            len(all_doc_tokens) - len(spans) * doc_stride,
+            max_seq_length - len(truncated_query) - sequence_pair_added_tokens,
+        )
 
-    def get_vocab(self) -> Dict[str, int]:
-        return self._tokenizer.get_vocab(with_added_tokens=True)
+        if tokenizer.pad_token_id in encoded_dict["input_ids"]:
+            if tokenizer.padding_side == "right":
+                non_padded_ids = encoded_dict["input_ids"][
+                    : encoded_dict["input_ids"].index(tokenizer.pad_token_id)
+                ]
+            else:
+                last_padding_id_position = (
+                    len(encoded_dict["input_ids"])
+                    - 1
+                    - encoded_dict["input_ids"][::-1].index(tokenizer.pad_token_id)
+                )
+                non_padded_ids = encoded_dict["input_ids"][
+                    last_padding_id_position + 1 :
+                ]
 
-    @property
-    def vocab(self) -> Dict[str, int]:
-        return self.get_vocab()
+        else:
+            non_padded_ids = encoded_dict["input_ids"]
 
-    def get_added_vocab(self) -> Dict[str, int]:
-        """
-        Returns the added tokens in the vocabulary as a dictionary of token to index.
+        tokens = tokenizer.convert_ids_to_tokens(non_padded_ids)
 
-        Returns:
-            `Dict[str, int]`: The added tokens.
-        """
-        base_vocab = self._tokenizer.get_vocab(with_added_tokens=False)
-        full_vocab = self._tokenizer.get_vocab(with_added_tokens=True)
-        added_vocab = dict(
-            (tok, index) for tok, index in full_vocab.items() if tok not in base_vocab
+        token_to_orig_map = {}
+        for i in range(paragraph_len):
+            index = (
+                len(truncated_query) + sequence_added_tokens + i
+                if tokenizer.padding_side == "right"
+                else i
+            )
+            token_to_orig_map[index] = tok_to_orig_index[len(spans) * doc_stride + i]
+
+        encoded_dict["paragraph_len"] = paragraph_len
+        encoded_dict["tokens"] = tokens
+        encoded_dict["token_to_orig_map"] = token_to_orig_map
+        encoded_dict["truncated_query_with_special_tokens_length"] = (
+            len(truncated_query) + sequence_added_tokens
         )
-        return added_vocab
-
-    def __len__(self) -> int:
-        """
-        Size of the full vocabulary with the added tokens.
-        """
-        return self._tokenizer.get_vocab_size(with_added_tokens=True)
+        encoded_dict["token_is_max_context"] = {}
+        encoded_dict["start"] = len(spans) * doc_stride
+        encoded_dict["length"] = paragraph_len
+
+        spans.append(encoded_dict)
+
+        if "overflowing_tokens" not in encoded_dict or (
+            "overflowing_tokens" in encoded_dict
+            and len(encoded_dict["overflowing_tokens"]) == 0
+        ):
+            break
+        span_doc_tokens = encoded_dict["overflowing_tokens"]
 
-    @property
-    def backend_tokenizer(self) -> TokenizerFast:
-        """
-        `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.
-        """
-        return self._tokenizer
+    for doc_span_index in range(len(spans)):
+        for j in range(spans[doc_span_index]["paragraph_len"]):
+            is_max_context = _new_check_is_max_context(
+                spans, doc_span_index, doc_span_index * doc_stride + j
+            )
+            index = (
+                j
+                if tokenizer.padding_side == "left"
+                else spans[doc_span_index]["truncated_query_with_special_tokens_length"]
+                + j
+            )
+            spans[doc_span_index]["token_is_max_context"][index] = is_max_context
+
+    for span in spans:
+        # Identify the position of the CLS token
+        cls_index = span["input_ids"].index(tokenizer.cls_token_id)
+
+        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)
+        # Original TF implementation also keep the classification token (set to 0)
+        p_mask = np.ones_like(span["token_type_ids"])
+        if tokenizer.padding_side == "right":
+            p_mask[len(truncated_query) + sequence_added_tokens :] = 0
+        else:
+            p_mask[
+                -len(span["tokens"]) : -(len(truncated_query) + sequence_added_tokens)
+            ] = 0
+
+        pad_token_indices = np.where(span["input_ids"] == tokenizer.pad_token_id)
+        special_token_indices = np.asarray(
+            tokenizer.get_special_tokens_mask(
+                span["input_ids"], already_has_special_tokens=True
+            )
+        ).nonzero()
+
+        p_mask[pad_token_indices] = 1
+        p_mask[special_token_indices] = 1
+
+        # Set the cls index to 0: the CLS index can be used for impossible answers
+        p_mask[cls_index] = 0
+
+        span_is_impossible = example.is_impossible
+        start_position = 0
+        end_position = 0
+        if is_training and not span_is_impossible:
+            # For training, if our document chunk does not contain an annotation
+            # we throw it out, since there is nothing to predict.
+            doc_start = span["start"]
+            doc_end = span["start"] + span["length"] - 1
+            out_of_span = False
+
+            if not (tok_start_position >= doc_start and tok_end_position <= doc_end):
+                out_of_span = True
+
+            if out_of_span:
+                start_position = cls_index
+                end_position = cls_index
+                span_is_impossible = True
+            else:
+                if tokenizer.padding_side == "left":
+                    doc_offset = 0
+                else:
+                    doc_offset = len(truncated_query) + sequence_added_tokens
 
-    @property
-    def decoder(self) -> DecoderFast:
-        """
-        `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.
-        """
-        return self._tokenizer.decoder
+                start_position = tok_start_position - doc_start + doc_offset
+                end_position = tok_end_position - doc_start + doc_offset
 
-    def _convert_encoding(
-        self,
-        encoding: EncodingFast,
-        return_token_type_ids: Optional[bool] = None,
-        return_attention_mask: Optional[bool] = None,
-        return_overflowing_tokens: bool = False,
-        return_special_tokens_mask: bool = False,
-        return_offsets_mapping: bool = False,
-        return_length: bool = False,
-        verbose: bool = True,
-    ) -> Tuple[Dict[str, Any], List[EncodingFast]]:
-        """
-        Convert the encoding representation (from low-level HuggingFace tokenizer output) to a python Dict and a list
-        of encodings, take care of building a batch from overflowing tokens.
+        features.append(
+            SquadFeatures(
+                span["input_ids"],
+                span["attention_mask"],
+                span["token_type_ids"],
+                cls_index,
+                p_mask.tolist(),
+                example_index=0,  # Can not set unique_id and example_index here. They will be set after multiple processing.
+                unique_id=0,
+                paragraph_len=span["paragraph_len"],
+                token_is_max_context=span["token_is_max_context"],
+                tokens=span["tokens"],
+                token_to_orig_map=span["token_to_orig_map"],
+                start_position=start_position,
+                end_position=end_position,
+                is_impossible=span_is_impossible,
+                qas_id=example.qas_id,
+            )
+        )
+    return features
 
-        Overflowing tokens are converted to additional examples (like batches) so the output values of the dict are
-        lists (overflows) of lists (tokens).
 
-        Output shape: (overflows, sequence length)
-        """
-        if return_token_type_ids is None:
-            return_token_type_ids = "token_type_ids" in self.model_input_names
-        if return_attention_mask is None:
-            return_attention_mask = "attention_mask" in self.model_input_names
+def squad_convert_example_to_features_init(
+    tokenizer_for_convert: PreTrainedTokenizerBase,
+):
+    global tokenizer
+    tokenizer = tokenizer_for_convert
+
+
+def squad_convert_examples_to_features(
+    examples,
+    tokenizer,
+    max_seq_length,
+    doc_stride,
+    max_query_length,
+    is_training,
+    padding_strategy="max_length",
+    return_dataset=False,
+    threads=1,
+    tqdm_enabled=True,
+):
+    """
+    Converts a list of examples into a list of features that can be directly given as input to a model. It is
+    model-dependant and takes advantage of many of the tokenizer's features to create the model's inputs.
 
-        if return_overflowing_tokens and encoding.overflowing is not None:
-            encodings = [encoding] + encoding.overflowing
-        else:
-            encodings = [encoding]
+    Args:
+        examples: list of [`~data.processors.squad.SquadExample`]
+        tokenizer: an instance of a child of [`PreTrainedTokenizer`]
+        max_seq_length: The maximum sequence length of the inputs.
+        doc_stride: The stride used when the context is too large and is split across several features.
+        max_query_length: The maximum length of the query.
+        is_training: whether to create features for model evaluation or model training.
+        padding_strategy: Default to "max_length". Which padding strategy to use
+        return_dataset: Default False. Either 'pt' or 'tf'.
+            if 'pt': returns a torch.data.TensorDataset, if 'tf': returns a tf.data.Dataset
+        threads: multiple processing threads.
+
+
+    Returns:
+        list of [`~data.processors.squad.SquadFeatures`]
+
+    Example:
+
+    ```python
+    processor = SquadV2Processor()
+    examples = processor.get_dev_examples(data_dir)
+
+    features = squad_convert_examples_to_features(
+        examples=examples,
+        tokenizer=tokenizer,
+        max_seq_length=args.max_seq_length,
+        doc_stride=args.doc_stride,
+        max_query_length=args.max_query_length,
+        is_training=not evaluate,
+    )
+    ```"""
+    # Defining helper methods
+    features = []
+
+    threads = min(threads, cpu_count())
+    with Pool(
+        threads,
+        initializer=squad_convert_example_to_features_init,
+        initargs=(tokenizer,),
+    ) as p:
+        annotate_ = partial(
+            squad_convert_example_to_features,
+            max_seq_length=max_seq_length,
+            doc_stride=doc_stride,
+            max_query_length=max_query_length,
+            padding_strategy=padding_strategy,
+            is_training=is_training,
+        )
+        features = list(
+            tqdm(
+                p.imap(annotate_, examples, chunksize=32),
+                total=len(examples),
+                desc="convert squad examples to features",
+                disable=not tqdm_enabled,
+            )
+        )
 
-        encoding_dict = defaultdict(list)
-        for e in encodings:
-            encoding_dict["input_ids"].append(e.ids)
-
-            if return_token_type_ids:
-                encoding_dict["token_type_ids"].append(e.type_ids)
-            if return_attention_mask:
-                encoding_dict["attention_mask"].append(e.attention_mask)
-            if return_special_tokens_mask:
-                encoding_dict["special_tokens_mask"].append(e.special_tokens_mask)
-            if return_offsets_mapping:
-                encoding_dict["offset_mapping"].append(e.offsets)
-            if return_length:
-                encoding_dict["length"].append(len(e.ids))
-
-        return encoding_dict, encodings
-
-    def convert_tokens_to_ids(
-        self, tokens: Union[str, List[str]]
-    ) -> Union[int, List[int]]:
-        """
-        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the
-        vocabulary.
+    new_features = []
+    unique_id = 1000000000
+    example_index = 0
+    for example_features in tqdm(
+        features,
+        total=len(features),
+        desc="add example index and unique id",
+        disable=not tqdm_enabled,
+    ):
+        if not example_features:
+            continue
+        for example_feature in example_features:
+            example_feature.example_index = example_index
+            example_feature.unique_id = unique_id
+            new_features.append(example_feature)
+            unique_id += 1
+        example_index += 1
+    features = new_features
+    del new_features
+    if return_dataset == "pt":
+        if not is_torch_available():
+            raise RuntimeError("PyTorch must be installed to return a PyTorch dataset.")
+
+        # Convert to Tensors and build dataset
+        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)
+        all_attention_masks = torch.tensor(
+            [f.attention_mask for f in features], dtype=torch.long
+        )
+        all_token_type_ids = torch.tensor(
+            [f.token_type_ids for f in features], dtype=torch.long
+        )
+        all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)
+        all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)
+        all_is_impossible = torch.tensor(
+            [f.is_impossible for f in features], dtype=torch.float
+        )
 
-        Args:
-            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).
+        if not is_training:
+            all_feature_index = torch.arange(all_input_ids.size(0), dtype=torch.long)
+            dataset = TensorDataset(
+                all_input_ids,
+                all_attention_masks,
+                all_token_type_ids,
+                all_feature_index,
+                all_cls_index,
+                all_p_mask,
+            )
+        else:
+            all_start_positions = torch.tensor(
+                [f.start_position for f in features], dtype=torch.long
+            )
+            all_end_positions = torch.tensor(
+                [f.end_position for f in features], dtype=torch.long
+            )
+            dataset = TensorDataset(
+                all_input_ids,
+                all_attention_masks,
+                all_token_type_ids,
+                all_start_positions,
+                all_end_positions,
+                all_cls_index,
+                all_p_mask,
+                all_is_impossible,
+            )
+
+        return features, dataset
+    elif return_dataset == "tf":
+        if not is_tf_available():
+            raise RuntimeError(
+                "TensorFlow must be installed to return a TensorFlow dataset."
+            )
+
+        def gen():
+            for i, ex in enumerate(features):
+                if ex.token_type_ids is None:
+                    yield (
+                        {
+                            "input_ids": ex.input_ids,
+                            "attention_mask": ex.attention_mask,
+                            "feature_index": i,
+                            "qas_id": ex.qas_id,
+                        },
+                        {
+                            "start_positions": ex.start_position,
+                            "end_positions": ex.end_position,
+                            "cls_index": ex.cls_index,
+                            "p_mask": ex.p_mask,
+                            "is_impossible": ex.is_impossible,
+                        },
+                    )
+                else:
+                    yield (
+                        {
+                            "input_ids": ex.input_ids,
+                            "attention_mask": ex.attention_mask,
+                            "token_type_ids": ex.token_type_ids,
+                            "feature_index": i,
+                            "qas_id": ex.qas_id,
+                        },
+                        {
+                            "start_positions": ex.start_position,
+                            "end_positions": ex.end_position,
+                            "cls_index": ex.cls_index,
+                            "p_mask": ex.p_mask,
+                            "is_impossible": ex.is_impossible,
+                        },
+                    )
 
-        Returns:
-            `int` or `List[int]`: The token id or list of token ids.
-        """
-        if tokens is None:
-            return None
+        # Why have we split the batch into a tuple? PyTorch just has a list of tensors.
+        if "token_type_ids" in tokenizer.model_input_names:
+            train_types = (
+                {
+                    "input_ids": tf.int32,
+                    "attention_mask": tf.int32,
+                    "token_type_ids": tf.int32,
+                    "feature_index": tf.int64,
+                    "qas_id": tf.string,
+                },
+                {
+                    "start_positions": tf.int64,
+                    "end_positions": tf.int64,
+                    "cls_index": tf.int64,
+                    "p_mask": tf.int32,
+                    "is_impossible": tf.int32,
+                },
+            )
 
-        if isinstance(tokens, str):
-            return self._convert_token_to_id_with_added_voc(tokens)
+            train_shapes = (
+                {
+                    "input_ids": tf.TensorShape([None]),
+                    "attention_mask": tf.TensorShape([None]),
+                    "token_type_ids": tf.TensorShape([None]),
+                    "feature_index": tf.TensorShape([]),
+                    "qas_id": tf.TensorShape([]),
+                },
+                {
+                    "start_positions": tf.TensorShape([]),
+                    "end_positions": tf.TensorShape([]),
+                    "cls_index": tf.TensorShape([]),
+                    "p_mask": tf.TensorShape([None]),
+                    "is_impossible": tf.TensorShape([]),
+                },
+            )
+        else:
+            train_types = (
+                {
+                    "input_ids": tf.int32,
+                    "attention_mask": tf.int32,
+                    "feature_index": tf.int64,
+                    "qas_id": tf.string,
+                },
+                {
+                    "start_positions": tf.int64,
+                    "end_positions": tf.int64,
+                    "cls_index": tf.int64,
+                    "p_mask": tf.int32,
+                    "is_impossible": tf.int32,
+                },
+            )
 
-        ids = []
-        for token in tokens:
-            ids.append(self._convert_token_to_id_with_added_voc(token))
-        return ids
-
-    def _convert_token_to_id_with_added_voc(self, token: str) -> int:
-        index = self._tokenizer.token_to_id(token)
-        if index is None:
-            return self.unk_token_id
-        return index
-
-    def _convert_id_to_token(self, index: int) -> Optional[str]:
-        return self._tokenizer.id_to_token(int(index))
-
-    def _add_tokens(
-        self, new_tokens: List[Union[str, AddedToken]], special_tokens=False
-    ) -> int:
-        if special_tokens:
-            return self._tokenizer.add_special_tokens(new_tokens)
+            train_shapes = (
+                {
+                    "input_ids": tf.TensorShape([None]),
+                    "attention_mask": tf.TensorShape([None]),
+                    "feature_index": tf.TensorShape([]),
+                    "qas_id": tf.TensorShape([]),
+                },
+                {
+                    "start_positions": tf.TensorShape([]),
+                    "end_positions": tf.TensorShape([]),
+                    "cls_index": tf.TensorShape([]),
+                    "p_mask": tf.TensorShape([None]),
+                    "is_impossible": tf.TensorShape([]),
+                },
+            )
 
-        return self._tokenizer.add_tokens(new_tokens)
+        return tf.data.Dataset.from_generator(gen, train_types, train_shapes)
+    else:
+        return features
 
-    def num_special_tokens_to_add(self, pair: bool = False) -> int:
-        """
-        Returns the number of added tokens when encoding a sequence with special tokens.
 
-        <Tip>
+class SquadProcessor(DataProcessor):
+    """
+    Processor for the SQuAD data set. overridden by SquadV1Processor and SquadV2Processor, used by the version 1.1 and
+    version 2.0 of SQuAD, respectively.
+    """
 
-        This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put
-        this inside your training loop.
+    train_file = None
+    dev_file = None
 
-        </Tip>
+    def _get_example_from_tensor_dict(self, tensor_dict, evaluate=False):
+        if not evaluate:
+            answer = tensor_dict["answers"]["text"][0].numpy().decode("utf-8")
+            answer_start = tensor_dict["answers"]["answer_start"][0].numpy()
+            answers = []
+        else:
+            answers = [
+                {"answer_start": start.numpy(), "text": text.numpy().decode("utf-8")}
+                for start, text in zip(
+                    tensor_dict["answers"]["answer_start"],
+                    tensor_dict["answers"]["text"],
+                )
+            ]
 
-        Args:
-            pair (`bool`, *optional*, defaults to `False`):
-                Whether the number of added tokens should be computed in the case of a sequence pair or a single
-                sequence.
+            answer = None
+            answer_start = None
 
-        Returns:
-            `int`: Number of special tokens added to sequences.
-        """
-        return self._tokenizer.num_special_tokens_to_add(pair)
+        return SquadExample(
+            qas_id=tensor_dict["id"].numpy().decode("utf-8"),
+            question_text=tensor_dict["question"].numpy().decode("utf-8"),
+            context_text=tensor_dict["context"].numpy().decode("utf-8"),
+            answer_text=answer,
+            start_position_character=answer_start,
+            title=tensor_dict["title"].numpy().decode("utf-8"),
+            answers=answers,
+        )
 
-    def convert_ids_to_tokens(
-        self, ids: Union[int, List[int]], skip_special_tokens: bool = False
-    ) -> Union[str, List[str]]:
+    def get_examples_from_dataset(self, dataset, evaluate=False):
         """
-        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
-        added tokens.
+        Creates a list of [`~data.processors.squad.SquadExample`] using a TFDS dataset.
 
         Args:
-            ids (`int` or `List[int]`):
-                The token id (or token ids) to convert to tokens.
-            skip_special_tokens (`bool`, *optional*, defaults to `False`):
-                Whether or not to remove special tokens in the decoding.
+            dataset: The tfds dataset loaded from *tensorflow_datasets.load("squad")*
+            evaluate: Boolean specifying if in evaluation mode or in training mode
 
         Returns:
-            `str` or `List[str]`: The decoded token(s).
-        """
-        if isinstance(ids, int):
-            return self._tokenizer.id_to_token(ids)
-        tokens = []
-        for index in ids:
-            index = int(index)
-            if skip_special_tokens and index in self.all_special_ids:
-                continue
-            tokens.append(self._tokenizer.id_to_token(index))
-        return tokens
+            List of SquadExample
 
-    def tokenize(
-        self,
-        text: str,
-        pair: Optional[str] = None,
-        add_special_tokens: bool = False,
-        **kwargs,
-    ) -> List[str]:
-        return self.encode_plus(
-            text=text, text_pair=pair, add_special_tokens=add_special_tokens, **kwargs
-        ).tokens()
+        Examples:
 
-    def set_truncation_and_padding(
-        self,
-        padding_strategy: PaddingStrategy,
-        truncation_strategy: TruncationStrategy,
-        max_length: int,
-        stride: int,
-        pad_to_multiple_of: Optional[int],
-    ):
-        """
-        Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers
-        library) and restore the tokenizer settings afterwards.
+        ```python
+        >>> import tensorflow_datasets as tfds
 
-        The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a
-        padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed
-        section.
+        >>> dataset = tfds.load("squad")
 
-        Args:
-            padding_strategy ([`~utils.PaddingStrategy`]):
-                The kind of padding that will be applied to the input
-            truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):
-                The kind of truncation that will be applied to the input
-            max_length (`int`):
-                The maximum size of a sequence.
-            stride (`int`):
-                The stride to use when handling overflow.
-            pad_to_multiple_of (`int`, *optional*):
-                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
-                the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).
-        """
-        _truncation = self._tokenizer.truncation
-        _padding = self._tokenizer.padding
-        # Set truncation and padding on the backend tokenizer
-        if truncation_strategy == TruncationStrategy.DO_NOT_TRUNCATE:
-            if _truncation is not None:
-                self._tokenizer.no_truncation()
-        else:
-            target = {
-                "max_length": max_length,
-                "stride": stride,
-                "strategy": truncation_strategy.value,
-                "direction": self.truncation_side,
-            }
-
-            # _truncation might contain more keys that the target `transformers`
-            # supports. Use only the target keys to trigger `enable_truncation`.
-            # This should enable this code to works on various `tokenizers`
-            # targets.
-            if _truncation is None:
-                current = None
-            else:
-                current = {k: _truncation.get(k, None) for k in target}
-
-            if current != target:
-                self._tokenizer.enable_truncation(**target)
+        >>> training_examples = get_examples_from_dataset(dataset, evaluate=False)
+        >>> evaluation_examples = get_examples_from_dataset(dataset, evaluate=True)
+        ```"""
 
-        if padding_strategy == PaddingStrategy.DO_NOT_PAD:
-            if _padding is not None:
-                self._tokenizer.no_padding()
+        if evaluate:
+            dataset = dataset["validation"]
         else:
-            length = (
-                max_length if padding_strategy == PaddingStrategy.MAX_LENGTH else None
-            )
-            target = {
-                "length": length,
-                "direction": self.padding_side,
-                "pad_id": self.pad_token_id,
-                "pad_token": self.pad_token,
-                "pad_type_id": self.pad_token_type_id,
-                "pad_to_multiple_of": pad_to_multiple_of,
-            }
-            if _padding != target:
-                self._tokenizer.enable_padding(**target)
+            dataset = dataset["train"]
 
-    def _batch_encode_plus(
-        self,
-        batch_text_or_text_pairs: Union[
-            List[TextInput],
-            List[TextInputPair],
-            List[PreTokenizedInput],
-            List[PreTokenizedInputPair],
-        ],
-        add_special_tokens: bool = True,
-        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
-        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
-        max_length: Optional[int] = None,
-        stride: int = 0,
-        is_split_into_words: bool = False,
-        pad_to_multiple_of: Optional[int] = None,
-        return_tensors: Optional[str] = None,
-        return_token_type_ids: Optional[bool] = None,
-        return_attention_mask: Optional[bool] = None,
-        return_overflowing_tokens: bool = False,
-        return_special_tokens_mask: bool = False,
-        return_offsets_mapping: bool = False,
-        return_length: bool = False,
-        verbose: bool = True,
-    ) -> BatchEncoding:
-        if not isinstance(batch_text_or_text_pairs, (tuple, list)):
-            raise TypeError(
-                f"batch_text_or_text_pairs has to be a list or a tuple (got {type(batch_text_or_text_pairs)})"
+        examples = []
+        for tensor_dict in tqdm(dataset):
+            examples.append(
+                self._get_example_from_tensor_dict(tensor_dict, evaluate=evaluate)
             )
 
-        # Set the truncation and padding strategy and restore the initial configuration
-        self.set_truncation_and_padding(
-            padding_strategy=padding_strategy,
-            truncation_strategy=truncation_strategy,
-            max_length=max_length,
-            stride=stride,
-            pad_to_multiple_of=pad_to_multiple_of,
-        )
-
-        encodings = self._tokenizer.encode_batch(
-            batch_text_or_text_pairs,
-            add_special_tokens=add_special_tokens,
-            is_pretokenized=is_split_into_words,
-        )
-
-        # Convert encoding to dict
-        # `Tokens` has type: Tuple[
-        #                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],
-        #                       List[EncodingFast]
-        #                    ]
-        # with nested dimensions corresponding to batch, overflows, sequence length
-        tokens_and_encodings = [
-            self._convert_encoding(
-                encoding=encoding,
-                return_token_type_ids=return_token_type_ids,
-                return_attention_mask=return_attention_mask,
-                return_overflowing_tokens=return_overflowing_tokens,
-                return_special_tokens_mask=return_special_tokens_mask,
-                return_offsets_mapping=return_offsets_mapping,
-                return_length=return_length,
-                verbose=verbose,
-            )
-            for encoding in encodings
-        ]
-
-        # Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension
-        # From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)
-        # (we say ~ because the number of overflow varies with the example in the batch)
-        #
-        # To match each overflowing sample with the original sample in the batch
-        # we add an overflow_to_sample_mapping array (see below)
-        sanitized_tokens = {}
-        for key in tokens_and_encodings[0][0].keys():
-            stack = [e for item, _ in tokens_and_encodings for e in item[key]]
-            sanitized_tokens[key] = stack
-        sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]
-
-        # If returning overflowing tokens, we need to return a mapping
-        # from the batch idx to the original sample
-        if return_overflowing_tokens:
-            overflow_to_sample_mapping = []
-            for i, (toks, _) in enumerate(tokens_and_encodings):
-                overflow_to_sample_mapping += [i] * len(toks["input_ids"])
-            sanitized_tokens["overflow_to_sample_mapping"] = overflow_to_sample_mapping
-
-        for input_ids in sanitized_tokens["input_ids"]:
-            self._eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)
-        return BatchEncoding(
-            sanitized_tokens, sanitized_encodings, tensor_type=return_tensors
-        )
+        return examples
 
-    def _encode_plus(
-        self,
-        text: Union[TextInput, PreTokenizedInput],
-        text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,
-        add_special_tokens: bool = True,
-        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
-        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
-        max_length: Optional[int] = None,
-        stride: int = 0,
-        is_split_into_words: bool = False,
-        pad_to_multiple_of: Optional[int] = None,
-        return_tensors: Optional[bool] = None,
-        return_token_type_ids: Optional[bool] = None,
-        return_attention_mask: Optional[bool] = None,
-        return_overflowing_tokens: bool = False,
-        return_special_tokens_mask: bool = False,
-        return_offsets_mapping: bool = False,
-        return_length: bool = False,
-        verbose: bool = True,
-        **kwargs,
-    ) -> BatchEncoding:
-        batched_input = [(text, text_pair)] if text_pair else [text]
-        batched_output = self._batch_encode_plus(
-            batched_input,
-            is_split_into_words=is_split_into_words,
-            add_special_tokens=add_special_tokens,
-            padding_strategy=padding_strategy,
-            truncation_strategy=truncation_strategy,
-            max_length=max_length,
-            stride=stride,
-            pad_to_multiple_of=pad_to_multiple_of,
-            return_tensors=return_tensors,
-            return_token_type_ids=return_token_type_ids,
-            return_attention_mask=return_attention_mask,
-            return_overflowing_tokens=return_overflowing_tokens,
-            return_special_tokens_mask=return_special_tokens_mask,
-            return_offsets_mapping=return_offsets_mapping,
-            return_length=return_length,
-            verbose=verbose,
-            **kwargs,
-        )
-
-        # Return tensor is None, then we can remove the leading batch axis
-        # Overflowing tokens are returned as a batch of output so we keep them in this case
-        if return_tensors is None and not return_overflowing_tokens:
-            batched_output = BatchEncoding(
-                {
-                    key: value[0]
-                    if len(value) > 0 and isinstance(value[0], list)
-                    else value
-                    for key, value in batched_output.items()
-                },
-                batched_output.encodings,
-            )
+    def get_train_examples(self, data_dir, filename=None):
+        """
+        Returns the training examples from the data directory.
 
-        self._eventual_warn_about_too_long_sequence(
-            batched_output["input_ids"], max_length, verbose
-        )
+        Args:
+            data_dir: Directory containing the data files used for training and evaluating.
+            filename: None by default, specify this if the training file has a different name than the original one
+                which is `train-v1.1.json` and `train-v2.0.json` for squad versions 1.1 and 2.0 respectively.
 
-        return batched_output
+        """
+        if data_dir is None:
+            data_dir = ""
 
-    def convert_tokens_to_string(self, tokens: List[str]) -> str:
-        return self.backend_tokenizer.decoder.decode(tokens)
+        if self.train_file is None:
+            raise ValueError(
+                "SquadProcessor should be instantiated via SquadV1Processor or SquadV2Processor"
+            )
 
-    def _decode(
-        self,
-        token_ids: Union[int, List[int]],
-        skip_special_tokens: bool = False,
-        clean_up_tokenization_spaces: bool = True,
-        **kwargs,
-    ) -> str:
-        self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)
-
-        if isinstance(token_ids, int):
-            token_ids = [token_ids]
-        text = self._tokenizer.decode(
-            token_ids, skip_special_tokens=skip_special_tokens
-        )
-
-        if clean_up_tokenization_spaces:
-            clean_text = self.clean_up_tokenization(text)
-            return clean_text
-        else:
-            return text
+        with open(
+            os.path.join(data_dir, self.train_file if filename is None else filename),
+            "r",
+            encoding="utf-8",
+        ) as reader:
+            input_data = json.load(reader)["data"]
+        return self._create_examples(input_data, "train")
 
-    def _save_pretrained(
-        self,
-        save_directory: Union[str, os.PathLike],
-        file_names: Tuple[str],
-        legacy_format: Optional[bool] = None,
-        filename_prefix: Optional[str] = None,
-    ) -> Tuple[str]:
+    def get_dev_examples(self, data_dir, filename=None):
         """
-        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON
-        file containing {config + vocab + added-tokens}.
+        Returns the evaluation example from the data directory.
+
+        Args:
+            data_dir: Directory containing the data files used for training and evaluating.
+            filename: None by default, specify this if the evaluation file has a different name than the original one
+                which is `dev-v1.1.json` and `dev-v2.0.json` for squad versions 1.1 and 2.0 respectively.
         """
-        save_directory = str(save_directory)
+        if data_dir is None:
+            data_dir = ""
 
-        if self.slow_tokenizer_class is None and legacy_format is True:
+        if self.dev_file is None:
             raise ValueError(
-                "Your tokenizer does not have a legacy version defined and therefore cannot register this version. You"
-                " might consider leaving the legacy_format at `None` or setting it to `False`."
+                "SquadProcessor should be instantiated via SquadV1Processor or SquadV2Processor"
             )
 
-        save_slow = (
-            (legacy_format is None or legacy_format is True)
-            and self.slow_tokenizer_class is not None
-            and self.can_save_slow_tokenizer
-        )
-        save_fast = legacy_format is None or legacy_format is False
-
-        if save_slow:
-            added_tokens_file = os.path.join(
-                save_directory,
-                (filename_prefix + "-" if filename_prefix else "") + ADDED_TOKENS_FILE,
-            )
-            added_vocab = self.get_added_vocab()
-            if added_vocab:
-                with open(added_tokens_file, "w", encoding="utf-8") as f:
-                    out_str = (
-                        json.dumps(
-                            added_vocab, indent=2, sort_keys=True, ensure_ascii=False
-                        )
-                        + "\n"
+        with open(
+            os.path.join(data_dir, self.dev_file if filename is None else filename),
+            "r",
+            encoding="utf-8",
+        ) as reader:
+            input_data = json.load(reader)["data"]
+        return self._create_examples(input_data, "dev")
+
+    def _create_examples(self, input_data, set_type):
+        is_training = set_type == "train"
+        examples = []
+        for entry in tqdm(input_data):
+            title = entry["title"]
+            for paragraph in entry["paragraphs"]:
+                context_text = paragraph["context"]
+                for qa in paragraph["qas"]:
+                    qas_id = qa["id"]
+                    question_text = qa["question"]
+                    start_position_character = None
+                    answer_text = None
+                    answers = []
+
+                    is_impossible = qa.get("is_impossible", False)
+                    if not is_impossible:
+                        if is_training:
+                            answer = qa["answers"][0]
+                            answer_text = answer["text"]
+                            start_position_character = answer["answer_start"]
+                        else:
+                            answers = qa["answers"]
+
+                    example = SquadExample(
+                        qas_id=qas_id,
+                        question_text=question_text,
+                        context_text=context_text,
+                        answer_text=answer_text,
+                        start_position_character=start_position_character,
+                        title=title,
+                        is_impossible=is_impossible,
+                        answers=answers,
                     )
-                    f.write(out_str)
+                    examples.append(example)
+        return examples
 
-            vocab_files = self.save_vocabulary(
-                save_directory, filename_prefix=filename_prefix
-            )
-            file_names = file_names + vocab_files + (added_tokens_file,)
 
-        if save_fast:
-            tokenizer_file = os.path.join(
-                save_directory,
-                (filename_prefix + "-" if filename_prefix else "") + TOKENIZER_FILE,
-            )
-            self.backend_tokenizer.save(tokenizer_file)
-            file_names = file_names + (tokenizer_file,)
+class SquadV1Processor(SquadProcessor):
+    train_file = "train-v1.1.json"
+    dev_file = "dev-v1.1.json"
+
+
+class SquadV2Processor(SquadProcessor):
+    train_file = "train-v2.0.json"
+    dev_file = "dev-v2.0.json"
 
-        return file_names
 
-    def train_new_from_iterator(
+class SquadExample:
+    """
+    A single training/test example for the Squad dataset, as loaded from disk.
+
+    Args:
+        qas_id: The example's unique identifier
+        question_text: The question string
+        context_text: The context string
+        answer_text: The answer string
+        start_position_character: The character position of the start of the answer
+        title: The title of the example
+        answers: None by default, this is used during evaluation. Holds answers as well as their start positions.
+        is_impossible: False by default, set to True if the example has no possible answer.
+    """
+
+    def __init__(
         self,
-        text_iterator,
-        vocab_size,
-        length=None,
-        new_special_tokens=None,
-        special_tokens_map=None,
-        **kwargs,
+        qas_id,
+        question_text,
+        context_text,
+        answer_text,
+        start_position_character,
+        title,
+        answers=[],
+        is_impossible=False,
     ):
-        """
-        Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)
-        as the current one.
+        self.qas_id = qas_id
+        self.question_text = question_text
+        self.context_text = context_text
+        self.answer_text = answer_text
+        self.title = title
+        self.is_impossible = is_impossible
+        self.answers = answers
+
+        self.start_position, self.end_position = 0, 0
+
+        doc_tokens = []
+        char_to_word_offset = []
+        prev_is_whitespace = True
+
+        # Split on whitespace so that different tokens may be attributed to their original position.
+        for c in self.context_text:
+            if _is_whitespace(c):
+                prev_is_whitespace = True
+            else:
+                if prev_is_whitespace:
+                    doc_tokens.append(c)
+                else:
+                    doc_tokens[-1] += c
+                prev_is_whitespace = False
+            char_to_word_offset.append(len(doc_tokens) - 1)
+
+        self.doc_tokens = doc_tokens
+        self.char_to_word_offset = char_to_word_offset
+
+        # Start and end positions only has a value during evaluation.
+        if start_position_character is not None and not is_impossible:
+            self.start_position = char_to_word_offset[start_position_character]
+            self.end_position = char_to_word_offset[
+                min(
+                    start_position_character + len(answer_text) - 1,
+                    len(char_to_word_offset) - 1,
+                )
+            ]
 
-        Args:
-            text_iterator (generator of `List[str]`):
-                The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts
-                if you have everything in memory.
-            vocab_size (`int`):
-                The size of the vocabulary you want for your tokenizer.
-            length (`int`, *optional*):
-                The total number of sequences in the iterator. This is used to provide meaningful progress tracking
-            new_special_tokens (list of `str` or `AddedToken`, *optional*):
-                A list of new special tokens to add to the tokenizer you are training.
-            special_tokens_map (`Dict[str, str]`, *optional*):
-                If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special
-                token name to new special token name in this argument.
-            kwargs:
-                Additional keyword arguments passed along to the trainer from the 🤗 Tokenizers library.
 
-        Returns:
-            [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on
-            `text_iterator`.
+class SquadFeatures:
+    """
+    Single squad example features to be fed to a model. Those features are model-specific and can be crafted from
+    [`~data.processors.squad.SquadExample`] using the
+    :method:*~transformers.data.processors.squad.squad_convert_examples_to_features* method.
+
+    Args:
+        input_ids: Indices of input sequence tokens in the vocabulary.
+        attention_mask: Mask to avoid performing attention on padding token indices.
+        token_type_ids: Segment token indices to indicate first and second portions of the inputs.
+        cls_index: the index of the CLS token.
+        p_mask: Mask identifying tokens that can be answers vs. tokens that cannot.
+            Mask with 1 for tokens than cannot be in the answer and 0 for token that can be in an answer
+        example_index: the index of the example
+        unique_id: The unique Feature identifier
+        paragraph_len: The length of the context
+        token_is_max_context:
+            List of booleans identifying which tokens have their maximum context in this feature object. If a token
+            does not have their maximum context in this feature object, it means that another feature object has more
+            information related to that token and should be prioritized over this feature for that token.
+        tokens: list of tokens corresponding to the input ids
+        token_to_orig_map: mapping between the tokens and the original text, needed in order to identify the answer.
+        start_position: start of the answer token index
+        end_position: end of the answer token index
+        encoding: optionally store the BatchEncoding with the fast-tokenizer alignment methods.
+    """
 
-        """
-        tokenizer_json = json.loads(self._tokenizer.to_str())
-        # Remove added tokens for now (uses IDs of tokens)
-        added_tokens = tokenizer_json.pop("added_tokens")
-        # Remove post processor for now (uses IDs of tokens)
-        post_processor = tokenizer_json.pop("post_processor")
-
-        unk_token = None
-        # Remove vocab
-        if tokenizer_json["model"]["type"] == "BPE":
-            tokenizer_json["model"]["vocab"] = {}
-            tokenizer_json["model"]["merges"] = []
-        elif tokenizer_json["model"]["type"] == "Unigram":
-            if tokenizer_json["model"]["unk_id"] is not None:
-                unk_id = tokenizer_json["model"]["unk_id"]
-                unk_token = tokenizer_json["model"]["vocab"][unk_id][0]
-                if special_tokens_map is not None and unk_token in special_tokens_map:
-                    unk_token = special_tokens_map[unk_token]
-                tokenizer_json["model"]["unk_id"] = 0
-                tokenizer_json["model"]["vocab"] = [[unk_token, 0.0]]
-        elif tokenizer_json["model"]["type"] in ["WordLevel", "WordPiece"]:
-            tokenizer_json["model"]["vocab"] = {}
-        else:
-            raise ValueError(
-                f"This method does not support this type of tokenizer (found {tokenizer_json['model']['type']}) "
-                "only BPE, Unigram, WordLevel and WordPiece."
-            )
+    def __init__(
+        self,
+        input_ids,
+        attention_mask,
+        token_type_ids,
+        cls_index,
+        p_mask,
+        example_index,
+        unique_id,
+        paragraph_len,
+        token_is_max_context,
+        tokens,
+        token_to_orig_map,
+        start_position,
+        end_position,
+        is_impossible,
+        qas_id: str = None,
+        encoding: BatchEncoding = None,
+    ):
+        self.input_ids = input_ids
+        self.attention_mask = attention_mask
+        self.token_type_ids = token_type_ids
+        self.cls_index = cls_index
+        self.p_mask = p_mask
+
+        self.example_index = example_index
+        self.unique_id = unique_id
+        self.paragraph_len = paragraph_len
+        self.token_is_max_context = token_is_max_context
+        self.tokens = tokens
+        self.token_to_orig_map = token_to_orig_map
+
+        self.start_position = start_position
+        self.end_position = end_position
+        self.is_impossible = is_impossible
+        self.qas_id = qas_id
 
-        if (
-            special_tokens_map is not None
-            and "unk_token" in tokenizer_json["model"]
-            and tokenizer_json["model"]["unk_token"] in special_tokens_map
-        ):
-            tokenizer_json["model"]["unk_token"] = special_tokens_map[
-                tokenizer_json["model"]["unk_token"]
-            ]
+        self.encoding = encoding
 
-        tokenizer = TokenizerFast.from_str(json.dumps(tokenizer_json))
 
-        # Get the special tokens from the current tokenizer if none are specified.
-        special_tokens = []
-        for added_token in added_tokens:
-            special = added_token.pop("special", None)
-            _ = added_token.pop("id", None)
-            if tokenizer_json["model"]["type"] != "Unigram" and not special:
-                continue
-            if (
-                special_tokens_map is not None
-                and added_token["content"] in special_tokens_map
-            ):
-                added_token["content"] = special_tokens_map[added_token["content"]]
-            special_tokens.append(AddedToken(**added_token))
-
-        if new_special_tokens is not None:
-            special_tokens.extend(new_special_tokens)
-
-        # Trainer needs to know the end of word / continuing subword thingies in BPE
-        if (
-            tokenizer_json["model"]["type"] == "BPE"
-            and "continuing_subword_prefix" not in kwargs
-            and tokenizer_json["model"]["continuing_subword_prefix"] is not None
-        ):
-            kwargs["continuing_subword_prefix"] = tokenizer_json["model"][
-                "continuing_subword_prefix"
-            ]
-        if (
-            tokenizer_json["model"]["type"] == "BPE"
-            and "end_of_word_suffix" not in kwargs
-            and tokenizer_json["model"]["end_of_word_suffix"] is not None
-        ):
-            kwargs["end_of_word_suffix"] = tokenizer_json["model"]["end_of_word_suffix"]
-        if tokenizer_json["model"]["type"] == "Unigram" and unk_token is not None:
-            kwargs["unk_token"] = unk_token
-        if tokenizer_json["pre_tokenizer"]["type"] == "ByteLevel":
-            kwargs["initial_alphabet"] = pre_tokenizers_fast.ByteLevel.alphabet()
-
-        trainer_class = MODEL_TO_TRAINER_MAPPING[tokenizer_json["model"]["type"]]
-        trainer = trainer_class(
-            vocab_size=vocab_size, special_tokens=special_tokens, **kwargs
-        )
-        tokenizer.train_from_iterator(text_iterator, length=length, trainer=trainer)
-
-        if post_processor is not None:
-            trained_tokenizer_json = json.loads(tokenizer.to_str())
-            # Almost done, we just have to adjust the token IDs in the post processor
-            if "special_tokens" in post_processor:
-                for key in post_processor["special_tokens"]:
-                    tokens = post_processor["special_tokens"][key]["tokens"]
-                    if special_tokens_map is not None:
-                        tokens = [
-                            special_tokens_map.get(token, token) for token in tokens
-                        ]
-                    post_processor["special_tokens"][key]["tokens"] = tokens
-                    post_processor["special_tokens"][key]["ids"] = [
-                        tokenizer.token_to_id(token) for token in tokens
-                    ]
-
-            for special_token in ["cls", "sep"]:
-                if special_token in post_processor:
-                    token, _ = post_processor[special_token]
-                    if special_tokens_map is not None and token in special_tokens_map:
-                        token = special_tokens_map[token]
-                    token_id = tokenizer.token_to_id(token)
-                    post_processor[special_token] = [token, token_id]
-
-            trained_tokenizer_json["post_processor"] = post_processor
-            tokenizer = TokenizerFast.from_str(json.dumps(trained_tokenizer_json))
-
-        kwargs = self.init_kwargs.copy()
-        # Map pad/cls/mask token at the Transformers level
-        special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()
-        special_tokens_list.remove("additional_special_tokens")
-        for token in special_tokens_list:
-            # Get the private one to avoid unnecessary warnings.
-            if getattr(self, f"_{token}") is not None:
-                special_token = getattr(self, token)
-                if (
-                    special_tokens_map is not None
-                    and special_token in special_tokens_map
-                ):
-                    special_token = special_tokens_map[special_token]
-
-                special_token_full = getattr(self, f"_{token}")
-                if isinstance(special_token_full, AddedToken):
-                    # Create an added token with the same parameters except the content
-                    kwargs[token] = AddedToken(
-                        special_token,
-                        single_word=special_token_full.single_word,
-                        lstrip=special_token_full.lstrip,
-                        rstrip=special_token_full.rstrip,
-                        normalized=special_token_full.normalized,
-                    )
-                else:
-                    kwargs[token] = special_token
+class SquadResult:
+    """
+    Constructs a SquadResult which can be used to evaluate a model's output on the SQuAD dataset.
 
-        additional_special_tokens = self.additional_special_tokens
-        if new_special_tokens is not None:
-            additional_special_tokens.extend(new_special_tokens)
-        if len(additional_special_tokens) > 0:
-            kwargs["additional_special_tokens"] = additional_special_tokens
+    Args:
+        unique_id: The unique identifier corresponding to that example.
+        start_logits: The logits corresponding to the start of the answer
+        end_logits: The logits corresponding to the end of the answer
+    """
 
-        return self.__class__(tokenizer_object=tokenizer, **kwargs)
+    def __init__(
+        self,
+        unique_id,
+        start_logits,
+        end_logits,
+        start_top_index=None,
+        end_top_index=None,
+        cls_logits=None,
+    ):
+        self.start_logits = start_logits
+        self.end_logits = end_logits
+        self.unique_id = unique_id
+
+        if start_top_index:
+            self.start_top_index = start_top_index
+            self.end_top_index = end_top_index
+            self.cls_logits = cls_logits
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `xs_transformers-1.0.0/xs_transformers/trainer_pt_utils.py` & `xs_transformers-1.0.1/xs_transformers/pipelines/base.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,1208 +1,1340 @@
 # coding=utf-8
-# Copyright 2020-present the HuggingFace Inc. team.
+# Copyright 2018 The HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Torch utilities for the Trainer class.
-"""
-
-import datetime
+import collections
+import csv
+import importlib
 import json
-import math
 import os
+import pickle
 import sys
+import types
 import warnings
-from collections.abc import Mapping
+from abc import ABC, abstractmethod
+from collections import UserDict
 from contextlib import contextmanager
-from dataclasses import dataclass
-from logging import StreamHandler
-from typing import Any, Dict, Iterator, List, Optional, Union
-
-import numpy as np
-import torch
-import torch.distributed as dist
-from torch import nn
-from torch.utils.data import Dataset, IterableDataset, RandomSampler, Sampler
-from torch.utils.data.distributed import DistributedSampler
-
-from .tokenization_utils_base import BatchEncoding
-from .utils import (
-    is_sagemaker_mp_enabled,
-    is_torch_tpu_available,
-    is_training_run_on_sagemaker,
+from os.path import abspath, exists
+from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union
+
+from packaging import version
+
+from ..dynamic_module_utils import custom_object_save
+from ..feature_extraction_utils import PreTrainedFeatureExtractor
+from ..modelcard import ModelCard
+from ..models.auto.configuration_auto import AutoConfig
+from ..tokenization_utils import PreTrainedTokenizer
+from ..utils import (
+    ModelOutput,
+    add_end_docstrings,
+    is_tf_available,
+    is_torch_available,
     logging,
 )
 
-if is_training_run_on_sagemaker():
-    logging.add_handler(StreamHandler(sys.stdout))
+GenericTensor = Union[List["GenericTensor"], "torch.Tensor", "tf.Tensor"]
+
+if is_tf_available():
+    import tensorflow as tf
+
+    from ..models.auto.modeling_tf_auto import TFAutoModel
+
+if is_torch_available():
+    import torch
+    from torch.utils.data import DataLoader, Dataset
+
+    from ..models.auto.modeling_auto import AutoModel
 
-if is_torch_tpu_available(check_device=False):
-    import torch_xla.core.xla_model as xm
+    # Re-export for backward compatibility
+    from .pt_utils import KeyDataset
+else:
+    Dataset = None
+    KeyDataset = None
+
+if TYPE_CHECKING:
+    from ..modeling_tf_utils import TFPreTrainedModel
+    from ..modeling_utils import PreTrainedModel
 
-# this is used to suppress an undesired warning emitted by pytorch versions 1.4.2-1.7.0
-try:
-    from torch.optim.lr_scheduler import SAVE_STATE_WARNING
-except ImportError:
-    SAVE_STATE_WARNING = ""
 
 logger = logging.get_logger(__name__)
 
 
-def atleast_1d(tensor_or_array: Union[torch.Tensor, np.ndarray]):
-    if isinstance(tensor_or_array, torch.Tensor):
-        if hasattr(torch, "atleast_1d"):
-            tensor_or_array = torch.atleast_1d(tensor_or_array)
-        elif tensor_or_array.ndim < 1:
-            tensor_or_array = tensor_or_array[None]
+def no_collate_fn(items):
+    if len(items) != 1:
+        raise ValueError("This collate_fn is meant to be used with batch_size=1")
+    return items[0]
+
+
+def _pad(items, key, padding_value, padding_side):
+    batch_size = len(items)
+    if isinstance(items[0][key], torch.Tensor):
+        # Others include `attention_mask` etc...
+        shape = items[0][key].shape
+        dim = len(shape)
+        if key == "pixel_values":
+            # This is probable image so padding shouldn't be necessary
+            # B, C, H, W
+            return torch.cat([item[key] for item in items], dim=0)
+        max_length = max(item[key].shape[1] for item in items)
+        min_length = min(item[key].shape[1] for item in items)
+        dtype = items[0][key].dtype
+
+        if dim == 2:
+            if max_length == min_length:
+                # Bypass for `ImageGPT` which doesn't provide a padding value, yet
+                # we can consistently pad since the size should be matching
+                return torch.cat([item[key] for item in items], dim=0)
+            tensor = torch.zeros((batch_size, max_length), dtype=dtype) + padding_value
+        elif dim == 3:
+            tensor = (
+                torch.zeros((batch_size, max_length, shape[-1]), dtype=dtype)
+                + padding_value
+            )
+
+        for i, item in enumerate(items):
+            if dim == 2:
+                if padding_side == "left":
+                    tensor[i, -len(item[key][0]) :] = item[key][0].clone()
+                else:
+                    tensor[i, : len(item[key][0])] = item[key][0].clone()
+            elif dim == 3:
+                if padding_side == "left":
+                    tensor[i, -len(item[key][0]) :, :] = item[key][0].clone()
+                else:
+                    tensor[i, : len(item[key][0]), :] = item[key][0].clone()
+        return tensor
     else:
-        tensor_or_array = np.atleast_1d(tensor_or_array)
-    return tensor_or_array
+        return [item[key] for item in items]
 
 
-def torch_pad_and_concatenate(tensor1, tensor2, padding_index=-100):
-    """Concatenates `tensor1` and `tensor2` on first axis, applying padding on the second if necessary."""
-    tensor1 = atleast_1d(tensor1)
-    tensor2 = atleast_1d(tensor2)
-
-    if len(tensor1.shape) == 1 or tensor1.shape[1] == tensor2.shape[1]:
-        return torch.cat((tensor1, tensor2), dim=0)
-
-    # Let's figure out the new shape
-    new_shape = (
-        tensor1.shape[0] + tensor2.shape[0],
-        max(tensor1.shape[1], tensor2.shape[1]),
-    ) + tensor1.shape[2:]
-
-    # Now let's fill the result tensor
-    result = tensor1.new_full(new_shape, padding_index)
-    result[: tensor1.shape[0], : tensor1.shape[1]] = tensor1
-    result[tensor1.shape[0] :, : tensor2.shape[1]] = tensor2
-    return result
-
-
-def numpy_pad_and_concatenate(array1, array2, padding_index=-100):
-    """Concatenates `array1` and `array2` on first axis, applying padding on the second if necessary."""
-    array1 = atleast_1d(array1)
-    array2 = atleast_1d(array2)
-
-    if len(array1.shape) == 1 or array1.shape[1] == array2.shape[1]:
-        return np.concatenate((array1, array2), axis=0)
-
-    # Let's figure out the new shape
-    new_shape = (
-        array1.shape[0] + array2.shape[0],
-        max(array1.shape[1], array2.shape[1]),
-    ) + array1.shape[2:]
-
-    # Now let's fill the result tensor
-    result = np.full_like(array1, padding_index, shape=new_shape)
-    result[: array1.shape[0], : array1.shape[1]] = array1
-    result[array1.shape[0] :, : array2.shape[1]] = array2
-    return result
-
-
-def nested_concat(tensors, new_tensors, padding_index=-100):
-    """
-    Concat the `new_tensors` to `tensors` on the first dim and pad them on the second if needed. Works for tensors or
-    nested list/tuples/dict of tensors.
-    """
-    assert type(tensors) == type(
-        new_tensors
-    ), f"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}."
-    if isinstance(tensors, (list, tuple)):
-        return type(tensors)(
-            nested_concat(t, n, padding_index=padding_index)
-            for t, n in zip(tensors, new_tensors)
+def pad_collate_fn(tokenizer, feature_extractor):
+    # Tokenizer
+    t_padding_side = None
+    # Feature extractor
+    f_padding_side = None
+    if tokenizer is None and feature_extractor is None:
+        raise ValueError(
+            "Pipeline without tokenizer or feature_extractor cannot do batching"
         )
-    elif isinstance(tensors, torch.Tensor):
-        return torch_pad_and_concatenate(
-            tensors, new_tensors, padding_index=padding_index
-        )
-    elif isinstance(tensors, Mapping):
-        return type(tensors)(
-            {
-                k: nested_concat(t, new_tensors[k], padding_index=padding_index)
-                for k, t in tensors.items()
-            }
-        )
-    elif isinstance(tensors, np.ndarray):
-        return numpy_pad_and_concatenate(
-            tensors, new_tensors, padding_index=padding_index
+    if tokenizer is not None:
+        if tokenizer.pad_token_id is None:
+            raise ValueError(
+                "Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with "
+                "`pipe.tokenizer.pad_token_id = model.config.eos_token_id`."
+            )
+        else:
+            t_padding_value = tokenizer.pad_token_id
+            t_padding_side = tokenizer.padding_side
+    if feature_extractor is not None:
+        # Feature extractor can be images, where no padding is expected
+        f_padding_value = getattr(feature_extractor, "padding_value", None)
+        f_padding_side = getattr(feature_extractor, "padding_side", None)
+
+    if (
+        t_padding_side is not None
+        and f_padding_side is not None
+        and t_padding_side != f_padding_side
+    ):
+        raise ValueError(
+            f"The feature extractor, and tokenizer don't agree on padding side {t_padding_side} != {f_padding_side}"
         )
-    else:
-        raise TypeError(f"Unsupported type for concatenation: got {type(tensors)}")
+    padding_side = "right"
+    if t_padding_side is not None:
+        padding_side = t_padding_side
+    if f_padding_side is not None:
+        padding_side = f_padding_side
+
+    def inner(items):
+        keys = set(items[0].keys())
+        for item in items:
+            if set(item.keys()) != keys:
+                raise ValueError(
+                    f"The elements of the batch contain different keys. Cannot batch them ({set(item.keys())} !="
+                    f" {keys})"
+                )
+        # input_values, input_pixels, input_ids, ...
+        padded = {}
+        for key in keys:
+            if key in {"input_ids"}:
+                # ImageGPT uses a feature extractor
+                if feature_extractor is not None:
+                    _padding_value = f_padding_value
+                else:
+                    _padding_value = t_padding_value
+            elif key in {"input_values", "pixel_values", "input_features"}:
+                _padding_value = f_padding_value
+            elif key in {"p_mask", "special_tokens_mask"}:
+                _padding_value = 1
+            elif key in {"attention_mask", "token_type_ids"}:
+                _padding_value = 0
+            else:
+                # This is likely another random key maybe even user provided
+                _padding_value = 0
+            padded[key] = _pad(items, key, _padding_value, padding_side)
+        return padded
+
+    return inner
 
 
-def find_batch_size(tensors):
+def infer_framework_load_model(
+    model,
+    config: AutoConfig,
+    model_classes: Optional[Dict[str, Tuple[type]]] = None,
+    task: Optional[str] = None,
+    framework: Optional[str] = None,
+    **model_kwargs,
+):
     """
-    Find the first dimension of a tensor in a nested list/tuple/dict of tensors.
+    Select framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).
+
+    If `model` is instantiated, this function will just infer the framework from the model class. Otherwise `model` is
+    actually a checkpoint name and this method will try to instantiate it using `model_classes`. Since we don't want to
+    instantiate the model twice, this model is returned for use by the pipeline.
+
+    If both frameworks are installed and available for `model`, PyTorch is selected.
+
+    Args:
+        model (`str`, [`PreTrainedModel`] or [`TFPreTrainedModel`]):
+            The model to infer the framework from. If `str`, a checkpoint name. The model to infer the framewrok from.
+        config ([`AutoConfig`]):
+            The config associated with the model to help using the correct class
+        model_classes (dictionary `str` to `type`, *optional*):
+            A mapping framework to class.
+        task (`str`):
+            The task defining which pipeline will be returned.
+        model_kwargs:
+            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,
+            **model_kwargs)` function.
+
+    Returns:
+        `Tuple`: A tuple framework, model.
     """
-    if isinstance(tensors, (list, tuple)):
-        for t in tensors:
-            result = find_batch_size(t)
-            if result is not None:
-                return result
-    elif isinstance(tensors, Mapping):
-        for key, value in tensors.items():
-            result = find_batch_size(value)
-            if result is not None:
-                return result
-    elif isinstance(tensors, torch.Tensor):
-        return tensors.shape[0] if len(tensors.shape) >= 1 else None
-    elif isinstance(tensors, np.ndarray):
-        return tensors.shape[0] if len(tensors.shape) >= 1 else None
-
-
-def nested_numpify(tensors):
-    "Numpify `tensors` (even if it's a nested list/tuple/dict of tensors)."
-    if isinstance(tensors, (list, tuple)):
-        return type(tensors)(nested_numpify(t) for t in tensors)
-    if isinstance(tensors, Mapping):
-        return type(tensors)({k: nested_numpify(t) for k, t in tensors.items()})
-
-    t = tensors.cpu()
-    if t.dtype == torch.bfloat16:
-        # As of Numpy 1.21.4, NumPy does not support bfloat16 (see
-        # https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).
-        # Until Numpy adds bfloat16, we must convert float32.
-        t = t.to(torch.float32)
-    return t.numpy()
-
-
-def nested_detach(tensors):
-    "Detach `tensors` (even if it's a nested list/tuple/dict of tensors)."
-    if isinstance(tensors, (list, tuple)):
-        return type(tensors)(nested_detach(t) for t in tensors)
-    elif isinstance(tensors, Mapping):
-        return type(tensors)({k: nested_detach(t) for k, t in tensors.items()})
-    return tensors.detach()
-
-
-def nested_xla_mesh_reduce(tensors, name):
-    if is_torch_tpu_available():
-        import torch_xla.core.xla_model as xm
-
-        if isinstance(tensors, (list, tuple)):
-            return type(tensors)(
-                nested_xla_mesh_reduce(t, f"{name}_{i}") for i, t in enumerate(tensors)
+    if not is_tf_available() and not is_torch_available():
+        raise RuntimeError(
+            "At least one of TensorFlow 2.0 or PyTorch should be installed. "
+            "To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ "
+            "To install PyTorch, read the instructions at https://pytorch.org/."
+        )
+    if isinstance(model, str):
+        model_kwargs["_from_pipeline"] = task
+        class_tuple = ()
+        look_pt = is_torch_available() and framework in {"pt", None}
+        look_tf = is_tf_available() and framework in {"tf", None}
+        if model_classes:
+            if look_pt:
+                class_tuple = class_tuple + model_classes.get("pt", (AutoModel,))
+            if look_tf:
+                class_tuple = class_tuple + model_classes.get("tf", (TFAutoModel,))
+        if config.architectures:
+            classes = []
+            for architecture in config.architectures:
+                transformers_module = importlib.import_module("transformers")
+                if look_pt:
+                    _class = getattr(transformers_module, architecture, None)
+                    if _class is not None:
+                        classes.append(_class)
+                if look_tf:
+                    _class = getattr(transformers_module, f"TF{architecture}", None)
+                    if _class is not None:
+                        classes.append(_class)
+            class_tuple = class_tuple + tuple(classes)
+
+        if len(class_tuple) == 0:
+            raise ValueError(
+                f"Pipeline cannot infer suitable model classes from {model}"
             )
-        if isinstance(tensors, Mapping):
-            return type(tensors)(
-                {
-                    k: nested_xla_mesh_reduce(t, f"{name}_{i}")
-                    for i, (k, t) in enumerate(tensors.items())
-                }
+
+        for model_class in class_tuple:
+            kwargs = model_kwargs.copy()
+            if framework == "pt" and model.endswith(".h5"):
+                kwargs["from_tf"] = True
+                logger.warning(
+                    "Model might be a TensorFlow model (ending with `.h5`) but TensorFlow is not available. "
+                    "Trying to load the model with PyTorch."
+                )
+            elif framework == "tf" and model.endswith(".bin"):
+                kwargs["from_pt"] = True
+                logger.warning(
+                    "Model might be a PyTorch model (ending with `.bin`) but PyTorch is not available. "
+                    "Trying to load the model with Tensorflow."
+                )
+
+            try:
+                model = model_class.from_pretrained(model, **kwargs)
+                if hasattr(model, "eval"):
+                    model = model.eval()
+                # Stop loading on the first successful load.
+                break
+            except (OSError, ValueError):
+                continue
+
+        if isinstance(model, str):
+            raise ValueError(
+                f"Could not load model {model} with any of the following classes: {class_tuple}."
             )
 
-        tensors = atleast_1d(tensors)
-        return xm.mesh_reduce(name, tensors, torch.cat)
-    else:
-        raise ImportError("Torch xla must be installed to use `nested_xla_mesh_reduce`")
+    framework = "tf" if model.__class__.__name__.startswith("TF") else "pt"
+    return framework, model
 
 
-def distributed_concat(tensor: Any, num_total_examples: Optional[int] = None) -> Any:
-    try:
-        if isinstance(tensor, (tuple, list)):
-            return type(tensor)(
-                distributed_concat(t, num_total_examples) for t in tensor
-            )
-        tensor = atleast_1d(tensor)
-        output_tensors = [tensor.clone() for _ in range(dist.get_world_size())]
-        dist.all_gather(output_tensors, tensor)
-        concat = torch.cat(output_tensors, dim=0)
-
-        # truncate the dummy elements added by SequentialDistributedSampler
-        if num_total_examples is not None:
-            concat = concat[:num_total_examples]
-        return concat
-    except AssertionError:
-        raise AssertionError("Not currently using distributed training")
-
-
-def distributed_broadcast_scalars(
-    scalars: List[Union[int, float]],
-    num_total_examples: Optional[int] = None,
-    device: Optional[torch.device] = torch.device("cuda"),
-) -> torch.Tensor:
-    try:
-        tensorized_scalar = torch.tensor(scalars).to(device)
-        output_tensors = [
-            tensorized_scalar.clone() for _ in range(dist.get_world_size())
-        ]
-        dist.all_gather(output_tensors, tensorized_scalar)
-        concat = torch.cat(output_tensors, dim=0)
+def infer_framework_from_model(
+    model,
+    model_classes: Optional[Dict[str, Tuple[type]]] = None,
+    task: Optional[str] = None,
+    framework: Optional[str] = None,
+    **model_kwargs,
+):
+    """
+    Select framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).
+
+    If `model` is instantiated, this function will just infer the framework from the model class. Otherwise `model` is
+    actually a checkpoint name and this method will try to instantiate it using `model_classes`. Since we don't want to
+    instantiate the model twice, this model is returned for use by the pipeline.
 
-        # truncate the dummy elements added by SequentialDistributedSampler
-        if num_total_examples is not None:
-            concat = concat[:num_total_examples]
-        return concat
-    except AssertionError:
-        raise AssertionError("Not currently using distributed training")
+    If both frameworks are installed and available for `model`, PyTorch is selected.
 
+    Args:
+        model (`str`, [`PreTrainedModel`] or [`TFPreTrainedModel`]):
+            The model to infer the framework from. If `str`, a checkpoint name. The model to infer the framewrok from.
+        model_classes (dictionary `str` to `type`, *optional*):
+            A mapping framework to class.
+        task (`str`):
+            The task defining which pipeline will be returned.
+        model_kwargs:
+            Additional dictionary of keyword arguments passed along to the model's `from_pretrained(...,
+            **model_kwargs)` function.
 
-def reissue_pt_warnings(caught_warnings):
-    # Reissue warnings that are not the SAVE_STATE_WARNING
-    if len(caught_warnings) > 1:
-        for w in caught_warnings:
-            if w.category != UserWarning or w.message != SAVE_STATE_WARNING:
-                warnings.warn(w.message, w.category)
+    Returns:
+        `Tuple`: A tuple framework, model.
+    """
+    if isinstance(model, str):
+        config = AutoConfig.from_pretrained(model, _from_pipeline=task, **model_kwargs)
+    else:
+        config = model.config
+    return infer_framework_load_model(
+        model,
+        config,
+        model_classes=model_classes,
+        _from_pipeline=task,
+        task=task,
+        framework=framework,
+        **model_kwargs,
+    )
 
 
-@contextmanager
-def torch_distributed_zero_first(local_rank: int):
+def get_framework(model, revision: Optional[str] = None):
     """
-    Decorator to make all processes in distributed training wait for each local_master to do something.
+    Select framework (TensorFlow or PyTorch) to use.
 
     Args:
-        local_rank (`int`): The rank of the local process.
-    """
-    if local_rank not in [-1, 0]:
-        dist.barrier()
-    yield
-    if local_rank == 0:
-        dist.barrier()
+        model (`str`, [`PreTrainedModel`] or [`TFPreTrainedModel`]):
+            If both frameworks are installed, picks the one corresponding to the model passed (either a model class or
+            the model name). If no specific model is provided, defaults to using PyTorch.
+    """
+    warnings.warn(
+        "`get_framework` is deprecated and will be removed in v5, use `infer_framework_from_model` instead.",
+        FutureWarning,
+    )
+    if not is_tf_available() and not is_torch_available():
+        raise RuntimeError(
+            "At least one of TensorFlow 2.0 or PyTorch should be installed. "
+            "To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ "
+            "To install PyTorch, read the instructions at https://pytorch.org/."
+        )
+    if isinstance(model, str):
+        if is_torch_available() and not is_tf_available():
+            model = AutoModel.from_pretrained(model, revision=revision)
+        elif is_tf_available() and not is_torch_available():
+            model = TFAutoModel.from_pretrained(model, revision=revision)
+        else:
+            try:
+                model = AutoModel.from_pretrained(model, revision=revision)
+            except OSError:
+                model = TFAutoModel.from_pretrained(model, revision=revision)
 
+    framework = "tf" if model.__class__.__name__.startswith("TF") else "pt"
+    return framework
 
-class DistributedSamplerWithLoop(DistributedSampler):
+
+def get_default_model_and_revision(
+    targeted_task: Dict, framework: Optional[str], task_options: Optional[Any]
+) -> Union[str, Tuple[str, str]]:
     """
-    Like a torch.utils.data.distributed.DistributedSampler` but loops at the end back to the beginning of the shuffled
-    samples to make each process have a round multiple of batch_size samples.
+    Select a default model to use for a given task. Defaults to pytorch if ambiguous.
 
     Args:
-        dataset (`torch.utils.data.Dataset`):
-            Dataset used for sampling.
-        batch_size (`int`):
-            The batch size used with this sampler
-        kwargs:
-            All other keyword arguments passed to `DistributedSampler`.
-    """
+        targeted_task (`Dict` ):
+           Dictionary representing the given task, that should contain default models
 
-    def __init__(self, dataset, batch_size, **kwargs):
-        super().__init__(dataset, **kwargs)
-        self.batch_size = batch_size
+        framework (`str`, None)
+           "pt", "tf" or None, representing a specific framework if it was specified, or None if we don't know yet.
 
-    def __iter__(self):
-        indices = list(super().__iter__())
-        remainder = (
-            0
-            if len(indices) % self.batch_size == 0
-            else self.batch_size - len(indices) % self.batch_size
+        task_options (`Any`, None)
+           Any further value required by the task to get fully specified, for instance (SRC, TGT) languages for
+           translation task.
+
+    Returns
+
+        `str` The model string representing the default model for this pipeline
+    """
+    if is_torch_available() and not is_tf_available():
+        framework = "pt"
+    elif is_tf_available() and not is_torch_available():
+        framework = "tf"
+
+    defaults = targeted_task["default"]
+    if task_options:
+        if task_options not in defaults:
+            raise ValueError(
+                f"The task does not provide any default models for options {task_options}"
+            )
+        default_models = defaults[task_options]["model"]
+    elif "model" in defaults:
+        default_models = targeted_task["default"]["model"]
+    else:
+        # XXX This error message needs to be updated to be more generic if more tasks are going to become
+        # parametrized
+        raise ValueError(
+            'The task defaults can\'t be correctly selected. You probably meant "translation_XX_to_YY"'
         )
-        # DistributedSampler already added samples from the beginning to make the number of samples a round multiple
-        # of the world size, so we skip those.
-        start_remainder = 1 if self.rank < len(self.dataset) % self.num_replicas else 0
-        indices += indices[start_remainder : start_remainder + remainder]
-        return iter(indices)
 
+    if framework is None:
+        framework = "pt"
 
-class SequentialDistributedSampler(Sampler):
-    """
-    Distributed Sampler that subsamples indices sequentially, making it easier to collate all results at the end.
+    return default_models[framework]
 
-    Even though we only use this sampler for eval and predict (no training), which means that the model params won't
-    have to be synced (i.e. will not hang for synchronization even if varied number of forward passes), we still add
-    extra samples to the sampler to make it evenly divisible (like in `DistributedSampler`) to make it easy to `gather`
-    or `reduce` resulting tensors at the end of the loop.
-    """
 
-    def __init__(self, dataset, num_replicas=None, rank=None, batch_size=None):
-        warnings.warn(
-            "SequentialDistributedSampler is deprecated and will be removed in v5 of Transformers.",
-            FutureWarning,
-        )
-        if num_replicas is None:
-            if not dist.is_available():
-                raise RuntimeError("Requires distributed package to be available")
-            num_replicas = dist.get_world_size()
-        if rank is None:
-            if not dist.is_available():
-                raise RuntimeError("Requires distributed package to be available")
-            rank = dist.get_rank()
-        self.dataset = dataset
-        self.num_replicas = num_replicas
-        self.rank = rank
-        num_samples = len(self.dataset)
-        # Add extra samples to make num_samples a multiple of batch_size if passed
-        if batch_size is not None:
-            self.num_samples = (
-                int(math.ceil(num_samples / (batch_size * num_replicas))) * batch_size
-            )
-        else:
-            self.num_samples = int(math.ceil(num_samples / num_replicas))
-        self.total_size = self.num_samples * self.num_replicas
-        self.batch_size = batch_size
+class PipelineException(Exception):
+    """
+    Raised by a [`Pipeline`] when handling __call__.
 
-    def __iter__(self):
-        indices = list(range(len(self.dataset)))
+    Args:
+        task (`str`): The task of the pipeline.
+        model (`str`): The model used by the pipeline.
+        reason (`str`): The error message to display.
+    """
 
-        # add extra samples to make it evenly divisible
-        indices += indices[: (self.total_size - len(indices))]
-        assert (
-            len(indices) == self.total_size
-        ), f"Indices length {len(indices)} and total size {self.total_size} mismatched"
-
-        # subsample
-        indices = indices[
-            self.rank * self.num_samples : (self.rank + 1) * self.num_samples
-        ]
-        assert (
-            len(indices) == self.num_samples
-        ), f"Indices length {len(indices)} and sample number {self.num_samples} mismatched"
+    def __init__(self, task: str, model: str, reason: str):
+        super().__init__(reason)
 
-        return iter(indices)
+        self.task = task
+        self.model = model
 
-    def __len__(self):
-        return self.num_samples
 
+class ArgumentHandler(ABC):
+    """
+    Base interface for handling arguments for each [`~pipelines.Pipeline`].
+    """
 
-def get_tpu_sampler(dataset: torch.utils.data.Dataset, batch_size: int):
-    if xm.xrt_world_size() <= 1:
-        return RandomSampler(dataset)
-    return DistributedSampler(
-        dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal()
-    )
+    @abstractmethod
+    def __call__(self, *args, **kwargs):
+        raise NotImplementedError()
 
 
-def nested_new_like(arrays, num_samples, padding_index=-100):
-    """Create the same nested structure as `arrays` with a first dimension always at `num_samples`."""
-    if isinstance(arrays, (list, tuple)):
-        return type(arrays)(nested_new_like(x, num_samples) for x in arrays)
-    return np.full_like(arrays, padding_index, shape=(num_samples, *arrays.shape[1:]))
+class PipelineDataFormat:
+    """
+    Base class for all the pipeline supported data format both for reading and writing. Supported data formats
+    currently includes:
 
+    - JSON
+    - CSV
+    - stdin/stdout (pipe)
 
-def expand_like(arrays, new_seq_length, padding_index=-100):
-    """Expand the `arrays` so that the second dimension grows to `new_seq_length`. Uses `padding_index` for padding."""
-    result = np.full_like(
-        arrays,
-        padding_index,
-        shape=(arrays.shape[0], new_seq_length) + arrays.shape[2:],
-    )
-    result[:, : arrays.shape[1]] = arrays
-    return result
+    `PipelineDataFormat` also includes some utilities to work with multi-columns like mapping from datasets columns to
+    pipelines keyword arguments through the `dataset_kwarg_1=dataset_column_1` format.
 
+    Args:
+        output_path (`str`, *optional*): Where to save the outgoing data.
+        input_path (`str`, *optional*): Where to look for the input data.
+        column (`str`, *optional*): The column to read.
+        overwrite (`bool`, *optional*, defaults to `False`):
+            Whether or not to overwrite the `output_path`.
+    """
 
-def nested_truncate(tensors, limit):
-    "Truncate `tensors` at `limit` (even if it's a nested list/tuple/dict of tensors)."
-    if isinstance(tensors, (list, tuple)):
-        return type(tensors)(nested_truncate(t, limit) for t in tensors)
-    if isinstance(tensors, Mapping):
-        return type(tensors)({k: nested_truncate(t, limit) for k, t in tensors.items()})
+    SUPPORTED_FORMATS = ["json", "csv", "pipe"]
 
-    return tensors[:limit]
+    def __init__(
+        self,
+        output_path: Optional[str],
+        input_path: Optional[str],
+        column: Optional[str],
+        overwrite: bool = False,
+    ):
+        self.output_path = output_path
+        self.input_path = input_path
+        self.column = column.split(",") if column is not None else [""]
+        self.is_multi_columns = len(self.column) > 1
+
+        if self.is_multi_columns:
+            self.column = [
+                tuple(c.split("=")) if "=" in c else (c, c) for c in self.column
+            ]
+
+        if output_path is not None and not overwrite:
+            if exists(abspath(self.output_path)):
+                raise OSError(f"{self.output_path} already exists on disk")
+
+        if input_path is not None:
+            if not exists(abspath(self.input_path)):
+                raise OSError(f"{self.input_path} doesnt exist on disk")
 
+    @abstractmethod
+    def __iter__(self):
+        raise NotImplementedError()
 
-class DistributedTensorGatherer:
-    """
-    A class responsible for properly gathering tensors (or nested list/tuple of tensors) on the CPU by chunks.
+    @abstractmethod
+    def save(self, data: Union[dict, List[dict]]):
+        """
+        Save the provided data object with the representation for the current [`~pipelines.PipelineDataFormat`].
 
-    If our dataset has 16 samples with a batch size of 2 on 3 processes and we gather then transfer on CPU at every
-    step, our sampler will generate the following indices:
+        Args:
+            data (`dict` or list of `dict`): The data to store.
+        """
+        raise NotImplementedError()
 
-        `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1]`
+    def save_binary(self, data: Union[dict, List[dict]]) -> str:
+        """
+        Save the provided data object as a pickle-formatted binary data on the disk.
 
-    to get something of size a multiple of 3 (so that each process gets the same dataset length). Then process 0, 1 and
-    2 will be responsible of making predictions for the following samples:
+        Args:
+            data (`dict` or list of `dict`): The data to store.
 
-        - P0: `[0, 1, 2, 3, 4, 5]`
-        - P1: `[6, 7, 8, 9, 10, 11]`
-        - P2: `[12, 13, 14, 15, 0, 1]`
+        Returns:
+            `str`: Path where the data has been saved.
+        """
+        path, _ = os.path.splitext(self.output_path)
+        binary_path = os.path.extsep.join((path, "pickle"))
 
-    The first batch treated on each process will be
+        with open(binary_path, "wb+") as f_output:
+            pickle.dump(data, f_output)
 
-        - P0: `[0, 1]`
-        - P1: `[6, 7]`
-        - P2: `[12, 13]`
+        return binary_path
 
-    So if we gather at the end of the first batch, we will get a tensor (nested list/tuple of tensor) corresponding to
-    the following indices:
+    @staticmethod
+    def from_str(
+        format: str,
+        output_path: Optional[str],
+        input_path: Optional[str],
+        column: Optional[str],
+        overwrite=False,
+    ) -> "PipelineDataFormat":
+        """
+        Creates an instance of the right subclass of [`~pipelines.PipelineDataFormat`] depending on `format`.
 
-        `[0, 1, 6, 7, 12, 13]`
+        Args:
+            format: (`str`):
+                The format of the desired pipeline. Acceptable values are `"json"`, `"csv"` or `"pipe"`.
+            output_path (`str`, *optional*):
+                Where to save the outgoing data.
+            input_path (`str`, *optional*):
+                Where to look for the input data.
+            column (`str`, *optional*):
+                The column to read.
+            overwrite (`bool`, *optional*, defaults to `False`):
+                Whether or not to overwrite the `output_path`.
 
-    If we directly concatenate our results without taking any precautions, the user will then get the predictions for
-    the indices in this order at the end of the prediction loop:
+        Returns:
+            [`~pipelines.PipelineDataFormat`]: The proper data format.
+        """
+        if format == "json":
+            return JsonPipelineDataFormat(
+                output_path, input_path, column, overwrite=overwrite
+            )
+        elif format == "csv":
+            return CsvPipelineDataFormat(
+                output_path, input_path, column, overwrite=overwrite
+            )
+        elif format == "pipe":
+            return PipedPipelineDataFormat(
+                output_path, input_path, column, overwrite=overwrite
+            )
+        else:
+            raise KeyError(
+                f"Unknown reader {format} (Available reader are json/csv/pipe)"
+            )
 
-        `[0, 1, 6, 7, 12, 13, 2, 3, 8, 9, 14, 15, 4, 5, 10, 11, 0, 1]`
 
-    For some reason, that's not going to roll their boat. This class is there to solve that problem.
+class CsvPipelineDataFormat(PipelineDataFormat):
+    """
+    Support for pipelines using CSV data format.
 
     Args:
-        world_size (`int`):
-            The number of processes used in the distributed training.
-        num_samples (`int`):
-            The number of samples in our dataset.
-        make_multiple_of (`int`, *optional*):
-            If passed, the class assumes the datasets passed to each process are made to be a multiple of this argument
-            (by adding samples).
-        padding_index (`int`, *optional*, defaults to -100):
-            The padding index to use if the arrays don't all have the same sequence length.
+        output_path (`str`, *optional*): Where to save the outgoing data.
+        input_path (`str`, *optional*): Where to look for the input data.
+        column (`str`, *optional*): The column to read.
+        overwrite (`bool`, *optional*, defaults to `False`):
+            Whether or not to overwrite the `output_path`.
     """
 
     def __init__(
-        self, world_size, num_samples, make_multiple_of=None, padding_index=-100
+        self,
+        output_path: Optional[str],
+        input_path: Optional[str],
+        column: Optional[str],
+        overwrite=False,
     ):
-        warnings.warn(
-            "DistributedTensorGatherer is deprecated and will be removed in v5 of Transformers.",
-            FutureWarning,
-        )
-        self.world_size = world_size
-        self.num_samples = num_samples
-        total_size = (
-            world_size if make_multiple_of is None else world_size * make_multiple_of
-        )
-        self.total_samples = int(np.ceil(num_samples / total_size)) * total_size
-        self.process_length = self.total_samples // world_size
-        self._storage = None
-        self._offsets = None
-        self.padding_index = padding_index
+        super().__init__(output_path, input_path, column, overwrite=overwrite)
 
-    def add_arrays(self, arrays):
-        """
-        Add `arrays` to the internal storage, Will initialize the storage to the full size at the first arrays passed
-        so that if we're bound to get an OOM, it happens at the beginning.
-        """
-        if arrays is None:
-            return
-        if self._storage is None:
-            self._storage = nested_new_like(
-                arrays, self.total_samples, padding_index=self.padding_index
-            )
-            self._offsets = list(range(0, self.total_samples, self.process_length))
-
-        slice_len, self._storage = self._nested_set_tensors(self._storage, arrays)
-        for i in range(self.world_size):
-            self._offsets[i] += slice_len
-
-    def _nested_set_tensors(self, storage, arrays):
-        if isinstance(arrays, (list, tuple)):
-            result = [self._nested_set_tensors(x, y) for x, y in zip(storage, arrays)]
-            return result[0][0], type(arrays)(r[1] for r in result)
-        assert (
-            arrays.shape[0] % self.world_size == 0
-        ), f"Arrays passed should all have a first dimension multiple of {self.world_size}, found {arrays.shape[0]}."
-
-        slice_len = arrays.shape[0] // self.world_size
-        for i in range(self.world_size):
-            if len(arrays.shape) == 1:
-                storage[self._offsets[i] : self._offsets[i] + slice_len] = arrays[
-                    i * slice_len : (i + 1) * slice_len
-                ]
-            else:
-                # Expand the array on the fly if needed.
-                if len(storage.shape) > 1 and storage.shape[1] < arrays.shape[1]:
-                    storage = expand_like(
-                        storage, arrays.shape[1], padding_index=self.padding_index
-                    )
-                storage[
-                    self._offsets[i] : self._offsets[i] + slice_len, : arrays.shape[1]
-                ] = arrays[i * slice_len : (i + 1) * slice_len]
-        return slice_len, storage
+    def __iter__(self):
+        with open(self.input_path, "r") as f:
+            reader = csv.DictReader(f)
+            for row in reader:
+                if self.is_multi_columns:
+                    yield {k: row[c] for k, c in self.column}
+                else:
+                    yield row[self.column[0]]
 
-    def finalize(self):
+    def save(self, data: List[dict]):
         """
-        Return the properly gathered arrays and truncate to the number of samples (since the sampler added some extras
-        to get each process a dataset of the same length).
+        Save the provided data object with the representation for the current [`~pipelines.PipelineDataFormat`].
+
+        Args:
+            data (`List[dict]`): The data to store.
         """
-        if self._storage is None:
-            return
-        if self._offsets[0] != self.process_length:
-            logger.warning(
-                "Not all data has been set. Are you sure you passed all values?"
-            )
-        return nested_truncate(self._storage, self.num_samples)
+        with open(self.output_path, "w") as f:
+            if len(data) > 0:
+                writer = csv.DictWriter(f, list(data[0].keys()))
+                writer.writeheader()
+                writer.writerows(data)
 
 
-@dataclass
-class LabelSmoother:
+class JsonPipelineDataFormat(PipelineDataFormat):
     """
-    Adds label-smoothing on a pre-computed output from a Transformers model.
+    Support for pipelines using JSON file format.
 
     Args:
-        epsilon (`float`, *optional*, defaults to 0.1):
-            The label smoothing factor.
-        ignore_index (`int`, *optional*, defaults to -100):
-            The index in the labels to ignore when computing the loss.
+        output_path (`str`, *optional*): Where to save the outgoing data.
+        input_path (`str`, *optional*): Where to look for the input data.
+        column (`str`, *optional*): The column to read.
+        overwrite (`bool`, *optional*, defaults to `False`):
+            Whether or not to overwrite the `output_path`.
     """
 
-    epsilon: float = 0.1
-    ignore_index: int = -100
-
-    def __call__(self, model_output, labels, shift_labels=False):
-        logits = (
-            model_output["logits"]
-            if isinstance(model_output, dict)
-            else model_output[0]
-        )
-        if shift_labels:
-            logits = logits[..., :-1, :].contiguous()
-            labels = labels[..., 1:].contiguous()
-
-        log_probs = -nn.functional.log_softmax(logits, dim=-1)
-        if labels.dim() == log_probs.dim() - 1:
-            labels = labels.unsqueeze(-1)
-
-        padding_mask = labels.eq(self.ignore_index)
-        # In case the ignore_index is -100, the gather will fail, so we replace labels by 0. The padding_mask
-        # will ignore them in any case.
-        labels = torch.clamp(labels, min=0)
-        nll_loss = log_probs.gather(dim=-1, index=labels)
-        # works for fp16 input tensor too, by internally upcasting it to fp32
-        smoothed_loss = log_probs.sum(dim=-1, keepdim=True, dtype=torch.float32)
-
-        nll_loss.masked_fill_(padding_mask, 0.0)
-        smoothed_loss.masked_fill_(padding_mask, 0.0)
-
-        # Take the mean over the label dimensions, then divide by the number of active elements (i.e. not-padded):
-        num_active_elements = padding_mask.numel() - padding_mask.long().sum()
-        nll_loss = nll_loss.sum() / num_active_elements
-        smoothed_loss = smoothed_loss.sum() / (
-            num_active_elements * log_probs.shape[-1]
-        )
-        return (1 - self.epsilon) * nll_loss + self.epsilon * smoothed_loss
+    def __init__(
+        self,
+        output_path: Optional[str],
+        input_path: Optional[str],
+        column: Optional[str],
+        overwrite=False,
+    ):
+        super().__init__(output_path, input_path, column, overwrite=overwrite)
 
+        with open(input_path, "r") as f:
+            self._entries = json.load(f)
 
-def get_length_grouped_indices(
-    lengths, batch_size, mega_batch_mult=None, generator=None
-):
-    """
-    Return a list of indices so that each slice of `batch_size` consecutive indices correspond to elements of similar
-    lengths. To do this, the indices are:
+    def __iter__(self):
+        for entry in self._entries:
+            if self.is_multi_columns:
+                yield {k: entry[c] for k, c in self.column}
+            else:
+                yield entry[self.column[0]]
 
-    - randomly permuted
-    - grouped in mega-batches of size `mega_batch_mult * batch_size`
-    - sorted by length in each mega-batch
-
-    The result is the concatenation of all mega-batches, with the batch of `batch_size` containing the element of
-    maximum length placed first, so that an OOM happens sooner rather than later.
-    """
-    # Default for mega_batch_mult: 50 or the number to get 4 megabatches, whichever is smaller.
-    if mega_batch_mult is None:
-        mega_batch_mult = min(len(lengths) // (batch_size * 4), 50)
-        # Just in case, for tiny datasets
-        if mega_batch_mult == 0:
-            mega_batch_mult = 1
-
-    # We need to use torch for the random part as a distributed sampler will set the random seed for torch.
-    indices = torch.randperm(len(lengths), generator=generator)
-    megabatch_size = mega_batch_mult * batch_size
-    megabatches = [
-        indices[i : i + megabatch_size].tolist()
-        for i in range(0, len(lengths), megabatch_size)
-    ]
-    megabatches = [
-        list(sorted(megabatch, key=lambda i: lengths[i], reverse=True))
-        for megabatch in megabatches
-    ]
-
-    # The rest is to get the biggest batch first.
-    # Since each megabatch is sorted by descending length, the longest element is the first
-    megabatch_maximums = [lengths[megabatch[0]] for megabatch in megabatches]
-    max_idx = torch.argmax(torch.tensor(megabatch_maximums)).item()
-    # Switch to put the longest element in first position
-    megabatches[0][0], megabatches[max_idx][0] = (
-        megabatches[max_idx][0],
-        megabatches[0][0],
-    )
+    def save(self, data: dict):
+        """
+        Save the provided data object in a json file.
 
-    return [i for megabatch in megabatches for i in megabatch]
+        Args:
+            data (`dict`): The data to store.
+        """
+        with open(self.output_path, "w") as f:
+            json.dump(data, f)
 
 
-class LengthGroupedSampler(Sampler):
-    r"""
-    Sampler that samples indices in a way that groups together features of the dataset of roughly the same length while
-    keeping a bit of randomness.
+class PipedPipelineDataFormat(PipelineDataFormat):
     """
+    Read data from piped input to the python process. For multi columns data, columns should separated by \t
 
-    def __init__(
-        self,
-        batch_size: int,
-        dataset: Optional[Dataset] = None,
-        lengths: Optional[List[int]] = None,
-        model_input_name: Optional[str] = None,
-        generator=None,
-    ):
-        if dataset is None and lengths is None:
-            raise ValueError("One of dataset and lengths must be provided.")
+    If columns are provided, then the output will be a dictionary with {column_x: value_x}
 
-        self.batch_size = batch_size
-        if lengths is None:
-            model_input_name = (
-                model_input_name if model_input_name is not None else "input_ids"
-            )
-            if (
-                not (
-                    isinstance(dataset[0], dict)
-                    or isinstance(dataset[0], BatchEncoding)
-                )
-                or model_input_name not in dataset[0]
-            ):
-                raise ValueError(
-                    "Can only automatically infer lengths for datasets whose items are dictionaries with an "
-                    f"'{model_input_name}' key."
-                )
-            lengths = [len(feature[model_input_name]) for feature in dataset]
-        elif isinstance(lengths, torch.Tensor):
-            logger.info(
-                "If lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]..."
-            )
-            lengths = lengths.tolist()
-
-        self.lengths = lengths
-        self.generator = generator
-
-    def __len__(self):
-        return len(self.lengths)
+    Args:
+        output_path (`str`, *optional*): Where to save the outgoing data.
+        input_path (`str`, *optional*): Where to look for the input data.
+        column (`str`, *optional*): The column to read.
+        overwrite (`bool`, *optional*, defaults to `False`):
+            Whether or not to overwrite the `output_path`.
+    """
 
     def __iter__(self):
-        indices = get_length_grouped_indices(
-            self.lengths, self.batch_size, generator=self.generator
-        )
-        return iter(indices)
+        for line in sys.stdin:
+            # Split for multi-columns
+            if "\t" in line:
+                line = line.split("\t")
+                if self.column:
+                    # Dictionary to map arguments
+                    yield {kwargs: l for (kwargs, _), l in zip(self.column, line)}
+                else:
+                    yield tuple(line)
 
+            # No dictionary to map arguments
+            else:
+                yield line
 
-class DistributedLengthGroupedSampler(DistributedSampler):
-    r"""
-    Distributed Sampler that samples indices in a way that groups together features of the dataset of roughly the same
-    length while keeping a bit of randomness.
-    """
+    def save(self, data: dict):
+        """
+        Print the data.
 
-    # Copied and adapted from PyTorch DistributedSampler.
-    def __init__(
-        self,
-        batch_size: int,
-        dataset: Optional[Dataset] = None,
-        num_replicas: Optional[int] = None,
-        rank: Optional[int] = None,
-        seed: int = 0,
-        drop_last: bool = False,
-        lengths: Optional[List[int]] = None,
-        model_input_name: Optional[str] = None,
-    ):
-        if dataset is None and lengths is None:
-            raise ValueError("One of dataset and lengths must be provided.")
-        if num_replicas is None:
-            if not dist.is_available():
-                raise RuntimeError("Requires distributed package to be available")
-            num_replicas = dist.get_world_size()
-        if rank is None:
-            if not dist.is_available():
-                raise RuntimeError("Requires distributed package to be available")
-            rank = dist.get_rank()
-
-        self.batch_size = batch_size
-        self.num_replicas = num_replicas
-        self.rank = rank
-        self.epoch = 0
-        self.drop_last = drop_last
-
-        if lengths is None:
-            model_input_name = (
-                model_input_name if model_input_name is not None else "input_ids"
-            )
-            if (
-                not (
-                    isinstance(dataset[0], dict)
-                    or isinstance(dataset[0], BatchEncoding)
-                )
-                or model_input_name not in dataset[0]
-            ):
-                raise ValueError(
-                    "Can only automatically infer lengths for datasets whose items are dictionaries with an "
-                    f"'{model_input_name}' key."
-                )
-            lengths = [len(feature[model_input_name]) for feature in dataset]
-        elif isinstance(lengths, torch.Tensor):
-            logger.info(
-                "If lengths is a torch.Tensor, DistributedLengthGroupedSampler will be slow. Converting lengths to"
-                " List[int]..."
+        Args:
+            data (`dict`): The data to store.
+        """
+        print(data)
+
+    def save_binary(self, data: Union[dict, List[dict]]) -> str:
+        if self.output_path is None:
+            raise KeyError(
+                "When using piped input on pipeline outputting large object requires an output file path. "
+                "Please provide such output path through --output argument."
             )
-            lengths = lengths.tolist()
 
-        self.lengths = lengths
+        return super().save_binary(data)
 
-        # If the dataset length is evenly divisible by # of replicas, then there
-        # is no need to drop any data, since the dataset will be split equally.
-        if self.drop_last and len(self.lengths) % self.num_replicas != 0:
-            # Split to nearest available length that is evenly divisible.
-            # This is to ensure each rank receives the same amount of data when
-            # using this Sampler.
-            self.num_samples = math.ceil(
-                (len(self.lengths) - self.num_replicas) / self.num_replicas
-            )
-        else:
-            self.num_samples = math.ceil(len(self.lengths) / self.num_replicas)
-        self.total_size = self.num_samples * self.num_replicas
-        self.seed = seed
-
-    def __iter__(self) -> Iterator:
-        # Deterministically shuffle based on epoch and seed
-        g = torch.Generator()
-        g.manual_seed(self.seed + self.epoch)
-        indices = get_length_grouped_indices(self.lengths, self.batch_size, generator=g)
-
-        if not self.drop_last:
-            # add extra samples to make it evenly divisible
-            indices += indices[: (self.total_size - len(indices))]
-        else:
-            # remove tail of data to make it evenly divisible.
-            indices = indices[: self.total_size]
-        assert len(indices) == self.total_size
 
-        # subsample
-        indices = indices[self.rank : self.total_size : self.num_replicas]
-        assert len(indices) == self.num_samples
+class _ScikitCompat(ABC):
+    """
+    Interface layer for the Scikit and Keras compatibility.
+    """
+
+    @abstractmethod
+    def transform(self, X):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def predict(self, X):
+        raise NotImplementedError()
+
+
+PIPELINE_INIT_ARGS = r"""
+    Arguments:
+        model ([`PreTrainedModel`] or [`TFPreTrainedModel`]):
+            The model that will be used by the pipeline to make predictions. This needs to be a model inheriting from
+            [`PreTrainedModel`] for PyTorch and [`TFPreTrainedModel`] for TensorFlow.
+        tokenizer ([`PreTrainedTokenizer`]):
+            The tokenizer that will be used by the pipeline to encode data for the model. This object inherits from
+            [`PreTrainedTokenizer`].
+        modelcard (`str` or [`ModelCard`], *optional*):
+            Model card attributed to the model for this pipeline.
+        framework (`str`, *optional*):
+            The framework to use, either `"pt"` for PyTorch or `"tf"` for TensorFlow. The specified framework must be
+            installed.
+
+            If no framework is specified, will default to the one currently installed. If no framework is specified and
+            both frameworks are installed, will default to the framework of the `model`, or to PyTorch if no model is
+            provided.
+        task (`str`, defaults to `""`):
+            A task-identifier for the pipeline.
+        num_workers (`int`, *optional*, defaults to 8):
+            When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the number of
+            workers to be used.
+        batch_size (`int`, *optional*, defaults to 1):
+            When the pipeline will use *DataLoader* (when passing a dataset, on GPU for a Pytorch model), the size of
+            the batch to use, for inference this is not always beneficial, please read [Batching with
+            pipelines](https://huggingface.co/transformers/main_classes/pipelines.html#pipeline-batching) .
+        args_parser ([`~pipelines.ArgumentHandler`], *optional*):
+            Reference to the object in charge of parsing supplied pipeline parameters.
+        device (`int`, *optional*, defaults to -1):
+            Device ordinal for CPU/GPU supports. Setting this to -1 will leverage CPU, a positive will run the model on
+            the associated CUDA device id. You can pass native `torch.device` or a `str` too.
+        binary_output (`bool`, *optional*, defaults to `False`):
+            Flag indicating if the output the pipeline should happen in a binary format (i.e., pickle) or as raw text.
+"""
 
-        return iter(indices)
+if is_torch_available():
+    from transformers.pipelines.pt_utils import (
+        PipelineChunkIterator,
+        PipelineDataset,
+        PipelineIterator,
+        PipelinePackIterator,
+    )
 
 
-class ShardSampler(Sampler):
+@add_end_docstrings(PIPELINE_INIT_ARGS)
+class Pipeline(_ScikitCompat):
     """
-    Sampler that shards batches between several processes. Dispatches indices batch by batch: on 2 processes with batch
-    size 4, the first two batches are `[0, 1, 2, 3, 4, 5, 6, 7]` and `[8, 9, 10, 11, 12, 13, 14, 15]`, which shard into
-    `[0, 1, 2, 3]` and `[8, 9, 10, 11]` for GPU-0 and `[4, 5, 6, 7]` and `[12, 13, 14, 15]` for GPU-1.
+    The Pipeline class is the class from which all pipelines inherit. Refer to this class for methods shared across
+    different pipelines.
+
+    Base class implementing pipelined operations. Pipeline workflow is defined as a sequence of the following
+    operations:
+
+        Input -> Tokenization -> Model Inference -> Post-Processing (task dependent) -> Output
 
-    The sampler thus yields `[0, 1, 2, 3, 8, 9, 10, 11]` on GPU-0 and `[4, 5, 6, 7, 12, 13, 14, 15]` on GPU-1.
+    Pipeline supports running on CPU or GPU through the device argument (see below).
+
+    Some pipeline, like for instance [`FeatureExtractionPipeline`] (`'feature-extraction'`) output large tensor object
+    as nested-lists. In order to avoid dumping such large structure as textual data we provide the `binary_output`
+    constructor argument. If set to `True`, the output will be stored in the pickle format.
     """
 
+    default_input_names = None
+
     def __init__(
         self,
-        dataset: Dataset,
-        batch_size: int = 1,
-        drop_last: bool = False,
-        num_processes: int = 1,
-        process_index: int = 0,
+        model: Union["PreTrainedModel", "TFPreTrainedModel"],
+        tokenizer: Optional[PreTrainedTokenizer] = None,
+        feature_extractor: Optional[PreTrainedFeatureExtractor] = None,
+        modelcard: Optional[ModelCard] = None,
+        framework: Optional[str] = None,
+        task: str = "",
+        args_parser: ArgumentHandler = None,
+        device: Union[int, str, "torch.device"] = -1,
+        binary_output: bool = False,
+        **kwargs,
     ):
-        self.dataset = dataset
-        self.batch_size = batch_size
-        self.drop_last = drop_last
-        self.num_processes = num_processes
-        self.process_index = process_index
-
-        self.total_batch_size = total_batch_size = batch_size * num_processes
-
-        num_batches = (
-            len(dataset) // total_batch_size
-            if drop_last
-            else math.ceil(len(dataset) / total_batch_size)
-        )
-        self.total_num_samples = num_batches * total_batch_size
+        if framework is None:
+            framework, model = infer_framework_load_model(model, config=model.config)
 
-    def __iter__(self):
-        indices = list(range(len(self.dataset)))
+        self.task = task
+        self.model = model
+        self.tokenizer = tokenizer
+        self.feature_extractor = feature_extractor
+        self.modelcard = modelcard
+        self.framework = framework
+        if is_torch_available() and self.framework == "pt":
+            if isinstance(device, torch.device):
+                self.device = device
+            elif isinstance(device, str):
+                self.device = torch.device(device)
+            elif device < 0:
+                self.device = torch.device("cpu")
+            else:
+                self.device = torch.device(f"cuda:{device}")
+        else:
+            self.device = device
+        self.binary_output = binary_output
 
-        # Add extra samples to make it evenly divisible. While loop is there in the edge case we have a tiny dataset
-        # and it needs to be done several times.
-        while len(indices) < self.total_num_samples:
-            indices += indices[: (self.total_num_samples - len(indices))]
-
-        result = []
-        for batch_start in range(
-            self.batch_size * self.process_index,
-            self.total_num_samples,
-            self.total_batch_size,
-        ):
-            result += indices[batch_start : batch_start + self.batch_size]
+        # Special handling
+        if self.framework == "pt" and self.device.type != "cpu":
+            self.model = self.model.to(self.device)
+
+        # Update config with task specific parameters
+        task_specific_params = self.model.config.task_specific_params
+        if task_specific_params is not None and task in task_specific_params:
+            self.model.config.update(task_specific_params.get(task))
+
+        self.call_count = 0
+        self._batch_size = kwargs.pop("batch_size", None)
+        self._num_workers = kwargs.pop("num_workers", None)
+        (
+            self._preprocess_params,
+            self._forward_params,
+            self._postprocess_params,
+        ) = self._sanitize_parameters(**kwargs)
 
-        return iter(result)
+    def save_pretrained(self, save_directory: str):
+        """
+        Save the pipeline's model and tokenizer.
 
-    def __len__(self):
-        # Each shard only sees a fraction of total_num_samples.
-        return self.total_num_samples // self.num_processes
+        Args:
+            save_directory (`str`):
+                A path to the directory where to saved. It will be created if it doesn't exist.
+        """
+        if os.path.isfile(save_directory):
+            logger.error(
+                f"Provided path ({save_directory}) should be a directory, not a file"
+            )
+            return
+        os.makedirs(save_directory, exist_ok=True)
 
+        if hasattr(self, "_registered_impl"):
+            # Add info to the config
+            pipeline_info = self._registered_impl.copy()
+            custom_pipelines = {}
+            for task, info in pipeline_info.items():
+                if info["impl"] != self.__class__:
+                    continue
+
+                info = info.copy()
+                module_name = info["impl"].__module__
+                last_module = module_name.split(".")[-1]
+                # Change classes into their names/full names
+                info["impl"] = f"{last_module}.{info['impl'].__name__}"
+                info["pt"] = tuple(c.__name__ for c in info["pt"])
+                info["tf"] = tuple(c.__name__ for c in info["tf"])
+
+                custom_pipelines[task] = info
+            self.model.config.custom_pipelines = custom_pipelines
+            # Save the pipeline custom code
+            custom_object_save(self, save_directory)
+
+        self.model.save_pretrained(save_directory)
+
+        if self.tokenizer is not None:
+            self.tokenizer.save_pretrained(save_directory)
 
-class IterableDatasetShard(IterableDataset):
-    """
-    Wraps a PyTorch `IterableDataset` to generate samples for one of the processes only. Instances of this class will
-    always yield a number of samples that is a round multiple of the actual batch size (which is `batch_size x
-    num_processes`). Depending on the value of the `drop_last` attribute, it will either stop the iteration at the
-    first batch that would be too small or loop with indices from the beginning.
+        if self.feature_extractor is not None:
+            self.feature_extractor.save_pretrained(save_directory)
 
-    On two processes with an iterable dataset yielding of `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]` with a batch size of
-    2:
+        if self.modelcard is not None:
+            self.modelcard.save_pretrained(save_directory)
 
-    - the shard on process 0 will yield `[0, 1, 4, 5, 8, 9]` so will see batches `[0, 1]`, `[4, 5]`, `[8, 9]`
-    - the shard on process 1 will yield `[2, 3, 6, 7, 10, 11]` so will see batches `[2, 3]`, `[6, 7]`, `[10, 11]`
+    def transform(self, X):
+        """
+        Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().
+        """
+        return self(X)
 
-    <Tip warning={true}>
+    def predict(self, X):
+        """
+        Scikit / Keras interface to transformers' pipelines. This method will forward to __call__().
+        """
+        return self(X)
 
-        If your IterableDataset implements some randomization that needs to be applied the same way on all processes
-        (for instance, a shuffling), you should use a `torch.Generator` in a `generator` attribute of the `dataset` to
-        generate your random numbers and call the [`~trainer_pt_utils.IterableDatasetShard.set_epoch`] method of this
-        object. It will set the seed of this `generator` to `seed + epoch` on all processes before starting the
-        iteration. Alternatively, you can also implement a `set_epoch()` method in your iterable dataset to deal with
-        this.
+    @contextmanager
+    def device_placement(self):
+        """
+        Context Manager allowing tensor allocation on the user-specified device in framework agnostic way.
 
-    </Tip>
+        Returns:
+            Context manager
 
-    Args:
-        dataset (`torch.utils.data.IterableDataset`):
-            The batch sampler to split in several shards.
-        batch_size (`int`, *optional*, defaults to 1):
-            The size of the batches per shard.
-        drop_last (`bool`, *optional*, defaults to `False`):
-            Whether or not to drop the last incomplete batch or complete the last batches by using the samples from the
-            beginning.
-        num_processes (`int`, *optional*, defaults to 1):
-            The number of processes running concurrently.
-        process_index (`int`, *optional*, defaults to 0):
-            The index of the current process.
-        seed (`int`, *optional*, defaults to 0):
-            A random seed that will be used for the random number generation in
-            [`~trainer_pt_utils.IterableDatasetShard.set_epoch`].
-    """
+        Examples:
 
-    def __init__(
-        self,
-        dataset: IterableDataset,
-        batch_size: int = 1,
-        drop_last: bool = False,
-        num_processes: int = 1,
-        process_index: int = 0,
-        seed: int = 0,
-    ):
-        self.dataset = dataset
-        self.batch_size = batch_size
-        self.drop_last = drop_last
-        self.num_processes = num_processes
-        self.process_index = process_index
-        self.seed = seed
-        self.epoch = 0
-        self.num_examples = 0
-
-    def set_epoch(self, epoch):
-        self.epoch = epoch
-        if hasattr(self.dataset, "set_epoch"):
-            self.dataset.set_epoch(epoch)
+        ```python
+        # Explicitly ask for tensor allocation on CUDA device :0
+        pipe = pipeline(..., device=0)
+        with pipe.device_placement():
+            # Every framework specific tensor allocation will be done on the request device
+            output = pipe(...)
+        ```"""
+        if self.framework == "tf":
+            with tf.device(
+                "/CPU:0" if self.device == -1 else f"/device:GPU:{self.device}"
+            ):
+                yield
+        else:
+            if self.device.type == "cuda":
+                torch.cuda.set_device(self.device)
 
-    def __iter__(self):
-        self.num_examples = 0
-        if (
-            not hasattr(self.dataset, "set_epoch")
-            and hasattr(self.dataset, "generator")
-            and isinstance(self.dataset.generator, torch.Generator)
-        ):
-            self.dataset.generator.manual_seed(self.seed + self.epoch)
-        real_batch_size = self.batch_size * self.num_processes
-        process_slice = range(
-            self.process_index * self.batch_size,
-            (self.process_index + 1) * self.batch_size,
-        )
+            yield
 
-        first_batch = None
-        current_batch = []
-        for element in self.dataset:
-            self.num_examples += 1
-            current_batch.append(element)
-            # Wait to have a full batch before yielding elements.
-            if len(current_batch) == real_batch_size:
-                for i in process_slice:
-                    yield current_batch[i]
-                if first_batch is None:
-                    first_batch = current_batch.copy()
-                current_batch = []
-
-        # Finished if drop_last is True, otherwise complete the last batch with elements from the beginning.
-        if not self.drop_last and len(current_batch) > 0:
-            if first_batch is None:
-                first_batch = current_batch.copy()
-            while len(current_batch) < real_batch_size:
-                current_batch += first_batch
-            for i in process_slice:
-                yield current_batch[i]
-
-    def __len__(self):
-        # Will raise an error if the underlying dataset is not sized.
-        if self.drop_last:
-            return (
-                len(self.dataset) // (self.batch_size * self.num_processes)
-            ) * self.batch_size
-        else:
-            return (
-                math.ceil(len(self.dataset) / (self.batch_size * self.num_processes))
-                * self.batch_size
-            )
+    def ensure_tensor_on_device(self, **inputs):
+        """
+        Ensure PyTorch tensors are on the specified device.
 
+        Args:
+            inputs (keyword arguments that should be `torch.Tensor`, the rest is ignored):
+                The tensors to place on `self.device`.
+            Recursive on lists **only**.
 
-# In order to keep `trainer.py` compact and easy to understand, place any secondary PT Trainer
-# helper methods here
+        Return:
+            `Dict[str, torch.Tensor]`: The same as `inputs` but on the proper device.
+        """
+        return self._ensure_tensor_on_device(inputs, self.device)
 
+    def _ensure_tensor_on_device(self, inputs, device):
+        if isinstance(inputs, ModelOutput):
+            return ModelOutput(
+                {
+                    name: self._ensure_tensor_on_device(tensor, device)
+                    for name, tensor in inputs.items()
+                }
+            )
+        elif isinstance(inputs, dict):
+            return {
+                name: self._ensure_tensor_on_device(tensor, device)
+                for name, tensor in inputs.items()
+            }
+        elif isinstance(inputs, UserDict):
+            return UserDict(
+                {
+                    name: self._ensure_tensor_on_device(tensor, device)
+                    for name, tensor in inputs.items()
+                }
+            )
+        elif isinstance(inputs, list):
+            return [self._ensure_tensor_on_device(item, device) for item in inputs]
+        elif isinstance(inputs, tuple):
+            return tuple(
+                [self._ensure_tensor_on_device(item, device) for item in inputs]
+            )
+        elif isinstance(inputs, torch.Tensor):
+            if device == torch.device("cpu") and inputs.dtype in {
+                torch.float16,
+                torch.bfloat16,
+            }:
+                inputs = inputs.float()
+            return inputs.to(device)
+        else:
+            return inputs
 
-def _get_learning_rate(self):
-    if self.deepspeed:
-        # with deepspeed's fp16 and dynamic loss scale enabled the optimizer/scheduler steps may
-        # not run for the first few dozen steps while loss scale is too large, and thus during
-        # that time `get_last_lr` will fail if called during that warm up stage, so work around it:
-        try:
-            last_lr = self.lr_scheduler.get_last_lr()[0]
-        except AssertionError as e:
-            if "need to call step" in str(e):
-                logger.warning(
-                    "tried to get lr value before scheduler/optimizer started stepping, returning lr=0"
-                )
-                last_lr = 0
-            else:
-                raise
-    else:
-        last_lr = self.lr_scheduler.get_last_lr()[0]
-        if torch.is_tensor(last_lr):
-            last_lr = last_lr.item()
-    return last_lr
+    def check_model_type(self, supported_models: Union[List[str], dict]):
+        """
+        Check if the model class is in supported by the pipeline.
 
+        Args:
+            supported_models (`List[str]` or `dict`):
+                The list of models supported by the pipeline, or a dictionary with model class values.
+        """
+        if not isinstance(supported_models, list):  # Create from a model mapping
+            supported_models_names = []
+            for config, model in supported_models.items():
+                # Mapping can now contain tuples of models for the same configuration.
+                if isinstance(model, tuple):
+                    supported_models_names.extend([_model.__name__ for _model in model])
+                else:
+                    supported_models_names.append(model.__name__)
+            supported_models = supported_models_names
+        if self.model.__class__.__name__ not in supported_models:
+            logger.error(
+                f"The model '{self.model.__class__.__name__}' is not supported for {self.task}. Supported models are"
+                f" {supported_models}."
+            )
 
-def _secs2timedelta(secs):
-    """
-    convert seconds to hh:mm:ss.msec, msecs rounded to 2 decimals
-    """
+    @abstractmethod
+    def _sanitize_parameters(self, **pipeline_parameters):
+        """
+        _sanitize_parameters will be called with any excessive named arguments from either `__init__` or `__call__`
+        methods. It should return 3 dictionnaries of the resolved parameters used by the various `preprocess`,
+        `forward` and `postprocess` methods. Do not fill dictionnaries if the caller didn't specify a kwargs. This
+        let's you keep defaults in function signatures, which is more "natural".
 
-    msec = int(abs(secs - int(secs)) * 100)
-    return f"{datetime.timedelta(seconds=int(secs))}.{msec:02d}"
+        It is not meant to be called directly, it will be automatically called and the final parameters resolved by
+        `__init__` and `__call__`
+        """
+        raise NotImplementedError("_sanitize_parameters not implemented")
 
+    @abstractmethod
+    def preprocess(
+        self, input_: Any, **preprocess_parameters: Dict
+    ) -> Dict[str, GenericTensor]:
+        """
+        Preprocess will take the `input_` of a specific pipeline and return a dictionnary of everything necessary for
+        `_forward` to run properly. It should contain at least one tensor, but might have arbitrary other items.
+        """
+        raise NotImplementedError("preprocess not implemented")
 
-def metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float]:
-    """
-    Reformat Trainer metrics values to a human-readable format
+    @abstractmethod
+    def _forward(
+        self, input_tensors: Dict[str, GenericTensor], **forward_parameters: Dict
+    ) -> ModelOutput:
+        """
+        _forward will receive the prepared dictionnary from `preprocess` and run it on the model. This method might
+        involve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`
+        and `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.
+
+        It is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional
+        code surrounding `_forward` making sure tensors and models are on the same device, disabling the training part
+        of the code (leading to faster inference).
+        """
+        raise NotImplementedError("_forward not implemented")
 
-    Args:
-        metrics (`Dict[str, float]`):
-            The metrics returned from train/evaluate/predict
+    @abstractmethod
+    def postprocess(
+        self, model_outputs: ModelOutput, **postprocess_parameters: Dict
+    ) -> Any:
+        """
+        Postprocess will receive the raw outputs of the `_forward` method, generally tensors, and reformat them into
+        something more friendly. Generally it will output a list or a dict or results (containing just strings and
+        numbers).
+        """
+        raise NotImplementedError("postprocess not implemented")
 
-    Returns:
-        metrics (`Dict[str, float]`): The reformatted metrics
-    """
+    def get_inference_context(self):
+        inference_context = (
+            torch.inference_mode
+            if version.parse(version.parse(torch.__version__).base_version)
+            >= version.parse("1.9.0")
+            else torch.no_grad
+        )
+        return inference_context
 
-    metrics_copy = metrics.copy()
-    for k, v in metrics_copy.items():
-        if "_mem_" in k:
-            metrics_copy[k] = f"{ v >> 20 }MB"
-        elif "_runtime" in k:
-            metrics_copy[k] = _secs2timedelta(v)
-        elif k == "total_flos":
-            metrics_copy[k] = f"{ int(v) >> 30 }GF"
-        elif type(metrics_copy[k]) == float:
-            metrics_copy[k] = round(v, 4)
+    def forward(self, model_inputs, **forward_params):
+        with self.device_placement():
+            if self.framework == "tf":
+                model_inputs["training"] = False
+                model_outputs = self._forward(model_inputs, **forward_params)
+            elif self.framework == "pt":
+                inference_context = self.get_inference_context()
+                with inference_context():
+                    model_inputs = self._ensure_tensor_on_device(
+                        model_inputs, device=self.device
+                    )
+                    model_outputs = self._forward(model_inputs, **forward_params)
+                    model_outputs = self._ensure_tensor_on_device(
+                        model_outputs, device=torch.device("cpu")
+                    )
+            else:
+                raise ValueError(f"Framework {self.framework} is not supported")
+        return model_outputs
 
-    return metrics_copy
+    def get_iterator(
+        self,
+        inputs,
+        num_workers: int,
+        batch_size: int,
+        preprocess_params,
+        forward_params,
+        postprocess_params,
+    ):
+        if isinstance(inputs, collections.abc.Sized):
+            dataset = PipelineDataset(inputs, self.preprocess, preprocess_params)
+        else:
+            if num_workers > 1:
+                logger.warning(
+                    "For iterable dataset using num_workers>1 is likely to result"
+                    " in errors since everything is iterable, setting `num_workers=1`"
+                    " to guarantee correctness."
+                )
+                num_workers = 1
+            dataset = PipelineIterator(inputs, self.preprocess, preprocess_params)
+        if "TOKENIZERS_PARALLELISM" not in os.environ:
+            logger.info(
+                "Disabling tokenizer parallelism, we're using DataLoader multithreading already"
+            )
+            os.environ["TOKENIZERS_PARALLELISM"] = "false"
+        collate_fn = (
+            no_collate_fn
+            if batch_size == 1
+            else pad_collate_fn(self.tokenizer, self.feature_extractor)
+        )
+        dataloader = DataLoader(
+            dataset,
+            num_workers=num_workers,
+            batch_size=batch_size,
+            collate_fn=collate_fn,
+        )
+        model_iterator = PipelineIterator(
+            dataloader, self.forward, forward_params, loader_batch_size=batch_size
+        )
+        final_iterator = PipelineIterator(
+            model_iterator, self.postprocess, postprocess_params
+        )
+        return final_iterator
 
+    def __call__(self, inputs, *args, num_workers=None, batch_size=None, **kwargs):
+        if args:
+            logger.warning(f"Ignoring args : {args}")
+
+        if num_workers is None:
+            if self._num_workers is None:
+                num_workers = 0
+            else:
+                num_workers = self._num_workers
+        if batch_size is None:
+            if self._batch_size is None:
+                batch_size = 1
+            else:
+                batch_size = self._batch_size
 
-def log_metrics(self, split, metrics):
-    """
-    Log metrics in a specially formatted way
+        (
+            preprocess_params,
+            forward_params,
+            postprocess_params,
+        ) = self._sanitize_parameters(**kwargs)
+
+        # Fuse __init__ params and __call__ params without modifying the __init__ ones.
+        preprocess_params = {**self._preprocess_params, **preprocess_params}
+        forward_params = {**self._forward_params, **forward_params}
+        postprocess_params = {**self._postprocess_params, **postprocess_params}
 
-    Under distributed environment this is done only for a process with rank 0.
+        self.call_count += 1
+        if (
+            self.call_count > 10
+            and self.framework == "pt"
+            and self.device.type == "cuda"
+        ):
+            warnings.warn(
+                "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a"
+                " dataset",
+                UserWarning,
+            )
 
-    Args:
-        split (`str`):
-            Mode/split name: one of `train`, `eval`, `test`
-        metrics (`Dict[str, float]`):
-            The metrics returned from train/evaluate/predictmetrics: metrics dict
-
-    Notes on memory reports:
-
-    In order to get memory usage report you need to install `psutil`. You can do that with `pip install psutil`.
-
-    Now when this method is run, you will see a report that will include: :
-
-    ```
-    init_mem_cpu_alloc_delta   =     1301MB
-    init_mem_cpu_peaked_delta  =      154MB
-    init_mem_gpu_alloc_delta   =      230MB
-    init_mem_gpu_peaked_delta  =        0MB
-    train_mem_cpu_alloc_delta  =     1345MB
-    train_mem_cpu_peaked_delta =        0MB
-    train_mem_gpu_alloc_delta  =      693MB
-    train_mem_gpu_peaked_delta =        7MB
-    ```
-
-    **Understanding the reports:**
-
-    - the first segment, e.g., `train__`, tells you which stage the metrics are for. Reports starting with `init_`
-        will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the
-        `__init__` will be reported along with the `eval_` metrics.
-    - the third segment, is either `cpu` or `gpu`, tells you whether it's the general RAM or the gpu0 memory
-        metric.
-    - `*_alloc_delta` - is the difference in the used/allocated memory counter between the end and the start of the
-        stage - it can be negative if a function released more memory than it allocated.
-    - `*_peaked_delta` - is any extra memory that was consumed and then freed - relative to the current allocated
-        memory counter - it is never negative. When you look at the metrics of any stage you add up `alloc_delta` +
-        `peaked_delta` and you know how much memory was needed to complete that stage.
-
-    The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the
-    main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may
-    use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more
-    memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the
-    future these reports will evolve to measure those too.
-
-    The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the
-    memory shared with other processes. It is important to note that it does not include swapped out memory, so the
-    reports could be imprecise.
-
-    The CPU peak memory is measured using a sampling thread. Due to python's GIL it may miss some of the peak memory if
-    that thread didn't get a chance to run when the highest memory was used. Therefore this report can be less than
-    reality. Using `tracemalloc` would have reported the exact peak memory, but it doesn't report memory allocations
-    outside of python. So if some C++ CUDA extension allocated its own memory it won't be reported. And therefore it
-    was dropped in favor of the memory sampling approach, which reads the current process memory usage.
-
-    The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()` and
-    `torch.cuda.max_memory_allocated()`. This metric reports only "deltas" for pytorch-specific allocations, as
-    `torch.cuda` memory management system doesn't track any memory allocated outside of pytorch. For example, the very
-    first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.
-
-    Note that this tracker doesn't account for memory allocations outside of [`Trainer`]'s `__init__`, `train`,
-    `evaluate` and `predict` calls.
-
-    Because `evaluation` calls may happen during `train`, we can't handle nested invocations because
-    `torch.cuda.max_memory_allocated` is a single counter, so if it gets reset by a nested eval call, `train`'s tracker
-    will report incorrect info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266) gets resolved
-    it will be possible to change this class to be re-entrant. Until then we will only track the outer level of
-    `train`, `evaluate` and `predict` methods. Which means that if `eval` is called during `train`, it's the latter
-    that will account for its memory usage and that of the former.
-
-    This also means that if any other tool that is used along the [`Trainer`] calls
-    `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be invalid. And the [`Trainer`] will disrupt
-    the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats` themselves.
-
-    For best performance you may want to consider turning the memory profiling off for production runs.
-    """
-    if not self.is_world_process_zero():
-        return
-
-    print(f"***** {split} metrics *****")
-    metrics_formatted = self.metrics_format(metrics)
-    k_width = max(len(str(x)) for x in metrics_formatted.keys())
-    v_width = max(len(str(x)) for x in metrics_formatted.values())
-    for key in sorted(metrics_formatted.keys()):
-        print(f"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}")
+        is_dataset = Dataset is not None and isinstance(inputs, Dataset)
+        is_generator = isinstance(inputs, types.GeneratorType)
+        is_list = isinstance(inputs, list)
+
+        is_iterable = is_dataset or is_generator or is_list
+
+        # TODO make the get_iterator work also for `tf` (and `flax`).
+        can_use_iterator = self.framework == "pt" and (
+            is_dataset or is_generator or is_list
+        )
+
+        if is_list:
+            if can_use_iterator:
+                final_iterator = self.get_iterator(
+                    inputs,
+                    num_workers,
+                    batch_size,
+                    preprocess_params,
+                    forward_params,
+                    postprocess_params,
+                )
+                outputs = [output for output in final_iterator]
+                return outputs
+            else:
+                return self.run_multi(
+                    inputs, preprocess_params, forward_params, postprocess_params
+                )
+        elif can_use_iterator:
+            return self.get_iterator(
+                inputs,
+                num_workers,
+                batch_size,
+                preprocess_params,
+                forward_params,
+                postprocess_params,
+            )
+        elif is_iterable:
+            return self.iterate(
+                inputs, preprocess_params, forward_params, postprocess_params
+            )
+        else:
+            return self.run_single(
+                inputs, preprocess_params, forward_params, postprocess_params
+            )
 
+    def run_multi(self, inputs, preprocess_params, forward_params, postprocess_params):
+        return [
+            self.run_single(item, preprocess_params, forward_params, postprocess_params)
+            for item in inputs
+        ]
 
-def save_metrics(self, split, metrics, combined=True):
-    """
-    Save metrics into a json file for that split, e.g. `train_results.json`.
+    def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
+        model_inputs = self.preprocess(inputs, **preprocess_params)
+        model_outputs = self.forward(model_inputs, **forward_params)
+        outputs = self.postprocess(model_outputs, **postprocess_params)
+        return outputs
+
+    def iterate(self, inputs, preprocess_params, forward_params, postprocess_params):
+        # This function should become `get_iterator` again, this is a temporary
+        # easy solution.
+        for input_ in inputs:
+            yield self.run_single(
+                input_, preprocess_params, forward_params, postprocess_params
+            )
 
-    Under distributed environment this is done only for a process with rank 0.
 
-    Args:
-        split (`str`):
-            Mode/split name: one of `train`, `eval`, `test`, `all`
-        metrics (`Dict[str, float]`):
-            The metrics returned from train/evaluate/predict
-        combined (`bool`, *optional*, defaults to `True`):
-            Creates combined metrics by updating `all_results.json` with metrics of this call
-
-    To understand the metrics please read the docstring of [`~Trainer.log_metrics`]. The only difference is that raw
-    unformatted numbers are saved in the current method.
-
-    """
-    if not self.is_world_process_zero():
-        return
-
-    path = os.path.join(self.args.output_dir, f"{split}_results.json")
-    with open(path, "w") as f:
-        json.dump(metrics, f, indent=4, sort_keys=True)
-
-    if combined:
-        path = os.path.join(self.args.output_dir, "all_results.json")
-        if os.path.exists(path):
-            with open(path, "r") as f:
-                all_metrics = json.load(f)
-        else:
-            all_metrics = {}
+class ChunkPipeline(Pipeline):
+    def run_single(self, inputs, preprocess_params, forward_params, postprocess_params):
+        all_outputs = []
+        for model_inputs in self.preprocess(inputs, **preprocess_params):
+            model_outputs = self.forward(model_inputs, **forward_params)
+            all_outputs.append(model_outputs)
+        outputs = self.postprocess(all_outputs, **postprocess_params)
+        return outputs
 
-        all_metrics.update(metrics)
-        with open(path, "w") as f:
-            json.dump(all_metrics, f, indent=4, sort_keys=True)
+    def get_iterator(
+        self,
+        inputs,
+        num_workers: int,
+        batch_size: int,
+        preprocess_params,
+        forward_params,
+        postprocess_params,
+    ):
+        if "TOKENIZERS_PARALLELISM" not in os.environ:
+            logger.info(
+                "Disabling tokenizer parallelism, we're using DataLoader multithreading already"
+            )
+            os.environ["TOKENIZERS_PARALLELISM"] = "false"
+        if num_workers > 1:
+            logger.warning(
+                "For ChunkPipeline using num_workers>0 is likely to result in errors since everything is iterable,"
+                " setting `num_workers=1` to guarantee correctness."
+            )
+            num_workers = 1
+        dataset = PipelineChunkIterator(inputs, self.preprocess, preprocess_params)
+        collate_fn = (
+            no_collate_fn
+            if batch_size == 1
+            else pad_collate_fn(self.tokenizer, self.feature_extractor)
+        )
+        dataloader = DataLoader(
+            dataset,
+            num_workers=num_workers,
+            batch_size=batch_size,
+            collate_fn=collate_fn,
+        )
+        model_iterator = PipelinePackIterator(
+            dataloader, self.forward, forward_params, loader_batch_size=batch_size
+        )
+        final_iterator = PipelineIterator(
+            model_iterator, self.postprocess, postprocess_params
+        )
+        return final_iterator
 
 
-def save_state(self):
-    """
-    Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model
+class PipelineRegistry:
+    def __init__(
+        self, supported_tasks: Dict[str, Any], task_aliases: Dict[str, str]
+    ) -> None:
+        self.supported_tasks = supported_tasks
+        self.task_aliases = task_aliases
+
+    def get_supported_tasks(self) -> List[str]:
+        supported_task = list(self.supported_tasks.keys()) + list(
+            self.task_aliases.keys()
+        )
+        supported_task.sort()
+        return supported_task
 
-    Under distributed environment this is done only for a process with rank 0.
-    """
-    if not self.is_world_process_zero():
-        return
+    def check_task(self, task: str) -> Tuple[str, Dict, Any]:
+        if task in self.task_aliases:
+            task = self.task_aliases[task]
+        if task in self.supported_tasks:
+            targeted_task = self.supported_tasks[task]
+            return task, targeted_task, None
+
+        if task.startswith("translation"):
+            tokens = task.split("_")
+            if len(tokens) == 4 and tokens[0] == "translation" and tokens[2] == "to":
+                targeted_task = self.supported_tasks["translation"]
+                task = "translation"
+                return task, targeted_task, (tokens[1], tokens[3])
+            raise KeyError(
+                f"Invalid translation task {task}, use 'translation_XX_to_YY' format"
+            )
 
-    path = os.path.join(self.args.output_dir, "trainer_state.json")
-    self.state.save_to_json(path)
+        raise KeyError(
+            f"Unknown task {task}, available tasks are {self.get_supported_tasks() + ['translation_XX_to_YY']}"
+        )
 
+    def register_pipeline(
+        self,
+        task: str,
+        pipeline_class: type,
+        pt_model: Optional[Union[type, Tuple[type]]] = None,
+        tf_model: Optional[Union[type, Tuple[type]]] = None,
+        default: Optional[Dict] = None,
+        type: Optional[str] = None,
+    ) -> None:
+        if task in self.supported_tasks:
+            logger.warning(
+                f"{task} is already registered. Overwriting pipeline for task {task}..."
+            )
 
-def get_parameter_names(model, forbidden_layer_types):
-    """
-    Returns the names of the model parameters that are not inside a forbidden layer.
-    """
-    result = []
-    for name, child in model.named_children():
-        result += [
-            f"{name}.{n}"
-            for n in get_parameter_names(child, forbidden_layer_types)
-            if not isinstance(child, tuple(forbidden_layer_types))
-        ]
-    # Add model specific parameters (defined with nn.Parameter) since they are not in any child.
-    result += list(model._parameters.keys())
-    return result
+        if pt_model is None:
+            pt_model = ()
+        elif not isinstance(pt_model, tuple):
+            pt_model = (pt_model,)
+
+        if tf_model is None:
+            tf_model = ()
+        elif not isinstance(tf_model, tuple):
+            tf_model = (tf_model,)
+
+        task_impl = {"impl": pipeline_class, "pt": pt_model, "tf": tf_model}
+
+        if default is not None:
+            if "model" not in default and ("pt" in default or "tf" in default):
+                default = {"model": default}
+            task_impl["default"] = default
 
+        if type is not None:
+            task_impl["type"] = type
 
-def get_module_class_from_name(module, name):
-    """
-    Gets a class from a module by its name.
+        self.supported_tasks[task] = task_impl
+        pipeline_class._registered_impl = {task: task_impl}
 
-    Args:
-        module (`torch.nn.Module`): The module to get the class from.
-        name (`str`): The name of the class.
-    """
-    modules_children = list(module.children())
-    if module.__class__.__name__ == name:
-        return module.__class__
-    elif len(modules_children) == 0:
-        return
-    else:
-        for child_module in modules_children:
-            module_class = get_module_class_from_name(child_module, name)
-            if module_class is not None:
-                return module_class
-
-
-if is_sagemaker_mp_enabled():
-    import smdistributed.modelparallel.torch as smp
-
-    @smp.step()
-    def smp_forward_backward(model, inputs, gradient_accumulation_steps=1):
-        outputs = model(**inputs)
-        loss = outputs["loss"] if isinstance(outputs, dict) else outputs[0]
-        loss /= gradient_accumulation_steps
-        model.backward(loss)
-        return loss
-
-    @smp.step()
-    def smp_forward_only(model, inputs):
-        return model(**inputs)
-
-    def smp_gather(tensor):
-        if isinstance(tensor, (list, tuple)):
-            return type(tensor)(smp_gather(t) for t in tensor)
-        elif isinstance(tensor, dict):
-            return type(tensor)({k: smp_gather(v) for k, v in tensor.items()})
-        elif not isinstance(tensor, torch.Tensor):
-            raise TypeError(
-                f"Can't gather the values of type {type(tensor)}, only of nested list/tuple/dicts of tensors."
-            )
-        all_tensors = smp.allgather(tensor, smp.CommGroup.DP_GROUP)
-        all_tensors = [atleast_1d(t) for t in all_tensors]
-        return torch.cat([t.cpu() for t in all_tensors], dim=0)
-
-    def smp_nested_concat(tensor):
-        if isinstance(tensor, (list, tuple)):
-            return type(tensor)(smp_nested_concat(t) for t in tensor)
-        elif isinstance(tensor, dict):
-            return type(tensor)({k: smp_nested_concat(v) for k, v in tensor.items()})
-        # It doesn't seem possible to check here if `tensor` is a StepOutput because StepOutput lives in `smp.step`
-        # which is also the name of the decorator so Python is confused.
-        return tensor.concat().detach().cpu()
+    def to_dict(self):
+        return self.supported_tasks
```

### Comparing `xs_transformers-1.0.0/xs_transformers/trainer_seq2seq.py` & `xs_transformers-1.0.1/xs_transformers/pipelines/zero_shot_object_detection.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,303 +1,326 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
-#
-# Licensed under the Apache License, Version 2.0 (the "License");
-# you may not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-from typing import Any, Dict, List, Optional, Tuple, Union
-
-import torch
-from torch import nn
-from torch.utils.data import Dataset
-
-from .deepspeed import is_deepspeed_zero3_enabled
-from .trainer import Trainer
-from .trainer_utils import PredictionOutput
-from .utils import logging
+from typing import Dict, List, Union
+
+import numpy as np
+
+from ..tokenization_utils_base import BatchEncoding
+from ..utils import (
+    add_end_docstrings,
+    is_tf_available,
+    is_torch_available,
+    is_vision_available,
+    logging,
+    requires_backends,
+)
+from .base import PIPELINE_INIT_ARGS, Pipeline
+
+if is_vision_available():
+    from PIL import Image
+
+    from ..image_utils import load_image
+
+if is_torch_available():
+    import torch
+
+    from ..models.auto.modeling_auto import MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING
 
 logger = logging.get_logger(__name__)
 
 
-class Seq2SeqTrainer(Trainer):
-    def evaluate(
+@add_end_docstrings(PIPELINE_INIT_ARGS)
+class ZeroShotObjectDetectionPipeline(Pipeline):
+    """
+    Zero shot object detection pipeline using `OwlViTForObjectDetection`. This pipeline predicts bounding boxes of
+    objects when you provide an image and a set of `candidate_labels`.
+
+    This object detection pipeline can currently be loaded from [`pipeline`] using the following task identifier:
+    `"zero-shot-object-detection"`.
+
+    See the list of available models on
+    [huggingface.co/models](https://huggingface.co/models?filter=zero-shot-object-detection).
+    """
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+        if self.framework == "tf":
+            raise ValueError(f"The {self.__class__} is only available in PyTorch.")
+
+        requires_backends(self, "vision")
+        self.check_model_type(MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING)
+
+    def __call__(
         self,
-        eval_dataset: Optional[Dataset] = None,
-        ignore_keys: Optional[List[str]] = None,
-        metric_key_prefix: str = "eval",
-        **gen_kwargs
-    ) -> Dict[str, float]:
+        images: Union[str, List[str], "Image.Image", List["Image.Image"]],
+        text_queries: Union[str, List[str], List[List[str]]] = None,
+        **kwargs,
+    ):
         """
-        Run evaluation and returns metrics.
+        Detect objects (bounding boxes & classes) in the image(s) passed as inputs.
 
-        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
-        (pass it to the init `compute_metrics` argument).
+        Args:
+            images (`str`, `List[str]`, `PIL.Image` or `List[PIL.Image]`):
+                The pipeline handles three types of images:
 
-        You can also subclass and override this method to inject custom behavior.
+                - A string containing an http url pointing to an image
+                - A string containing a local path to an image
+                - An image loaded in PIL directly
+
+            text_queries (`str` or `List[str]` or `List[List[str]]`): Text queries to query the target image with.
+            If given multiple images, `text_queries` should be provided as a list of lists, where each nested list
+            contains the text queries for the corresponding image.
+
+            threshold (`float`, *optional*, defaults to 0.1):
+                The probability necessary to make a prediction.
+
+            top_k (`int`, *optional*, defaults to None):
+                The number of top predictions that will be returned by the pipeline. If the provided number is `None`
+                or higher than the number of predictions available, it will default to the number of predictions.
 
-        Args:
-            eval_dataset (`Dataset`, *optional*):
-                Pass a dataset if you wish to override `self.eval_dataset`. If it is an [`~datasets.Dataset`], columns
-                not accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`
-                method.
-            ignore_keys (`List[str]`, *optional*):
-                A list of keys in the output of your model (if it is a dictionary) that should be ignored when
-                gathering predictions.
-            metric_key_prefix (`str`, *optional*, defaults to `"eval"`):
-                An optional prefix to be used as the metrics key prefix. For example the metrics "bleu" will be named
-                "eval_bleu" if the prefix is `"eval"` (default)
-            max_length (`int`, *optional*):
-                The maximum target length to use when predicting with the generate method.
-            num_beams (`int`, *optional*):
-                Number of beams for beam search that will be used when predicting with the generate method. 1 means no
-                beam search.
-            gen_kwargs:
-                Additional `generate` specific kwargs.
 
-        Returns:
-            A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The
-            dictionary also contains the epoch number which comes from the training state.
+        Return:
+            A list of lists containing prediction results, one list per input image. Each list contains dictionaries
+            with the following keys:
+
+            - **label** (`str`) -- Text query corresponding to the found object.
+            - **score** (`float`) -- Score corresponding to the object (between 0 and 1).
+            - **box** (`Dict[str,int]`) -- Bounding box of the detected object in image's original size. It is a
+              dictionary with `x_min`, `x_max`, `y_min`, `y_max` keys.
         """
+        if isinstance(text_queries, str) or (
+            isinstance(text_queries, List) and not isinstance(text_queries[0], List)
+        ):
+            if isinstance(images, (str, Image.Image)):
+                inputs = {"images": images, "text_queries": text_queries}
+            elif isinstance(images, List):
+                assert (
+                    len(images) == 1
+                ), "Input text_queries and images must have correspondance"
+                inputs = {"images": images[0], "text_queries": text_queries}
+            else:
+                raise TypeError(f"Innapropriate type of images: {type(images)}")
 
-        gen_kwargs = gen_kwargs.copy()
-        if (
-            gen_kwargs.get("max_length") is None
-            and gen_kwargs.get("max_new_tokens") is None
+        elif isinstance(text_queries, str) or (
+            isinstance(text_queries, List) and isinstance(text_queries[0], List)
         ):
-            gen_kwargs["max_length"] = self.args.generation_max_length
-        gen_kwargs["num_beams"] = (
-            gen_kwargs["num_beams"]
-            if gen_kwargs.get("num_beams") is not None
-            else self.args.generation_num_beams
+            if isinstance(images, (Image.Image, str)):
+                images = [images]
+            assert len(images) == len(
+                text_queries
+            ), "Input text_queries and images must have correspondance"
+            inputs = {"images": images, "text_queries": text_queries}
+        else:
+            """
+            Supports the following format
+            - {"images": images, "text_queries": text_queries}
+            """
+            inputs = images
+        results = super().__call__(inputs, **kwargs)
+        return results
+
+    def _sanitize_parameters(self, **kwargs):
+        postprocess_params = {}
+        if "threshold" in kwargs:
+            postprocess_params["threshold"] = kwargs["threshold"]
+        if "top_k" in kwargs:
+            postprocess_params["top_k"] = kwargs["top_k"]
+        return {}, {}, postprocess_params
+
+    def preprocess(self, inputs):
+        if not isinstance(inputs["images"], List):
+            inputs["images"] = [inputs["images"]]
+        images = [load_image(img) for img in inputs["images"]]
+        text_queries = inputs["text_queries"]
+        if isinstance(text_queries, str) or isinstance(text_queries[0], str):
+            text_queries = [text_queries]
+
+        target_sizes = [torch.IntTensor([[img.height, img.width]]) for img in images]
+        target_sizes = torch.cat(target_sizes)
+        inputs = self._processor(
+            text=inputs["text_queries"], images=images, return_tensors="pt"
         )
-        self._gen_kwargs = gen_kwargs
+        return {"target_sizes": target_sizes, "text_queries": text_queries, **inputs}
+
+    def _forward(self, model_inputs):
+        target_sizes = model_inputs.pop("target_sizes")
+        text_queries = model_inputs.pop("text_queries")
+        outputs = self.model(**model_inputs)
 
-        return super().evaluate(
-            eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix
+        model_outputs = outputs.__class__(
+            {"target_sizes": target_sizes, "text_queries": text_queries, **outputs}
         )
+        return model_outputs
 
-    def predict(
-        self,
-        test_dataset: Dataset,
-        ignore_keys: Optional[List[str]] = None,
-        metric_key_prefix: str = "test",
-        **gen_kwargs
-    ) -> PredictionOutput:
-        """
-        Run prediction and returns predictions and potential metrics.
+    def postprocess(self, model_outputs, threshold=0.1, top_k=None):
+        texts = model_outputs["text_queries"]
 
-        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
-        will also return metrics, like in `evaluate()`.
+        outputs = self.feature_extractor.post_process(
+            outputs=model_outputs, target_sizes=model_outputs["target_sizes"]
+        )
 
-        Args:
-            test_dataset (`Dataset`):
-                Dataset to run the predictions on. If it is a [`~datasets.Dataset`], columns not accepted by the
-                `model.forward()` method are automatically removed. Has to implement the method `__len__`
-            ignore_keys (`List[str]`, *optional*):
-                A list of keys in the output of your model (if it is a dictionary) that should be ignored when
-                gathering predictions.
-            metric_key_prefix (`str`, *optional*, defaults to `"eval"`):
-                An optional prefix to be used as the metrics key prefix. For example the metrics "bleu" will be named
-                "eval_bleu" if the prefix is `"eval"` (default)
-            max_length (`int`, *optional*):
-                The maximum target length to use when predicting with the generate method.
-            num_beams (`int`, *optional*):
-                Number of beams for beam search that will be used when predicting with the generate method. 1 means no
-                beam search.
-            gen_kwargs:
-                Additional `generate` specific kwargs.
-
-        <Tip>
-
-        If your predictions or labels have different sequence lengths (for instance because you're doing dynamic
-        padding in a token classification task) the predictions will be padded (on the right) to allow for
-        concatenation into one array. The padding index is -100.
-
-        </Tip>
-
-        Returns: *NamedTuple* A namedtuple with the following keys:
-
-            - predictions (`np.ndarray`): The predictions on `test_dataset`.
-            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).
-            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained
-              labels).
+        results = []
+        for i in range(len(outputs)):
+            keep = outputs[i]["scores"] >= threshold
+            labels = outputs[i]["labels"][keep].tolist()
+            scores = outputs[i]["scores"][keep].tolist()
+            boxes = [self._get_bounding_box(box) for box in outputs[i]["boxes"][keep]]
+
+            result = [
+                {"score": score, "label": texts[i][label], "box": box}
+                for score, label, box in zip(scores, labels, boxes)
+            ]
+
+            result = sorted(result, key=lambda x: x["score"], reverse=True)
+            if top_k:
+                result = result[:top_k]
+            results.append(result)
+
+        return results
+
+    def _get_bounding_box(self, box: "torch.Tensor") -> Dict[str, int]:
         """
+        Turns list [xmin, xmax, ymin, ymax] into dict { "xmin": xmin, ... }
 
-        gen_kwargs = gen_kwargs.copy()
-        if (
-            gen_kwargs.get("max_length") is None
-            and gen_kwargs.get("max_new_tokens") is None
-        ):
-            gen_kwargs["max_length"] = self.args.generation_max_length
-        gen_kwargs["num_beams"] = (
-            gen_kwargs["num_beams"]
-            if gen_kwargs.get("num_beams") is not None
-            else self.args.generation_num_beams
-        )
-        self._gen_kwargs = gen_kwargs
+        Args:
+            box (`torch.Tensor`): Tensor containing the coordinates in corners format.
 
-        return super().predict(
-            test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix
-        )
+        Returns:
+            bbox (`Dict[str, int]`): Dict containing the coordinates in corners format.
+        """
+        if self.framework != "pt":
+            raise ValueError(
+                "The ZeroShotObjectDetectionPipeline is only available in PyTorch."
+            )
+        xmin, ymin, xmax, ymax = box.int().tolist()
+        bbox = {
+            "xmin": xmin,
+            "ymin": ymin,
+            "xmax": xmax,
+            "ymax": ymax,
+        }
+        return bbox
 
-    def prediction_step(
+    # Replication of OwlViTProcessor __call__ method, since pipelines don't auto infer processor's yet!
+    def _processor(
         self,
-        model: nn.Module,
-        inputs: Dict[str, Union[torch.Tensor, Any]],
-        prediction_loss_only: bool,
-        ignore_keys: Optional[List[str]] = None,
-    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:
+        text=None,
+        images=None,
+        padding="max_length",
+        return_tensors="np",
+        **kwargs,
+    ):
         """
-        Perform an evaluation step on `model` using `inputs`.
-
-        Subclass and override to inject custom behavior.
+        Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and
+        `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:
+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to
+        CLIPFeatureExtractor's [`~CLIPFeatureExtractor.__call__`] if `images` is not `None`. Please refer to the
+        doctsring of the above two methods for more information.
 
         Args:
-            model (`nn.Module`):
-                The model to evaluate.
-            inputs (`Dict[str, Union[torch.Tensor, Any]]`):
-                The inputs and targets of the model.
-
-                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
-                argument `labels`. Check your model's documentation for all accepted arguments.
-            prediction_loss_only (`bool`):
-                Whether or not to return the loss only.
-
-        Return:
-            Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss, logits and
-            labels (each being optional).
+            text (`str`, `List[str]`, `List[List[str]]`):
+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).
+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`,
+            `List[torch.Tensor]`):
+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
+                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
+                number of channels, H and W are image height and width.
+            return_tensors (`str` or [`~utils.TensorType`], *optional*):
+                If set, will return tensors of a particular framework. Acceptable values are:
+                - `'tf'`: Return TensorFlow `tf.constant` objects.
+                - `'pt'`: Return PyTorch `torch.Tensor` objects.
+                - `'np'`: Return NumPy `np.ndarray` objects.
+                - `'jax'`: Return JAX `jnp.ndarray` objects.
+        Returns:
+            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:
+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.
+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when
+              `return_attention_mask=True` or if *"attention_mask"* is in `self.model_input_names` and if `text` is not
+              `None`).
+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.
         """
 
-        if not self.args.predict_with_generate or prediction_loss_only:
-            return super().prediction_step(
-                model,
-                inputs,
-                prediction_loss_only=prediction_loss_only,
-                ignore_keys=ignore_keys,
+        if text is None and images is None:
+            raise ValueError(
+                "You have to specify at least one text or image. Both cannot be none."
             )
 
-        has_labels = "labels" in inputs
-        inputs = self._prepare_inputs(inputs)
-
-        # XXX: adapt synced_gpus for fairscale as well
-        gen_kwargs = self._gen_kwargs.copy()
-        if (
-            gen_kwargs.get("max_length") is None
-            and gen_kwargs.get("max_new_tokens") is None
-        ):
-            gen_kwargs["max_length"] = self.model.config.max_length
-        gen_kwargs["num_beams"] = (
-            gen_kwargs["num_beams"]
-            if gen_kwargs.get("num_beams") is not None
-            else self.model.config.num_beams
-        )
-        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False
-        gen_kwargs["synced_gpus"] = (
-            gen_kwargs["synced_gpus"]
-            if gen_kwargs.get("synced_gpus") is not None
-            else default_synced_gpus
-        )
+        if text is not None:
+            if isinstance(text, str) or (
+                isinstance(text, List) and not isinstance(text[0], List)
+            ):
+                encodings = [
+                    self.tokenizer(
+                        text, padding=padding, return_tensors=return_tensors, **kwargs
+                    )
+                ]
 
-        if "attention_mask" in inputs:
-            gen_kwargs["attention_mask"] = inputs.get("attention_mask", None)
-        if "global_attention_mask" in inputs:
-            gen_kwargs["global_attention_mask"] = inputs.get(
-                "global_attention_mask", None
-            )
+            elif isinstance(text, List) and isinstance(text[0], List):
+                encodings = []
 
-        # prepare generation inputs
-        # some encoder-decoder models can have varying encoder's and thus
-        # varying model input names
-        if (
-            hasattr(self.model, "encoder")
-            and self.model.encoder.main_input_name != self.model.main_input_name
-        ):
-            generation_inputs = inputs[self.model.encoder.main_input_name]
-        else:
-            generation_inputs = inputs[self.model.main_input_name]
+                # Maximum number of queries across batch
+                max_num_queries = max([len(t) for t in text])
 
-        generated_tokens = self.model.generate(
-            generation_inputs,
-            **gen_kwargs,
-        )
-        # in case the batch is shorter than max length, the output should be padded
-        if (
-            gen_kwargs.get("max_length") is not None
-            and generated_tokens.shape[-1] < gen_kwargs["max_length"]
-        ):
-            generated_tokens = self._pad_tensors_to_max_len(
-                generated_tokens, gen_kwargs["max_length"]
-            )
-        elif gen_kwargs.get("max_new_tokens") is not None and generated_tokens.shape[
-            -1
-        ] < (gen_kwargs["max_new_tokens"] + 1):
-            generated_tokens = self._pad_tensors_to_max_len(
-                generated_tokens, gen_kwargs["max_new_tokens"] + 1
-            )
+                # Pad all batch samples to max number of text queries
+                for t in text:
+                    if len(t) != max_num_queries:
+                        t = t + [" "] * (max_num_queries - len(t))
 
-        with torch.no_grad():
-            if has_labels:
-                with self.compute_loss_context_manager():
-                    outputs = model(**inputs)
-                if self.label_smoother is not None:
-                    loss = (
-                        self.label_smoother(outputs, inputs["labels"]).mean().detach()
-                    )
-                else:
-                    loss = (
-                        (outputs["loss"] if isinstance(outputs, dict) else outputs[0])
-                        .mean()
-                        .detach()
+                    encoding = self.tokenizer(
+                        t, padding=padding, return_tensors=return_tensors, **kwargs
                     )
+                    encodings.append(encoding)
             else:
-                loss = None
+                raise TypeError(
+                    "Input text should be a string, a list of strings or a nested list of strings"
+                )
 
-        if self.args.prediction_loss_only:
-            return (loss, None, None)
+            if return_tensors == "np":
+                input_ids = np.concatenate(
+                    [encoding["input_ids"] for encoding in encodings], axis=0
+                )
+                attention_mask = np.concatenate(
+                    [encoding["attention_mask"] for encoding in encodings], axis=0
+                )
 
-        if has_labels:
-            labels = inputs["labels"]
-            if (
-                gen_kwargs.get("max_length") is not None
-                and labels.shape[-1] < gen_kwargs["max_length"]
-            ):
-                labels = self._pad_tensors_to_max_len(labels, gen_kwargs["max_length"])
-            elif gen_kwargs.get("max_new_tokens") is not None and labels.shape[-1] < (
-                gen_kwargs["max_new_tokens"] + 1
-            ):
-                labels = self._pad_tensors_to_max_len(
-                    labels, (gen_kwargs["max_new_tokens"] + 1)
+            elif return_tensors == "pt" and is_torch_available():
+                import torch
+
+                input_ids = torch.cat(
+                    [encoding["input_ids"] for encoding in encodings], dim=0
+                )
+                attention_mask = torch.cat(
+                    [encoding["attention_mask"] for encoding in encodings], dim=0
                 )
-        else:
-            labels = None
 
-        return (loss, generated_tokens, labels)
+            elif return_tensors == "tf" and is_tf_available():
+                import tensorflow as tf
 
-    def _pad_tensors_to_max_len(self, tensor, max_length):
-        if self.tokenizer is not None and hasattr(self.tokenizer, "pad_token_id"):
-            # If PAD token is not defined at least EOS token has to be defined
-            pad_token_id = (
-                self.tokenizer.pad_token_id
-                if self.tokenizer.pad_token_id is not None
-                else self.tokenizer.eos_token_id
-            )
-        else:
-            if self.model.config.pad_token_id is not None:
-                pad_token_id = self.model.config.pad_token_id
-            else:
-                raise ValueError(
-                    "Pad_token_id must be set in the configuration of the model, in order to pad tensors"
+                input_ids = tf.stack(
+                    [encoding["input_ids"] for encoding in encodings], axis=0
+                )
+                attention_mask = tf.stack(
+                    [encoding["attention_mask"] for encoding in encodings], axis=0
                 )
 
-        padded_tensor = pad_token_id * torch.ones(
-            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device
-        )
-        padded_tensor[:, : tensor.shape[-1]] = tensor
-        return padded_tensor
+            else:
+                raise ValueError("Target return tensor type could not be returned")
+
+            encoding = BatchEncoding()
+            encoding["input_ids"] = input_ids
+            encoding["attention_mask"] = attention_mask
+
+        if images is not None:
+            image_features = self.feature_extractor(
+                images, return_tensors=return_tensors, **kwargs
+            )
+
+        if text is not None and images is not None:
+            encoding["pixel_values"] = image_features.pixel_values
+            return encoding
+        elif text is not None:
+            return encoding
+        else:
+            return BatchEncoding(
+                data=dict(**image_features), tensor_type=return_tensors
+            )
```

### Comparing `xs_transformers-1.0.0/xs_transformers/trainer_tf.py` & `xs_transformers-1.0.1/xs_transformers/models/vit/modeling_tf_vit.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,914 +1,935 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# coding=utf-8
+# Copyright 2021 Google AI, Ross Wightman, The HuggingFace Inc. team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""Tensorflow trainer class."""
+""" TF 2.0 ViT model."""
 
-import datetime
+
+import collections.abc
 import math
-import os
-import warnings
-from typing import Callable, Dict, Optional, Tuple
-
-from .utils import ENV_VARS_TRUE_VALUES
-
-# Integrations must be imported before ML frameworks:
-from .integrations import (  # isort: split
-    is_comet_available,
-    is_wandb_available,
-)
+from typing import Dict, Optional, Tuple, Union
 
 import numpy as np
 import tensorflow as tf
-from tensorflow.python.distribute.values import PerReplica
 
-from .modeling_tf_utils import TFPreTrainedModel
-from .optimization_tf import GradientAccumulator, create_optimizer
-from .trainer_utils import (
-    PREFIX_CHECKPOINT_DIR,
-    EvalPrediction,
-    IntervalStrategy,
-    PredictionOutput,
-    enable_full_determinism,
-    set_seed,
+from ...activations_tf import get_tf_activation
+from ...modeling_tf_outputs import (
+    TFBaseModelOutput,
+    TFBaseModelOutputWithPooling,
+    TFSequenceClassifierOutput,
+)
+from ...modeling_tf_utils import (
+    TFModelInputType,
+    TFPreTrainedModel,
+    TFSequenceClassificationLoss,
+    get_initializer,
+    keras_serializable,
+    unpack_inputs,
 )
-from .training_args_tf import TFTrainingArguments
-from .utils import logging
+from ...tf_utils import shape_list, stable_softmax
+from ...utils import (
+    add_code_sample_docstrings,
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
+    logging,
+)
+from .configuration_vit import ViTConfig
 
-if is_wandb_available():
-    import wandb
+logger = logging.get_logger(__name__)
 
-if is_comet_available():
-    import comet_ml
+# General docstring
+_CONFIG_FOR_DOC = "ViTConfig"
+_FEAT_EXTRACTOR_FOR_DOC = "ViTFeatureExtractor"
 
-logger = logging.get_logger(__name__)
+# Base docstring
+_CHECKPOINT_FOR_DOC = "google/vit-base-patch16-224-in21k"
+_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]
+
+# Image classification docstring
+_IMAGE_CLASS_CHECKPOINT = "google/vit-base-patch16-224"
+_IMAGE_CLASS_EXPECTED_OUTPUT = "Egyptian cat"
 
 
-class TFTrainer:
+class TFViTEmbeddings(tf.keras.layers.Layer):
     """
-    TFTrainer is a simple but feature-complete training and eval loop for TensorFlow, optimized for 🤗 Transformers.
+    Construct the CLS token, position and patch embeddings.
 
-    Args:
-        model ([`TFPreTrainedModel`]):
-            The model to train, evaluate or use for predictions.
-        args ([`TFTrainingArguments`]):
-            The arguments to tweak training.
-        train_dataset ([`~tf.data.Dataset`], *optional*):
-            The dataset to use for training. The dataset should yield tuples of `(features, labels)` where `features`
-            is a dict of input features and `labels` is the labels. If `labels` is a tensor, the loss is calculated by
-            the model by calling `model(features, labels=labels)`. If `labels` is a dict, such as when using a
-            QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
-            `model(features, **labels)`.
-        eval_dataset ([`~tf.data.Dataset`], *optional*):
-            The dataset to use for evaluation. The dataset should yield tuples of `(features, labels)` where `features`
-            is a dict of input features and `labels` is the labels. If `labels` is a tensor, the loss is calculated by
-            the model by calling `model(features, labels=labels)`. If `labels` is a dict, such as when using a
-            QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
-            `model(features, **labels)`.
-        compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):
-            The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return
-            a dictionary string to metric values.
-        tb_writer (`tf.summary.SummaryWriter`, *optional*):
-            Object to write to TensorBoard.
-        optimizers (`Tuple[tf.keras.optimizers.Optimizer, tf.keras.optimizers.schedules.LearningRateSchedule]`, *optional*):
-            A tuple containing the optimizer and the scheduler to use. The optimizer default to an instance of
-            [`tf.keras.optimizers.Adam`] if `args.weight_decay_rate` is 0 else an instance of [`AdamWeightDecay`]. The
-            scheduler will default to an instance of [`tf.keras.optimizers.schedules.PolynomialDecay`] if
-            `args.num_warmup_steps` is 0 else an instance of [`WarmUp`].
     """
 
-    def __init__(
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
+
+        self.patch_embeddings = TFViTPatchEmbeddings(config, name="patch_embeddings")
+        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)
+        self.config = config
+
+    def build(self, input_shape: tf.TensorShape):
+        num_patches = self.patch_embeddings.num_patches
+        self.cls_token = self.add_weight(
+            shape=(1, 1, self.config.hidden_size),
+            initializer=get_initializer(self.config.initializer_range),
+            trainable=True,
+            name="cls_token",
+        )
+        self.position_embeddings = self.add_weight(
+            shape=(1, num_patches + 1, self.config.hidden_size),
+            initializer=get_initializer(self.config.initializer_range),
+            trainable=True,
+            name="position_embeddings",
+        )
+
+        super().build(input_shape)
+
+    def interpolate_pos_encoding(self, embeddings, height, width) -> tf.Tensor:
+        """
+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher
+        resolution images.
+
+        Source:
+        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174
+        """
+
+        batch_size, seq_len, dim = shape_list(embeddings)
+        num_patches = seq_len - 1
+
+        _, num_positions, _ = shape_list(self.position_embeddings)
+        num_positions -= 1
+
+        if num_patches == num_positions and height == width:
+            return self.position_embeddings
+        class_pos_embed = self.position_embeddings[:, :1]
+        patch_pos_embed = self.position_embeddings[:, 1:]
+        h0 = height // self.config.patch_size
+        w0 = width // self.config.patch_size
+        patch_pos_embed = tf.image.resize(
+            images=tf.reshape(
+                patch_pos_embed,
+                shape=(
+                    1,
+                    int(math.sqrt(num_positions)),
+                    int(math.sqrt(num_positions)),
+                    dim,
+                ),
+            ),
+            size=(h0, w0),
+            method="bicubic",
+        )
+
+        shape = shape_list(patch_pos_embed)
+        assert h0 == shape[-3] and w0 == shape[-2]
+        patch_pos_embed = tf.reshape(tensor=patch_pos_embed, shape=(1, -1, dim))
+        return tf.concat(values=(class_pos_embed, patch_pos_embed), axis=1)
+
+    def call(
         self,
-        model: TFPreTrainedModel,
-        args: TFTrainingArguments,
-        train_dataset: Optional[tf.data.Dataset] = None,
-        eval_dataset: Optional[tf.data.Dataset] = None,
-        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
-        tb_writer: Optional[tf.summary.SummaryWriter] = None,
-        optimizers: Tuple[
-            tf.keras.optimizers.Optimizer,
-            tf.keras.optimizers.schedules.LearningRateSchedule,
-        ] = (
-            None,
-            None,
-        ),
-    ):
-        self.model = model
-        self.args = args
-        self.train_dataset = train_dataset
-        self.eval_dataset = eval_dataset
-        self.compute_metrics = compute_metrics
-        self.optimizer, self.lr_scheduler = optimizers
-        self.gradient_accumulator = GradientAccumulator()
-        self.global_step = 0
-        self.epoch_logging = 0
-        self.eval_loss = tf.keras.metrics.Sum()
-
-        warnings.warn(
-            "The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. "
-            "We recommend using native Keras instead, by calling methods like `fit()` and `predict()` "
-            "directly on the model object. Detailed examples of the Keras style can be found in our "
-            "examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow",
-            FutureWarning,
+        pixel_values: tf.Tensor,
+        interpolate_pos_encoding: bool = False,
+        training: bool = False,
+    ) -> tf.Tensor:
+        batch_size, num_channels, height, width = shape_list(pixel_values)
+        embeddings = self.patch_embeddings(
+            pixel_values,
+            interpolate_pos_encoding=interpolate_pos_encoding,
+            training=training,
         )
 
-        if tb_writer is not None:
-            self.tb_writer = tb_writer
+        # add the [CLS] token to the embedded patch tokens
+        cls_tokens = tf.repeat(self.cls_token, repeats=batch_size, axis=0)
+        embeddings = tf.concat((cls_tokens, embeddings), axis=1)
+
+        # add positional encoding to each token
+        if interpolate_pos_encoding:
+            embeddings = embeddings + self.interpolate_pos_encoding(
+                embeddings, height, width
+            )
         else:
-            self.tb_writer = tf.summary.create_file_writer(self.args.logging_dir)
+            embeddings = embeddings + self.position_embeddings
 
-        if is_wandb_available():
-            self.setup_wandb()
-        elif os.getenv("WANDB_DISABLED", "").upper() not in ENV_VARS_TRUE_VALUES:
-            logger.info(
-                "You are instantiating a Trainer but W&B is not installed. To use wandb logging, "
-                "run `pip install wandb && wandb login` see https://docs.wandb.com/huggingface."
-            )
+        embeddings = self.dropout(embeddings, training=training)
 
-        if is_comet_available():
-            self.setup_comet()
-        elif os.environ.get("COMET_MODE") != "DISABLED":
-            logger.info(
-                "To use comet_ml logging, run `pip/conda install comet_ml` "
-                "see https://www.comet.ml/docs/python-sdk/huggingface/"
-            )
+        return embeddings
 
-        enable_full_determinism(
-            self.args.seed
-        ) if self.args.full_determinism else set_seed(self.args.seed)
 
-    def get_train_tfdataset(self) -> tf.data.Dataset:
-        """
-        Returns the training [`~tf.data.Dataset`].
-
-        Subclass and override this method if you want to inject some custom behavior.
-        """
-        if self.train_dataset is None:
-            raise ValueError("Trainer: training requires a train_dataset.")
+# Based on timm implementation, which can be found here:
+# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
+class TFViTPatchEmbeddings(tf.keras.layers.Layer):
+    """
+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial
+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a
+    Transformer.
+    """
 
-        self.total_train_batch_size = (
-            self.args.train_batch_size * self.args.gradient_accumulation_steps
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
+        image_size, patch_size = config.image_size, config.patch_size
+        num_channels, hidden_size = config.num_channels, config.hidden_size
+
+        image_size = (
+            image_size
+            if isinstance(image_size, collections.abc.Iterable)
+            else (image_size, image_size)
+        )
+        patch_size = (
+            patch_size
+            if isinstance(patch_size, collections.abc.Iterable)
+            else (patch_size, patch_size)
+        )
+        num_patches = (image_size[1] // patch_size[1]) * (
+            image_size[0] // patch_size[0]
+        )
+        self.image_size = image_size
+        self.patch_size = patch_size
+        self.num_patches = num_patches
+        self.num_channels = num_channels
+        self.config = config
+
+        self.projection = tf.keras.layers.Conv2D(
+            filters=hidden_size,
+            kernel_size=patch_size,
+            strides=patch_size,
+            padding="valid",
+            data_format="channels_last",
+            use_bias=True,
+            kernel_initializer=get_initializer(self.config.initializer_range),
+            bias_initializer="zeros",
+            name="projection",
         )
-        self.num_train_examples = self.train_dataset.cardinality().numpy()
 
-        if self.num_train_examples < 0:
-            raise ValueError("The training dataset must have an asserted cardinality")
-
-        ds = (
-            self.train_dataset.repeat()
-            .shuffle(self.num_train_examples, seed=self.args.seed)
-            .batch(
-                self.total_train_batch_size,
-                drop_remainder=self.args.dataloader_drop_last,
-            )
-            .prefetch(tf.data.experimental.AUTOTUNE)
-        )
+    def call(
+        self,
+        pixel_values: tf.Tensor,
+        interpolate_pos_encoding: bool = False,
+        training: bool = False,
+    ) -> tf.Tensor:
+        batch_size, num_channels, height, width = shape_list(pixel_values)
+        if tf.executing_eagerly() and num_channels != self.num_channels:
+            raise ValueError(
+                "Make sure that the channel dimension of the pixel values match with the one set in the configuration."
+            )
+        if not interpolate_pos_encoding:
+            if tf.executing_eagerly():
+                if height != self.image_size[0] or width != self.image_size[1]:
+                    raise ValueError(
+                        f"Input image size ({height}*{width}) doesn't match model"
+                        f" ({self.image_size[0]}*{self.image_size[1]})."
+                    )
 
-        return self.args.strategy.experimental_distribute_dataset(ds)
+        # When running on CPU, `tf.keras.layers.Conv2D` doesn't support `NCHW` format.
+        # So change the input format from `NCHW` to `NHWC`.
+        # shape = (batch_size, in_height, in_width, in_channels=num_channels)
+        pixel_values = tf.transpose(pixel_values, perm=(0, 2, 3, 1))
 
-    def get_eval_tfdataset(
-        self, eval_dataset: Optional[tf.data.Dataset] = None
-    ) -> tf.data.Dataset:
-        """
-        Returns the evaluation [`~tf.data.Dataset`].
+        projection = self.projection(pixel_values)
 
-        Args:
-            eval_dataset ([`~tf.data.Dataset`], *optional*):
-                If provided, will override *self.eval_dataset*. The dataset should yield tuples of `(features, labels)`
-                where `features` is a dict of input features and `labels` is the labels. If `labels` is a tensor, the
-                loss is calculated by the model by calling `model(features, labels=labels)`. If `labels` is a dict,
-                such as when using a QuestionAnswering head model with multiple targets, the loss is instead calculated
-                by calling `model(features, **labels)`.
+        # Change the 2D spatial dimensions to a single temporal dimension.
+        # shape = (batch_size, num_patches, out_channels=embed_dim)
+        num_patches = (width // self.patch_size[1]) * (height // self.patch_size[0])
+        embeddings = tf.reshape(tensor=projection, shape=(batch_size, num_patches, -1))
 
-        Subclass and override this method if you want to inject some custom behavior.
-        """
-        if eval_dataset is None and self.eval_dataset is None:
-            raise ValueError("Trainer: evaluation requires an eval_dataset.")
+        return embeddings
 
-        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
-        num_examples = eval_dataset.cardinality().numpy()
 
-        if num_examples < 0:
-            raise ValueError("The training dataset must have an asserted cardinality")
+class TFViTSelfAttention(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
 
-        approx = math.floor if self.args.dataloader_drop_last else math.ceil
-        steps = approx(num_examples / self.args.eval_batch_size)
-        ds = (
-            eval_dataset.repeat()
-            .batch(
-                self.args.eval_batch_size, drop_remainder=self.args.dataloader_drop_last
+        if config.hidden_size % config.num_attention_heads != 0:
+            raise ValueError(
+                f"The hidden size ({config.hidden_size}) is not a multiple of the number "
+                f"of attention heads ({config.num_attention_heads})"
             )
-            .prefetch(tf.data.experimental.AUTOTUNE)
+
+        self.num_attention_heads = config.num_attention_heads
+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
+        self.all_head_size = self.num_attention_heads * self.attention_head_size
+        self.sqrt_att_head_size = math.sqrt(self.attention_head_size)
+
+        self.query = tf.keras.layers.Dense(
+            units=self.all_head_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="query",
+        )
+        self.key = tf.keras.layers.Dense(
+            units=self.all_head_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="key",
+        )
+        self.value = tf.keras.layers.Dense(
+            units=self.all_head_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="value",
         )
+        self.dropout = tf.keras.layers.Dropout(rate=config.attention_probs_dropout_prob)
 
-        return (
-            self.args.strategy.experimental_distribute_dataset(ds),
-            steps,
-            num_examples,
+    def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:
+        # Reshape from [batch_size, seq_length, all_head_size] to [batch_size, seq_length, num_attention_heads, attention_head_size]
+        tensor = tf.reshape(
+            tensor=tensor,
+            shape=(batch_size, -1, self.num_attention_heads, self.attention_head_size),
         )
 
-    def get_test_tfdataset(self, test_dataset: tf.data.Dataset) -> tf.data.Dataset:
-        """
-        Returns a test [`~tf.data.Dataset`].
+        # Transpose the tensor from [batch_size, seq_length, num_attention_heads, attention_head_size] to [batch_size, num_attention_heads, seq_length, attention_head_size]
+        return tf.transpose(tensor, perm=[0, 2, 1, 3])
 
-        Args:
-            test_dataset ([`~tf.data.Dataset`]):
-                The dataset to use. The dataset should yield tuples of `(features, labels)` where `features` is a dict
-                of input features and `labels` is the labels. If `labels` is a tensor, the loss is calculated by the
-                model by calling `model(features, labels=labels)`. If `labels` is a dict, such as when using a
-                QuestionAnswering head model with multiple targets, the loss is instead calculated by calling
-                `model(features, **labels)`.
+    def call(
+        self,
+        hidden_states: tf.Tensor,
+        head_mask: tf.Tensor,
+        output_attentions: bool,
+        training: bool = False,
+    ) -> Tuple[tf.Tensor]:
+        batch_size = shape_list(hidden_states)[0]
+        mixed_query_layer = self.query(inputs=hidden_states)
+        mixed_key_layer = self.key(inputs=hidden_states)
+        mixed_value_layer = self.value(inputs=hidden_states)
+        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)
+        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)
+        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)
+
+        # Take the dot product between "query" and "key" to get the raw attention scores.
+        # (batch size, num_heads, seq_len_q, seq_len_k)
+        attention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)
+        dk = tf.cast(self.sqrt_att_head_size, dtype=attention_scores.dtype)
+        attention_scores = tf.divide(attention_scores, dk)
+
+        # Normalize the attention scores to probabilities.
+        attention_probs = stable_softmax(logits=attention_scores, axis=-1)
+
+        # This is actually dropping out entire tokens to attend to, which might
+        # seem a bit unusual, but is taken from the original Transformer paper.
+        attention_probs = self.dropout(inputs=attention_probs, training=training)
+
+        # Mask heads if we want to
+        if head_mask is not None:
+            attention_probs = tf.multiply(attention_probs, head_mask)
+
+        attention_output = tf.matmul(attention_probs, value_layer)
+        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])
+
+        # (batch_size, seq_len_q, all_head_size)
+        attention_output = tf.reshape(
+            tensor=attention_output, shape=(batch_size, -1, self.all_head_size)
+        )
+        outputs = (
+            (attention_output, attention_probs)
+            if output_attentions
+            else (attention_output,)
+        )
 
-        Subclass and override this method if you want to inject some custom behavior.
-        """
+        return outputs
 
-        num_examples = test_dataset.cardinality().numpy()
 
-        if num_examples < 0:
-            raise ValueError("The training dataset must have an asserted cardinality")
+class TFViTSelfOutput(tf.keras.layers.Layer):
+    """
+    The residual connection is defined in TFViTLayer instead of here (as is the case with other models), due to the
+    layernorm applied before each block.
+    """
 
-        steps = math.ceil(num_examples / self.args.eval_batch_size)
-        ds = test_dataset.batch(self.args.eval_batch_size).prefetch(
-            tf.data.experimental.AUTOTUNE
-        )
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
 
-        return (
-            self.args.strategy.experimental_distribute_dataset(ds),
-            steps,
-            num_examples,
+        self.dense = tf.keras.layers.Dense(
+            units=config.hidden_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="dense",
         )
+        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)
 
-    def create_optimizer_and_scheduler(self, num_training_steps: int):
-        """
-        Setup the optimizer and the learning rate scheduler.
+    def call(
+        self, hidden_states: tf.Tensor, input_tensor: tf.Tensor, training: bool = False
+    ) -> tf.Tensor:
+        hidden_states = self.dense(inputs=hidden_states)
+        hidden_states = self.dropout(inputs=hidden_states, training=training)
 
-        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the
-        TFTrainer's init through `optimizers`, or subclass and override this method.
-        """
-        if not self.optimizer and not self.lr_scheduler:
-            warmup_steps = (
-                self.args.warmup_steps
-                if self.args.warmup_steps > 0
-                else math.ceil(num_training_steps * self.args.warmup_ratio)
-            )
+        return hidden_states
 
-            self.optimizer, self.lr_scheduler = create_optimizer(
-                self.args.learning_rate,
-                num_training_steps,
-                warmup_steps,
-                adam_beta1=self.args.adam_beta1,
-                adam_beta2=self.args.adam_beta2,
-                adam_epsilon=self.args.adam_epsilon,
-                weight_decay_rate=self.args.weight_decay,
-                power=self.args.poly_power,
-            )
 
-    def setup_wandb(self):
-        """
-        Setup the optional Weights & Biases (`wandb`) integration.
+class TFViTAttention(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
 
-        One can subclass and override this method to customize the setup if needed. Find more information `here
-        <https://docs.wandb.com/huggingface>`__. You can also override the following environment variables:
+        self.self_attention = TFViTSelfAttention(config, name="attention")
+        self.dense_output = TFViTSelfOutput(config, name="output")
 
-        Environment:
-            WANDB_PROJECT:
-                (Optional): str - "huggingface" by default, set this to a custom string to store results in a different
-                project.
-            WANDB_DISABLED:
-                (Optional): boolean - defaults to false, set to "true" to disable wandb entirely.
-        """
+    def prune_heads(self, heads):
+        raise NotImplementedError
 
-        logger.info(
-            'Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"'
-        )
-        combined_dict = {**self.model.config.to_dict(), **self.args.to_sanitized_dict()}
-        wandb.init(
-            project=os.getenv("WANDB_PROJECT", "huggingface"),
-            config=combined_dict,
-            name=self.args.run_name,
+    def call(
+        self,
+        input_tensor: tf.Tensor,
+        head_mask: tf.Tensor,
+        output_attentions: bool,
+        training: bool = False,
+    ) -> Tuple[tf.Tensor]:
+        self_outputs = self.self_attention(
+            hidden_states=input_tensor,
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+            training=training,
+        )
+        attention_output = self.dense_output(
+            hidden_states=self_outputs[0], input_tensor=input_tensor, training=training
+        )
+        outputs = (attention_output,) + self_outputs[
+            1:
+        ]  # add attentions if we output them
+
+        return outputs
+
+
+class TFViTIntermediate(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
+
+        self.dense = tf.keras.layers.Dense(
+            units=config.intermediate_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="dense",
         )
 
-    def setup_comet(self):
-        """
-        Setup the optional Comet.ml integration.
+        if isinstance(config.hidden_act, str):
+            self.intermediate_act_fn = get_tf_activation(config.hidden_act)
+        else:
+            self.intermediate_act_fn = config.hidden_act
 
-        Environment:
-            COMET_MODE:
-                (Optional): str - "OFFLINE", "ONLINE", or "DISABLED"
-            COMET_PROJECT_NAME:
-                (Optional): str - Comet.ml project name for experiments
-            COMET_OFFLINE_DIRECTORY:
-                (Optional): str - folder to use for saving offline experiments when `COMET_MODE` is "OFFLINE"
+    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
+        hidden_states = self.dense(inputs=hidden_states)
+        hidden_states = self.intermediate_act_fn(hidden_states)
 
-        For a number of configurable items in the environment, see `here
-        <https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables>`__
-        """
-        comet_mode = os.getenv("COMET_MODE", "ONLINE").upper()
-        args = {"project_name": os.getenv("COMET_PROJECT_NAME", "huggingface")}
-        experiment = None
-        if comet_mode == "ONLINE":
-            experiment = comet_ml.Experiment(**args)
-            logger.info("Automatic Comet.ml online logging enabled")
-        elif comet_mode == "OFFLINE":
-            args["offline_directory"] = os.getenv("COMET_OFFLINE_DIRECTORY", "./")
-            experiment = comet_ml.OfflineExperiment(**args)
-            logger.info(
-                "Automatic Comet.ml offline logging enabled; use `comet upload` when finished"
-            )
-        if experiment is not None:
-            experiment._set_model_graph(self.model, framework="transformers")
-            experiment._log_parameters(
-                self.args, prefix="args/", framework="transformers"
-            )
-            experiment._log_parameters(
-                self.model.config, prefix="config/", framework="transformers"
-            )
+        return hidden_states
 
-    def prediction_loop(
-        self,
-        dataset: tf.data.Dataset,
-        steps: int,
-        num_examples: int,
-        description: str,
-        prediction_loss_only: Optional[bool] = None,
-    ) -> PredictionOutput:
-        """
-        Prediction/evaluation loop, shared by [`~TFTrainer.evaluate`] and [`~TFTrainer.predict`].
 
-        Works both with or without labels.
-        """
+class TFViTOutput(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
 
-        prediction_loss_only = (
-            prediction_loss_only
-            if prediction_loss_only is not None
-            else self.args.prediction_loss_only
+        self.dense = tf.keras.layers.Dense(
+            units=config.hidden_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="dense",
         )
+        self.dropout = tf.keras.layers.Dropout(rate=config.hidden_dropout_prob)
 
-        logger.info(f"***** Running {description} *****")
-        logger.info(f"  Num examples in dataset = {num_examples}")
-        if description == "Evaluation":
-            logger.info(
-                f"  Num examples in used in evaluation = {self.args.eval_batch_size * steps}"
-            )
-        logger.info(f"  Batch size = {self.args.eval_batch_size}")
-
-        label_ids: np.ndarray = None
-        preds: np.ndarray = None
-        self.eval_loss.reset_states()
-
-        # Reset the past mems state at the beginning of the evaluation if necessary.
-        if self.args.past_index >= 0:
-            self._past = None
-
-        for step, batch in enumerate(dataset):
-            logits = self.distributed_prediction_steps(batch)
-            _, labels = batch
-
-            if not prediction_loss_only:
-                if isinstance(logits, tuple):
-                    logits = logits[0]
-
-                if isinstance(labels, tuple):
-                    labels = labels[0]
-
-                if self.args.n_replicas > 1:
-                    for val in logits.values:
-                        if preds is None:
-                            preds = val.numpy()
-                        else:
-                            preds = np.append(preds, val.numpy(), axis=0)
-
-                    for val in labels.values:
-                        if label_ids is None:
-                            label_ids = val.numpy()
-                        else:
-                            label_ids = np.append(label_ids, val.numpy(), axis=0)
-                else:
-                    if preds is None:
-                        preds = logits.numpy()
-                    else:
-                        preds = np.append(preds, logits.numpy(), axis=0)
-
-                    if label_ids is None:
-                        label_ids = labels.numpy()
-                    else:
-                        label_ids = np.append(label_ids, labels.numpy(), axis=0)
-
-                if step == steps - 1:
-                    break
-
-        if (
-            self.compute_metrics is not None
-            and preds is not None
-            and label_ids is not None
-        ):
-            metrics = self.compute_metrics(
-                EvalPrediction(predictions=preds, label_ids=label_ids)
-            )
-        else:
-            metrics = {}
-
-        metrics["eval_loss"] = self.eval_loss.result().numpy() / steps
-
-        for key in list(metrics.keys()):
-            if not key.startswith("eval_"):
-                metrics[f"eval_{key}"] = metrics.pop(key)
+    def call(
+        self, hidden_states: tf.Tensor, input_tensor: tf.Tensor, training: bool = False
+    ) -> tf.Tensor:
+        hidden_states = self.dense(inputs=hidden_states)
+        hidden_states = self.dropout(inputs=hidden_states, training=training)
+        hidden_states = hidden_states + input_tensor
 
-        if self.args.past_index and hasattr(self, "_past"):
-            # Clean the state at the end of training
-            delattr(self, "_past")
+        return hidden_states
 
-        return PredictionOutput(predictions=preds, label_ids=label_ids, metrics=metrics)
 
-    def log(self, logs: Dict[str, float]) -> None:
-        """
-        Log `logs` on the various objects watching training.
+class TFViTLayer(tf.keras.layers.Layer):
+    """This corresponds to the Block class in the timm implementation."""
 
-        Subclass and override this method to inject custom behavior.
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
 
-        Args:
-            logs (`Dict[str, float]`):
-                The values to log.
-        """
-        logs["epoch"] = self.epoch_logging
+        self.attention = TFViTAttention(config, name="attention")
+        self.intermediate = TFViTIntermediate(config, name="intermediate")
+        self.vit_output = TFViTOutput(config, name="output")
 
-        if self.tb_writer:
-            with self.tb_writer.as_default():
-                for k, v in logs.items():
-                    tf.summary.scalar(k, v, step=self.global_step)
-            self.tb_writer.flush()
-
-        if is_wandb_available():
-            wandb.log(logs, step=self.global_step)
-
-        if is_comet_available():
-            experiment = comet_ml.config.get_global_experiment()
-            if experiment is not None:
-                experiment._log_metrics(
-                    logs,
-                    step=self.global_step,
-                    epoch=self.epoch_logging,
-                    framework="transformers",
-                )
-
-        output = {**logs, **{"step": self.global_step}}
-
-        logger.info(output)
-
-    def evaluate(
-        self, eval_dataset: Optional[tf.data.Dataset] = None
-    ) -> Dict[str, float]:
-        """
-        Run evaluation and returns metrics.
+        self.layernorm_before = tf.keras.layers.LayerNormalization(
+            epsilon=config.layer_norm_eps, name="layernorm_before"
+        )
+        self.layernorm_after = tf.keras.layers.LayerNormalization(
+            epsilon=config.layer_norm_eps, name="layernorm_after"
+        )
 
-        The calling script will be responsible for providing a method to compute metrics, as they are task-dependent
-        (pass it to the init `compute_metrics` argument).
+    def call(
+        self,
+        hidden_states: tf.Tensor,
+        head_mask: tf.Tensor,
+        output_attentions: bool,
+        training: bool = False,
+    ) -> Tuple[tf.Tensor]:
+        attention_outputs = self.attention(
+            # in ViT, layernorm is applied before self-attention
+            input_tensor=self.layernorm_before(inputs=hidden_states),
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+            training=training,
+        )
+        attention_output = attention_outputs[0]
+
+        # first residual connection
+        hidden_states = attention_output + hidden_states
+
+        # in ViT, layernorm is also applied after self-attention
+        layer_output = self.layernorm_after(inputs=hidden_states)
+
+        intermediate_output = self.intermediate(hidden_states=layer_output)
+
+        # second residual connection is done here
+        layer_output = self.vit_output(
+            hidden_states=intermediate_output,
+            input_tensor=hidden_states,
+            training=training,
+        )
+        outputs = (layer_output,) + attention_outputs[
+            1:
+        ]  # add attentions if we output them
+
+        return outputs
+
+
+class TFViTEncoder(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
+
+        self.layer = [
+            TFViTLayer(config, name=f"layer_._{i}")
+            for i in range(config.num_hidden_layers)
+        ]
 
-        Args:
-            eval_dataset ([`~tf.data.Dataset`], *optional*):
-                Pass a dataset if you wish to override `self.eval_dataset`. The dataset should yield tuples of
-                `(features, labels)` where `features` is a dict of input features and `labels` is the labels. If
-                `labels` is a tensor, the loss is calculated by the model by calling `model(features, labels=labels)`.
-                If `labels` is a dict, such as when using a QuestionAnswering head model with multiple targets, the
-                loss is instead calculated by calling `model(features, **labels)`.
+    def call(
+        self,
+        hidden_states: tf.Tensor,
+        head_mask: tf.Tensor,
+        output_attentions: bool,
+        output_hidden_states: bool,
+        return_dict: bool,
+        training: bool = False,
+    ) -> Union[TFBaseModelOutput, Tuple[tf.Tensor]]:
+        all_hidden_states = () if output_hidden_states else None
+        all_attentions = () if output_attentions else None
 
-        Returns:
-            A dictionary containing the evaluation loss and the potential metrics computed from the predictions.
-        """
-        eval_ds, steps, num_examples = self.get_eval_tfdataset(eval_dataset)
+        for i, layer_module in enumerate(self.layer):
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
 
-        output = self.prediction_loop(
-            eval_ds, steps, num_examples, description="Evaluation"
-        )
-        logs = {**output.metrics}
-        logs["epoch"] = self.epoch_logging
+            layer_outputs = layer_module(
+                hidden_states=hidden_states,
+                head_mask=head_mask[i],
+                output_attentions=output_attentions,
+                training=training,
+            )
+            hidden_states = layer_outputs[0]
 
-        self.log(logs)
+            if output_attentions:
+                all_attentions = all_attentions + (layer_outputs[1],)
 
-        return output.metrics
+        # Add last layer
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
 
-    def prediction_step(
-        self,
-        features: tf.Tensor,
-        labels: tf.Tensor,
-        nb_instances_in_global_batch: tf.Tensor,
-    ) -> tf.Tensor:
-        """
-        Compute the prediction on features and update the loss with labels.
+        if not return_dict:
+            return tuple(
+                v
+                for v in [hidden_states, all_hidden_states, all_attentions]
+                if v is not None
+            )
 
-        Subclass and override to inject some custom behavior.
-        """
-        per_example_loss, logits = self.run_model(features, labels, False)
-        scaled_loss = per_example_loss / tf.cast(
-            nb_instances_in_global_batch, dtype=per_example_loss.dtype
+        return TFBaseModelOutput(
+            last_hidden_state=hidden_states,
+            hidden_states=all_hidden_states,
+            attentions=all_attentions,
         )
 
-        self.eval_loss.update_state(scaled_loss)
 
-        return logits
+@keras_serializable
+class TFViTMainLayer(tf.keras.layers.Layer):
+    config_class = ViTConfig
 
-    @tf.function
-    def distributed_prediction_steps(self, batch):
-        nb_instances_in_batch = self._compute_nb_instances(batch)
-        inputs = self._get_step_inputs(batch, nb_instances_in_batch)
+    def __init__(self, config: ViTConfig, add_pooling_layer: bool = True, **kwargs):
+        super().__init__(**kwargs)
 
-        logits = self.args.strategy.run(self.prediction_step, inputs)
+        self.config = config
 
-        return logits
+        self.embeddings = TFViTEmbeddings(config, name="embeddings")
+        self.encoder = TFViTEncoder(config, name="encoder")
+        self.layernorm = tf.keras.layers.LayerNormalization(
+            epsilon=config.layer_norm_eps, name="layernorm"
+        )
+        self.pooler = TFViTPooler(config, name="pooler") if add_pooling_layer else None
 
-    def train(self) -> None:
+    def get_input_embeddings(self) -> tf.keras.layers.Layer:
+        return self.embeddings.patch_embeddings
+
+    def _prune_heads(self, heads_to_prune):
         """
-        Train method to train the model.
+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
+        class PreTrainedModel
         """
-        train_ds = self.get_train_tfdataset()
-
-        if self.args.debug:
-            tf.summary.trace_on(graph=True, profiler=True)
+        raise NotImplementedError
 
-        self.gradient_accumulator.reset()
+    @unpack_inputs
+    def call(
+        self,
+        pixel_values: Optional[TFModelInputType] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        interpolate_pos_encoding: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        training: bool = False,
+    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:
+        if pixel_values is None:
+            raise ValueError("You have to specify pixel_values")
+
+        embedding_output = self.embeddings(
+            pixel_values=pixel_values,
+            interpolate_pos_encoding=interpolate_pos_encoding,
+            training=training,
+        )
+
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
+        if head_mask is not None:
+            raise NotImplementedError
+        else:
+            head_mask = [None] * self.config.num_hidden_layers
 
-        num_update_steps_per_epoch = (
-            self.num_train_examples / self.total_train_batch_size
+        encoder_outputs = self.encoder(
+            hidden_states=embedding_output,
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+            training=training,
         )
 
-        # In fact, ``self.args.dataloader_drop_last`` has no effect in `trainer_tf.py`, because
-        # the dataset is repeated before being batched.
-        # It has the effect only when TPU is used which requires explicit tensor shape in order to make
-        # the gradient accumulation implementation work.
-        approx = math.floor if self.args.dataloader_drop_last else math.ceil
-        num_update_steps_per_epoch = approx(num_update_steps_per_epoch)
+        sequence_output = encoder_outputs[0]
+        sequence_output = self.layernorm(inputs=sequence_output)
+        pooled_output = (
+            self.pooler(hidden_states=sequence_output)
+            if self.pooler is not None
+            else None
+        )
 
-        # At least one update for each epoch.
-        num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)
-        self.steps_per_epoch = num_update_steps_per_epoch
+        if not return_dict:
+            return (sequence_output, pooled_output) + encoder_outputs[1:]
 
-        if self.args.max_steps > 0:
-            t_total = self.args.max_steps
-            epochs = (self.args.max_steps // self.steps_per_epoch) + int(
-                self.args.max_steps % self.steps_per_epoch > 0
-            )
-        else:
-            t_total = self.steps_per_epoch * self.args.num_train_epochs
-            epochs = self.args.num_train_epochs
+        return TFBaseModelOutputWithPooling(
+            last_hidden_state=sequence_output,
+            pooler_output=pooled_output,
+            hidden_states=encoder_outputs.hidden_states,
+            attentions=encoder_outputs.attentions,
+        )
 
-        # Since ``self.args.num_train_epochs`` can be `float`, we make ``epochs`` be a `float` always.
-        epochs = float(epochs)
 
-        with self.args.strategy.scope():
-            self.create_optimizer_and_scheduler(num_training_steps=t_total)
-            folder = os.path.join(self.args.output_dir, PREFIX_CHECKPOINT_DIR)
-            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)
-            self.model.ckpt_manager = tf.train.CheckpointManager(
-                ckpt, folder, max_to_keep=self.args.save_total_limit
-            )
-
-            iterations = self.optimizer.iterations
-            epochs_trained = 0
-            steps_trained_in_current_epoch = 0
-            if self.model.ckpt_manager.latest_checkpoint:
-                logger.info(
-                    f"Checkpoint file {self.model.ckpt_manager.latest_checkpoint} found and restoring from checkpoint"
-                )
-                ckpt.restore(self.model.ckpt_manager.latest_checkpoint).expect_partial()
-
-                self.global_step = iterations.numpy()
-
-                epochs_trained = self.global_step // self.steps_per_epoch
-                steps_trained_in_current_epoch = self.global_step % self.steps_per_epoch
-
-                logger.info(
-                    "  Continuing training from checkpoint, will skip to saved global_step"
-                )
-                logger.info(f"  Continuing training from epoch {epochs_trained}")
-                logger.info(
-                    f"  Continuing training from global step {self.global_step}"
-                )
-                logger.info(
-                    f"  Will skip the first {steps_trained_in_current_epoch} steps in the first epoch"
-                )
-
-            tf.summary.experimental.set_step(self.global_step)
-
-            with self.tb_writer.as_default():
-                tf.summary.text("args", self.args.to_json_string())
-
-            self.tb_writer.flush()
-
-            logger.info("***** Running training *****")
-            logger.info(f"  Num examples = {self.num_train_examples}")
-            # TODO: We might want to print a more precise ``epochs`` if self.args.max_steps > 0 ?
-            logger.info(f"  Num Epochs = {epochs}")
-            logger.info(
-                f"  Instantaneous batch size per device = {self.args.per_device_train_batch_size}"
-            )
-            logger.info(
-                f"  Total train batch size (w. parallel, distributed & accumulation) = {self.total_train_batch_size}"
-            )
-            logger.info(
-                f"  Gradient Accumulation steps = {self.args.gradient_accumulation_steps}"
-            )
-            logger.info(f"  Steps per epoch = {self.steps_per_epoch}")
-            logger.info(f"  Total optimization steps = {t_total}")
-
-            self.train_loss = tf.keras.metrics.Sum()
-            start_time = datetime.datetime.now()
+class TFViTPreTrainedModel(TFPreTrainedModel):
+    """
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
+    """
 
-            for epoch_iter in range(epochs_trained, int(epochs)):
-                # Reset the past mems state at the beginning of each epoch if necessary.
-                if self.args.past_index >= 0:
-                    self._past = None
-
-                for step, batch in enumerate(train_ds):
-                    # Skip past any already trained steps if resuming training
-                    if steps_trained_in_current_epoch > 0:
-                        steps_trained_in_current_epoch -= 1
-                        continue
-
-                    self.distributed_training_steps(batch)
-
-                    self.global_step = iterations.numpy()
-                    self.epoch_logging = epoch_iter + (step + 1) / self.steps_per_epoch
-
-                    training_loss = self.train_loss.result() / (step + 1)
-
-                    if self.args.debug:
-                        logs = {}
-                        logs["loss"] = training_loss.numpy()
-                        logs["epoch"] = self.epoch_logging
-
-                        self.log(logs)
-
-                    if self.global_step == 1 and self.args.debug:
-                        with self.tb_writer.as_default():
-                            tf.summary.trace_export(
-                                name="training",
-                                step=self.global_step,
-                                profiler_outdir=self.args.logging_dir,
-                            )
-
-                    if (
-                        self.args.eval_steps > 0
-                        and self.args.evaluation_strategy == IntervalStrategy.STEPS
-                        and self.global_step % self.args.eval_steps == 0
-                    ):
-                        self.evaluate()
-
-                    if (
-                        self.args.logging_steps > 0
-                        and self.global_step % self.args.logging_steps == 0
-                    ) or (self.global_step == 1 and self.args.logging_first_step):
-                        logs = {}
-                        logs["loss"] = training_loss.numpy()
-                        logs["learning_rate"] = self.lr_scheduler(
-                            self.global_step
-                        ).numpy()
-                        logs["epoch"] = self.epoch_logging
-
-                        self.log(logs)
-
-                    if (
-                        self.args.save_steps > 0
-                        and self.global_step % self.args.save_steps == 0
-                    ):
-                        ckpt_save_path = self.model.ckpt_manager.save()
-
-                        logger.info(
-                            f"Saving checkpoint for step {self.global_step} at {ckpt_save_path}"
-                        )
-
-                    if self.args.max_steps > 0 and self.global_step >= t_total:
-                        break
-
-                    if self.global_step % self.steps_per_epoch == 0:
-                        break
-
-                self.train_loss.reset_states()
-
-                if self.args.max_steps > 0 and self.global_step >= self.args.max_steps:
-                    break
-
-            end_time = datetime.datetime.now()
-
-            logger.info(f"Training took: {str(end_time - start_time)}")
-
-        if self.args.past_index and hasattr(self, "_past"):
-            # Clean the state at the end of training
-            delattr(self, "_past")
+    config_class = ViTConfig
+    base_model_prefix = "vit"
+    main_input_name = "pixel_values"
 
-    def training_step(self, features, labels, nb_instances_in_global_batch):
+    @property
+    def dummy_inputs(self) -> Dict[str, tf.Tensor]:
         """
-        Perform a training step on features and labels.
+        Dummy inputs to build the network.
 
-        Subclass and override to inject some custom behavior.
+        Returns:
+            `Dict[str, tf.Tensor]`: The dummy inputs.
         """
-        per_example_loss, _ = self.run_model(features, labels, True)
-        scaled_loss = per_example_loss / tf.cast(
-            nb_instances_in_global_batch, dtype=per_example_loss.dtype
-        )
-        gradients = tf.gradients(scaled_loss, self.model.trainable_variables)
-        gradients = [
-            g if g is not None else tf.zeros_like(v)
-            for g, v in zip(gradients, self.model.trainable_variables)
+        VISION_DUMMY_INPUTS = tf.random.uniform(
+            shape=(
+                3,
+                self.config.num_channels,
+                self.config.image_size,
+                self.config.image_size,
+            ),
+            dtype=tf.float32,
+        )
+        return {"pixel_values": tf.constant(VISION_DUMMY_INPUTS)}
+
+    @tf.function(
+        input_signature=[
+            {
+                "pixel_values": tf.TensorSpec(
+                    (None, None, None, None), tf.float32, name="pixel_values"
+                ),
+            }
         ]
+    )
+    def serving(self, inputs):
+        """
+        Method used for serving the model.
 
-        if self.args.gradient_accumulation_steps > 1:
-            self.gradient_accumulator(gradients)
-
-        self.train_loss.update_state(scaled_loss)
-
-        if self.args.gradient_accumulation_steps == 1:
-            return gradients
-
-    def apply_gradients(self, features, labels, nb_instances_in_global_batch):
-        if self.args.gradient_accumulation_steps == 1:
-            gradients = self.training_step(
-                features, labels, nb_instances_in_global_batch
-            )
-
-            self.optimizer.apply_gradients(
-                list(zip(gradients, self.model.trainable_variables))
-            )
-        else:
-            for _ in tf.range(self.args.gradient_accumulation_steps):
-                reduced_features = {
-                    k: ft[: self.args.train_batch_size // self.args.n_replicas]
-                    for k, ft in features.items()
-                }
-
-                if tf.is_tensor(labels):
-                    reduced_labels = labels[
-                        : self.args.train_batch_size // self.args.n_replicas
-                    ]
-                elif isinstance(labels, dict):
-                    reduced_labels = {
-                        k: lbl[: self.args.train_batch_size // self.args.n_replicas]
-                        for k, lbl in labels.items()
-                    }
-                else:
-                    raise ValueError("The labels must be either a tf.Tensor or a dict.")
-
-                self.training_step(
-                    reduced_features, reduced_labels, nb_instances_in_global_batch
-                )
-
-                features = {
-                    k: tf.concat(
-                        [
-                            ft[self.args.train_batch_size // self.args.n_replicas :],
-                            reduced_features[k],
-                        ],
-                        axis=0,
-                    )
-                    for k, ft in features.items()
-                }
-
-                if tf.is_tensor(labels):
-                    labels = tf.concat(
-                        [
-                            labels[
-                                self.args.train_batch_size // self.args.n_replicas :
-                            ],
-                            reduced_labels,
-                        ],
-                        axis=0,
-                    )
-                elif isinstance(labels, dict):
-                    labels = {
-                        k: tf.concat(
-                            [
-                                lbl[
-                                    self.args.train_batch_size // self.args.n_replicas :
-                                ],
-                                reduced_labels[k],
-                            ],
-                            axis=0,
-                        )
-                        for k, lbl in labels.items()
-                    }
-                else:
-                    raise ValueError("The labels must be either a tf.Tensor or a dict.")
-
-            gradients = self.gradient_accumulator.gradients
-            gradients = [
-                (
-                    tf.clip_by_value(
-                        grad, -self.args.max_grad_norm, self.args.max_grad_norm
-                    )
-                )
-                for grad in gradients
-            ]
-
-            self.optimizer.apply_gradients(
-                list(zip(gradients, self.model.trainable_variables))
-            )
-            self.gradient_accumulator.reset()
-
-    @tf.function
-    def distributed_training_steps(self, batch):
-        with self.args.strategy.scope():
-            nb_instances_in_batch = self._compute_nb_instances(batch)
-            inputs = self._get_step_inputs(batch, nb_instances_in_batch)
+        Args:
+            inputs (`Dict[str, tf.Tensor]`):
+                The input of the saved model as a dictionary of tensors.
+        """
+        output = self.call(inputs)
 
-            self.args.strategy.run(self.apply_gradients, inputs)
+        return self.serving_output(output)
 
-    @staticmethod
-    def _compute_nb_instances(batch):
-        labels = batch[-1]
-        if isinstance(labels, PerReplica):
-            labels = tf.concat(labels.values, axis=0)
 
-        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))
+VIT_START_DOCSTRING = r"""
 
-        return nb_instances
+    This model inherits from [`TFPreTrainedModel`]. Check the superclass documentation for the generic methods the
+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
+    etc.)
 
-    @staticmethod
-    def _get_step_inputs(batch, nb_instances):
-        features, labels = batch
+    This model is also a [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model) subclass. Use it
+    as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
+    behavior.
 
-        if isinstance(labels, PerReplica):
-            # need to make a `PerReplica` objects for ``nb_instances``
-            nb_instances = PerReplica([nb_instances] * len(labels.values))
+    <Tip>
 
-        step_inputs = (features, labels, nb_instances)
+    TensorFlow models and layers in `transformers` accept two formats as input:
 
-        return step_inputs
+    - having all inputs as keyword arguments (like PyTorch models), or
+    - having all inputs as a list, tuple or dict in the first positional argument.
 
-    def run_model(self, features, labels, training):
-        """
-        Computes the loss of the given features and labels pair.
+    The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
+    and layers. Because of this support, when using methods like `model.fit()` things should "just work" for you - just
+    pass your inputs and labels in any format that `model.fit()` supports! If, however, you want to use the second
+    format outside of Keras methods like `fit()` and `predict()`, such as when creating your own layers or models with
+    the Keras `Functional` API, there are three possibilities you can use to gather all the input Tensors in the first
+    positional argument:
 
-        Subclass and override this method if you want to inject some custom behavior.
+    - a single Tensor with `pixel_values` only and nothing else: `model(pixel_values)`
+    - a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
+    `model([pixel_values, attention_mask])` or `model([pixel_values, attention_mask, token_type_ids])`
+    - a dictionary with one or several input Tensors associated to the input names given in the docstring:
+    `model({"pixel_values": pixel_values, "token_type_ids": token_type_ids})`
 
-        Args:
-            features (`tf.Tensor`): A batch of input features.
-            labels (`tf.Tensor`): A batch of labels.
-            training (`bool`): Whether or not to run the model in training mode.
+    Note that when creating models and layers with
+    [subclassing](https://keras.io/guides/making_new_layers_and_models_via_subclassing/) then you don't need to worry
+    about any of this, as you can just pass inputs like you would to any other Python function!
 
-        Returns:
-            A tuple of two `tf.Tensor`: The loss and logits.
-        """
+    </Tip>
 
-        if self.args.past_index >= 0 and getattr(self, "_past", None) is not None:
-            features["mems"] = self._past
+    Args:
+        config ([`ViTConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~TFPreTrainedModel.from_pretrained`] method to load the model weights.
+"""
 
-        if isinstance(labels, (dict)):
-            outputs = self.model(features, training=training, **labels)[:2]
-        else:
-            outputs = self.model(features, labels=labels, training=training)[:2]
+VIT_INPUTS_DOCSTRING = r"""
+    Args:
+        pixel_values (`np.ndarray`, `tf.Tensor`, `List[tf.Tensor]` ``Dict[str, tf.Tensor]` or `Dict[str, np.ndarray]` and each example must have the shape `(batch_size, num_channels, height, width)`):
+            Pixel values. Pixel values can be obtained using [`ViTFeatureExtractor`]. See
+            [`ViTFeatureExtractor.__call__`] for details.
+
+        head_mask (`np.ndarray` or `tf.Tensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
+
+            - 1 indicates the head is **not masked**,
+            - 0 indicates the head is **masked**.
+
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
+            tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
+            config will be used instead.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
+            used instead.
+        interpolate_pos_encoding (`bool`, *optional*):
+            Whether to interpolate the pre-trained position encodings.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple. This argument can be used in
+            eager mode, in graph mode the value will always be set to True.
+        training (`bool`, *optional*, defaults to `False``):
+            Whether or not to use the model in training mode (some modules like dropout modules have different
+            behaviors between training and evaluation).
+"""
+
+
+@add_start_docstrings(
+    "The bare ViT Model transformer outputting raw hidden-states without any specific head on top.",
+    VIT_START_DOCSTRING,
+)
+class TFViTModel(TFViTPreTrainedModel):
+    def __init__(self, config: ViTConfig, *inputs, add_pooling_layer=True, **kwargs):
+        super().__init__(config, *inputs, **kwargs)
+
+        self.vit = TFViTMainLayer(
+            config, add_pooling_layer=add_pooling_layer, name="vit"
+        )
+
+    @unpack_inputs
+    @add_start_docstrings_to_model_forward(VIT_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_FEAT_EXTRACTOR_FOR_DOC,
+        checkpoint=_CHECKPOINT_FOR_DOC,
+        output_type=TFBaseModelOutputWithPooling,
+        config_class=_CONFIG_FOR_DOC,
+        modality="vision",
+        expected_output=_EXPECTED_OUTPUT_SHAPE,
+    )
+    def call(
+        self,
+        pixel_values: Optional[TFModelInputType] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        interpolate_pos_encoding: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        training: bool = False,
+    ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor]]:
+        outputs = self.vit(
+            pixel_values=pixel_values,
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            interpolate_pos_encoding=interpolate_pos_encoding,
+            return_dict=return_dict,
+            training=training,
+        )
+
+        return outputs
+
+    def serving_output(
+        self, output: TFBaseModelOutputWithPooling
+    ) -> TFBaseModelOutputWithPooling:
+        hs = (
+            tf.convert_to_tensor(output.hidden_states)
+            if self.config.output_hidden_states
+            else None
+        )
+        attns = (
+            tf.convert_to_tensor(output.attentions)
+            if self.config.output_attentions
+            else None
+        )
+
+        return TFBaseModelOutputWithPooling(
+            last_hidden_state=output.last_hidden_state,
+            pooler_output=output.pooler_output,
+            hidden_states=hs,
+            attentions=attns,
+        )
 
-        loss, logits = outputs[:2]
 
-        if self.args.past_index >= 0:
-            self._past = outputs[self.args.past_index]
+class TFViTPooler(tf.keras.layers.Layer):
+    def __init__(self, config: ViTConfig, **kwargs):
+        super().__init__(**kwargs)
+
+        self.dense = tf.keras.layers.Dense(
+            units=config.hidden_size,
+            kernel_initializer=get_initializer(config.initializer_range),
+            activation="tanh",
+            name="dense",
+        )
+
+    def call(self, hidden_states: tf.Tensor) -> tf.Tensor:
+        # We "pool" the model by simply taking the hidden state corresponding
+        # to the first token.
+        first_token_tensor = hidden_states[:, 0]
+        pooled_output = self.dense(inputs=first_token_tensor)
 
-        return loss, logits
+        return pooled_output
 
-    def predict(self, test_dataset: tf.data.Dataset) -> PredictionOutput:
-        """
-        Run prediction and returns predictions and potential metrics.
 
-        Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method
-        will also return metrics, like in `evaluate()`.
+@add_start_docstrings(
+    """
+    ViT Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
+    the [CLS] token) e.g. for ImageNet.
 
-        Args:
-            test_dataset ([`~tf.data.Dataset`]):
-                Dataset to run the predictions on. The dataset should yield tuples of `(features, labels)` where
-                `features` is a dict of input features and `labels` is the labels. If `labels` is a tensor, the loss is
-                calculated by the model by calling `model(features, labels=labels)`. If `labels` is a dict, such as
-                when using a QuestionAnswering head model with multiple targets, the loss is instead calculated by
-                calling `model(features, **labels)`
-
-        Returns: *NamedTuple* A namedtuple with the following keys:
-
-            - predictions (`np.ndarray`): The predictions on `test_dataset`.
-            - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).
-            - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained
-              labels).
-        """
-        test_ds, steps, num_examples = self.get_test_tfdataset(test_dataset)
+    <Tip>
 
-        return self.prediction_loop(
-            test_ds, steps, num_examples, description="Prediction"
+        Note that it's possible to fine-tune ViT on higher resolution images than the ones it has been trained on, by
+        setting `interpolate_pos_encoding` to `True` in the forward of the model. This will interpolate the pre-trained
+        position embeddings to the higher resolution.
+
+    </Tip>
+    """,
+    VIT_START_DOCSTRING,
+)
+class TFViTForImageClassification(TFViTPreTrainedModel, TFSequenceClassificationLoss):
+    def __init__(self, config: ViTConfig, *inputs, **kwargs):
+        super().__init__(config, *inputs, **kwargs)
+
+        self.num_labels = config.num_labels
+        self.vit = TFViTMainLayer(config, add_pooling_layer=False, name="vit")
+
+        # Classifier head
+        self.classifier = tf.keras.layers.Dense(
+            units=config.num_labels,
+            kernel_initializer=get_initializer(config.initializer_range),
+            name="classifier",
+        )
+
+    @unpack_inputs
+    @add_start_docstrings_to_model_forward(VIT_INPUTS_DOCSTRING)
+    @add_code_sample_docstrings(
+        processor_class=_FEAT_EXTRACTOR_FOR_DOC,
+        checkpoint=_IMAGE_CLASS_CHECKPOINT,
+        output_type=TFSequenceClassifierOutput,
+        config_class=_CONFIG_FOR_DOC,
+        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,
+    )
+    def call(
+        self,
+        pixel_values: Optional[TFModelInputType] = None,
+        head_mask: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        output_attentions: Optional[bool] = None,
+        output_hidden_states: Optional[bool] = None,
+        interpolate_pos_encoding: Optional[bool] = None,
+        return_dict: Optional[bool] = None,
+        labels: Optional[Union[np.ndarray, tf.Tensor]] = None,
+        training: Optional[bool] = False,
+    ) -> Union[TFSequenceClassifierOutput, Tuple[tf.Tensor]]:
+        r"""
+        labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):
+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,
+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If
+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).
+        """
+
+        outputs = self.vit(
+            pixel_values=pixel_values,
+            head_mask=head_mask,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            interpolate_pos_encoding=interpolate_pos_encoding,
+            return_dict=return_dict,
+            training=training,
+        )
+        sequence_output = outputs[0]
+        logits = self.classifier(inputs=sequence_output[:, 0, :])
+        loss = (
+            None
+            if labels is None
+            else self.hf_compute_loss(labels=labels, logits=logits)
+        )
+
+        if not return_dict:
+            output = (logits,) + outputs[2:]
+            return ((loss,) + output) if loss is not None else output
+
+        return TFSequenceClassifierOutput(
+            loss=loss,
+            logits=logits,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
+        )
+
+    def serving_output(
+        self, output: TFSequenceClassifierOutput
+    ) -> TFSequenceClassifierOutput:
+        hs = (
+            tf.convert_to_tensor(output.hidden_states)
+            if self.config.output_hidden_states
+            else None
+        )
+        attns = (
+            tf.convert_to_tensor(output.attentions)
+            if self.config.output_attentions
+            else None
         )
 
-    def save_model(self, output_dir: Optional[str] = None):
-        """
-        Will save the model, so you can reload it using `from_pretrained()`.
-        """
-        output_dir = output_dir if output_dir is not None else self.args.output_dir
-
-        logger.info(f"Saving model in {output_dir}")
-
-        if not isinstance(self.model, TFPreTrainedModel):
-            raise ValueError("Trainer.model appears to not be a PreTrainedModel")
-
-        self.model.save_pretrained(output_dir)
+        return TFSequenceClassifierOutput(
+            logits=output.logits, hidden_states=hs, attentions=attns
+        )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `xs_transformers-1.0.0/xs_transformers/trainer_utils.py` & `xs_transformers-1.0.1/xs_transformers/models/layoutlm/tokenization_layoutlm.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,725 +1,544 @@
 # coding=utf-8
-# Copyright 2020-present the HuggingFace Inc. team.
+# Copyright 2018 The Microsoft Research Asia LayoutLM Team Authors.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""
-Utilities for the Trainer and TFTrainer class. Should be independent from PyTorch and TensorFlow.
-"""
-
-import copy
-import functools
-import gc
-import inspect
-import os
-import random
-import re
-import threading
-import time
-from typing import Any, Dict, List, NamedTuple, Optional, Tuple, Union
-
-import numpy as np
-
-from .utils import (
-    ExplicitEnum,
-    is_psutil_available,
-    is_tf_available,
-    is_torch_available,
-    is_torch_cuda_available,
-    is_torch_tpu_available,
-    requires_backends,
-)
-
-if is_torch_available():
-    import torch
-
-if is_tf_available():
-    import tensorflow as tf
-
-
-def seed_worker(_):
-    """
-    Helper function to set worker seed during Dataloader initialization.
-    """
-    worker_seed = torch.initial_seed() % 2**32
-    set_seed(worker_seed)
-
-
-def enable_full_determinism(seed: int):
-    """
-    Helper function for reproducible behavior during distributed training. See
-    - https://pytorch.org/docs/stable/notes/randomness.html for pytorch
-    - https://www.tensorflow.org/api_docs/python/tf/config/experimental/enable_op_determinism for tensorflow
-    """
-    # set seed first
-    set_seed(seed)
-
-    if is_torch_available():
-        #  Enable PyTorch deterministic mode. This potentially requires either the environment
-        #  variable 'CUDA_LAUNCH_BLOCKING' or 'CUBLAS_WORKSPACE_CONFIG' to be set,
-        # depending on the CUDA version, so we set them both here
-        os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
-        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":16:8"
-        torch.use_deterministic_algorithms(True)
-
-        # Enable CUDNN deterministic mode
-        torch.backends.cudnn.deterministic = True
-        torch.backends.cudnn.benchmark = False
-
-    if is_tf_available():
-        tf.config.experimental.enable_op_determinism()
-
-
-def set_seed(seed: int):
-    """
-    Helper function for reproducible behavior to set the seed in `random`, `numpy`, `torch` and/or `tf` (if installed).
-
-    Args:
-        seed (`int`): The seed to set.
-    """
-    random.seed(seed)
-    np.random.seed(seed)
-    if is_torch_available():
-        torch.manual_seed(seed)
-        torch.cuda.manual_seed_all(seed)
-        # ^^ safe to call this function even if cuda is not available
-    if is_tf_available():
-        tf.random.set_seed(seed)
-
-
-class EvalPrediction:
-    """
-    Evaluation output (always contains labels), to be used to compute metrics.
-
-    Parameters:
-        predictions (`np.ndarray`): Predictions of the model.
-        label_ids (`np.ndarray`): Targets to be matched.
-        inputs (`np.ndarray`, *optional*)
-    """
-
-    def __init__(
-        self,
-        predictions: Union[np.ndarray, Tuple[np.ndarray]],
-        label_ids: Union[np.ndarray, Tuple[np.ndarray]],
-        inputs: Optional[Union[np.ndarray, Tuple[np.ndarray]]] = None,
-    ):
-        self.predictions = predictions
-        self.label_ids = label_ids
-        self.inputs = inputs
-
-    def __iter__(self):
-        if self.inputs is not None:
-            return iter((self.predictions, self.label_ids, self.inputs))
-        else:
-            return iter((self.predictions, self.label_ids))
-
-    def __getitem__(self, idx):
-        if idx < 0 or idx > 2:
-            raise IndexError("tuple index out of range")
-        if idx == 2 and self.inputs is None:
-            raise IndexError("tuple index out of range")
-        if idx == 0:
-            return self.predictions
-        elif idx == 1:
-            return self.label_ids
-        elif idx == 2:
-            return self.inputs
-
-
-class EvalLoopOutput(NamedTuple):
-    predictions: Union[np.ndarray, Tuple[np.ndarray]]
-    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
-    metrics: Optional[Dict[str, float]]
-    num_samples: Optional[int]
-
+""" Tokenization class for model LayoutLM."""
 
-class PredictionOutput(NamedTuple):
-    predictions: Union[np.ndarray, Tuple[np.ndarray]]
-    label_ids: Optional[Union[np.ndarray, Tuple[np.ndarray]]]
-    metrics: Optional[Dict[str, float]]
-
-
-class TrainOutput(NamedTuple):
-    global_step: int
-    training_loss: float
-    metrics: Dict[str, float]
-
-
-PREFIX_CHECKPOINT_DIR = "checkpoint"
-_re_checkpoint = re.compile(r"^" + PREFIX_CHECKPOINT_DIR + r"\-(\d+)$")
-
-
-def get_last_checkpoint(folder):
-    content = os.listdir(folder)
-    checkpoints = [
-        path
-        for path in content
-        if _re_checkpoint.search(path) is not None
-        and os.path.isdir(os.path.join(folder, path))
-    ]
-    if len(checkpoints) == 0:
-        return
-    return os.path.join(
-        folder,
-        max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0])),
-    )
-
-
-class IntervalStrategy(ExplicitEnum):
-    NO = "no"
-    STEPS = "steps"
-    EPOCH = "epoch"
-
-
-class EvaluationStrategy(ExplicitEnum):
-    NO = "no"
-    STEPS = "steps"
-    EPOCH = "epoch"
-
-
-class HubStrategy(ExplicitEnum):
-    END = "end"
-    EVERY_SAVE = "every_save"
-    CHECKPOINT = "checkpoint"
-    ALL_CHECKPOINTS = "all_checkpoints"
-
-
-class BestRun(NamedTuple):
-    """
-    The best run found by an hyperparameter search (see [`~Trainer.hyperparameter_search`]).
-
-    Parameters:
-        run_id (`str`):
-            The id of the best run (if models were saved, the corresponding checkpoint will be in the folder ending
-            with run-{run_id}).
-        objective (`float`):
-            The objective that was obtained for this run.
-        hyperparameters (`Dict[str, Any]`):
-            The hyperparameters picked to get this run.
-    """
+import collections
+import os
+import unicodedata
+from typing import List, Optional, Tuple
 
-    run_id: str
-    objective: float
-    hyperparameters: Dict[str, Any]
+from ...tokenization_utils import (
+    PreTrainedTokenizer,
+    _is_control,
+    _is_punctuation,
+    _is_whitespace,
+)
+from ...utils import logging
 
+logger = logging.get_logger(__name__)
 
-def default_compute_objective(metrics: Dict[str, float]) -> float:
-    """
-    The default objective to maximize/minimize when doing an hyperparameter search. It is the evaluation loss if no
-    metrics are provided to the [`Trainer`], the sum of all metrics otherwise.
-
-    Args:
-        metrics (`Dict[str, float]`): The metrics returned by the evaluate method.
+VOCAB_FILES_NAMES = {"vocab_file": "vocab.txt"}
 
-    Return:
-        `float`: The objective to minimize or maximize
-    """
-    metrics = copy.deepcopy(metrics)
-    loss = metrics.pop("eval_loss", None)
-    _ = metrics.pop("epoch", None)
-    # Remove speed metrics
-    speed_metrics = [
-        m for m in metrics.keys() if m.endswith("_runtime") or m.endswith("_per_second")
-    ]
-    for sm in speed_metrics:
-        _ = metrics.pop(sm, None)
-    return loss if len(metrics) == 0 else sum(metrics.values())
-
-
-def default_hp_space_optuna(trial) -> Dict[str, float]:
-    from .integrations import is_optuna_available
-
-    assert (
-        is_optuna_available()
-    ), "This function needs Optuna installed: `pip install optuna`"
-    return {
-        "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-4, log=True),
-        "num_train_epochs": trial.suggest_int("num_train_epochs", 1, 5),
-        "seed": trial.suggest_int("seed", 1, 40),
-        "per_device_train_batch_size": trial.suggest_categorical(
-            "per_device_train_batch_size", [4, 8, 16, 32, 64]
+PRETRAINED_VOCAB_FILES_MAP = {
+    "vocab_file": {
+        "microsoft/layoutlm-base-uncased": (
+            "https://huggingface.co/microsoft/layoutlm-base-uncased/resolve/main/vocab.txt"
+        ),
+        "microsoft/layoutlm-large-uncased": (
+            "https://huggingface.co/microsoft/layoutlm-large-uncased/resolve/main/vocab.txt"
         ),
     }
-
-
-def default_hp_space_ray(trial) -> Dict[str, float]:
-    from .integrations import is_ray_tune_available
-
-    assert (
-        is_ray_tune_available()
-    ), "This function needs ray installed: `pip install ray[tune]`"
-    from ray import tune
-
-    return {
-        "learning_rate": tune.loguniform(1e-6, 1e-4),
-        "num_train_epochs": tune.choice(list(range(1, 6))),
-        "seed": tune.uniform(1, 40),
-        "per_device_train_batch_size": tune.choice([4, 8, 16, 32, 64]),
-    }
-
-
-def default_hp_space_sigopt(trial):
-    return [
-        {
-            "bounds": {"min": 1e-6, "max": 1e-4},
-            "name": "learning_rate",
-            "type": "double",
-            "transformamtion": "log",
-        },
-        {"bounds": {"min": 1, "max": 6}, "name": "num_train_epochs", "type": "int"},
-        {"bounds": {"min": 1, "max": 40}, "name": "seed", "type": "int"},
-        {
-            "categorical_values": ["4", "8", "16", "32", "64"],
-            "name": "per_device_train_batch_size",
-            "type": "categorical",
-        },
-    ]
-
-
-def default_hp_space_wandb(trial) -> Dict[str, float]:
-    from .integrations import is_wandb_available
-
-    if not is_wandb_available():
-        raise ImportError("This function needs wandb installed: `pip install wandb`")
-
-    return {
-        "method": "random",
-        "metric": {"name": "objective", "goal": "minimize"},
-        "parameters": {
-            "learning_rate": {"distribution": "uniform", "min": 1e-6, "max": 1e-4},
-            "num_train_epochs": {"distribution": "int_uniform", "min": 1, "max": 6},
-            "seed": {"distribution": "int_uniform", "min": 1, "max": 40},
-            "per_device_train_batch_size": {"values": [4, 8, 16, 32, 64]},
-        },
-    }
-
-
-class HPSearchBackend(ExplicitEnum):
-    OPTUNA = "optuna"
-    RAY = "ray"
-    SIGOPT = "sigopt"
-    WANDB = "wandb"
-
-
-default_hp_space = {
-    HPSearchBackend.OPTUNA: default_hp_space_optuna,
-    HPSearchBackend.RAY: default_hp_space_ray,
-    HPSearchBackend.SIGOPT: default_hp_space_sigopt,
-    HPSearchBackend.WANDB: default_hp_space_wandb,
 }
 
+PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {
+    "microsoft/layoutlm-base-uncased": 512,
+    "microsoft/layoutlm-large-uncased": 512,
+}
 
-def is_main_process(local_rank):
-    """
-    Whether or not the current process is the local process, based on `xm.get_ordinal()` (for TPUs) first, then on
-    `local_rank`.
-    """
-    if is_torch_tpu_available(check_device=True):
-        import torch_xla.core.xla_model as xm
-
-        return xm.get_ordinal() == 0
-    return local_rank in [-1, 0]
-
-
-def total_processes_number(local_rank):
-    """
-    Return the number of processes launched in parallel. Works with `torch.distributed` and TPUs.
-    """
-    if is_torch_tpu_available(check_device=True):
-        import torch_xla.core.xla_model as xm
-
-        return xm.xrt_world_size()
-    elif local_rank != -1 and is_torch_available():
-        import torch
-
-        return torch.distributed.get_world_size()
-    return 1
+PRETRAINED_INIT_CONFIGURATION = {
+    "microsoft/layoutlm-base-uncased": {"do_lower_case": True},
+    "microsoft/layoutlm-large-uncased": {"do_lower_case": True},
+}
 
 
-def speed_metrics(split, start_time, num_samples=None, num_steps=None):
-    """
-    Measure and return speed performance metrics.
+# Copied from transformers.models.bert.tokenization_bert.load_vocab
+def load_vocab(vocab_file):
+    """Loads a vocabulary file into a dictionary."""
+    vocab = collections.OrderedDict()
+    with open(vocab_file, "r", encoding="utf-8") as reader:
+        tokens = reader.readlines()
+    for index, token in enumerate(tokens):
+        token = token.rstrip("\n")
+        vocab[token] = index
+    return vocab
+
+
+# Copied from transformers.models.bert.tokenization_bert.whitespace_tokenize
+def whitespace_tokenize(text):
+    """Runs basic whitespace cleaning and splitting on a piece of text."""
+    text = text.strip()
+    if not text:
+        return []
+    tokens = text.split()
+    return tokens
+
+
+# Copied from transformers.models.bert.tokenization_bert.BertTokenizer with Bert->LayoutLM,BERT->LayoutLM
+class LayoutLMTokenizer(PreTrainedTokenizer):
+    r"""
+    Construct a LayoutLM tokenizer. Based on WordPiece.
 
-    This function requires a time snapshot `start_time` before the operation to be measured starts and this function
-    should be run immediately after the operation to be measured has completed.
+    This tokenizer inherits from [`PreTrainedTokenizer`] which contains most of the main methods. Users should refer to
+    this superclass for more information regarding those methods.
 
     Args:
-    - split: name to prefix metric (like train, eval, test...)
-    - start_time: operation start time
-    - num_samples: number of samples processed
-    """
-    runtime = time.time() - start_time
-    result = {f"{split}_runtime": round(runtime, 4)}
-    if num_samples is not None:
-        samples_per_second = num_samples / runtime
-        result[f"{split}_samples_per_second"] = round(samples_per_second, 3)
-    if num_steps is not None:
-        steps_per_second = num_steps / runtime
-        result[f"{split}_steps_per_second"] = round(steps_per_second, 3)
-    return result
-
-
-class SchedulerType(ExplicitEnum):
-    LINEAR = "linear"
-    COSINE = "cosine"
-    COSINE_WITH_RESTARTS = "cosine_with_restarts"
-    POLYNOMIAL = "polynomial"
-    CONSTANT = "constant"
-    CONSTANT_WITH_WARMUP = "constant_with_warmup"
-
-
-class TrainerMemoryTracker:
-    """
-    A helper class that tracks cpu and gpu memory.
+        vocab_file (`str`):
+            File containing the vocabulary.
+        do_lower_case (`bool`, *optional*, defaults to `True`):
+            Whether or not to lowercase the input when tokenizing.
+        do_basic_tokenize (`bool`, *optional*, defaults to `True`):
+            Whether or not to do basic tokenization before WordPiece.
+        never_split (`Iterable`, *optional*):
+            Collection of tokens which will never be split during tokenization. Only has an effect when
+            `do_basic_tokenize=True`
+        unk_token (`str`, *optional*, defaults to `"[UNK]"`):
+            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
+            token instead.
+        sep_token (`str`, *optional*, defaults to `"[SEP]"`):
+            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
+            sequence classification or for a text and a question for question answering. It is also used as the last
+            token of a sequence built with special tokens.
+        pad_token (`str`, *optional*, defaults to `"[PAD]"`):
+            The token used for padding, for example when batching sequences of different lengths.
+        cls_token (`str`, *optional*, defaults to `"[CLS]"`):
+            The classifier token which is used when doing sequence classification (classification of the whole sequence
+            instead of per-token classification). It is the first token of the sequence when built with special tokens.
+        mask_token (`str`, *optional*, defaults to `"[MASK]"`):
+            The token used for masking values. This is the token used when training this model with masked language
+            modeling. This is the token which the model will try to predict.
+        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):
+            Whether or not to tokenize Chinese characters.
+
+            This should likely be deactivated for Japanese (see this
+            [issue](https://github.com/huggingface/transformers/issues/328)).
+        strip_accents (`bool`, *optional*):
+            Whether or not to strip all accents. If this option is not specified, then it will be determined by the
+            value for `lowercase` (as in the original LayoutLM).
+    """
+
+    vocab_files_names = VOCAB_FILES_NAMES
+    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP
+    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION
+    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES
 
-    This class will silently skip unless `psutil` is available. Install with `pip install psutil`.
-
-    When a stage completes, it can pass metrics dict to update with the memory metrics gathered during this stage.
-
-    Example :
-
-    ```python
-    self._memory_tracker = TrainerMemoryTracker(self.args.skip_memory_metrics)
-    self._memory_tracker.start()
-    # code ...
-    metrics = {"train_runtime": 10.5}
-    self._memory_tracker.stop_and_update_metrics(metrics)
-    ```
-
-    At the moment GPU tracking is only for `pytorch`, but can be extended to support `tensorflow`.
-
-    To understand this class' intricacies please read the documentation of [`~Trainer.log_metrics`].
-    """
-
-    # map trainer methods to metrics prefix
-    stages = {
-        "__init__": "init",
-        "train": "train",
-        "_inner_training_loop": "train",
-        "evaluate": "eval",
-        "predict": "test",
-    }
-
-    def __init__(self, skip_memory_metrics=False):
-        self.skip_memory_metrics = skip_memory_metrics
-
-        if not is_psutil_available():
-            # soft dependency on psutil
-            self.skip_memory_metrics = True
-
-        if self.skip_memory_metrics:
-            return
-
-        import psutil  # noqa
-
-        if is_torch_cuda_available():
-            import torch
-
-            self.torch = torch
-            self.gpu = {}
-        else:
-            self.torch = None
-
-        self.process = psutil.Process()
+    def __init__(
+        self,
+        vocab_file,
+        do_lower_case=True,
+        do_basic_tokenize=True,
+        never_split=None,
+        unk_token="[UNK]",
+        sep_token="[SEP]",
+        pad_token="[PAD]",
+        cls_token="[CLS]",
+        mask_token="[MASK]",
+        tokenize_chinese_chars=True,
+        strip_accents=None,
+        **kwargs,
+    ):
+        super().__init__(
+            do_lower_case=do_lower_case,
+            do_basic_tokenize=do_basic_tokenize,
+            never_split=never_split,
+            unk_token=unk_token,
+            sep_token=sep_token,
+            pad_token=pad_token,
+            cls_token=cls_token,
+            mask_token=mask_token,
+            tokenize_chinese_chars=tokenize_chinese_chars,
+            strip_accents=strip_accents,
+            **kwargs,
+        )
 
-        self.cur_stage = None
-        self.cpu = {}
-        self.init_reported = False
-
-    def derive_stage(self):
-        """derives the stage/caller name automatically"""
-        caller = inspect.currentframe().f_back.f_back.f_code.co_name
-        if caller in self.stages:
-            return self.stages[caller]
-        else:
+        if not os.path.isfile(vocab_file):
             raise ValueError(
-                f"was called from {caller}, but only expect to be called from one of {self.stages.keys()}"
+                f"Can't find a vocabulary file at path '{vocab_file}'. To load the vocabulary from a Google pretrained"
+                " model use `tokenizer = LayoutLMTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
             )
-
-    def cpu_mem_used(self):
-        """get resident set size memory for the current process"""
-        return self.process.memory_info().rss
-
-    def peak_monitor_func(self):
-        self.cpu_mem_used_peak = -1
-
-        while True:
-            self.cpu_mem_used_peak = max(self.cpu_mem_used(), self.cpu_mem_used_peak)
-
-            # can't sleep or will not catch the peak right (this comment is here on purpose)
-            # time.sleep(0.001) # 1msec
-
-            if not self.peak_monitoring:
-                break
-
-    def start(self):
-        """start tracking for the caller's stage"""
-        if self.skip_memory_metrics:
-            return
-
-        stage = self.derive_stage()
-        # deal with nested calls of eval during train - simply ignore those
-        if self.cur_stage is not None and self.cur_stage != stage:
-            return
-
-        self.cur_stage = stage
-
-        gc.collect()
-
-        if self.torch is not None:
-            self.torch.cuda.reset_peak_memory_stats()
-            self.torch.cuda.empty_cache()
-
-        # gpu
-        if self.torch is not None:
-            self.gpu_mem_used_at_start = self.torch.cuda.memory_allocated()
-
-        # cpu
-        self.cpu_mem_used_at_start = self.cpu_mem_used()
-
-        self.peak_monitoring = True
-        peak_monitor_thread = threading.Thread(target=self.peak_monitor_func)
-        peak_monitor_thread.daemon = True
-        peak_monitor_thread.start()
-
-    def stop(self, stage):
-        """stop tracking for the passed stage"""
-
-        # deal with nested calls of eval during train - simply ignore those
-        if self.cur_stage is not None and self.cur_stage != stage:
-            return
-
-        # this sends a signal to peak_monitor_func to complete its loop
-        self.peak_monitoring = False
-
-        # first ensure all objects get collected and their memory is freed
-        gc.collect()
-
-        if self.torch is not None:
-            self.torch.cuda.empty_cache()
-
-        # concepts:
-        # - alloc_delta:  the difference of allocated memory between the end and the start
-        # - peaked_delta: the difference between the peak memory and the current memory
-        # in order to know how much memory the measured code consumed one needs to sum these two
-
-        # gpu
-        if self.torch is not None:
-            self.gpu_mem_used_now = self.torch.cuda.memory_allocated()
-            self.gpu_mem_used_peak = self.torch.cuda.max_memory_allocated()
-            self.gpu[self.cur_stage] = dict(
-                begin=self.gpu_mem_used_at_start,
-                end=self.gpu_mem_used_now,
-                alloc=(self.gpu_mem_used_now - self.gpu_mem_used_at_start),
-                peaked=max(0, self.gpu_mem_used_peak - self.gpu_mem_used_now),
+        self.vocab = load_vocab(vocab_file)
+        self.ids_to_tokens = collections.OrderedDict(
+            [(ids, tok) for tok, ids in self.vocab.items()]
+        )
+        self.do_basic_tokenize = do_basic_tokenize
+        if do_basic_tokenize:
+            self.basic_tokenizer = BasicTokenizer(
+                do_lower_case=do_lower_case,
+                never_split=never_split,
+                tokenize_chinese_chars=tokenize_chinese_chars,
+                strip_accents=strip_accents,
             )
-
-        # cpu
-        self.cpu_mem_used_now = self.cpu_mem_used()
-        self.cpu[self.cur_stage] = dict(
-            begin=self.cpu_mem_used_at_start,
-            end=self.cpu_mem_used_now,
-            alloc=(self.cpu_mem_used_now - self.cpu_mem_used_at_start),
-            peaked=max(0, self.cpu_mem_used_peak - self.cpu_mem_used_now),
+        self.wordpiece_tokenizer = WordpieceTokenizer(
+            vocab=self.vocab, unk_token=self.unk_token
         )
 
-        # reset - cycle finished
-        self.cur_stage = None
+    @property
+    def do_lower_case(self):
+        return self.basic_tokenizer.do_lower_case
+
+    @property
+    def vocab_size(self):
+        return len(self.vocab)
+
+    def get_vocab(self):
+        return dict(self.vocab, **self.added_tokens_encoder)
+
+    def _tokenize(self, text):
+        split_tokens = []
+        if self.do_basic_tokenize:
+            for token in self.basic_tokenizer.tokenize(
+                text, never_split=self.all_special_tokens
+            ):
+                # If the token is part of the never_split set
+                if token in self.basic_tokenizer.never_split:
+                    split_tokens.append(token)
+                else:
+                    split_tokens += self.wordpiece_tokenizer.tokenize(token)
+        else:
+            split_tokens = self.wordpiece_tokenizer.tokenize(text)
+        return split_tokens
 
-    def update_metrics(self, stage, metrics):
-        """updates the metrics"""
-        if self.skip_memory_metrics:
-            return
-
-        # deal with nested calls of eval during train - simply ignore those
-        if self.cur_stage is not None and self.cur_stage != stage:
-            return
-
-        # since we don't have a way to return init metrics, we push them into the first of train/val/predict
-        stages = [stage]
-        if not self.init_reported:
-            stages.insert(0, "init")
-            self.init_reported = True
-
-        for stage in stages:
-            for t in ["alloc", "peaked"]:
-                if stage in self.cpu and t in self.cpu[stage]:
-                    metrics[f"{stage}_mem_cpu_{t}_delta"] = self.cpu[stage][t]
-                if (
-                    self.torch is not None
-                    and stage in self.gpu
-                    and t in self.gpu[stage]
-                ):
-                    metrics[f"{stage}_mem_gpu_{t}_delta"] = self.gpu[stage][t]
-            # if we need additional debug info, enable the following
-            # for t in ["begin", "end"]:
-            #     if stage in self.cpu and t in self.cpu[stage]:
-            #         metrics[f"{stage}_mem_cpu_{t}"] = self.cpu[stage][t]
-            #     if self.torch is not None and stage in self.gpu and t in self.gpu[stage]:
-            #         metrics[f"{stage}_mem_gpu_{t}"] = self.gpu[stage][t]
-
-        # since memory can be allocated before init, and it might be difficult to track overall
-        # memory usage, in particular for GPU, let's report memory usage at the point init was called
-        if stages[0] == "init":
-            metrics["before_init_mem_cpu"] = self.cpu["init"]["begin"]
-            if self.torch is not None:
-                metrics["before_init_mem_gpu"] = self.gpu["init"]["begin"]
-            # if we also wanted to report any additional memory allocations in between init and
-            # whatever the next stage was we could also report this:
-            # if self.cpu["init"]["end"] != self.cpu[stage]["begin"]:
-            #     metrics[f"after_init_mem_cpu_delta"] = self.cpu[stage]["begin"] - self.cpu["init"]["end"]
-            # if self.torch is not None and self.gpu["init"]["end"] != self.gpu[stage]["begin"]:
-            #     metrics[f"after_init_mem_gpu_delta"] = self.gpu[stage]["begin"] - self.gpu["init"]["end"]
-
-    def stop_and_update_metrics(self, metrics=None):
-        """combine stop and metrics update in one call for simpler code"""
-        if self.skip_memory_metrics:
-            return
-
-        stage = self.derive_stage()
-        self.stop(stage)
-
-        # init doesn't have metrics to update so we just save that data for later stages to retrieve
-        if metrics is not None:
-            self.update_metrics(stage, metrics)
+    def _convert_token_to_id(self, token):
+        """Converts a token (str) in an id using the vocab."""
+        return self.vocab.get(token, self.vocab.get(self.unk_token))
+
+    def _convert_id_to_token(self, index):
+        """Converts an index (integer) in a token (str) using the vocab."""
+        return self.ids_to_tokens.get(index, self.unk_token)
+
+    def convert_tokens_to_string(self, tokens):
+        """Converts a sequence of tokens (string) in a single string."""
+        out_string = " ".join(tokens).replace(" ##", "").strip()
+        return out_string
+
+    def build_inputs_with_special_tokens(
+        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
+    ) -> List[int]:
+        """
+        Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
+        adding special tokens. A LayoutLM sequence has the following format:
+
+        - single sequence: `[CLS] X [SEP]`
+        - pair of sequences: `[CLS] A [SEP] B [SEP]`
+
+        Args:
+            token_ids_0 (`List[int]`):
+                List of IDs to which the special tokens will be added.
+            token_ids_1 (`List[int]`, *optional*):
+                Optional second list of IDs for sequence pairs.
+
+        Returns:
+            `List[int]`: List of [input IDs](../glossary#input-ids) with the appropriate special tokens.
+        """
+        if token_ids_1 is None:
+            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]
+        cls = [self.cls_token_id]
+        sep = [self.sep_token_id]
+        return cls + token_ids_0 + sep + token_ids_1 + sep
 
+    def get_special_tokens_mask(
+        self,
+        token_ids_0: List[int],
+        token_ids_1: Optional[List[int]] = None,
+        already_has_special_tokens: bool = False,
+    ) -> List[int]:
+        """
+        Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
+        special tokens using the tokenizer `prepare_for_model` method.
+
+        Args:
+            token_ids_0 (`List[int]`):
+                List of IDs.
+            token_ids_1 (`List[int]`, *optional*):
+                Optional second list of IDs for sequence pairs.
+            already_has_special_tokens (`bool`, *optional*, defaults to `False`):
+                Whether or not the token list is already formatted with special tokens for the model.
+
+        Returns:
+            `List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.
+        """
+
+        if already_has_special_tokens:
+            return super().get_special_tokens_mask(
+                token_ids_0=token_ids_0,
+                token_ids_1=token_ids_1,
+                already_has_special_tokens=True,
+            )
 
-def has_length(dataset):
-    """
-    Checks if the dataset implements __len__() and it doesn't raise an error
-    """
-    try:
-        return len(dataset) is not None
-    except TypeError:
-        # TypeError: len() of unsized object
-        return False
+        if token_ids_1 is not None:
+            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]
+        return [1] + ([0] * len(token_ids_0)) + [1]
+
+    def create_token_type_ids_from_sequences(
+        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None
+    ) -> List[int]:
+        """
+        Create a mask from the two sequences passed to be used in a sequence-pair classification task. A LayoutLM
+        sequence pair mask has the following format:
+
+        ```
+        0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1
+        | first sequence    | second sequence |
+        ```
+
+        If `token_ids_1` is `None`, this method only returns the first portion of the mask (0s).
+
+        Args:
+            token_ids_0 (`List[int]`):
+                List of IDs.
+            token_ids_1 (`List[int]`, *optional*):
+                Optional second list of IDs for sequence pairs.
+
+        Returns:
+            `List[int]`: List of [token type IDs](../glossary#token-type-ids) according to the given sequence(s).
+        """
+        sep = [self.sep_token_id]
+        cls = [self.cls_token_id]
+        if token_ids_1 is None:
+            return len(cls + token_ids_0 + sep) * [0]
+        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]
+
+    def save_vocabulary(
+        self, save_directory: str, filename_prefix: Optional[str] = None
+    ) -> Tuple[str]:
+        index = 0
+        if os.path.isdir(save_directory):
+            vocab_file = os.path.join(
+                save_directory,
+                (filename_prefix + "-" if filename_prefix else "")
+                + VOCAB_FILES_NAMES["vocab_file"],
+            )
+        else:
+            vocab_file = (
+                filename_prefix + "-" if filename_prefix else ""
+            ) + save_directory
+        with open(vocab_file, "w", encoding="utf-8") as writer:
+            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):
+                if index != token_index:
+                    logger.warning(
+                        f"Saving vocabulary to {vocab_file}: vocabulary indices are not consecutive."
+                        " Please check that the vocabulary is not corrupted!"
+                    )
+                    index = token_index
+                writer.write(token + "\n")
+                index += 1
+        return (vocab_file,)
 
 
-def denumpify_detensorize(metrics):
+# Copied from transformers.models.bert.tokenization_bert.BasicTokenizer
+class BasicTokenizer(object):
     """
-    Recursively calls `.item()` on the element of the dictionary passed
-    """
-    if isinstance(metrics, (list, tuple)):
-        return type(metrics)(denumpify_detensorize(m) for m in metrics)
-    elif isinstance(metrics, dict):
-        return type(metrics)({k: denumpify_detensorize(v) for k, v in metrics.items()})
-    elif isinstance(metrics, np.generic):
-        return metrics.item()
-    elif (
-        is_torch_available()
-        and isinstance(metrics, torch.Tensor)
-        and metrics.numel() == 1
-    ):
-        return metrics.item()
-    return metrics
+    Constructs a BasicTokenizer that will run basic tokenization (punctuation splitting, lower casing, etc.).
 
-
-def number_of_arguments(func):
-    """
-    Return the number of arguments of the passed function, even if it's a partial function.
-    """
-    if isinstance(func, functools.partial):
-        total_args = len(inspect.signature(func.func).parameters)
-        return total_args - len(func.args) - len(func.keywords)
-    return len(inspect.signature(func).parameters)
-
-
-class ShardedDDPOption(ExplicitEnum):
-    SIMPLE = "simple"
-    ZERO_DP_2 = "zero_dp_2"
-    ZERO_DP_3 = "zero_dp_3"
-    OFFLOAD = "offload"
-    AUTO_WRAP = "auto_wrap"
-
-
-def find_executable_batch_size(
-    function: callable = None,
-    starting_batch_size: int = 128,
-    auto_find_batch_size: bool = False,
-):
-    """
     Args:
-    A basic decorator that will try to execute `function`. If it fails from exceptions related to out-of-memory or
-    CUDNN, the batch size is cut in half and passed to `function` `function` must take in a `batch_size` parameter as
-    its first argument.
-        function (`callable`, *optional*)
-            A function to wrap
-        starting_batch_size (`int`, *optional*)
-            The batch size to try and fit into memory
-        auto_find_batch_size (`bool`, *optional*)
-            If False, will just execute `function`
+        do_lower_case (`bool`, *optional*, defaults to `True`):
+            Whether or not to lowercase the input when tokenizing.
+        never_split (`Iterable`, *optional*):
+            Collection of tokens which will never be split during tokenization. Only has an effect when
+            `do_basic_tokenize=True`
+        tokenize_chinese_chars (`bool`, *optional*, defaults to `True`):
+            Whether or not to tokenize Chinese characters.
+
+            This should likely be deactivated for Japanese (see this
+            [issue](https://github.com/huggingface/transformers/issues/328)).
+        strip_accents (`bool`, *optional*):
+            Whether or not to strip all accents. If this option is not specified, then it will be determined by the
+            value for `lowercase` (as in the original BERT).
     """
-    if function is None:
-        return functools.partial(
-            find_executable_batch_size,
-            starting_batch_size=starting_batch_size,
-            auto_find_batch_size=auto_find_batch_size,
-        )
-
-    if auto_find_batch_size:
-        requires_backends(find_executable_batch_size, "accelerate")
-        import accelerate.memory_utils as mem_utils
 
-        return mem_utils.find_executable_batch_size(
-            function=function, starting_batch_size=starting_batch_size
+    def __init__(
+        self,
+        do_lower_case=True,
+        never_split=None,
+        tokenize_chinese_chars=True,
+        strip_accents=None,
+    ):
+        if never_split is None:
+            never_split = []
+        self.do_lower_case = do_lower_case
+        self.never_split = set(never_split)
+        self.tokenize_chinese_chars = tokenize_chinese_chars
+        self.strip_accents = strip_accents
+
+    def tokenize(self, text, never_split=None):
+        """
+        Basic Tokenization of a piece of text. Split on "white spaces" only, for sub-word tokenization, see
+        WordPieceTokenizer.
+
+        Args:
+            never_split (`List[str]`, *optional*)
+                Kept for backward compatibility purposes. Now implemented directly at the base class level (see
+                [`PreTrainedTokenizer.tokenize`]) List of token not to split.
+        """
+        # union() returns a new set by concatenating the two sets.
+        never_split = (
+            self.never_split.union(set(never_split))
+            if never_split
+            else self.never_split
         )
+        text = self._clean_text(text)
 
-    return functools.partial(function, batch_size=starting_batch_size)
-
-
-class FSDPOption(ExplicitEnum):
-    FULL_SHARD = "full_shard"
-    SHARD_GRAD_OP = "shard_grad_op"
-    NO_SHARD = "no_shard"
-    OFFLOAD = "offload"
-    AUTO_WRAP = "auto_wrap"
-
+        # This was added on November 1st, 2018 for the multilingual and Chinese
+        # models. This is also applied to the English models now, but it doesn't
+        # matter since the English models were not trained on any Chinese data
+        # and generally don't have any Chinese data in them (there are Chinese
+        # characters in the vocabulary because Wikipedia does have some Chinese
+        # words in the English Wikipedia.).
+        if self.tokenize_chinese_chars:
+            text = self._tokenize_chinese_chars(text)
+        orig_tokens = whitespace_tokenize(text)
+        split_tokens = []
+        for token in orig_tokens:
+            if token not in never_split:
+                if self.do_lower_case:
+                    token = token.lower()
+                    if self.strip_accents is not False:
+                        token = self._run_strip_accents(token)
+                elif self.strip_accents:
+                    token = self._run_strip_accents(token)
+            split_tokens.extend(self._run_split_on_punc(token, never_split))
+
+        output_tokens = whitespace_tokenize(" ".join(split_tokens))
+        return output_tokens
+
+    def _run_strip_accents(self, text):
+        """Strips accents from a piece of text."""
+        text = unicodedata.normalize("NFD", text)
+        output = []
+        for char in text:
+            cat = unicodedata.category(char)
+            if cat == "Mn":
+                continue
+            output.append(char)
+        return "".join(output)
+
+    def _run_split_on_punc(self, text, never_split=None):
+        """Splits punctuation on a piece of text."""
+        if never_split is not None and text in never_split:
+            return [text]
+        chars = list(text)
+        i = 0
+        start_new_word = True
+        output = []
+        while i < len(chars):
+            char = chars[i]
+            if _is_punctuation(char):
+                output.append([char])
+                start_new_word = True
+            else:
+                if start_new_word:
+                    output.append([])
+                start_new_word = False
+                output[-1].append(char)
+            i += 1
+
+        return ["".join(x) for x in output]
+
+    def _tokenize_chinese_chars(self, text):
+        """Adds whitespace around any CJK character."""
+        output = []
+        for char in text:
+            cp = ord(char)
+            if self._is_chinese_char(cp):
+                output.append(" ")
+                output.append(char)
+                output.append(" ")
+            else:
+                output.append(char)
+        return "".join(output)
+
+    def _is_chinese_char(self, cp):
+        """Checks whether CP is the codepoint of a CJK character."""
+        # This defines a "chinese character" as anything in the CJK Unicode block:
+        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
+        #
+        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
+        # despite its name. The modern Korean Hangul alphabet is a different block,
+        # as is Japanese Hiragana and Katakana. Those alphabets are used to write
+        # space-separated words, so they are not treated specially and handled
+        # like the all of the other languages.
+        if (
+            (cp >= 0x4E00 and cp <= 0x9FFF)
+            or (cp >= 0x3400 and cp <= 0x4DBF)  #
+            or (cp >= 0x20000 and cp <= 0x2A6DF)  #
+            or (cp >= 0x2A700 and cp <= 0x2B73F)  #
+            or (cp >= 0x2B740 and cp <= 0x2B81F)  #
+            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #
+            or (cp >= 0xF900 and cp <= 0xFAFF)
+            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #
+        ):  #
+            return True
 
-class RemoveColumnsCollator:
-    """Wrap the data collator to remove unused columns before they are passed to the collator."""
+        return False
 
-    def __init__(
-        self,
-        data_collator,
-        signature_columns,
-        logger=None,
-        model_name: Optional[str] = None,
-        description: Optional[str] = None,
-    ):
-        self.data_collator = data_collator
-        self.signature_columns = signature_columns
-        self.logger = logger
-        self.description = description
-        self.model_name = model_name
-        self.message_logged = False
-
-    def _remove_columns(self, feature: dict) -> dict:
-        if not isinstance(feature, dict):
-            return feature
-        if not self.message_logged and self.logger and self.model_name:
-            ignored_columns = list(set(feature.keys()) - set(self.signature_columns))
-            if len(ignored_columns) > 0:
-                dset_description = (
-                    "" if self.description is None else f"in the {self.description} set"
-                )
-                self.logger.info(
-                    f"The following columns {dset_description} don't have a corresponding argument in "
-                    f"`{self.model_name}.forward` and have been ignored: {', '.join(ignored_columns)}."
-                    f" If {', '.join(ignored_columns)} are not expected by `{self.model_name}.forward`, "
-                    " you can safely ignore this message."
-                )
-                self.message_logged = True
-        return {k: v for k, v in feature.items() if k in self.signature_columns}
-
-    def __call__(self, features: List[dict]):
-        features = [self._remove_columns(feature) for feature in features]
-        return self.data_collator(features)
+    def _clean_text(self, text):
+        """Performs invalid character removal and whitespace cleanup on text."""
+        output = []
+        for char in text:
+            cp = ord(char)
+            if cp == 0 or cp == 0xFFFD or _is_control(char):
+                continue
+            if _is_whitespace(char):
+                output.append(" ")
+            else:
+                output.append(char)
+        return "".join(output)
+
+
+# Copied from transformers.models.bert.tokenization_bert.WordpieceTokenizer
+class WordpieceTokenizer(object):
+    """Runs WordPiece tokenization."""
+
+    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):
+        self.vocab = vocab
+        self.unk_token = unk_token
+        self.max_input_chars_per_word = max_input_chars_per_word
+
+    def tokenize(self, text):
+        """
+        Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform
+        tokenization using the given vocabulary.
+
+        For example, `input = "unaffable"` wil return as output `["un", "##aff", "##able"]`.
+
+        Args:
+            text: A single token or whitespace separated tokens. This should have
+                already been passed through *BasicTokenizer*.
+
+        Returns:
+            A list of wordpiece tokens.
+        """
+
+        output_tokens = []
+        for token in whitespace_tokenize(text):
+            chars = list(token)
+            if len(chars) > self.max_input_chars_per_word:
+                output_tokens.append(self.unk_token)
+                continue
+
+            is_bad = False
+            start = 0
+            sub_tokens = []
+            while start < len(chars):
+                end = len(chars)
+                cur_substr = None
+                while start < end:
+                    substr = "".join(chars[start:end])
+                    if start > 0:
+                        substr = "##" + substr
+                    if substr in self.vocab:
+                        cur_substr = substr
+                        break
+                    end -= 1
+                if cur_substr is None:
+                    is_bad = True
+                    break
+                sub_tokens.append(cur_substr)
+                start = end
+
+            if is_bad:
+                output_tokens.append(self.unk_token)
+            else:
+                output_tokens.extend(sub_tokens)
+        return output_tokens
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `xs_transformers-1.0.0/xs_transformers/training_args.py` & `xs_transformers-1.0.1/xs_transformers/models/realm/modeling_realm.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,1960 +1,2136 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# coding=utf-8
+# Copyright 2022 The REALM authors and The HuggingFace Inc. team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+""" PyTorch REALM model."""
 
-import contextlib
-import json
 import math
 import os
-import warnings
-from dataclasses import asdict, dataclass, field, fields
-from datetime import timedelta
-from enum import Enum
-from pathlib import Path
-from typing import Any, Dict, List, Optional, Union
-
-from packaging import version
-
-from .debug_utils import DebugOption
-from .trainer_utils import (
-    EvaluationStrategy,
-    FSDPOption,
-    HubStrategy,
-    IntervalStrategy,
-    SchedulerType,
-    ShardedDDPOption,
+from dataclasses import dataclass
+from typing import Optional, Tuple, Union
+
+import torch
+from torch import nn
+from torch.nn import CrossEntropyLoss
+
+from ...activations import ACT2FN
+from ...modeling_outputs import (
+    BaseModelOutputWithPastAndCrossAttentions,
+    BaseModelOutputWithPoolingAndCrossAttentions,
+    MaskedLMOutput,
+    ModelOutput,
+)
+from ...modeling_utils import PreTrainedModel
+from ...pytorch_utils import (
+    apply_chunking_to_forward,
+    find_pruneable_heads_and_indices,
+    prune_linear_layer,
 )
-from .utils import (
-    ExplicitEnum,
-    cached_property,
-    ccl_version,
-    get_full_repo_name,
-    is_accelerate_available,
-    is_psutil_available,
-    is_sagemaker_dp_enabled,
-    is_sagemaker_mp_enabled,
-    is_torch_available,
-    is_torch_bf16_cpu_available,
-    is_torch_bf16_gpu_available,
-    is_torch_tf32_available,
-    is_torch_tpu_available,
+from ...utils import (
+    add_start_docstrings,
+    add_start_docstrings_to_model_forward,
     logging,
-    requires_backends,
-    torch_required,
+    replace_return_docstrings,
 )
+from .configuration_realm import RealmConfig
+
+logger = logging.get_logger(__name__)
+_EMBEDDER_CHECKPOINT_FOR_DOC = "google/realm-cc-news-pretrained-embedder"
+_ENCODER_CHECKPOINT_FOR_DOC = "google/realm-cc-news-pretrained-encoder"
+_SCORER_CHECKPOINT_FOR_DOC = "google/realm-cc-news-pretrained-scorer"
+_CONFIG_FOR_DOC = "RealmConfig"
+_TOKENIZER_FOR_DOC = "RealmTokenizer"
+
+REALM_PRETRAINED_MODEL_ARCHIVE_LIST = [
+    "google/realm-cc-news-pretrained-embedder",
+    "google/realm-cc-news-pretrained-encoder",
+    "google/realm-cc-news-pretrained-scorer",
+    "google/realm-cc-news-pretrained-openqa",
+    "google/realm-orqa-nq-openqa",
+    "google/realm-orqa-nq-reader",
+    "google/realm-orqa-wq-openqa",
+    "google/realm-orqa-wq-reader",
+    # See all REALM models at https://huggingface.co/models?filter=realm
+]
+
+
+def load_tf_weights_in_realm(model, config, tf_checkpoint_path):
+    """Load tf checkpoints in a pytorch model."""
+    try:
+        import re
+
+        import numpy as np
+        import tensorflow as tf
+    except ImportError:
+        logger.error(
+            "Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see "
+            "https://www.tensorflow.org/install/ for installation instructions."
+        )
+        raise
+    tf_path = os.path.abspath(tf_checkpoint_path)
+    logger.info(f"Converting TensorFlow checkpoint from {tf_path}")
+    # Load weights from TF model
+    init_vars = tf.train.list_variables(tf_path)
+    names = []
+    arrays = []
+
+    for name, shape in init_vars:
+        logger.info(f"Loading TF weight {name} with shape {shape}")
+        array = tf.train.load_variable(tf_path, name)
+        names.append(name)
+        arrays.append(array)
 
-if is_torch_available():
-    import torch
-    import torch.distributed as dist
+    for name, array in zip(names, arrays):
+        if isinstance(model, RealmReader) and "reader" not in name:
+            logger.info(
+                f"Skipping {name} as it is not {model.__class__.__name__}'s parameter"
+            )
+            continue
 
-if is_torch_tpu_available(check_device=False):
-    import torch_xla.core.xla_model as xm
+        # For pretrained openqa reader
+        if (name.startswith("bert") or name.startswith("cls")) and isinstance(
+            model, RealmForOpenQA
+        ):
+            name = name.replace("bert/", "reader/realm/")
+            name = name.replace("cls/", "reader/cls/")
 
+        # For pretrained encoder
+        if (name.startswith("bert") or name.startswith("cls")) and isinstance(
+            model, RealmKnowledgeAugEncoder
+        ):
+            name = name.replace("bert/", "realm/")
+
+        # For finetuned reader
+        if name.startswith("reader"):
+            reader_prefix = "" if isinstance(model, RealmReader) else "reader/"
+            name = name.replace("reader/module/bert/", f"{reader_prefix}realm/")
+            name = name.replace("reader/module/cls/", f"{reader_prefix}cls/")
+            name = name.replace(
+                "reader/dense/", f"{reader_prefix}qa_outputs/dense_intermediate/"
+            )
+            name = name.replace(
+                "reader/dense_1/", f"{reader_prefix}qa_outputs/dense_output/"
+            )
+            name = name.replace(
+                "reader/layer_normalization",
+                f"{reader_prefix}qa_outputs/layer_normalization",
+            )
+
+        # For embedder and scorer
+        if name.startswith("module/module/module/"):  # finetuned
+            embedder_prefix = "" if isinstance(model, RealmEmbedder) else "embedder/"
+            name = name.replace(
+                "module/module/module/module/bert/", f"{embedder_prefix}realm/"
+            )
+            name = name.replace(
+                "module/module/module/LayerNorm/", f"{embedder_prefix}cls/LayerNorm/"
+            )
+            name = name.replace(
+                "module/module/module/dense/", f"{embedder_prefix}cls/dense/"
+            )
+            name = name.replace(
+                "module/module/module/module/cls/predictions/",
+                f"{embedder_prefix}cls/predictions/",
+            )
+            name = name.replace(
+                "module/module/module/bert/", f"{embedder_prefix}realm/"
+            )
+            name = name.replace(
+                "module/module/module/cls/predictions/",
+                f"{embedder_prefix}cls/predictions/",
+            )
+        elif name.startswith("module/module/"):  # pretrained
+            embedder_prefix = "" if isinstance(model, RealmEmbedder) else "embedder/"
+            name = name.replace(
+                "module/module/LayerNorm/", f"{embedder_prefix}cls/LayerNorm/"
+            )
+            name = name.replace("module/module/dense/", f"{embedder_prefix}cls/dense/")
+
+        name = name.split("/")
+        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v
+        # which are not required for using pretrained model
+        if any(
+            n
+            in [
+                "adam_v",
+                "adam_m",
+                "AdamWeightDecayOptimizer",
+                "AdamWeightDecayOptimizer_1",
+                "global_step",
+            ]
+            for n in name
+        ):
+            logger.info(f"Skipping {'/'.join(name)}")
+            continue
+        pointer = model
+        for m_name in name:
+            if re.fullmatch(r"[A-Za-z]+_\d+", m_name):
+                scope_names = re.split(r"_(\d+)", m_name)
+            else:
+                scope_names = [m_name]
+            if scope_names[0] == "kernel" or scope_names[0] == "gamma":
+                pointer = getattr(pointer, "weight")
+            elif scope_names[0] == "output_bias" or scope_names[0] == "beta":
+                pointer = getattr(pointer, "bias")
+            else:
+                try:
+                    pointer = getattr(pointer, scope_names[0])
+                except AttributeError:
+                    logger.info(f"Skipping {'/'.join(name)}")
+                    continue
+            if len(scope_names) >= 2:
+                num = int(scope_names[1])
+                pointer = pointer[num]
+        if m_name[-11:] == "_embeddings":
+            pointer = getattr(pointer, "weight")
+        elif m_name == "kernel":
+            array = np.transpose(array)
+        try:
+            assert (
+                pointer.shape == array.shape
+            ), f"Pointer shape {pointer.shape} and array shape {array.shape} mismatched"
+        except AssertionError as e:
+            e.args += (pointer.shape, array.shape)
+            raise
+        logger.info(f"Initialize PyTorch weight {name}")
+        pointer.data = torch.from_numpy(array)
+    return model
+
+
+# Copied from transformers.models.bert.modeling_bert.BertEmbeddings with Bert->Realm
+class RealmEmbeddings(nn.Module):
+    """Construct the embeddings from word, position and token_type embeddings."""
+
+    def __init__(self, config):
+        super().__init__()
+        self.word_embeddings = nn.Embedding(
+            config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id
+        )
+        self.position_embeddings = nn.Embedding(
+            config.max_position_embeddings, config.hidden_size
+        )
+        self.token_type_embeddings = nn.Embedding(
+            config.type_vocab_size, config.hidden_size
+        )
 
-if is_sagemaker_mp_enabled():
-    import smdistributed.modelparallel.torch as smp
+        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
+        # any TensorFlow checkpoint file
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized
+        self.position_embedding_type = getattr(
+            config, "position_embedding_type", "absolute"
+        )
+        self.register_buffer(
+            "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1))
+        )
+        self.register_buffer(
+            "token_type_ids",
+            torch.zeros(self.position_ids.size(), dtype=torch.long),
+            persistent=False,
+        )
 
-    smp.init()
+    def forward(
+        self,
+        input_ids: Optional[torch.LongTensor] = None,
+        token_type_ids: Optional[torch.LongTensor] = None,
+        position_ids: Optional[torch.LongTensor] = None,
+        inputs_embeds: Optional[torch.FloatTensor] = None,
+        past_key_values_length: int = 0,
+    ) -> torch.Tensor:
+        if input_ids is not None:
+            input_shape = input_ids.size()
+        else:
+            input_shape = inputs_embeds.size()[:-1]
 
+        seq_length = input_shape[1]
 
-logger = logging.get_logger(__name__)
-log_levels = logging.get_log_levels_dict().copy()
-trainer_log_levels = dict(**log_levels, passive=-1)
+        if position_ids is None:
+            position_ids = self.position_ids[
+                :, past_key_values_length : seq_length + past_key_values_length
+            ]
 
+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs
+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves
+        # issue #5664
+        if token_type_ids is None:
+            if hasattr(self, "token_type_ids"):
+                buffered_token_type_ids = self.token_type_ids[:, :seq_length]
+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(
+                    input_shape[0], seq_length
+                )
+                token_type_ids = buffered_token_type_ids_expanded
+            else:
+                token_type_ids = torch.zeros(
+                    input_shape, dtype=torch.long, device=self.position_ids.device
+                )
 
-def default_logdir() -> str:
-    """
-    Same default as PyTorch
+        if inputs_embeds is None:
+            inputs_embeds = self.word_embeddings(input_ids)
+        token_type_embeddings = self.token_type_embeddings(token_type_ids)
+
+        embeddings = inputs_embeds + token_type_embeddings
+        if self.position_embedding_type == "absolute":
+            position_embeddings = self.position_embeddings(position_ids)
+            embeddings += position_embeddings
+        embeddings = self.LayerNorm(embeddings)
+        embeddings = self.dropout(embeddings)
+        return embeddings
+
+
+# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Realm
+class RealmSelfAttention(nn.Module):
+    def __init__(self, config, position_embedding_type=None):
+        super().__init__()
+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(
+            config, "embedding_size"
+        ):
+            raise ValueError(
+                f"The hidden size ({config.hidden_size}) is not a multiple of the number of attention "
+                f"heads ({config.num_attention_heads})"
+            )
+
+        self.num_attention_heads = config.num_attention_heads
+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
+        self.all_head_size = self.num_attention_heads * self.attention_head_size
+
+        self.query = nn.Linear(config.hidden_size, self.all_head_size)
+        self.key = nn.Linear(config.hidden_size, self.all_head_size)
+        self.value = nn.Linear(config.hidden_size, self.all_head_size)
+
+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)
+        self.position_embedding_type = position_embedding_type or getattr(
+            config, "position_embedding_type", "absolute"
+        )
+        if (
+            self.position_embedding_type == "relative_key"
+            or self.position_embedding_type == "relative_key_query"
+        ):
+            self.max_position_embeddings = config.max_position_embeddings
+            self.distance_embedding = nn.Embedding(
+                2 * config.max_position_embeddings - 1, self.attention_head_size
+            )
+
+        self.is_decoder = config.is_decoder
+
+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:
+        new_x_shape = x.size()[:-1] + (
+            self.num_attention_heads,
+            self.attention_head_size,
+        )
+        x = x.view(new_x_shape)
+        return x.permute(0, 2, 1, 3)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        output_attentions: Optional[bool] = False,
+    ) -> Tuple[torch.Tensor]:
+        mixed_query_layer = self.query(hidden_states)
+
+        # If this is instantiated as a cross-attention module, the keys
+        # and values come from an encoder; the attention mask needs to be
+        # such that the encoder's padding tokens are not attended to.
+        is_cross_attention = encoder_hidden_states is not None
+
+        if is_cross_attention and past_key_value is not None:
+            # reuse k,v, cross_attentions
+            key_layer = past_key_value[0]
+            value_layer = past_key_value[1]
+            attention_mask = encoder_attention_mask
+        elif is_cross_attention:
+            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))
+            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))
+            attention_mask = encoder_attention_mask
+        elif past_key_value is not None:
+            key_layer = self.transpose_for_scores(self.key(hidden_states))
+            value_layer = self.transpose_for_scores(self.value(hidden_states))
+            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)
+            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)
+        else:
+            key_layer = self.transpose_for_scores(self.key(hidden_states))
+            value_layer = self.transpose_for_scores(self.value(hidden_states))
+
+        query_layer = self.transpose_for_scores(mixed_query_layer)
+
+        if self.is_decoder:
+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.
+            # Further calls to cross_attention layer can then reuse all cross-attention
+            # key/value_states (first "if" case)
+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of
+            # all previous decoder key/value_states. Further calls to uni-directional self-attention
+            # can concat previous decoder key/value_states to current projected key/value_states (third "elif" case)
+            # if encoder bi-directional self-attention `past_key_value` is always `None`
+            past_key_value = (key_layer, value_layer)
+
+        # Take the dot product between "query" and "key" to get the raw attention scores.
+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
+
+        if (
+            self.position_embedding_type == "relative_key"
+            or self.position_embedding_type == "relative_key_query"
+        ):
+            seq_length = hidden_states.size()[1]
+            position_ids_l = torch.arange(
+                seq_length, dtype=torch.long, device=hidden_states.device
+            ).view(-1, 1)
+            position_ids_r = torch.arange(
+                seq_length, dtype=torch.long, device=hidden_states.device
+            ).view(1, -1)
+            distance = position_ids_l - position_ids_r
+            positional_embedding = self.distance_embedding(
+                distance + self.max_position_embeddings - 1
+            )
+            positional_embedding = positional_embedding.to(
+                dtype=query_layer.dtype
+            )  # fp16 compatibility
+
+            if self.position_embedding_type == "relative_key":
+                relative_position_scores = torch.einsum(
+                    "bhld,lrd->bhlr", query_layer, positional_embedding
+                )
+                attention_scores = attention_scores + relative_position_scores
+            elif self.position_embedding_type == "relative_key_query":
+                relative_position_scores_query = torch.einsum(
+                    "bhld,lrd->bhlr", query_layer, positional_embedding
+                )
+                relative_position_scores_key = torch.einsum(
+                    "bhrd,lrd->bhlr", key_layer, positional_embedding
+                )
+                attention_scores = (
+                    attention_scores
+                    + relative_position_scores_query
+                    + relative_position_scores_key
+                )
+
+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)
+        if attention_mask is not None:
+            # Apply the attention mask is (precomputed for all layers in RealmModel forward() function)
+            attention_scores = attention_scores + attention_mask
+
+        # Normalize the attention scores to probabilities.
+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+
+        # This is actually dropping out entire tokens to attend to, which might
+        # seem a bit unusual, but is taken from the original Transformer paper.
+        attention_probs = self.dropout(attention_probs)
+
+        # Mask heads if we want to
+        if head_mask is not None:
+            attention_probs = attention_probs * head_mask
+
+        context_layer = torch.matmul(attention_probs, value_layer)
+
+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()
+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
+        context_layer = context_layer.view(new_context_layer_shape)
+
+        outputs = (
+            (context_layer, attention_probs) if output_attentions else (context_layer,)
+        )
+
+        if self.is_decoder:
+            outputs = outputs + (past_key_value,)
+        return outputs
+
+
+# Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Realm
+class RealmSelfOutput(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+
+    def forward(
+        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor
+    ) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.dropout(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        return hidden_states
+
+
+# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Realm
+class RealmAttention(nn.Module):
+    def __init__(self, config, position_embedding_type=None):
+        super().__init__()
+        self.self = RealmSelfAttention(
+            config, position_embedding_type=position_embedding_type
+        )
+        self.output = RealmSelfOutput(config)
+        self.pruned_heads = set()
+
+    def prune_heads(self, heads):
+        if len(heads) == 0:
+            return
+        heads, index = find_pruneable_heads_and_indices(
+            heads,
+            self.self.num_attention_heads,
+            self.self.attention_head_size,
+            self.pruned_heads,
+        )
+
+        # Prune linear layers
+        self.self.query = prune_linear_layer(self.self.query, index)
+        self.self.key = prune_linear_layer(self.self.key, index)
+        self.self.value = prune_linear_layer(self.self.value, index)
+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)
+
+        # Update hyper params and store pruned heads
+        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)
+        self.self.all_head_size = (
+            self.self.attention_head_size * self.self.num_attention_heads
+        )
+        self.pruned_heads = self.pruned_heads.union(heads)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        output_attentions: Optional[bool] = False,
+    ) -> Tuple[torch.Tensor]:
+        self_outputs = self.self(
+            hidden_states,
+            attention_mask,
+            head_mask,
+            encoder_hidden_states,
+            encoder_attention_mask,
+            past_key_value,
+            output_attentions,
+        )
+        attention_output = self.output(self_outputs[0], hidden_states)
+        outputs = (attention_output,) + self_outputs[
+            1:
+        ]  # add attentions if we output them
+        return outputs
+
+
+# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Realm
+class RealmIntermediate(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
+        if isinstance(config.hidden_act, str):
+            self.intermediate_act_fn = ACT2FN[config.hidden_act]
+        else:
+            self.intermediate_act_fn = config.hidden_act
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.intermediate_act_fn(hidden_states)
+        return hidden_states
+
+
+# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Realm
+class RealmOutput(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
+        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+
+    def forward(
+        self, hidden_states: torch.Tensor, input_tensor: torch.Tensor
+    ) -> torch.Tensor:
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.dropout(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        return hidden_states
+
+
+# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Realm
+class RealmLayer(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.chunk_size_feed_forward = config.chunk_size_feed_forward
+        self.seq_len_dim = 1
+        self.attention = RealmAttention(config)
+        self.is_decoder = config.is_decoder
+        self.add_cross_attention = config.add_cross_attention
+        if self.add_cross_attention:
+            if not self.is_decoder:
+                raise ValueError(
+                    f"{self} should be used as a decoder model if cross attention is added"
+                )
+            self.crossattention = RealmAttention(
+                config, position_embedding_type="absolute"
+            )
+        self.intermediate = RealmIntermediate(config)
+        self.output = RealmOutput(config)
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        output_attentions: Optional[bool] = False,
+    ) -> Tuple[torch.Tensor]:
+        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
+        self_attn_past_key_value = (
+            past_key_value[:2] if past_key_value is not None else None
+        )
+        self_attention_outputs = self.attention(
+            hidden_states,
+            attention_mask,
+            head_mask,
+            output_attentions=output_attentions,
+            past_key_value=self_attn_past_key_value,
+        )
+        attention_output = self_attention_outputs[0]
+
+        # if decoder, the last output is tuple of self-attn cache
+        if self.is_decoder:
+            outputs = self_attention_outputs[1:-1]
+            present_key_value = self_attention_outputs[-1]
+        else:
+            outputs = self_attention_outputs[
+                1:
+            ]  # add self attentions if we output attention weights
+
+        cross_attn_present_key_value = None
+        if self.is_decoder and encoder_hidden_states is not None:
+            if not hasattr(self, "crossattention"):
+                raise ValueError(
+                    f"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers"
+                    " by setting `config.add_cross_attention=True`"
+                )
+
+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple
+            cross_attn_past_key_value = (
+                past_key_value[-2:] if past_key_value is not None else None
+            )
+            cross_attention_outputs = self.crossattention(
+                attention_output,
+                attention_mask,
+                head_mask,
+                encoder_hidden_states,
+                encoder_attention_mask,
+                cross_attn_past_key_value,
+                output_attentions,
+            )
+            attention_output = cross_attention_outputs[0]
+            outputs = (
+                outputs + cross_attention_outputs[1:-1]
+            )  # add cross attentions if we output attention weights
+
+            # add cross-attn cache to positions 3,4 of present_key_value tuple
+            cross_attn_present_key_value = cross_attention_outputs[-1]
+            present_key_value = present_key_value + cross_attn_present_key_value
+
+        layer_output = apply_chunking_to_forward(
+            self.feed_forward_chunk,
+            self.chunk_size_feed_forward,
+            self.seq_len_dim,
+            attention_output,
+        )
+        outputs = (layer_output,) + outputs
+
+        # if decoder, return the attn key/values as the last output
+        if self.is_decoder:
+            outputs = outputs + (present_key_value,)
+
+        return outputs
+
+    def feed_forward_chunk(self, attention_output):
+        intermediate_output = self.intermediate(attention_output)
+        layer_output = self.output(intermediate_output, attention_output)
+        return layer_output
+
+
+# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Realm
+class RealmEncoder(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.config = config
+        self.layer = nn.ModuleList(
+            [RealmLayer(config) for _ in range(config.num_hidden_layers)]
+        )
+        self.gradient_checkpointing = False
+
+    def forward(
+        self,
+        hidden_states: torch.Tensor,
+        attention_mask: Optional[torch.FloatTensor] = None,
+        head_mask: Optional[torch.FloatTensor] = None,
+        encoder_hidden_states: Optional[torch.FloatTensor] = None,
+        encoder_attention_mask: Optional[torch.FloatTensor] = None,
+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
+        use_cache: Optional[bool] = None,
+        output_attentions: Optional[bool] = False,
+        output_hidden_states: Optional[bool] = False,
+        return_dict: Optional[bool] = True,
+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:
+        all_hidden_states = () if output_hidden_states else None
+        all_self_attentions = () if output_attentions else None
+        all_cross_attentions = (
+            () if output_attentions and self.config.add_cross_attention else None
+        )
+
+        next_decoder_cache = () if use_cache else None
+        for i, layer_module in enumerate(self.layer):
+            if output_hidden_states:
+                all_hidden_states = all_hidden_states + (hidden_states,)
+
+            layer_head_mask = head_mask[i] if head_mask is not None else None
+            past_key_value = past_key_values[i] if past_key_values is not None else None
+
+            if self.gradient_checkpointing and self.training:
+                if use_cache:
+                    logger.warning(
+                        "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`..."
+                    )
+                    use_cache = False
+
+                def create_custom_forward(module):
+                    def custom_forward(*inputs):
+                        return module(*inputs, past_key_value, output_attentions)
+
+                    return custom_forward
+
+                layer_outputs = torch.utils.checkpoint.checkpoint(
+                    create_custom_forward(layer_module),
+                    hidden_states,
+                    attention_mask,
+                    layer_head_mask,
+                    encoder_hidden_states,
+                    encoder_attention_mask,
+                )
+            else:
+                layer_outputs = layer_module(
+                    hidden_states,
+                    attention_mask,
+                    layer_head_mask,
+                    encoder_hidden_states,
+                    encoder_attention_mask,
+                    past_key_value,
+                    output_attentions,
+                )
+
+            hidden_states = layer_outputs[0]
+            if use_cache:
+                next_decoder_cache += (layer_outputs[-1],)
+            if output_attentions:
+                all_self_attentions = all_self_attentions + (layer_outputs[1],)
+                if self.config.add_cross_attention:
+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)
+
+        if output_hidden_states:
+            all_hidden_states = all_hidden_states + (hidden_states,)
+
+        if not return_dict:
+            return tuple(
+                v
+                for v in [
+                    hidden_states,
+                    next_decoder_cache,
+                    all_hidden_states,
+                    all_self_attentions,
+                    all_cross_attentions,
+                ]
+                if v is not None
+            )
+        return BaseModelOutputWithPastAndCrossAttentions(
+            last_hidden_state=hidden_states,
+            past_key_values=next_decoder_cache,
+            hidden_states=all_hidden_states,
+            attentions=all_self_attentions,
+            cross_attentions=all_cross_attentions,
+        )
+
+
+# Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->Realm
+class RealmPooler(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        self.activation = nn.Tanh()
+
+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
+        # We "pool" the model by simply taking the hidden state corresponding
+        # to the first token.
+        first_token_tensor = hidden_states[:, 0]
+        pooled_output = self.dense(first_token_tensor)
+        pooled_output = self.activation(pooled_output)
+        return pooled_output
+
+
+@dataclass
+class RealmEmbedderOutput(ModelOutput):
     """
-    import socket
-    from datetime import datetime
+    Outputs of [`RealmEmbedder`] models.
 
-    current_time = datetime.now().strftime("%b%d_%H-%M-%S")
-    return os.path.join("runs", current_time + "_" + socket.gethostname())
+    Args:
+        projected_score (`torch.FloatTensor` of shape `(batch_size, config.retriever_proj_size)`):
 
+            Projected score.
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
+    """
 
-def get_int_from_env(env_keys, default):
-    """Returns the first positive env value found in the `env_keys` list or the default."""
-    for e in env_keys:
-        val = int(os.environ.get(e, -1))
-        if val >= 0:
-            return val
-    return default
+    projected_score: torch.FloatTensor = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
 
 
-def get_xla_device_type(device: "torch.device") -> Optional[str]:
+@dataclass
+class RealmScorerOutput(ModelOutput):
     """
-    Returns the xla device type (CPU|GPU|TPU) or None if the device is a non-xla device.
+    Outputs of [`RealmScorer`] models.
+
+    Args:
+        relevance_score (`torch.FloatTensor` of shape `(batch_size, config.num_candidates)`):
+            The relevance score of document candidates (before softmax).
+        query_score (`torch.FloatTensor` of shape `(batch_size, config.retriever_proj_size)`):
+            Query score derived from the query embedder.
+        candidate_score (`torch.FloatTensor` of shape `(batch_size, config.num_candidates, config.retriever_proj_size)`):
+            Candidate score derived from the embedder.
     """
-    if is_torch_tpu_available():
-        return xm.xla_real_devices([device])[0].split(":")[0]
-    return None
 
+    relevance_score: torch.FloatTensor = None
+    query_score: torch.FloatTensor = None
+    candidate_score: torch.FloatTensor = None
 
-class OptimizerNames(ExplicitEnum):
+
+@dataclass
+class RealmReaderOutput(ModelOutput):
     """
-    Stores the acceptable string identifiers for optimizers.
+    Outputs of [`RealmReader`] models.
+
+    Args:
+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `start_positions`, `end_positions`, `has_answers` are provided):
+            Total loss.
+        retriever_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `start_positions`, `end_positions`, `has_answers` are provided):
+            Retriever loss.
+        reader_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `start_positions`, `end_positions`, `has_answers` are provided):
+            Reader loss.
+        retriever_correct (`torch.BoolTensor` of shape `(config.searcher_beam_size,)`, *optional*):
+            Whether or not an evidence block contains answer.
+        reader_correct (`torch.BoolTensor` of shape `(config.reader_beam_size, num_candidates)`, *optional*):
+            Whether or not a span candidate contains answer.
+        block_idx (`torch.LongTensor` of shape `()`):
+            The index of the retrieved evidence block in which the predicted answer is most likely.
+        candidate (`torch.LongTensor` of shape `()`):
+            The index of the retrieved span candidates in which the predicted answer is most likely.
+        start_pos (`torch.IntTensor` of shape `()`):
+            Predicted answer starting position in *RealmReader*'s inputs.
+        end_pos: (`torch.IntTensor` of shape `()`):
+            Predicted answer ending position in *RealmReader*'s inputs.
+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):
+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of
+            shape `(batch_size, sequence_length, hidden_size)`.
+
+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.
+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):
+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,
+            sequence_length)`.
+
+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
+            heads.
     """
 
-    ADAMW_HF = "adamw_hf"
-    ADAMW_TORCH = "adamw_torch"
-    ADAMW_TORCH_XLA = "adamw_torch_xla"
-    ADAMW_APEX_FUSED = "adamw_apex_fused"
-    ADAFACTOR = "adafactor"
-    ADAMW_BNB = "adamw_bnb_8bit"
-    SGD = "sgd"
-    ADAGRAD = "adagrad"
+    loss: torch.FloatTensor = None
+    retriever_loss: torch.FloatTensor = None
+    reader_loss: torch.FloatTensor = None
+    retriever_correct: torch.BoolTensor = None
+    reader_correct: torch.BoolTensor = None
+    block_idx: torch.LongTensor = None
+    candidate: torch.LongTensor = None
+    start_pos: torch.int32 = None
+    end_pos: torch.int32 = None
+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None
+    attentions: Optional[Tuple[torch.FloatTensor]] = None
 
 
 @dataclass
-class TrainingArguments:
+class RealmForOpenQAOutput(ModelOutput):
     """
-    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop
-    itself**.
 
-    Using [`HfArgumentParser`] we can turn this class into
-    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the
-    command line.
+    Outputs of [`RealmForOpenQA`] models.
 
-    Parameters:
-        output_dir (`str`):
-            The output directory where the model predictions and checkpoints will be written.
-        overwrite_output_dir (`bool`, *optional*, defaults to `False`):
-            If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`
-            points to a checkpoint directory.
-        do_train (`bool`, *optional*, defaults to `False`):
-            Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used
-            by your training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        do_eval (`bool`, *optional*):
-            Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is
-            different from `"no"`. This argument is not directly used by [`Trainer`], it's intended to be used by your
-            training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        do_predict (`bool`, *optional*, defaults to `False`):
-            Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's
-            intended to be used by your training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        evaluation_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"no"`):
-            The evaluation strategy to adopt during training. Possible values are:
-
-                - `"no"`: No evaluation is done during training.
-                - `"steps"`: Evaluation is done (and logged) every `eval_steps`.
-                - `"epoch"`: Evaluation is done at the end of each epoch.
-
-        prediction_loss_only (`bool`, *optional*, defaults to `False`):
-            When performing evaluation and generating predictions, only returns the loss.
-        per_device_train_batch_size (`int`, *optional*, defaults to 8):
-            The batch size per GPU/TPU core/CPU for training.
-        per_device_eval_batch_size (`int`, *optional*, defaults to 8):
-            The batch size per GPU/TPU core/CPU for evaluation.
-        gradient_accumulation_steps (`int`, *optional*, defaults to 1):
-            Number of updates steps to accumulate the gradients for, before performing a backward/update pass.
-
-            <Tip warning={true}>
-
-            When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,
-            evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.
-
-            </Tip>
-
-        eval_accumulation_steps (`int`, *optional*):
-            Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If
-            left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster but
-            requires more memory).
-        eval_delay (`float`, *optional*):
-            Number of epochs or steps to wait for before the first evaluation can be performed, depending on the
-            evaluation_strategy.
-        learning_rate (`float`, *optional*, defaults to 5e-5):
-            The initial learning rate for [`AdamW`] optimizer.
-        weight_decay (`float`, *optional*, defaults to 0):
-            The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]
-            optimizer.
-        adam_beta1 (`float`, *optional*, defaults to 0.9):
-            The beta1 hyperparameter for the [`AdamW`] optimizer.
-        adam_beta2 (`float`, *optional*, defaults to 0.999):
-            The beta2 hyperparameter for the [`AdamW`] optimizer.
-        adam_epsilon (`float`, *optional*, defaults to 1e-8):
-            The epsilon hyperparameter for the [`AdamW`] optimizer.
-        max_grad_norm (`float`, *optional*, defaults to 1.0):
-            Maximum gradient norm (for gradient clipping).
-        num_train_epochs(`float`, *optional*, defaults to 3.0):
-            Total number of training epochs to perform (if not an integer, will perform the decimal part percents of
-            the last epoch before stopping training).
-        max_steps (`int`, *optional*, defaults to -1):
-            If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.
-            In case of using a finite iterable dataset the training may stop before reaching the set number of steps
-            when all data is exhausted
-        lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `"linear"`):
-            The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.
-        warmup_ratio (`float`, *optional*, defaults to 0.0):
-            Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.
-        warmup_steps (`int`, *optional*, defaults to 0):
-            Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.
-        log_level (`str`, *optional*, defaults to `passive`):
-            Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',
-            'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and lets the
-            application set the level.
-        log_level_replica (`str`, *optional*, defaults to `passive`):
-            Logger log level to use on replicas. Same choices as `log_level`"
-        log_on_each_node (`bool`, *optional*, defaults to `True`):
-            In multinode distributed training, whether to log using `log_level` once per node, or only on the main
-            node.
-        logging_dir (`str`, *optional*):
-            [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to
-            *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.
-        logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
-            The logging strategy to adopt during training. Possible values are:
-
-                - `"no"`: No logging is done during training.
-                - `"epoch"`: Logging is done at the end of each epoch.
-                - `"steps"`: Logging is done every `logging_steps`.
-
-        logging_first_step (`bool`, *optional*, defaults to `False`):
-            Whether to log and evaluate the first `global_step` or not.
-        logging_steps (`int`, *optional*, defaults to 500):
-            Number of update steps between two logs if `logging_strategy="steps"`.
-        logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):
-            Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`
-            or `inf` is filtered and the average loss of the current logging window is taken instead.
-
-            <Tip>
-
-            `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the
-            gradient is computed or applied to the model.
-
-            </Tip>
-
-        save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
-            The checkpoint save strategy to adopt during training. Possible values are:
-
-                - `"no"`: No save is done during training.
-                - `"epoch"`: Save is done at the end of each epoch.
-                - `"steps"`: Save is done every `save_steps`.
-        save_steps (`int`, *optional*, defaults to 500):
-            Number of updates steps before two checkpoint saves if `save_strategy="steps"`.
-        save_total_limit (`int`, *optional*):
-            If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
-            `output_dir`.
-        save_on_each_node (`bool`, *optional*, defaults to `False`):
-            When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on
-            the main one.
-
-            This should not be activated when the different nodes use the same storage as the files will be saved with
-            the same names for each node.
-        no_cuda (`bool`, *optional*, defaults to `False`):
-            Whether to not use CUDA even when it is available or not.
-        seed (`int`, *optional*, defaults to 42):
-            Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the
-            [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.
-        data_seed (`int`, *optional*):
-            Random seed to be used with data samplers. If not set, random generators for data sampling will use the
-            same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model
-            seed.
-        jit_mode_eval (`bool`, *optional*, defaults to `False`):
-            Whether or not to use PyTorch jit trace for inference.
-        use_ipex (`bool`, *optional*, defaults to `False`):
-            Use Intel extension for PyTorch when it is available. [IPEX
-            installation](https://github.com/intel/intel-extension-for-pytorch).
-        bf16 (`bool`, *optional*, defaults to `False`):
-            Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher
-            NVIDIA architecture or using CPU (no_cuda). This is an experimental API and it may change.
-        fp16 (`bool`, *optional*, defaults to `False`):
-            Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.
-        fp16_opt_level (`str`, *optional*, defaults to 'O1'):
-            For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on
-            the [Apex documentation](https://nvidia.github.io/apex/amp).
-        fp16_backend (`str`, *optional*, defaults to `"auto"`):
-            This argument is deprecated. Use `half_precision_backend` instead.
-        half_precision_backend (`str`, *optional*, defaults to `"auto"`):
-            The backend to use for mixed precision training. Must be one of `"auto", "cuda_amp", "apex", "cpu_amp"`.
-            `"auto"` will use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices
-            will force the requested backend.
-        bf16_full_eval (`bool`, *optional*, defaults to `False`):
-            Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm
-            metric values. This is an experimental API and it may change.
-        fp16_full_eval (`bool`, *optional*, defaults to `False`):
-            Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm
-            metric values.
-        tf32 (`bool`, *optional*):
-            Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends
-            on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to
-            the [TF32](https://huggingface.co/docs/transformers/performance#tf32) documentation. This is an
-            experimental API and it may change.
-        local_rank (`int`, *optional*, defaults to -1):
-            Rank of the process during distributed training.
-        xpu_backend (`str`, *optional*):
-            The backend to use for xpu distributed training. Must be one of `"mpi"` or `"ccl"` or `"gloo"`.
-        tpu_num_cores (`int`, *optional*):
-            When training on TPU, the number of TPU cores (automatically passed by launcher script).
-        dataloader_drop_last (`bool`, *optional*, defaults to `False`):
-            Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
-            or not.
-        eval_steps (`int`, *optional*):
-            Number of update steps between two evaluations if `evaluation_strategy="steps"`. Will default to the same
-            value as `logging_steps` if not set.
-        dataloader_num_workers (`int`, *optional*, defaults to 0):
-            Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the
-            main process.
-        past_index (`int`, *optional*, defaults to -1):
-            Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of
-            the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will
-            use the corresponding output (usually index 2) as the past state and feed it to the model at the next
-            training step under the keyword argument `mems`.
-        run_name (`str`, *optional*):
-            A descriptor for the run. Typically used for [wandb](https://www.wandb.com/) and
-            [mlflow](https://www.mlflow.org/) logging.
-        disable_tqdm (`bool`, *optional*):
-            Whether or not to disable the tqdm progress bars and table of metrics produced by
-            [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is
-            set to warn or lower (default), `False` otherwise.
-        remove_unused_columns (`bool`, *optional*, defaults to `True`):
-            Whether or not to automatically remove the columns unused by the model forward method.
-
-            (Note that this behavior is not implemented for [`TFTrainer`] yet.)
-        label_names (`List[str]`, *optional*):
-            The list of keys in your dictionary of inputs that correspond to the labels.
-
-            Will eventually default to `["labels"]` except if the model used is one of the `XxxForQuestionAnswering` in
-            which case it will default to `["start_positions", "end_positions"]`.
-        load_best_model_at_end (`bool`, *optional*, defaults to `False`):
-            Whether or not to load the best model found during training at the end of training.
-
-            <Tip>
-
-            When set to `True`, the parameters `save_strategy` needs to be the same as `evaluation_strategy`, and in
-            the case it is "steps", `save_steps` must be a round multiple of `eval_steps`.
-
-            </Tip>
-
-        metric_for_best_model (`str`, *optional*):
-            Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different
-            models. Must be the name of a metric returned by the evaluation with or without the prefix `"eval_"`. Will
-            default to `"loss"` if unspecified and `load_best_model_at_end=True` (to use the evaluation loss).
-
-            If you set this value, `greater_is_better` will default to `True`. Don't forget to set it to `False` if
-            your metric is better when lower.
-        greater_is_better (`bool`, *optional*):
-            Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models
-            should have a greater metric or not. Will default to:
-
-            - `True` if `metric_for_best_model` is set to a value that isn't `"loss"` or `"eval_loss"`.
-            - `False` if `metric_for_best_model` is not set, or set to `"loss"` or `"eval_loss"`.
-        ignore_data_skip (`bool`, *optional*, defaults to `False`):
-            When resuming training, whether or not to skip the epochs and batches to get the data loading at the same
-            stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step
-            can take a long time) but will not yield the same results as the interrupted training would have.
-        sharded_ddp (`bool`, `str` or list of [`~trainer_utils.ShardedDDPOption`], *optional*, defaults to `False`):
-            Use Sharded DDP training from [FairScale](https://github.com/facebookresearch/fairscale) (in distributed
-            training only). This is an experimental feature.
-
-            A list of options along the following:
-
-            - `"simple"`: to use first instance of sharded DDP released by fairscale (`ShardedDDP`) similar to ZeRO-2.
-            - `"zero_dp_2"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in
-              Zero-2 mode (with `reshard_after_forward=False`).
-            - `"zero_dp_3"`: to use the second instance of sharded DPP released by fairscale (`FullyShardedDDP`) in
-              Zero-3 mode (with `reshard_after_forward=True`).
-            - `"offload"`: to add ZeRO-offload (only compatible with `"zero_dp_2"` and `"zero_dp_3"`).
-
-            If a string is passed, it will be split on space. If a bool is passed, it will be converted to an empty
-            list for `False` and `["simple"]` for `True`.
-        fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `False`):
-            Use PyTorch Distributed Parallel Training (in distributed training only).
-
-            A list of options along the following:
-
-            - `"full_shard"`: Shard parameters, gradients and optimizer states.
-            - `"shard_grad_op"`: Shard optimizer states and gradients.
-            - `"offload"`: Offload parameters and gradients to CPUs (only compatible with `"full_shard"` and
-              `"shard_grad_op"`).
-            - `"auto_wrap"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.
-        fsdp_min_num_params (`int`, *optional*, defaults to `0`):
-            FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is passed).
-        deepspeed (`str` or `dict`, *optional*):
-            Use [Deepspeed](https://github.com/microsoft/deepspeed). This is an experimental feature and its API may
-            evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,
-            `ds_config.json`) or an already loaded json file as a `dict`"
-        label_smoothing_factor (`float`, *optional*, defaults to 0.0):
-            The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded
-            labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +
-            label_smoothing_factor/num_labels` respectively.
-        debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `""`):
-            Enable one or more debug features. This is an experimental feature.
-
-            Possible options are:
-
-            - `"underflow_overflow"`: detects overflow in model's input/outputs and reports the last frames that led to
-              the event
-            - `"tpu_metrics_debug"`: print debug metrics on TPU
-
-            The options should be separated by whitespaces.
-        optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `"adamw_hf"`):
-            The optimizer to use: adamw_hf, adamw_torch, adamw_apex_fused, or adafactor.
-        adafactor (`bool`, *optional*, defaults to `False`):
-            This argument is deprecated. Use `--optim adafactor` instead.
-        group_by_length (`bool`, *optional*, defaults to `False`):
-            Whether or not to group together samples of roughly the same length in the training dataset (to minimize
-            padding applied and be more efficient). Only useful if applying dynamic padding.
-        length_column_name (`str`, *optional*, defaults to `"length"`):
-            Column name for precomputed lengths. If the column exists, grouping by length will use these values rather
-            than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an
-            instance of `Dataset`.
-        report_to (`str` or `List[str]`, *optional*, defaults to `"all"`):
-            The list of integrations to report the results and logs to. Supported platforms are `"azure_ml"`,
-            `"comet_ml"`, `"mlflow"`, `"neptune"`, `"tensorboard"` and `"wandb"`. Use `"all"` to report to all
-            integrations installed, `"none"` for no integrations.
-        ddp_find_unused_parameters (`bool`, *optional*):
-            When using distributed training, the value of the flag `find_unused_parameters` passed to
-            `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.
-        ddp_bucket_cap_mb (`int`, *optional*):
-            When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.
-        dataloader_pin_memory (`bool`, *optional*, defaults to `True`):
-            Whether you want to pin memory in data loaders or not. Will default to `True`.
-        skip_memory_metrics (`bool`, *optional*, defaults to `True`):
-            Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows
-            down the training and evaluation speed.
-        push_to_hub (`bool`, *optional*, defaults to `False`):
-            Whether or not to push the model to the Hub every time the model is saved. If this is activated,
-            `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content
-            will be pushed each time a save is triggered (depending on your `save_strategy`). Calling
-            [`~Trainer.save_model`] will also trigger a push.
-
-            <Tip warning={true}>
-
-            If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be
-            pushed.
-
-            </Tip>
-
-        resume_from_checkpoint (`str`, *optional*):
-            The path to a folder with a valid checkpoint for your model. This argument is not directly used by
-            [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        hub_model_id (`str`, *optional*):
-            The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in
-            which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,
-            for instance `"user_name/model"`, which allows you to push to an organization you are a member of with
-            `"organization_name/model"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the
-            name of `output_dir`.
-
-            Will default to the name of `output_dir`.
-        hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `"every_save"`):
-            Defines the scope of what is pushed to the Hub and when. Possible values are:
-
-            - `"end"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a
-              draft of a model card when the [`~Trainer.save_model`] method is called.
-            - `"every_save"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and
-              a draft of a model card each time there is a model save. The pushes are asynchronous to not block
-              training, and in case the save are very frequent, a new push is only attempted if the previous one is
-              finished. A last push is made with the final model at the end of training.
-            - `"checkpoint"`: like `"every_save"` but the latest checkpoint is also pushed in a subfolder named
-              last-checkpoint, allowing you to resume training easily with
-              `trainer.train(resume_from_checkpoint="last-checkpoint")`.
-            - `"all_checkpoints"`: like `"checkpoint"` but all checkpoints are pushed like they appear in the output
-              folder (so you will get one checkpoint folder per folder in your final repository)
-
-        hub_token (`str`, *optional*):
-            The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with
-            `huggingface-cli login`.
-        hub_private_repo (`bool`, *optional*, defaults to `False`):
-            If True, the Hub repo will be set to private.
-        gradient_checkpointing (`bool`, *optional*, defaults to `False`):
-            If True, use gradient checkpointing to save memory at the expense of slower backward pass.
-        include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):
-            Whether or not the inputs will be passed to the `compute_metrics` function. This is intended for metrics
-            that need inputs, predictions and references for scoring calculation in Metric class.
-        auto_find_batch_size (`bool`, *optional*, defaults to `False`)
-            Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding
-            CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)
-        full_determinism (`bool`, *optional*, defaults to `False`)
-            If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in
-            distributed training
-        torchdynamo (`str`, *optional*):
-            The token that is used to set the backend compiler for TorchDynamo. Possible choices are ["eager",
-            "nvfuser]. This is an experimental API and subject to change.
-        ray_scope (`str`, *optional*, defaults to `"last"`):
-            The scope to use when doing hyperparameter search with Ray. By default, `"last"` will be used. Ray will
-            then use the last checkpoint of all trials, compare those, and select the best one. However, other options
-            are also available. See the [Ray documentation](
-            https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for
-            more options.
-        ddp_timeout (`int`, *optional*, defaults to 1800):
-            The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when
-            performing slow operations in distributed runnings. Please refer the [PyTorch documentation]
-            (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more
-            information.
-        use_mps_device (`bool`, *optional*, defaults to `False`):
-            Whether to use Apple Silicon chip based `mps` device.
+    Args:
+        reader_output (`dict`):
+            Reader output.
+        predicted_answer_ids (`torch.LongTensor` of shape `(answer_sequence_length)`):
+            Predicted answer ids.
     """
 
-    framework = "pt"
-    output_dir: str = field(
-        metadata={
-            "help": "The output directory where the model predictions and checkpoints will be written."
-        },
-    )
-    overwrite_output_dir: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Overwrite the content of the output directory. "
-                "Use this to continue training if output_dir points to a checkpoint directory."
-            )
-        },
-    )
+    reader_output: dict = None
+    predicted_answer_ids: torch.LongTensor = None
 
-    do_train: bool = field(default=False, metadata={"help": "Whether to run training."})
-    do_eval: bool = field(
-        default=False, metadata={"help": "Whether to run eval on the dev set."}
-    )
-    do_predict: bool = field(
-        default=False, metadata={"help": "Whether to run predictions on the test set."}
-    )
-    evaluation_strategy: Union[IntervalStrategy, str] = field(
-        default="no",
-        metadata={"help": "The evaluation strategy to use."},
-    )
-    prediction_loss_only: bool = field(
-        default=False,
-        metadata={
-            "help": "When performing evaluation and predictions, only returns the loss."
-        },
-    )
 
-    per_device_train_batch_size: int = field(
-        default=8, metadata={"help": "Batch size per GPU/TPU core/CPU for training."}
-    )
-    per_device_eval_batch_size: int = field(
-        default=8, metadata={"help": "Batch size per GPU/TPU core/CPU for evaluation."}
-    )
+class RealmPredictionHeadTransform(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
+        if isinstance(config.hidden_act, str):
+            self.transform_act_fn = ACT2FN[config.hidden_act]
+        else:
+            self.transform_act_fn = config.hidden_act
+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
 
-    per_gpu_train_batch_size: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Deprecated, the use of `--per_device_train_batch_size` is preferred. "
-                "Batch size per GPU/TPU core/CPU for training."
-            )
-        },
-    )
-    per_gpu_eval_batch_size: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Deprecated, the use of `--per_device_eval_batch_size` is preferred. "
-                "Batch size per GPU/TPU core/CPU for evaluation."
-            )
-        },
-    )
+    def forward(self, hidden_states):
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.transform_act_fn(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states)
+        return hidden_states
 
-    gradient_accumulation_steps: int = field(
-        default=1,
-        metadata={
-            "help": "Number of updates steps to accumulate before performing a backward/update pass."
-        },
-    )
-    eval_accumulation_steps: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "Number of predictions steps to accumulate before moving the tensors to the CPU."
-        },
-    )
 
-    eval_delay: Optional[float] = field(
-        default=0,
-        metadata={
-            "help": (
-                "Number of epochs or steps to wait for before the first evaluation can be performed, depending on the"
-                " evaluation_strategy."
-            )
-        },
-    )
+class RealmLMPredictionHead(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.transform = RealmPredictionHeadTransform(config)
 
-    learning_rate: float = field(
-        default=5e-5, metadata={"help": "The initial learning rate for AdamW."}
-    )
-    weight_decay: float = field(
-        default=0.0, metadata={"help": "Weight decay for AdamW if we apply some."}
-    )
-    adam_beta1: float = field(
-        default=0.9, metadata={"help": "Beta1 for AdamW optimizer"}
-    )
-    adam_beta2: float = field(
-        default=0.999, metadata={"help": "Beta2 for AdamW optimizer"}
-    )
-    adam_epsilon: float = field(
-        default=1e-8, metadata={"help": "Epsilon for AdamW optimizer."}
-    )
-    max_grad_norm: float = field(default=1.0, metadata={"help": "Max gradient norm."})
+        # The output weights are the same as the input embeddings, but there is
+        # an output-only bias for each token.
+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
 
-    num_train_epochs: float = field(
-        default=3.0, metadata={"help": "Total number of training epochs to perform."}
-    )
-    max_steps: int = field(
-        default=-1,
-        metadata={
-            "help": "If > 0: set total number of training steps to perform. Override num_train_epochs."
-        },
-    )
-    lr_scheduler_type: Union[SchedulerType, str] = field(
-        default="linear",
-        metadata={"help": "The scheduler type to use."},
-    )
-    warmup_ratio: float = field(
-        default=0.0,
-        metadata={"help": "Linear warmup over warmup_ratio fraction of total steps."},
-    )
-    warmup_steps: int = field(
-        default=0, metadata={"help": "Linear warmup over warmup_steps."}
-    )
+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
 
-    log_level: Optional[str] = field(
-        default="passive",
-        metadata={
-            "help": (
-                "Logger log level to use on the main node. Possible choices are the log levels as strings: 'debug',"
-                " 'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and"
-                " lets the application set the level. Defaults to 'passive'."
-            ),
-            "choices": trainer_log_levels.keys(),
-        },
-    )
-    log_level_replica: Optional[str] = field(
-        default="passive",
-        metadata={
-            "help": "Logger log level to use on replica nodes. Same choices and defaults as ``log_level``",
-            "choices": trainer_log_levels.keys(),
-        },
-    )
-    log_on_each_node: bool = field(
-        default=True,
-        metadata={
-            "help": (
-                "When doing a multinode distributed training, whether to log once per node or just once on the main"
-                " node."
-            )
-        },
-    )
-    logging_dir: Optional[str] = field(
-        default=None, metadata={"help": "Tensorboard log dir."}
-    )
-    logging_strategy: Union[IntervalStrategy, str] = field(
-        default="steps",
-        metadata={"help": "The logging strategy to use."},
-    )
-    logging_first_step: bool = field(
-        default=False, metadata={"help": "Log the first global_step"}
-    )
-    logging_steps: int = field(
-        default=500, metadata={"help": "Log every X updates steps."}
-    )
-    logging_nan_inf_filter: bool = field(
-        default=True, metadata={"help": "Filter nan and inf losses for logging."}
-    )
-    save_strategy: Union[IntervalStrategy, str] = field(
-        default="steps",
-        metadata={"help": "The checkpoint save strategy to use."},
-    )
-    save_steps: int = field(
-        default=500, metadata={"help": "Save checkpoint every X updates steps."}
-    )
-    save_total_limit: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Limit the total amount of checkpoints. "
-                "Deletes the older checkpoints in the output_dir. Default is unlimited checkpoints"
-            )
-        },
-    )
-    save_on_each_node: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "When doing multi-node distributed training, whether to save models and checkpoints on each node, or"
-                " only on the main one"
-            )
-        },
-    )
-    no_cuda: bool = field(
-        default=False, metadata={"help": "Do not use CUDA even when it is available"}
-    )
-    use_mps_device: bool = field(
-        default=False,
-        metadata={"help": "Whether to use Apple Silicon chip based `mps` device."},
-    )
-    seed: int = field(
-        default=42,
-        metadata={"help": "Random seed that will be set at the beginning of training."},
-    )
-    data_seed: Optional[int] = field(
-        default=None, metadata={"help": "Random seed to be used with data samplers."}
-    )
-    jit_mode_eval: bool = field(
-        default=False,
-        metadata={"help": "Whether or not to use PyTorch jit trace for inference"},
-    )
-    use_ipex: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Use Intel extension for PyTorch when it is available, installation:"
-                " 'https://github.com/intel/intel-extension-for-pytorch'"
-            )
-        },
-    )
-    bf16: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Whether to use bf16 (mixed) precision instead of 32-bit. Requires Ampere or higher NVIDIA"
-                " architecture or using CPU (no_cuda). This is an experimental API and it may change."
-            )
-        },
-    )
-    fp16: bool = field(
-        default=False,
-        metadata={"help": "Whether to use fp16 (mixed) precision instead of 32-bit"},
-    )
-    fp16_opt_level: str = field(
-        default="O1",
-        metadata={
-            "help": (
-                "For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. "
-                "See details at https://nvidia.github.io/apex/amp.html"
-            )
-        },
-    )
-    half_precision_backend: str = field(
-        default="auto",
-        metadata={
-            "help": "The backend to be used for half precision.",
-            "choices": ["auto", "cuda_amp", "apex", "cpu_amp"],
-        },
-    )
-    bf16_full_eval: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Whether to use full bfloat16 evaluation instead of 32-bit. This is an experimental API and it may"
-                " change."
-            )
-        },
-    )
-    fp16_full_eval: bool = field(
-        default=False,
-        metadata={"help": "Whether to use full float16 evaluation instead of 32-bit"},
-    )
-    tf32: Optional[bool] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Whether to enable tf32 mode, available in Ampere and newer GPU architectures. This is an experimental"
-                " API and it may change."
-            )
-        },
-    )
-    local_rank: int = field(
-        default=-1, metadata={"help": "For distributed training: local_rank"}
-    )
-    xpu_backend: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": "The backend to be used for distributed training on Intel XPU.",
-            "choices": ["mpi", "ccl", "gloo"],
-        },
-    )
-    tpu_num_cores: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": "TPU: Number of TPU cores (automatically passed by launcher script)"
-        },
-    )
-    tpu_metrics_debug: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Deprecated, the use of `--debug tpu_metrics_debug` is preferred. TPU: Whether to print debug metrics"
-            )
-        },
-    )
-    debug: str = field(
-        default="",
-        metadata={
-            "help": (
-                "Whether or not to enable debug mode. Current options: "
-                "`underflow_overflow` (Detect underflow and overflow in activations and weights), "
-                "`tpu_metrics_debug` (print debug metrics on TPU)."
-            )
-        },
-    )
+        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`
+        self.decoder.bias = self.bias
 
-    dataloader_drop_last: bool = field(
-        default=False,
-        metadata={
-            "help": "Drop the last incomplete batch if it is not divisible by the batch size."
-        },
-    )
-    eval_steps: Optional[int] = field(
-        default=None, metadata={"help": "Run an evaluation every X steps."}
-    )
-    dataloader_num_workers: int = field(
-        default=0,
-        metadata={
-            "help": (
-                "Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded"
-                " in the main process."
-            )
-        },
-    )
+    def forward(self, hidden_states):
+        hidden_states = self.transform(hidden_states)
+        hidden_states = self.decoder(hidden_states)
+        return hidden_states
 
-    past_index: int = field(
-        default=-1,
-        metadata={
-            "help": "If >=0, uses the corresponding part of the output as the past state for next step."
-        },
-    )
 
-    run_name: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": "An optional descriptor for the run. Notably used for wandb logging."
-        },
-    )
-    disable_tqdm: Optional[bool] = field(
-        default=None,
-        metadata={"help": "Whether or not to disable the tqdm progress bars."},
-    )
+class RealmOnlyMLMHead(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.predictions = RealmLMPredictionHead(config)
 
-    remove_unused_columns: Optional[bool] = field(
-        default=True,
-        metadata={
-            "help": "Remove columns not required by the model when using an nlp.Dataset."
-        },
-    )
-    label_names: Optional[List[str]] = field(
-        default=None,
-        metadata={
-            "help": "The list of keys in your dictionary of inputs that correspond to the labels."
-        },
-    )
+    def forward(self, sequence_output):
+        prediction_scores = self.predictions(sequence_output)
+        return prediction_scores
 
-    load_best_model_at_end: Optional[bool] = field(
-        default=False,
-        metadata={
-            "help": "Whether or not to load the best model found during training at the end of training."
-        },
-    )
-    metric_for_best_model: Optional[str] = field(
-        default=None,
-        metadata={"help": "The metric to use to compare two different models."},
-    )
-    greater_is_better: Optional[bool] = field(
-        default=None,
-        metadata={
-            "help": "Whether the `metric_for_best_model` should be maximized or not."
-        },
-    )
-    ignore_data_skip: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "When resuming training, whether or not to skip the first epochs and batches to get to the same"
-                " training data."
-            )
-        },
-    )
-    sharded_ddp: str = field(
-        default="",
-        metadata={
-            "help": (
-                "Whether or not to use sharded DDP training (in distributed training only). The base option should be"
-                " `simple`, `zero_dp_2` or `zero_dp_3` and you can add CPU-offload to `zero_dp_2` or `zero_dp_3` like"
-                " this: zero_dp_2 offload` or `zero_dp_3 offload`. You can add auto-wrap to `zero_dp_2` or `zero_dp_3`"
-                " with the same syntax: zero_dp_2 auto_wrap` or `zero_dp_3 auto_wrap`."
-            ),
-        },
-    )
-    fsdp: str = field(
-        default="",
-        metadata={
-            "help": (
-                "Whether or not to use PyTorch Fully Sharded Data Parallel (FSDP) training (in distributed training"
-                " only). The base option should be `full_shard`, `shard_grad_op` or `no_shard` and you can add"
-                " CPU-offload to `full_shard` or `shard_grad_op` like this: full_shard offload` or `shard_grad_op"
-                " offload`. You can add auto-wrap to `full_shard` or `shard_grad_op` with the same syntax: full_shard"
-                " auto_wrap` or `shard_grad_op auto_wrap`."
-            ),
-        },
-    )
-    fsdp_min_num_params: int = field(
-        default=0,
-        metadata={
-            "help": (
-                "FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is"
-                " passed)."
-            )
-        },
-    )
-    fsdp_transformer_layer_cls_to_wrap: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Transformer layer class name (case-sensitive) to wrap ,e.g, `BertLayer`, `GPTJBlock`, `T5Block` .... "
-                "(useful only when `fsdp` flag is passed)."
-            )
-        },
-    )
-    deepspeed: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Enable deepspeed and pass the path to deepspeed json config file (e.g. ds_config.json) or an already"
-                " loaded json file as a dict"
-            )
-        },
-    )
-    label_smoothing_factor: float = field(
-        default=0.0,
-        metadata={
-            "help": "The label smoothing epsilon to apply (zero means no label smoothing)."
-        },
-    )
-    optim: Union[OptimizerNames, str] = field(
-        default="adamw_hf",
-        metadata={"help": "The optimizer to use."},
-    )
-    adafactor: bool = field(
-        default=False,
-        metadata={"help": "Whether or not to replace AdamW by Adafactor."},
-    )
-    group_by_length: bool = field(
-        default=False,
-        metadata={
-            "help": "Whether or not to group samples of roughly the same length together when batching."
-        },
-    )
-    length_column_name: Optional[str] = field(
-        default="length",
-        metadata={
-            "help": "Column name with precomputed lengths to use when grouping by length."
-        },
-    )
-    report_to: Optional[List[str]] = field(
-        default=None,
-        metadata={
-            "help": "The list of integrations to report the results and logs to."
-        },
-    )
-    ddp_find_unused_parameters: Optional[bool] = field(
-        default=None,
-        metadata={
-            "help": (
-                "When using distributed training, the value of the flag `find_unused_parameters` passed to "
-                "`DistributedDataParallel`."
-            )
-        },
-    )
-    ddp_bucket_cap_mb: Optional[int] = field(
-        default=None,
-        metadata={
-            "help": (
-                "When using distributed training, the value of the flag `bucket_cap_mb` passed to "
-                "`DistributedDataParallel`."
-            )
-        },
-    )
-    dataloader_pin_memory: bool = field(
-        default=True, metadata={"help": "Whether or not to pin memory for DataLoader."}
-    )
-    skip_memory_metrics: bool = field(
-        default=True,
-        metadata={
-            "help": "Whether or not to skip adding of memory profiler reports to metrics."
-        },
-    )
-    use_legacy_prediction_loop: bool = field(
-        default=False,
-        metadata={
-            "help": "Whether or not to use the legacy prediction_loop in the Trainer."
-        },
-    )
-    push_to_hub: bool = field(
-        default=False,
-        metadata={
-            "help": "Whether or not to upload the trained model to the model hub after training."
-        },
-    )
-    resume_from_checkpoint: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": "The path to a folder with a valid checkpoint for your model."
-        },
-    )
-    hub_model_id: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": "The name of the repository to keep in sync with the local `output_dir`."
-        },
-    )
-    hub_strategy: Union[HubStrategy, str] = field(
-        default="every_save",
-        metadata={"help": "The hub strategy to use when `--push_to_hub` is activated."},
-    )
-    hub_token: Optional[str] = field(
-        default=None, metadata={"help": "The token to use to push to the Model Hub."}
-    )
-    hub_private_repo: bool = field(
-        default=False,
-        metadata={"help": "Whether the model repository is private or not."},
-    )
-    gradient_checkpointing: bool = field(
-        default=False,
-        metadata={
-            "help": "If True, use gradient checkpointing to save memory at the expense of slower backward pass."
-        },
-    )
-    include_inputs_for_metrics: bool = field(
-        default=False,
-        metadata={
-            "help": "Whether or not the inputs will be passed to the `compute_metrics` function."
-        },
-    )
-    # Deprecated arguments
-    fp16_backend: str = field(
-        default="auto",
-        metadata={
-            "help": "Deprecated. Use half_precision_backend instead",
-            "choices": ["auto", "cuda_amp", "apex", "cpu_amp"],
-        },
-    )
-    push_to_hub_model_id: Optional[str] = field(
-        default=None,
-        metadata={"help": "The name of the repository to which push the `Trainer`."},
-    )
-    push_to_hub_organization: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": "The name of the organization in with to which push the `Trainer`."
-        },
-    )
-    push_to_hub_token: Optional[str] = field(
-        default=None, metadata={"help": "The token to use to push to the Model Hub."}
-    )
-    _n_gpu: int = field(init=False, repr=False, default=-1)
-    mp_parameters: str = field(
-        default="",
-        metadata={
-            "help": "Used by the SageMaker launcher to send mp-specific args. Ignored in Trainer"
-        },
-    )
 
-    auto_find_batch_size: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Whether to automatically decrease the batch size in half and rerun the training loop again each time"
-                " a CUDA Out-of-Memory was reached"
-            )
-        },
-    )
-    full_determinism: bool = field(
-        default=False,
-        metadata={
-            "help": (
-                "Whether to call enable_full_determinism instead of set_seed for reproducibility in distributed"
-                " training"
-            )
-        },
-    )
-    torchdynamo: Optional[str] = field(
-        default=None,
-        metadata={
-            "help": (
-                "Sets up the backend compiler for TorchDynamo. TorchDynamo is a Python level JIT compiler designed to"
-                " make unmodified PyTorch programs faster. TorchDynamo dynamically modifies the Python bytecode right"
-                " before its executed. It rewrites Python bytecode to extract sequences of PyTorch operations"
-                " and lifts them up into Fx graph. We can then pass these Fx graphs to other backend compilers. There"
-                " are two options - eager and nvfuser. Eager defaults to pytorch eager and is useful for debugging."
-                " nvfuser path uses AOT Autograd and nvfuser compiler to optimize the models."
-            ),
-            "choices": ["eager", "nvfuser", "fx2trt", "fx2trt-fp16"],
-        },
-    )
-    ray_scope: Optional[str] = field(
-        default="last",
-        metadata={
-            "help": (
-                'The scope to use when doing hyperparameter search with Ray. By default, `"last"` will be used. Ray'
-                " will then use the last checkpoint of all trials, compare those, and select the best one. However,"
-                " other options are also available. See the Ray documentation"
-                " (https://docs.ray.io/en/latest/tune/api_docs/analysis.html"
-                "#ray.tune.ExperimentAnalysis.get_best_trial)"
-                " for more options."
-            )
-        },
-    )
-    ddp_timeout: Optional[int] = field(
-        default=1800,
-        metadata={
-            "help": "Overrides the default timeout for distributed training (value should be given in seconds)."
-        },
-    )
+class RealmScorerProjection(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.predictions = RealmLMPredictionHead(config)
+        self.dense = nn.Linear(config.hidden_size, config.retriever_proj_size)
+        self.LayerNorm = nn.LayerNorm(
+            config.retriever_proj_size, eps=config.layer_norm_eps
+        )
 
-    def __post_init__(self):
-        # Handle --use_env option in torch.distributed.launch (local_rank not passed as an arg then).
-        # This needs to happen before any call to self.device or self.n_gpu.
-        env_local_rank = int(os.environ.get("LOCAL_RANK", -1))
-        if env_local_rank != -1 and env_local_rank != self.local_rank:
-            self.local_rank = env_local_rank
-
-        # expand paths, if not os.makedirs("~/bar") will make directory
-        # in the current directory instead of the actual home
-        #  see https://github.com/huggingface/transformers/issues/10628
-        if self.output_dir is not None:
-            self.output_dir = os.path.expanduser(self.output_dir)
-        if self.logging_dir is None and self.output_dir is not None:
-            self.logging_dir = os.path.join(self.output_dir, default_logdir())
-        if self.logging_dir is not None:
-            self.logging_dir = os.path.expanduser(self.logging_dir)
-
-        if self.disable_tqdm is None:
-            self.disable_tqdm = logger.getEffectiveLevel() > logging.WARN
-
-        if isinstance(self.evaluation_strategy, EvaluationStrategy):
-            warnings.warn(
-                "using `EvaluationStrategy` for `evaluation_strategy` is deprecated and will be removed in version 5"
-                " of 🤗 Transformers. Use `IntervalStrategy` instead",
-                FutureWarning,
-            )
-            # Go back to the underlying string or we won't be able to instantiate `IntervalStrategy` on it.
-            self.evaluation_strategy = self.evaluation_strategy.value
-
-        self.evaluation_strategy = IntervalStrategy(self.evaluation_strategy)
-        self.logging_strategy = IntervalStrategy(self.logging_strategy)
-        self.save_strategy = IntervalStrategy(self.save_strategy)
-        self.hub_strategy = HubStrategy(self.hub_strategy)
-
-        self.lr_scheduler_type = SchedulerType(self.lr_scheduler_type)
-        if self.do_eval is False and self.evaluation_strategy != IntervalStrategy.NO:
-            self.do_eval = True
-
-        # eval_steps has to be defined and non-zero, fallbacks to logging_steps if the latter is non-zero
-        if self.evaluation_strategy == IntervalStrategy.STEPS and (
-            self.eval_steps is None or self.eval_steps == 0
-        ):
-            if self.logging_steps > 0:
-                logger.info(
-                    f"using `logging_steps` to initialize `eval_steps` to {self.logging_steps}"
+    def forward(self, hidden_states):
+        hidden_states = self.dense(hidden_states)
+        hidden_states = self.LayerNorm(hidden_states)
+        return hidden_states
+
+
+class RealmReaderProjection(nn.Module):
+    def __init__(self, config):
+        super().__init__()
+        self.config = config
+        self.dense_intermediate = nn.Linear(
+            config.hidden_size, config.span_hidden_size * 2
+        )
+        self.dense_output = nn.Linear(config.span_hidden_size, 1)
+        self.layer_normalization = nn.LayerNorm(
+            config.span_hidden_size, eps=config.reader_layer_norm_eps
+        )
+        self.relu = nn.ReLU()
+
+    def forward(self, hidden_states, block_mask):
+        def span_candidates(masks):
+            """
+            Generate span candidates.
+
+            Args:
+                masks: <bool> [num_retrievals, max_sequence_len]
+
+            Returns:
+                starts: <int32> [num_spans] ends: <int32> [num_spans] span_masks: <int32> [num_retrievals, num_spans]
+                whether spans locate in evidence block.
+            """
+            _, max_sequence_len = masks.shape
+
+            def _spans_given_width(width):
+                current_starts = torch.arange(
+                    max_sequence_len - width + 1, device=masks.device
                 )
-                self.eval_steps = self.logging_steps
-            else:
-                raise ValueError(
-                    f"evaluation strategy {self.evaluation_strategy} requires either non-zero --eval_steps or"
-                    " --logging_steps"
+                current_ends = torch.arange(
+                    width - 1, max_sequence_len, device=masks.device
                 )
+                return current_starts, current_ends
 
-        # logging_steps must be non-zero for logging_strategy that is other than 'no'
-        if self.logging_strategy == IntervalStrategy.STEPS and self.logging_steps == 0:
-            raise ValueError(
-                f"logging strategy {self.logging_strategy} requires non-zero --logging_steps"
+            starts, ends = zip(
+                *(_spans_given_width(w + 1) for w in range(self.config.max_span_width))
             )
 
-        # Sanity checks for load_best_model_at_end: we require save and eval strategies to be compatible.
-        if self.load_best_model_at_end:
-            if self.evaluation_strategy != self.save_strategy:
-                raise ValueError(
-                    "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation "
-                    f"strategy: {self.evaluation_strategy}\n- Save strategy: {self.save_strategy}"
-                )
-            if (
-                self.evaluation_strategy == IntervalStrategy.STEPS
-                and self.save_steps % self.eval_steps != 0
-            ):
-                raise ValueError(
-                    "--load_best_model_at_end requires the saving steps to be a round multiple of the evaluation "
-                    f"steps, but found {self.save_steps}, which is not a round multiple of {self.eval_steps}."
-                )
+            # [num_spans]
+            starts = torch.cat(starts, 0)
+            ends = torch.cat(ends, 0)
 
-        if self.load_best_model_at_end and self.metric_for_best_model is None:
-            self.metric_for_best_model = "loss"
-        if self.greater_is_better is None and self.metric_for_best_model is not None:
-            self.greater_is_better = self.metric_for_best_model not in [
-                "loss",
-                "eval_loss",
-            ]
-        if self.run_name is None:
-            self.run_name = self.output_dir
-        if self.framework == "pt" and is_torch_available():
-            if self.fp16_backend and self.fp16_backend != "auto":
-                warnings.warn(
-                    "`fp16_backend` is deprecated and will be removed in version 5 of 🤗 Transformers. Use"
-                    " `half_precision_backend` instead",
-                    FutureWarning,
-                )
-                self.half_precision_backend = self.fp16_backend
+            # [num_retrievals, num_spans]
+            start_masks = torch.index_select(masks, dim=-1, index=starts)
+            end_masks = torch.index_select(masks, dim=-1, index=ends)
+            span_masks = start_masks * end_masks
 
-            if self.bf16 or self.bf16_full_eval:
-                if self.no_cuda and not is_torch_bf16_cpu_available():
-                    # cpu
-                    raise ValueError(
-                        "Your setup doesn't support bf16/cpu. You need torch>=1.10"
-                    )
-                elif not self.no_cuda and not is_torch_bf16_gpu_available():
-                    # gpu
-                    raise ValueError(
-                        "Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0"
-                    )
+            return starts, ends, span_masks
 
-        if self.fp16 and self.bf16:
-            raise ValueError("At most one of fp16 and bf16 can be True, but not both")
+        def mask_to_score(mask, dtype=torch.float32):
+            return (1.0 - mask.type(dtype)) * torch.finfo(dtype).min
 
-        if self.fp16_full_eval and self.bf16_full_eval:
-            raise ValueError(
-                "At most one of fp16 and bf16 can be True for full eval, but not both"
-            )
+        # [reader_beam_size, max_sequence_len, span_hidden_size * 2]
+        hidden_states = self.dense_intermediate(hidden_states)
+        # [reader_beam_size, max_sequence_len, span_hidden_size]
+        start_projection, end_projection = hidden_states.chunk(2, dim=-1)
 
-        if self.bf16:
-            if self.half_precision_backend == "apex":
-                raise ValueError(
-                    " `--half_precision_backend apex`: GPU bf16 is not supported by apex. Use"
-                    " `--half_precision_backend cuda_amp` instead"
-                )
-            if not (self.sharded_ddp == "" or not self.sharded_ddp):
-                raise ValueError("sharded_ddp is not supported with bf16")
+        candidate_starts, candidate_ends, candidate_mask = span_candidates(block_mask)
 
-        self.optim = OptimizerNames(self.optim)
-        if self.adafactor:
-            warnings.warn(
-                "`--adafactor` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--optim"
-                " adafactor` instead",
-                FutureWarning,
-            )
-            self.optim = OptimizerNames.ADAFACTOR
+        candidate_start_projections = torch.index_select(
+            start_projection, dim=1, index=candidate_starts
+        )
+        candidate_end_projections = torch.index_select(
+            end_projection, dim=1, index=candidate_ends
+        )
+        candidate_hidden = candidate_start_projections + candidate_end_projections
 
-        if (
-            self.framework == "pt"
-            and is_torch_available()
-            and (self.device.type != "cuda")
-            and (get_xla_device_type(self.device) != "GPU")
-            and (self.fp16 or self.fp16_full_eval)
-        ):
-            raise ValueError(
-                "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation"
-                " (`--fp16_full_eval`) can only be used on CUDA devices."
-            )
+        # [reader_beam_size, num_candidates, span_hidden_size]
+        candidate_hidden = self.relu(candidate_hidden)
+        # [reader_beam_size, num_candidates, span_hidden_size]
+        candidate_hidden = self.layer_normalization(candidate_hidden)
+        # [reader_beam_size, num_candidates]
+        reader_logits = self.dense_output(candidate_hidden).squeeze(-1)
+        # [reader_beam_size, num_candidates]
+        reader_logits += mask_to_score(candidate_mask, dtype=reader_logits.dtype)
+
+        return reader_logits, candidate_starts, candidate_ends
+
+
+REALM_START_DOCSTRING = r"""
+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use
+    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
+    behavior.
 
-        if (
-            self.framework == "pt"
-            and is_torch_available()
-            and (self.device.type != "cuda")
-            and (get_xla_device_type(self.device) != "GPU")
-            and (self.device.type != "cpu")
-            and (self.bf16 or self.bf16_full_eval)
-        ):
-            raise ValueError(
-                "BF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation"
-                " (`--bf16_full_eval`) can only be used on CUDA or CPU devices."
-            )
+    Parameters:
+        config ([`RealmConfig`]): Model configuration class with all the parameters of the model.
+            Initializing with a config file does not load the weights associated with the model, only the
+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.
+"""
+
+REALM_INPUTS_DOCSTRING = r"""
+    Args:
+        input_ids (`torch.LongTensor` of shape `({0})`):
+            Indices of input sequence tokens in the vocabulary.
+
+            Indices can be obtained using [`RealmTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+            [`PreTrainedTokenizer.__call__`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+            [What are attention masks?](../glossary#attention-mask)
+        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
+            1]`:
+
+            - 0 corresponds to a *sentence A* token,
+            - 1 corresponds to a *sentence B* token.
+
+            [What are token type IDs?](../glossary#token-type-ids)
+        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):
+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,
+            config.max_position_embeddings - 1]`.
+
+            [What are position IDs?](../glossary#position-ids)
+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):
+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:
+
+            - 1 indicates the head is **not masked**,
+            - 0 indicates the head is **masked**.
+
+        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):
+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This
+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the
+            model's internal embedding lookup matrix.
+        output_attentions (`bool`, *optional*):
+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned
+            tensors for more detail.
+        output_hidden_states (`bool`, *optional*):
+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for
+            more detail.
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+"""
 
-        if self.framework == "pt" and is_torch_available() and self.tf32 is not None:
-            if self.tf32:
-                if is_torch_tf32_available():
-                    torch.backends.cuda.matmul.allow_tf32 = True
-                else:
-                    raise ValueError(
-                        "--tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7"
-                    )
+
+class RealmPreTrainedModel(PreTrainedModel):
+    """
+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
+    models.
+    """
+
+    config_class = RealmConfig
+    load_tf_weights = load_tf_weights_in_realm
+    base_model_prefix = "realm"
+    _keys_to_ignore_on_load_missing = [r"position_ids"]
+
+    def _init_weights(self, module):
+        """Initialize the weights"""
+        if isinstance(module, nn.Linear):
+            # Slightly different from the TF version which uses truncated_normal for initialization
+            # cf https://github.com/pytorch/pytorch/pull/5617
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.bias is not None:
+                module.bias.data.zero_()
+        elif isinstance(module, nn.Embedding):
+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
+            if module.padding_idx is not None:
+                module.weight.data[module.padding_idx].zero_()
+        elif isinstance(module, nn.LayerNorm):
+            module.bias.data.zero_()
+            module.weight.data.fill_(1.0)
+
+    def _flatten_inputs(self, *inputs):
+        """Flatten inputs' shape to (-1, input_shape[-1])"""
+        flattened_inputs = []
+        for tensor in inputs:
+            if tensor is None:
+                flattened_inputs.append(None)
             else:
-                if is_torch_tf32_available():
-                    torch.backends.cuda.matmul.allow_tf32 = False
-                # no need to assert on else
+                input_shape = tensor.shape
+                if len(input_shape) > 2:
+                    tensor = tensor.view((-1, input_shape[-1]))
+                flattened_inputs.append(tensor)
+        return flattened_inputs
 
-        if self.report_to is None:
-            logger.info(
-                "The default value for the training argument `--report_to` will change in v5 (from all installed "
-                "integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as "
-                "now. You should start updating your code and make this info disappear :-)."
-            )
-            self.report_to = "all"
-        if self.report_to == "all" or self.report_to == ["all"]:
-            # Import at runtime to avoid a circular import.
-            from .integrations import get_available_reporting_integrations
-
-            self.report_to = get_available_reporting_integrations()
-        elif self.report_to == "none" or self.report_to == ["none"]:
-            self.report_to = []
-        elif not isinstance(self.report_to, list):
-            self.report_to = [self.report_to]
-
-        if self.warmup_ratio < 0 or self.warmup_ratio > 1:
-            raise ValueError("warmup_ratio must lie in range [0,1]")
-        elif self.warmup_ratio > 0 and self.warmup_steps > 0:
-            logger.info(
-                "Both warmup_ratio and warmup_steps given, warmup_steps will override any effect of warmup_ratio"
-                " during training"
-            )
 
-        if isinstance(self.sharded_ddp, bool):
-            self.sharded_ddp = "simple" if self.sharded_ddp else ""
-        if isinstance(self.sharded_ddp, str):
-            self.sharded_ddp = [ShardedDDPOption(s) for s in self.sharded_ddp.split()]
-        if self.sharded_ddp == [ShardedDDPOption.OFFLOAD]:
-            raise ValueError(
-                "`--sharded_ddp offload` can't work on its own. It needs to be added to `--sharded_ddp zero_dp_2` or "
-                '`--sharded_ddp zero_dp_3`. For example, `--sharded_ddp "zero_dp_2 offload"`.'
-            )
-        elif len(self.sharded_ddp) > 1 and ShardedDDPOption.SIMPLE in self.sharded_ddp:
-            raise ValueError(
-                "`--sharded_ddp simple` is not compatible with any other option."
-            )
-        elif (
-            ShardedDDPOption.ZERO_DP_2 in self.sharded_ddp
-            and ShardedDDPOption.ZERO_DP_3 in self.sharded_ddp
-        ):
-            raise ValueError(
-                "`--sharded_ddp zero_dp_2` is not compatible with `--sharded_ddp zero_dp_3`."
-            )
+class RealmBertModel(RealmPreTrainedModel):
+    """
+    Same as the original BertModel but remove docstrings.
+    """
 
-        if isinstance(self.fsdp, bool):
-            self.fsdp = "full_shard" if self.fsdp else ""
-        if isinstance(self.fsdp, str):
-            self.fsdp = [FSDPOption(s) for s in self.fsdp.split()]
-        if self.fsdp == [FSDPOption.OFFLOAD]:
-            raise ValueError(
-                "`--fsdp offload` can't work on its own. It needs to be added to `--fsdp full_shard` or "
-                '`--fsdp shard_grad_op`. For example, `--fsdp "full_shard offload"`.'
-            )
-        elif (
-            FSDPOption.FULL_SHARD in self.fsdp and FSDPOption.SHARD_GRAD_OP in self.fsdp
-        ):
+    def __init__(self, config, add_pooling_layer=True):
+        super().__init__(config)
+        self.config = config
+
+        self.embeddings = RealmEmbeddings(config)
+        self.encoder = RealmEncoder(config)
+
+        self.pooler = RealmPooler(config) if add_pooling_layer else None
+
+        # Weights initialization is mostly managed by other Realm models,
+        # but we also have them initialized here to keep a consistency.
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.embeddings.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.embeddings.word_embeddings = value
+
+    def _prune_heads(self, heads_to_prune):
+        """
+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base
+        class PreTrainedModel
+        """
+        for layer, heads in heads_to_prune.items():
+            self.encoder.layer[layer].attention.prune_heads(heads)
+
+    def forward(
+        self,
+        input_ids=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        encoder_hidden_states=None,
+        encoder_attention_mask=None,
+        past_key_values=None,
+        use_cache=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        output_attentions = (
+            output_attentions
+            if output_attentions is not None
+            else self.config.output_attentions
+        )
+        output_hidden_states = (
+            output_hidden_states
+            if output_hidden_states is not None
+            else self.config.output_hidden_states
+        )
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        if self.config.is_decoder:
+            use_cache = use_cache if use_cache is not None else self.config.use_cache
+        else:
+            use_cache = False
+
+        if input_ids is not None and inputs_embeds is not None:
             raise ValueError(
-                "`--fsdp full_shard` is not compatible with `--fsdp shard_grad_op`."
+                "You cannot specify both input_ids and inputs_embeds at the same time"
             )
+        elif input_ids is not None:
+            input_shape = input_ids.size()
+        elif inputs_embeds is not None:
+            input_shape = inputs_embeds.size()[:-1]
+        else:
+            raise ValueError("You have to specify either input_ids or inputs_embeds")
 
-        if len(self.fsdp) == 0 and self.fsdp_min_num_params > 0:
-            warnings.warn(
-                "`--fsdp_min_num_params` is useful only when `--fsdp` is specified."
-            )
+        batch_size, seq_length = input_shape
+        device = input_ids.device if input_ids is not None else inputs_embeds.device
 
-        if len(self.fsdp) == 0 and self.fsdp_transformer_layer_cls_to_wrap is not None:
-            warnings.warn(
-                "`--fsdp_transformer_layer_cls_to_wrap` is useful only when `--fsdp` is specified."
-            )
+        # past_key_values_length
+        past_key_values_length = (
+            past_key_values[0][0].shape[2] if past_key_values is not None else 0
+        )
 
-        if (
-            len(self.fsdp) > 0
-            and self.fsdp_min_num_params > 0
-            and self.fsdp_transformer_layer_cls_to_wrap is not None
-        ):
-            raise ValueError(
-                "`--fsdp_min_num_params` and `--fsdp_transformer_layer_cls_to_wrap` are mutually exclusive."
+        if attention_mask is None:
+            attention_mask = torch.ones(
+                ((batch_size, seq_length + past_key_values_length)), device=device
             )
 
-        if self.tpu_metrics_debug:
-            warnings.warn(
-                "using `--tpu_metrics_debug` is deprecated and will be removed in version 5 of 🤗 Transformers. Use"
-                " `--debug tpu_metrics_debug` instead",
-                FutureWarning,
-            )
-            self.debug += " tpu_metrics_debug"
-            self.tpu_metrics_debug = False
-        if isinstance(self.debug, str):
-            self.debug = [DebugOption(s) for s in self.debug.split()]
-
-        if self.deepspeed:
-            # - must be run very last in arg parsing, since it will use a lot of these settings.
-            # - must be run before the model is created.
-            if not is_accelerate_available():
-                raise ValueError(
-                    "--deepspeed requires Accelerate to be installed: `pip install accelerate`."
-                )
-            from transformers.deepspeed import HfTrainerDeepSpeedConfig
-
-            # will be used later by the Trainer
-            # note: leave self.deepspeed unmodified in case a user relies on it not to be modified)
-            self.hf_deepspeed_config = HfTrainerDeepSpeedConfig(self.deepspeed)
-            self.hf_deepspeed_config.trainer_config_process(self)
-
-        if self.push_to_hub_token is not None:
-            warnings.warn(
-                "`--push_to_hub_token` is deprecated and will be removed in version 5 of 🤗 Transformers. Use "
-                "`--hub_token` instead.",
-                FutureWarning,
-            )
-            self.hub_token = self.push_to_hub_token
-
-        if self.push_to_hub_model_id is not None:
-            self.hub_model_id = get_full_repo_name(
-                self.push_to_hub_model_id,
-                organization=self.push_to_hub_organization,
-                token=self.hub_token,
-            )
-            if self.push_to_hub_organization is not None:
-                warnings.warn(
-                    "`--push_to_hub_model_id` and `--push_to_hub_organization` are deprecated and will be removed in "
-                    "version 5 of 🤗 Transformers. Use `--hub_model_id` instead and pass the full repo name to this "
-                    f"argument (in this case {self.hub_model_id}).",
-                    FutureWarning,
+        if token_type_ids is None:
+            if hasattr(self.embeddings, "token_type_ids"):
+                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]
+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(
+                    batch_size, seq_length
                 )
+                token_type_ids = buffered_token_type_ids_expanded
             else:
-                warnings.warn(
-                    "`--push_to_hub_model_id` is deprecated and will be removed in version 5 of 🤗 Transformers. Use "
-                    "`--hub_model_id` instead and pass the full repo name to this argument (in this case "
-                    f"{self.hub_model_id}).",
-                    FutureWarning,
+                token_type_ids = torch.zeros(
+                    input_shape, dtype=torch.long, device=device
                 )
-        elif self.push_to_hub_organization is not None:
-            self.hub_model_id = (
-                f"{self.push_to_hub_organization}/{Path(self.output_dir).name}"
-            )
-            warnings.warn(
-                "`--push_to_hub_organization` is deprecated and will be removed in version 5 of 🤗 Transformers. Use "
-                "`--hub_model_id` instead and pass the full repo name to this argument (in this case "
-                f"{self.hub_model_id}).",
-                FutureWarning,
-            )
-
-    def __str__(self):
-        self_as_dict = asdict(self)
-
-        # Remove deprecated arguments. That code should be removed once
-        # those deprecated arguments are removed from TrainingArguments. (TODO: v5)
-        del self_as_dict["per_gpu_train_batch_size"]
-        del self_as_dict["per_gpu_eval_batch_size"]
-
-        self_as_dict = {
-            k: f"<{k.upper()}>" if k.endswith("_token") else v
-            for k, v in self_as_dict.items()
-        }
 
-        attrs_as_str = [f"{k}={v},\n" for k, v in sorted(self_as_dict.items())]
-        return f"{self.__class__.__name__}(\n{''.join(attrs_as_str)})"
+        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]
+        # ourselves in which case we just need to make it broadcastable to all heads.
+        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(
+            attention_mask, input_shape
+        )
 
-    __repr__ = __str__
+        # If a 2D or 3D attention mask is provided for the cross-attention
+        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]
+        if self.config.is_decoder and encoder_hidden_states is not None:
+            (
+                encoder_batch_size,
+                encoder_sequence_length,
+                _,
+            ) = encoder_hidden_states.size()
+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)
+            if encoder_attention_mask is None:
+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)
+            encoder_extended_attention_mask = self.invert_attention_mask(
+                encoder_attention_mask
+            )
+        else:
+            encoder_extended_attention_mask = None
 
-    @property
-    def train_batch_size(self) -> int:
-        """
-        The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).
+        # Prepare head mask if needed
+        # 1.0 in head_mask indicate we keep the head
+        # attention_probs has shape bsz x n_heads x N x N
+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]
+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]
+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)
+
+        embedding_output = self.embeddings(
+            input_ids=input_ids,
+            position_ids=position_ids,
+            token_type_ids=token_type_ids,
+            inputs_embeds=inputs_embeds,
+            past_key_values_length=past_key_values_length,
+        )
+        encoder_outputs = self.encoder(
+            embedding_output,
+            attention_mask=extended_attention_mask,
+            head_mask=head_mask,
+            encoder_hidden_states=encoder_hidden_states,
+            encoder_attention_mask=encoder_extended_attention_mask,
+            past_key_values=past_key_values,
+            use_cache=use_cache,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+        sequence_output = encoder_outputs[0]
+        pooled_output = (
+            self.pooler(sequence_output) if self.pooler is not None else None
+        )
+
+        if not return_dict:
+            return (sequence_output, pooled_output) + encoder_outputs[1:]
+
+        return BaseModelOutputWithPoolingAndCrossAttentions(
+            last_hidden_state=sequence_output,
+            pooler_output=pooled_output,
+            past_key_values=encoder_outputs.past_key_values,
+            hidden_states=encoder_outputs.hidden_states,
+            attentions=encoder_outputs.attentions,
+            cross_attentions=encoder_outputs.cross_attentions,
+        )
+
+
+@add_start_docstrings(
+    "The embedder of REALM outputting projected score that will be used to calculate relevance score.",
+    REALM_START_DOCSTRING,
+)
+class RealmEmbedder(RealmPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+
+        self.realm = RealmBertModel(self.config)
+        self.cls = RealmScorerProjection(self.config)
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.realm.embeddings.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.realm.embeddings.word_embeddings = value
+
+    @add_start_docstrings_to_model_forward(
+        REALM_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=RealmEmbedderOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        r"""
+        Returns:
+
+        Example:
+
+        ```python
+        >>> from transformers import RealmTokenizer, RealmEmbedder
+        >>> import torch
+
+        >>> tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-embedder")
+        >>> model = RealmEmbedder.from_pretrained("google/realm-cc-news-pretrained-embedder")
+
+        >>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
+        >>> outputs = model(**inputs)
+
+        >>> projected_score = outputs.projected_score
+        ```
         """
-        if self.per_gpu_train_batch_size:
-            logger.warning(
-                "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future "
-                "version. Using `--per_device_train_batch_size` is preferred."
+
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        realm_outputs = self.realm(
+            input_ids,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
+
+        # [batch_size, hidden_size]
+        pooler_output = realm_outputs[1]
+        # [batch_size, retriever_proj_size]
+        projected_score = self.cls(pooler_output)
+
+        if not return_dict:
+            return (projected_score,) + realm_outputs[2:4]
+        else:
+            return RealmEmbedderOutput(
+                projected_score=projected_score,
+                hidden_states=realm_outputs.hidden_states,
+                attentions=realm_outputs.attentions,
             )
-        per_device_batch_size = (
-            self.per_gpu_train_batch_size or self.per_device_train_batch_size
+
+
+@add_start_docstrings(
+    "The scorer of REALM outputting relevance scores representing the score of document candidates (before softmax).",
+    REALM_START_DOCSTRING,
+)
+class RealmScorer(RealmPreTrainedModel):
+    r"""
+    Args:
+        query_embedder ([`RealmEmbedder`]):
+            Embedder for input sequences. If not specified, it will use the same embedder as candidate sequences.
+    """
+
+    def __init__(self, config, query_embedder=None):
+        super().__init__(config)
+
+        self.embedder = RealmEmbedder(self.config)
+
+        self.query_embedder = (
+            query_embedder if query_embedder is not None else self.embedder
         )
-        train_batch_size = per_device_batch_size * max(1, self.n_gpu)
-        return train_batch_size
 
-    @property
-    def eval_batch_size(self) -> int:
-        """
-        The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).
-        """
-        if self.per_gpu_eval_batch_size:
-            logger.warning(
-                "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future "
-                "version. Using `--per_device_eval_batch_size` is preferred."
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        REALM_INPUTS_DOCSTRING.format("batch_size, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=RealmScorerOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        candidate_input_ids=None,
+        candidate_attention_mask=None,
+        candidate_token_type_ids=None,
+        candidate_inputs_embeds=None,
+        head_mask=None,
+        inputs_embeds=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        r"""
+        candidate_input_ids (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`):
+            Indices of candidate input sequence tokens in the vocabulary.
+
+            Indices can be obtained using [`RealmTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+            [`PreTrainedTokenizer.__call__`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        candidate_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_candidates, sequence_length)`, *optional*):
+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+            [What are attention masks?](../glossary#attention-mask)
+        candidate_token_type_ids (`torch.LongTensor` of shape `(batch_size, num_candidates, sequence_length)`, *optional*):
+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
+            1]`:
+
+            - 0 corresponds to a *sentence A* token,
+            - 1 corresponds to a *sentence B* token.
+
+            [What are token type IDs?](../glossary#token-type-ids)
+        candidate_inputs_embeds (`torch.FloatTensor` of shape `(batch_size * num_candidates, sequence_length, hidden_size)`, *optional*):
+            Optionally, instead of passing `candidate_input_ids` you can choose to directly pass an embedded
+            representation. This is useful if you want more control over how to convert *candidate_input_ids* indices
+            into associated vectors than the model's internal embedding lookup matrix.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> import torch
+        >>> from transformers import RealmTokenizer, RealmScorer
+
+        >>> tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-scorer")
+        >>> model = RealmScorer.from_pretrained("google/realm-cc-news-pretrained-scorer", num_candidates=2)
+
+        >>> # batch_size = 2, num_candidates = 2
+        >>> input_texts = ["How are you?", "What is the item in the picture?"]
+        >>> candidates_texts = [["Hello world!", "Nice to meet you!"], ["A cute cat.", "An adorable dog."]]
+
+        >>> inputs = tokenizer(input_texts, return_tensors="pt")
+        >>> candidates_inputs = tokenizer.batch_encode_candidates(candidates_texts, max_length=10, return_tensors="pt")
+
+        >>> outputs = model(
+        ...     **inputs,
+        ...     candidate_input_ids=candidates_inputs.input_ids,
+        ...     candidate_attention_mask=candidates_inputs.attention_mask,
+        ...     candidate_token_type_ids=candidates_inputs.token_type_ids,
+        ... )
+        >>> relevance_score = outputs.relevance_score
+        ```"""
+
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
+
+        if input_ids is None and inputs_embeds is None:
+            raise ValueError("You have to specify either input_ids or input_embeds.")
+
+        if candidate_input_ids is None and candidate_inputs_embeds is None:
+            raise ValueError(
+                "You have to specify either candidate_input_ids or candidate_inputs_embeds."
             )
-        per_device_batch_size = (
-            self.per_gpu_eval_batch_size or self.per_device_eval_batch_size
+
+        query_outputs = self.query_embedder(
+            input_ids,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
         )
-        eval_batch_size = per_device_batch_size * max(1, self.n_gpu)
-        return eval_batch_size
 
-    @property
-    def ddp_timeout_delta(self) -> timedelta:
-        """
-        The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.
-        """
-        return timedelta(seconds=self.ddp_timeout)
+        # [batch_size * num_candidates, candidate_seq_len]
+        (
+            flattened_input_ids,
+            flattened_attention_mask,
+            flattened_token_type_ids,
+        ) = self._flatten_inputs(
+            candidate_input_ids, candidate_attention_mask, candidate_token_type_ids
+        )
 
-    @cached_property
-    @torch_required
-    def _setup_devices(self) -> "torch.device":
-        logger.info("PyTorch: setting up devices")
-        if (
-            torch.distributed.is_available()
-            and torch.distributed.is_initialized()
-            and self.local_rank == -1
-        ):
-            logger.warning(
-                "torch.distributed process group is initialized, but local_rank == -1. "
-                "In order to use Torch DDP, launch your script with `python -m torch.distributed.launch"
-            )
-        if self.no_cuda:
-            device = torch.device("cpu")
-            self._n_gpu = 0
-            self.local_rank = get_int_from_env(
-                [
-                    "LOCAL_RANK",
-                    "MPI_LOCALRANKID",
-                    "OMPI_COMM_WORLD_LOCAL_RANK",
-                    "MV2_COMM_WORLD_LOCAL_RANK",
-                ],
-                self.local_rank,
-            )
-            if self.local_rank != -1 and not torch.distributed.is_initialized():
-                # Initializes distributed backend for cpu
-                if self.xpu_backend not in ("mpi", "ccl", "gloo"):
-                    raise ValueError(
-                        "CPU distributed training backend is not properly set. "
-                        "Please set '--xpu_backend' to either 'mpi' or 'ccl' or 'gloo'."
-                    )
-                if self.xpu_backend == "ccl":
-                    requires_backends(self, "oneccl_bind_pt")
-                    if ccl_version >= "1.12":
-                        import oneccl_bindings_for_pytorch  # noqa: F401
-                    else:
-                        import torch_ccl  # noqa: F401
-                    if int(os.environ.get("CCL_WORKER_COUNT", 0)) < 1:
-                        raise ValueError(
-                            "CPU distributed training backend is ccl. but CCL_WORKER_COUNT is not correctly set. "
-                            "Please use like 'export CCL_WORKER_COUNT = 1' to set."
-                        )
-
-                # Try to get launch configuration from environment variables set by MPI launcher - works for Intel MPI, OpenMPI and MVAPICH
-                rank = get_int_from_env(
-                    ["RANK", "PMI_RANK", "OMPI_COMM_WORLD_RANK", "MV2_COMM_WORLD_RANK"],
-                    0,
-                )
-                size = get_int_from_env(
-                    [
-                        "WORLD_SIZE",
-                        "PMI_SIZE",
-                        "OMPI_COMM_WORLD_SIZE",
-                        "MV2_COMM_WORLD_SIZE",
-                    ],
-                    1,
-                )
-                local_size = get_int_from_env(
-                    [
-                        "MPI_LOCALNRANKS",
-                        "OMPI_COMM_WORLD_LOCAL_SIZE",
-                        "MV2_COMM_WORLD_LOCAL_SIZE",
-                    ],
-                    1,
-                )
-                os.environ["RANK"] = str(rank)
-                os.environ["WORLD_SIZE"] = str(size)
-                os.environ["LOCAL_RANK"] = str(self.local_rank)
-                if not os.environ.get("MASTER_PORT", None):
-                    os.environ["MASTER_PORT"] = "29500"
-                if not os.environ.get("MASTER_ADDR", None):
-                    if local_size != size or self.xpu_backend != "mpi":
-                        raise ValueError(
-                            "Looks like distributed multinode run but MASTER_ADDR env not set, "
-                            "please try exporting rank 0's hostname as MASTER_ADDR"
-                        )
-                if (
-                    torch.get_num_threads() == 1
-                    and get_int_from_env(["OMP_NUM_THREADS", "MKL_NUM_THREADS"], 0) == 0
-                    and is_psutil_available()
-                ):
-                    import psutil
+        candidate_outputs = self.embedder(
+            flattened_input_ids,
+            attention_mask=flattened_attention_mask,
+            token_type_ids=flattened_token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=candidate_inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-                    num_cpu_threads_per_process = int(
-                        psutil.cpu_count(logical=False) / local_size
-                    )
-                    if num_cpu_threads_per_process == 0:
-                        num_cpu_threads_per_process = 1
-                    torch.set_num_threads(num_cpu_threads_per_process)
-                    logger.info(
-                        f"num_cpu_threads_per_process unset, we set it at {num_cpu_threads_per_process} to improve oob"
-                        " performance."
-                    )
-                torch.distributed.init_process_group(
-                    backend=self.xpu_backend,
-                    rank=rank,
-                    world_size=size,
-                    timeout=self.ddp_timeout_delta,
-                )
-        elif is_torch_tpu_available():
-            device = xm.xla_device()
-            self._n_gpu = 0
-        elif is_sagemaker_mp_enabled():
-            local_rank = smp.local_rank()
-            device = torch.device("cuda", local_rank)
-            self._n_gpu = 1
-        elif is_sagemaker_dp_enabled():
-            import smdistributed.dataparallel.torch.torch_smddp  # noqa: F401
-
-            dist.init_process_group(backend="smddp", timeout=self.ddp_timeout_delta)
-            self.local_rank = int(os.getenv("SMDATAPARALLEL_LOCAL_RANK"))
-            device = torch.device("cuda", self.local_rank)
-            self._n_gpu = 1
-        elif self.deepspeed:
-            # deepspeed inits torch.distributed internally
-            from .deepspeed import is_deepspeed_available
-
-            if not is_deepspeed_available():
-                raise ImportError(
-                    "--deepspeed requires deepspeed: `pip install deepspeed`."
-                )
-            import deepspeed
+        # [batch_size, retriever_proj_size]
+        query_score = query_outputs[0]
+        # [batch_size * num_candidates, retriever_proj_size]
+        candidate_score = candidate_outputs[0]
+        # [batch_size, num_candidates, retriever_proj_size]
+        candidate_score = candidate_score.view(
+            -1, self.config.num_candidates, self.config.retriever_proj_size
+        )
+        # [batch_size, num_candidates]
+        relevance_score = torch.einsum("BD,BND->BN", query_score, candidate_score)
 
-            deepspeed.init_distributed()
+        if not return_dict:
+            return relevance_score, query_score, candidate_score
 
-            # workaround for setups like notebooks where the launcher can't be used,
-            # but deepspeed requires a dist env.
-            # env LOCAL_RANK could be set manually by the user, or via init_distributed if mpi4py is installed
-            self.local_rank = int(os.environ.get("LOCAL_RANK", "-1"))
-
-            device = torch.device("cuda", self.local_rank)
-            self._n_gpu = 1
-        elif self.local_rank == -1:
-            if self.use_mps_device:
-                if not torch.backends.mps.is_available():
-                    if not torch.backends.mps.is_built():
-                        raise AssertionError(
-                            "MPS not available because the current PyTorch install was not "
-                            "built with MPS enabled. Please install torch version >=1.12.0 on "
-                            "your Apple silicon Mac running macOS 12.3 or later with a native "
-                            "version (arm64) of Python"
-                        )
-                    else:
-                        raise AssertionError(
-                            "MPS not available because the current MacOS version is not 12.3+ "
-                            "and/or you do not have an MPS-enabled device on this machine."
-                        )
-                else:
-                    if not version.parse(
-                        version.parse(torch.__version__).base_version
-                    ) > version.parse("1.12.0"):
-                        warnings.warn(
-                            "We strongly recommend to install PyTorch >= 1.13 (nightly version at the time of writing)"
-                            " on your MacOS machine. It has major fixes related to model correctness and performance"
-                            " improvements for transformer based models. Please refer to"
-                            " https://github.com/pytorch/pytorch/issues/82707 for more details."
-                        )
-                    device = torch.device("mps")
-                    self._n_gpu = 1
+        return RealmScorerOutput(
+            relevance_score=relevance_score,
+            query_score=query_score,
+            candidate_score=candidate_score,
+        )
 
-            else:
-                # if n_gpu is > 1 we'll use nn.DataParallel.
-                # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`
-                # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will
-                # trigger an error that a device index is missing. Index 0 takes into account the
-                # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`
-                # will use the first GPU in that env, i.e. GPU#1
-                device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-                # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at
-                # the default value.
-                self._n_gpu = torch.cuda.device_count()
-        else:
-            # Here, we'll use torch.distributed.
-            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs
-            if not torch.distributed.is_initialized():
-                torch.distributed.init_process_group(
-                    backend="nccl", timeout=self.ddp_timeout_delta
-                )
-            device = torch.device("cuda", self.local_rank)
-            self._n_gpu = 1
 
-        if device.type == "cuda":
-            torch.cuda.set_device(device)
+@add_start_docstrings(
+    "The knowledge-augmented encoder of REALM outputting masked language model logits and marginal log-likelihood"
+    " loss.",
+    REALM_START_DOCSTRING,
+)
+class RealmKnowledgeAugEncoder(RealmPreTrainedModel):
+    def __init__(self, config):
+        super().__init__(config)
+        self.realm = RealmBertModel(self.config)
+        self.cls = RealmOnlyMLMHead(self.config)
+        self.post_init()
+
+    def get_input_embeddings(self):
+        return self.realm.embeddings.word_embeddings
+
+    def set_input_embeddings(self, value):
+        self.realm.embeddings.word_embeddings = value
+
+    def get_output_embeddings(self):
+        return self.cls.predictions.decoder
+
+    def set_output_embeddings(self, new_embeddings):
+        self.cls.predictions.decoder = new_embeddings
+
+    @add_start_docstrings_to_model_forward(
+        REALM_INPUTS_DOCSTRING.format("batch_size, num_candidates, sequence_length")
+    )
+    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)
+    def forward(
+        self,
+        input_ids=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        relevance_score=None,
+        labels=None,
+        mlm_mask=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        r"""
+        relevance_score (`torch.FloatTensor` of shape `(batch_size, num_candidates)`, *optional*):
+            Relevance score derived from RealmScorer, must be specified if you want to compute the masked language
+            modeling loss.
+
+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,
+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the
+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
+
+        mlm_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):
+            Mask to avoid calculating joint loss on certain positions. If not specified, the loss will not be masked.
+            Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+        Returns:
+
+        Example:
+
+        ```python
+        >>> import torch
+        >>> from transformers import RealmTokenizer, RealmKnowledgeAugEncoder
+
+        >>> tokenizer = RealmTokenizer.from_pretrained("google/realm-cc-news-pretrained-encoder")
+        >>> model = RealmKnowledgeAugEncoder.from_pretrained(
+        ...     "google/realm-cc-news-pretrained-encoder", num_candidates=2
+        ... )
+
+        >>> # batch_size = 2, num_candidates = 2
+        >>> text = [["Hello world!", "Nice to meet you!"], ["The cute cat.", "The adorable dog."]]
+
+        >>> inputs = tokenizer.batch_encode_candidates(text, max_length=10, return_tensors="pt")
+        >>> outputs = model(**inputs)
+        >>> logits = outputs.logits
+        ```"""
 
-        return device
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
 
-    @property
-    @torch_required
-    def device(self) -> "torch.device":
-        """
-        The device used by this process.
-        """
-        return self._setup_devices
+        (
+            flattened_input_ids,
+            flattened_attention_mask,
+            flattened_token_type_ids,
+        ) = self._flatten_inputs(input_ids, attention_mask, token_type_ids)
+
+        joint_outputs = self.realm(
+            flattened_input_ids,
+            attention_mask=flattened_attention_mask,
+            token_type_ids=flattened_token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-    @property
-    @torch_required
-    def n_gpu(self):
-        """
-        The number of GPUs used by this process.
+        # [batch_size * num_candidates, joint_seq_len, hidden_size]
+        joint_output = joint_outputs[0]
+        # [batch_size * num_candidates, joint_seq_len, vocab_size]
+        prediction_scores = self.cls(joint_output)
+        # [batch_size, num_candidates]
+        candidate_score = relevance_score
+
+        masked_lm_loss = None
+        if labels is not None:
+            if candidate_score is None:
+                raise ValueError(
+                    "You have to specify `relevance_score` when `labels` is specified in order to compute loss."
+                )
 
-        Note:
-            This will only be greater than one when you have multiple GPUs available but are not using distributed
-            training. For distributed training, it will always be 1.
-        """
-        # Make sure `self._n_gpu` is properly setup.
-        _ = self._setup_devices
-        return self._n_gpu
+            batch_size, seq_length = labels.size()
 
-    @property
-    @torch_required
-    def parallel_mode(self):
-        """
-        The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:
+            if mlm_mask is None:
+                mlm_mask = torch.ones_like(labels, dtype=torch.float32)
+            else:
+                mlm_mask = mlm_mask.type(torch.float32)
 
-        - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).
-        - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).
-        - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses
-          `torch.nn.DistributedDataParallel`).
-        - `ParallelMode.TPU`: several TPU cores.
-        """
-        if is_torch_tpu_available():
-            return ParallelMode.TPU
-        elif is_sagemaker_mp_enabled():
-            return ParallelMode.SAGEMAKER_MODEL_PARALLEL
-        elif is_sagemaker_dp_enabled():
-            return ParallelMode.SAGEMAKER_DATA_PARALLEL
-        elif self.local_rank != -1:
-            return ParallelMode.DISTRIBUTED
-        elif self.n_gpu > 1:
-            return ParallelMode.NOT_DISTRIBUTED
-        else:
-            return ParallelMode.NOT_PARALLEL
+            # Compute marginal log-likelihood
+            loss_fct = CrossEntropyLoss(reduction="none")  # -100 index = padding token
 
-    @property
-    @torch_required
-    def world_size(self):
-        """
-        The number of processes used in parallel.
-        """
-        if is_torch_tpu_available():
-            return xm.xrt_world_size()
-        elif is_sagemaker_mp_enabled():
-            return (
-                smp.dp_size() if not smp.state.cfg.prescaled_batch else smp.rdp_size()
+            # [batch_size * num_candidates * joint_seq_len, vocab_size]
+            mlm_logits = prediction_scores.view(-1, self.config.vocab_size)
+            # [batch_size * num_candidates * joint_seq_len]
+            mlm_targets = labels.tile(1, self.config.num_candidates).view(-1)
+            # [batch_size, num_candidates, joint_seq_len]
+            masked_lm_log_prob = -loss_fct(mlm_logits, mlm_targets).view(
+                batch_size, self.config.num_candidates, seq_length
+            )
+            # [batch_size, num_candidates, 1]
+            candidate_log_prob = candidate_score.log_softmax(-1).unsqueeze(-1)
+            # [batch_size, num_candidates, joint_seq_len]
+            joint_gold_log_prob = candidate_log_prob + masked_lm_log_prob
+            # [batch_size, joint_seq_len]
+            marginal_gold_log_probs = joint_gold_log_prob.logsumexp(1)
+            # []
+            masked_lm_loss = -torch.nansum(
+                torch.sum(marginal_gold_log_probs * mlm_mask) / torch.sum(mlm_mask)
             )
-        elif is_sagemaker_dp_enabled():
-            return dist.get_world_size()
-        elif self.local_rank != -1:
-            return torch.distributed.get_world_size()
-        return 1
 
-    @property
-    @torch_required
-    def process_index(self):
-        """
-        The index of the current process used.
-        """
-        if is_torch_tpu_available():
-            return xm.get_ordinal()
-        elif is_sagemaker_mp_enabled():
+        if not return_dict:
+            output = (prediction_scores,) + joint_outputs[2:4]
             return (
-                smp.dp_rank() if not smp.state.cfg.prescaled_batch else smp.rdp_rank()
+                ((masked_lm_loss,) + output) if masked_lm_loss is not None else output
             )
-        elif is_sagemaker_dp_enabled():
-            return dist.get_rank()
-        elif self.local_rank != -1:
-            return torch.distributed.get_rank()
-        return 0
 
-    @property
-    @torch_required
-    def local_process_index(self):
-        """
-        The index of the local process used.
-        """
-        if is_torch_tpu_available():
-            return xm.get_local_ordinal()
-        elif is_sagemaker_mp_enabled():
-            return smp.local_rank()
-        elif is_sagemaker_dp_enabled():
-            return dist.get_rank()
-        elif self.local_rank != -1:
-            return self.local_rank
-        return 0
+        return MaskedLMOutput(
+            loss=masked_lm_loss,
+            logits=prediction_scores,
+            hidden_states=joint_outputs.hidden_states,
+            attentions=joint_outputs.attentions,
+        )
 
-    @property
-    def should_log(self):
-        """
-        Whether or not the current process should produce log.
-        """
-        if self.log_on_each_node:
-            return self.local_process_index == 0
-        else:
-            if is_sagemaker_mp_enabled():
-                return smp.rank() == 0
-            else:
-                return self.process_index == 0
 
-    @property
-    def should_save(self):
-        """
-        Whether or not the current process should write to disk, e.g., to save models and checkpoints.
-        """
-        if self.save_on_each_node:
-            return self.local_process_index == 0
-        else:
-            if is_sagemaker_mp_enabled():
-                return smp.rank() == 0
-            else:
-                return self.process_index == 0
+@add_start_docstrings("The reader of REALM.", REALM_START_DOCSTRING)
+class RealmReader(RealmPreTrainedModel):
+    _keys_to_ignore_on_load_unexpected = [r"pooler", "cls"]
+
+    def __init__(self, config):
+        super().__init__(config)
+        self.num_labels = config.num_labels
+
+        self.realm = RealmBertModel(config)
+        self.cls = RealmOnlyMLMHead(config)
+        self.qa_outputs = RealmReaderProjection(config)
+
+        self.post_init()
+
+    @add_start_docstrings_to_model_forward(
+        REALM_INPUTS_DOCSTRING.format("reader_beam_size, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=RealmReaderOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids=None,
+        attention_mask=None,
+        token_type_ids=None,
+        position_ids=None,
+        head_mask=None,
+        inputs_embeds=None,
+        relevance_score=None,
+        block_mask=None,
+        start_positions=None,
+        end_positions=None,
+        has_answers=None,
+        output_attentions=None,
+        output_hidden_states=None,
+        return_dict=None,
+    ):
+        r"""
+        relevance_score (`torch.FloatTensor` of shape `(searcher_beam_size,)`, *optional*):
+            Relevance score, which must be specified if you want to compute the logits and marginal log loss.
+        block_mask (`torch.BoolTensor` of shape `(searcher_beam_size, sequence_length)`, *optional*):
+            The mask of the evidence block, which must be specified if you want to compute the logits and marginal log
+            loss.
+        start_positions (`torch.LongTensor` of shape `(searcher_beam_size,)`, *optional*):
+            Labels for position (index) of the start of the labelled span for computing the token classification loss.
+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
+            are not taken into account for computing the loss.
+        end_positions (`torch.LongTensor` of shape `(searcher_beam_size,)`, *optional*):
+            Labels for position (index) of the end of the labelled span for computing the token classification loss.
+            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence
+            are not taken into account for computing the loss.
+        has_answers (`torch.BoolTensor` of shape `(searcher_beam_size,)`, *optional*):
+            Whether or not the evidence block has answer(s).
 
-    def get_process_log_level(self):
+        Returns:
         """
-        Returns the log level to be used depending on whether this process is the main process of node 0, main process
-        of node non-0, or a non-main process.
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
+        )
 
-        For the main process the log level defaults to `logging.INFO` unless overridden by `log_level` argument.
+        if relevance_score is None:
+            raise ValueError(
+                "You have to specify `relevance_score` to calculate logits and loss."
+            )
+        if block_mask is None:
+            raise ValueError(
+                "You have to specify `block_mask` to separate question block and evidence block."
+            )
+        if token_type_ids.size(1) < self.config.max_span_width:
+            raise ValueError(
+                "The input sequence length must be greater than or equal to config.max_span_width."
+            )
+        outputs = self.realm(
+            input_ids,
+            attention_mask=attention_mask,
+            token_type_ids=token_type_ids,
+            position_ids=position_ids,
+            head_mask=head_mask,
+            inputs_embeds=inputs_embeds,
+            output_attentions=output_attentions,
+            output_hidden_states=output_hidden_states,
+            return_dict=return_dict,
+        )
 
-        For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`
-        argument.
+        # [reader_beam_size, joint_seq_len, hidden_size]
+        sequence_output = outputs[0]
 
-        The choice between the main and replica process settings is made according to the return value of `should_log`.
-        """
+        # [reader_beam_size, num_candidates], [num_candidates], [num_candidates]
+        reader_logits, candidate_starts, candidate_ends = self.qa_outputs(
+            sequence_output, block_mask[0 : self.config.reader_beam_size]
+        )
+        # [searcher_beam_size, 1]
+        retriever_logits = torch.unsqueeze(
+            relevance_score[0 : self.config.reader_beam_size], -1
+        )
+        # [reader_beam_size, num_candidates]
+        reader_logits += retriever_logits
+        # []
+        predicted_block_index = torch.argmax(torch.max(reader_logits, dim=1).values)
+        # []
+        predicted_candidate = torch.argmax(torch.max(reader_logits, dim=0).values)
+        # [1]
+        predicted_start = torch.index_select(
+            candidate_starts, dim=0, index=predicted_candidate
+        )
+        # [1]
+        predicted_end = torch.index_select(
+            candidate_ends, dim=0, index=predicted_candidate
+        )
+
+        total_loss = None
+        retriever_loss = None
+        reader_loss = None
+        retriever_correct = None
+        reader_correct = None
+        if (
+            start_positions is not None
+            and end_positions is not None
+            and has_answers is not None
+        ):
+
+            def compute_correct_candidates(
+                candidate_starts, candidate_ends, gold_starts, gold_ends
+            ):
+                """Compute correct span."""
+                # [reader_beam_size, num_answers, num_candidates]
+                is_gold_start = torch.eq(
+                    torch.unsqueeze(torch.unsqueeze(candidate_starts, 0), 0),
+                    torch.unsqueeze(gold_starts, -1),
+                )
+                is_gold_end = torch.eq(
+                    torch.unsqueeze(torch.unsqueeze(candidate_ends, 0), 0),
+                    torch.unsqueeze(gold_ends, -1),
+                )
+
+                # [reader_beam_size, num_candidates]
+                return torch.any(torch.logical_and(is_gold_start, is_gold_end), 1)
+
+            def marginal_log_loss(logits, is_correct):
+                """Loss based on the negative marginal log-likelihood."""
+
+                def mask_to_score(mask, dtype=torch.float32):
+                    return (1.0 - mask.type(dtype)) * torch.finfo(dtype).min
+
+                # []
+                log_numerator = torch.logsumexp(
+                    logits + mask_to_score(is_correct, dtype=logits.dtype), dim=-1
+                )
+                log_denominator = torch.logsumexp(logits, dim=-1)
+                return log_denominator - log_numerator
+
+            # sometimes the start/end positions are outside our model inputs, we ignore these terms
+            # `-1` is reserved for no answer.
+            ignored_index = sequence_output.size(1)
+            start_positions = start_positions.clamp(-1, ignored_index)
+            end_positions = end_positions.clamp(-1, ignored_index)
+
+            retriever_correct = has_answers
+            any_retriever_correct = torch.any(retriever_correct)
+
+            reader_correct = compute_correct_candidates(
+                candidate_starts=candidate_starts,
+                candidate_ends=candidate_ends,
+                gold_starts=start_positions[0 : self.config.reader_beam_size],
+                gold_ends=end_positions[0 : self.config.reader_beam_size],
+            )
+            any_reader_correct = torch.any(reader_correct)
+
+            retriever_loss = marginal_log_loss(relevance_score, retriever_correct)
+            reader_loss = marginal_log_loss(
+                reader_logits.view(-1), reader_correct.view(-1)
+            )
+            retriever_loss *= any_retriever_correct.type(torch.float32)
+            reader_loss *= any_reader_correct.type(torch.float32)
+
+            total_loss = (retriever_loss + reader_loss).mean()
+
+        if not return_dict:
+            output = (
+                predicted_block_index,
+                predicted_candidate,
+                predicted_start,
+                predicted_end,
+            ) + outputs[2:]
+            return (
+                (
+                    (
+                        total_loss,
+                        retriever_loss,
+                        reader_loss,
+                        retriever_correct,
+                        reader_correct,
+                    )
+                    + output
+                )
+                if total_loss is not None
+                else output
+            )
 
-        # convert to int
-        log_level = trainer_log_levels[self.log_level]
-        log_level_replica = trainer_log_levels[self.log_level_replica]
-
-        log_level_main_node = logging.INFO if log_level == -1 else log_level
-        log_level_replica_node = (
-            logging.WARNING if log_level_replica == -1 else log_level_replica
+        return RealmReaderOutput(
+            loss=total_loss,
+            retriever_loss=retriever_loss,
+            reader_loss=reader_loss,
+            retriever_correct=retriever_correct,
+            reader_correct=reader_correct,
+            block_idx=predicted_block_index,
+            candidate=predicted_candidate,
+            start_pos=predicted_start,
+            end_pos=predicted_end,
+            hidden_states=outputs.hidden_states,
+            attentions=outputs.attentions,
         )
-        return log_level_main_node if self.should_log else log_level_replica_node
 
-    @property
-    def place_model_on_device(self):
-        """
-        Can be subclassed and overridden for some specific integrations.
-        """
-        return not is_sagemaker_mp_enabled()
 
-    @property
-    def _no_sync_in_gradient_accumulation(self):
-        """
-        Whether or not to use no_sync for the gradients when doing gradient accumulation.
-        """
-        return not (
-            self.deepspeed or is_sagemaker_dp_enabled() or is_sagemaker_mp_enabled()
+REALM_FOR_OPEN_QA_DOCSTRING = r"""
+    Args:
+        input_ids (`torch.LongTensor` of shape `({0})`):
+            Indices of input sequence tokens in the vocabulary.
+
+            Indices can be obtained using [`RealmTokenizer`]. See [`PreTrainedTokenizer.encode`] and
+            [`PreTrainedTokenizer.__call__`] for details.
+
+            [What are input IDs?](../glossary#input-ids)
+        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):
+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:
+
+            - 1 for tokens that are **not masked**,
+            - 0 for tokens that are **masked**.
+
+            [What are attention masks?](../glossary#attention-mask)
+        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):
+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,
+            1]`:
+
+            - 0 corresponds to a *sentence A* token,
+            - 1 corresponds to a *sentence B* token (should not be used in this model by design).
+
+            [What are token type IDs?](../glossary#token-type-ids)
+        answer_ids (`list` of shape `(num_answers, answer_length)`, *optional*):
+            Answer ids for computing the marginal log-likelihood loss. Indices should be in `[-1, 0, ...,
+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-1` are ignored (masked), the
+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`
+        return_dict (`bool`, *optional*):
+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.
+"""
+
+
+@add_start_docstrings(
+    "`RealmForOpenQA` for end-to-end open domain question answering.",
+    REALM_START_DOCSTRING,
+)
+class RealmForOpenQA(RealmPreTrainedModel):
+    def __init__(self, config, retriever=None):
+        super().__init__(config)
+        self.embedder = RealmEmbedder(config)
+        self.reader = RealmReader(config)
+        self.register_buffer(
+            "block_emb",
+            torch.zeros(()).new_empty(
+                size=(config.num_block_records, config.retriever_proj_size),
+                dtype=torch.float32,
+                device=torch.device("cpu"),
+            ),
         )
+        self.retriever = retriever
 
-    @contextlib.contextmanager
-    def main_process_first(self, local=True, desc="work"):
-        """
-        A context manager for torch distributed environment where on needs to do something on the main process, while
-        blocking replicas, and when it's finished releasing the replicas.
+        self.post_init()
 
-        One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,
-        which upon completion saves a cached version of results and which then automatically gets loaded by the
-        replicas.
+    @property
+    def searcher_beam_size(self):
+        if self.training:
+            return self.config.searcher_beam_size
+        return self.config.reader_beam_size
 
-        Args:
-            local (`bool`, *optional*, defaults to `True`):
-                if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node
-                rank 0 In multi-node environment with a shared filesystem you most likely will want to use
-                `local=False` so that only the main process of the first node will do the processing. If however, the
-                filesystem is not shared, then the main process of each node will need to do the processing, which is
-                the default behavior.
-            desc (`str`, *optional*, defaults to `"work"`):
-                a work description to be used in debug logs
+    def block_embedding_to(self, device):
+        """Send `self.block_emb` to a specific device.
 
+        Args:
+            device (`str` or `torch.device`):
+                The device to which `self.block_emb` will be sent.
         """
-        if is_torch_available() and self.world_size > 1:
-            main_process_desc = "main process"
-            if local:
-                is_main_process = self.local_process_index == 0
-                main_process_desc = "main local process"
-            elif is_sagemaker_mp_enabled():
-                is_main_process = smp.rank() == 0
-            else:
-                is_main_process = self.process_index == 0
 
-            try:
-                if not is_main_process:
-                    # tell all replicas to wait
-                    logger.debug(
-                        f"{self.process_index}: waiting for the {main_process_desc} to perform {desc}"
-                    )
-                    if is_torch_tpu_available():
-                        xm.rendezvous(desc)
-                    elif is_sagemaker_dp_enabled():
-                        dist.barrier()
-                    else:
-                        torch.distributed.barrier()
-                yield
-            finally:
-                if is_main_process:
-                    # the wait is over
-                    logger.debug(
-                        f"{self.process_index}: {main_process_desc} completed {desc}, releasing all replicas"
-                    )
-                    if is_torch_tpu_available():
-                        xm.rendezvous(desc)
-                    elif is_sagemaker_dp_enabled():
-                        dist.barrier()
-                    else:
-                        torch.distributed.barrier()
-        else:
-            yield
+        self.block_emb = self.block_emb.to(device)
 
-    def get_warmup_steps(self, num_training_steps: int):
-        """
-        Get number of steps used for a linear warmup.
-        """
-        warmup_steps = (
-            self.warmup_steps
-            if self.warmup_steps > 0
-            else math.ceil(num_training_steps * self.warmup_ratio)
+    @add_start_docstrings_to_model_forward(
+        REALM_FOR_OPEN_QA_DOCSTRING.format("1, sequence_length")
+    )
+    @replace_return_docstrings(
+        output_type=RealmForOpenQAOutput, config_class=_CONFIG_FOR_DOC
+    )
+    def forward(
+        self,
+        input_ids,
+        attention_mask=None,
+        token_type_ids=None,
+        answer_ids=None,
+        return_dict=None,
+    ):
+        r"""
+        Returns:
+
+        Example:
+
+        ```python
+        >>> import torch
+        >>> from transformers import RealmForOpenQA, RealmRetriever, RealmTokenizer
+
+        >>> retriever = RealmRetriever.from_pretrained("google/realm-orqa-nq-openqa")
+        >>> tokenizer = RealmTokenizer.from_pretrained("google/realm-orqa-nq-openqa")
+        >>> model = RealmForOpenQA.from_pretrained("google/realm-orqa-nq-openqa", retriever=retriever)
+
+        >>> question = "Who is the pioneer in modern computer science?"
+        >>> question_ids = tokenizer([question], return_tensors="pt")
+        >>> answer_ids = tokenizer(
+        ...     ["alan mathison turing"],
+        ...     add_special_tokens=False,
+        ...     return_token_type_ids=False,
+        ...     return_attention_mask=False,
+        ... ).input_ids
+
+        >>> reader_output, predicted_answer_ids = model(**question_ids, answer_ids=answer_ids, return_dict=False)
+        >>> predicted_answer = tokenizer.decode(predicted_answer_ids)
+        >>> loss = reader_output.loss
+        ```"""
+
+        return_dict = (
+            return_dict if return_dict is not None else self.config.use_return_dict
         )
-        return warmup_steps
 
-    def to_dict(self):
-        """
-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates
-        the token values by removing their value.
-        """
-        # filter out fields that are defined as field(init=False)
-        d = dict(
-            (field.name, getattr(self, field.name))
-            for field in fields(self)
-            if field.init
-        )
-
-        for k, v in d.items():
-            if isinstance(v, Enum):
-                d[k] = v.value
-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):
-                d[k] = [x.value for x in v]
-            if k.endswith("_token"):
-                d[k] = f"<{k.upper()}>"
-        return d
+        if input_ids is not None and input_ids.shape[0] != 1:
+            raise ValueError("The batch_size of the inputs must be 1.")
 
-    def to_json_string(self):
-        """
-        Serializes this instance to a JSON string.
-        """
-        return json.dumps(self.to_dict(), indent=2)
+        question_outputs = self.embedder(
+            input_ids=input_ids,
+            token_type_ids=token_type_ids,
+            attention_mask=attention_mask,
+            return_dict=True,
+        )
+        # [1, projection_size]
+        question_projection = question_outputs[0]
 
-    def to_sanitized_dict(self) -> Dict[str, Any]:
-        """
-        Sanitized serialization to use with TensorBoard’s hparams
-        """
-        d = self.to_dict()
-        d = {
-            **d,
-            **{
-                "train_batch_size": self.train_batch_size,
-                "eval_batch_size": self.eval_batch_size,
-            },
-        }
-
-        valid_types = [bool, int, float, str]
-        if is_torch_available():
-            valid_types.append(torch.Tensor)
-
-        return {k: v if type(v) in valid_types else str(v) for k, v in d.items()}
-
-
-class ParallelMode(Enum):
-    NOT_PARALLEL = "not_parallel"
-    NOT_DISTRIBUTED = "not_distributed"
-    DISTRIBUTED = "distributed"
-    SAGEMAKER_MODEL_PARALLEL = "sagemaker_model_parallel"
-    SAGEMAKER_DATA_PARALLEL = "sagemaker_data_parallel"
-    TPU = "tpu"
+        # CPU computation starts.
+        # [1, block_emb_size]
+        batch_scores = torch.einsum(
+            "BD,QD->QB", self.block_emb, question_projection.to(self.block_emb.device)
+        )
+        # [1, searcher_beam_size]
+        _, retrieved_block_ids = torch.topk(
+            batch_scores, k=self.searcher_beam_size, dim=-1
+        )
+        # [searcher_beam_size]
+        retrieved_block_ids = retrieved_block_ids.squeeze()
+        # [searcher_beam_size, projection_size]
+        retrieved_block_emb = torch.index_select(
+            self.block_emb, dim=0, index=retrieved_block_ids
+        )
+        # CPU computation ends.
+
+        # Retrieve possible answers
+        has_answers, start_pos, end_pos, concat_inputs = self.retriever(
+            retrieved_block_ids.cpu(),
+            input_ids,
+            answer_ids,
+            max_length=self.config.reader_seq_len,
+        )
+
+        concat_inputs = concat_inputs.to(self.reader.device)
+        block_mask = concat_inputs.special_tokens_mask.type(torch.bool).to(
+            device=self.reader.device
+        )
+        block_mask.logical_not_().logical_and_(
+            concat_inputs.token_type_ids.type(torch.bool)
+        )
+
+        if has_answers is not None:
+            has_answers = torch.tensor(
+                has_answers, dtype=torch.bool, device=self.reader.device
+            )
+            start_pos = torch.tensor(
+                start_pos, dtype=torch.long, device=self.reader.device
+            )
+            end_pos = torch.tensor(end_pos, dtype=torch.long, device=self.reader.device)
+
+        # [searcher_beam_size]
+        retrieved_logits = torch.einsum(
+            "D,BD->B",
+            question_projection.squeeze(),
+            retrieved_block_emb.to(self.reader.device),
+        )
+
+        reader_output = self.reader(
+            input_ids=concat_inputs.input_ids[0 : self.config.reader_beam_size],
+            attention_mask=concat_inputs.attention_mask[
+                0 : self.config.reader_beam_size
+            ],
+            token_type_ids=concat_inputs.token_type_ids[
+                0 : self.config.reader_beam_size
+            ],
+            relevance_score=retrieved_logits,
+            block_mask=block_mask,
+            has_answers=has_answers,
+            start_positions=start_pos,
+            end_positions=end_pos,
+            return_dict=True,
+        )
+
+        predicted_block = concat_inputs.input_ids[reader_output.block_idx]
+        predicted_answer_ids = predicted_block[
+            reader_output.start_pos : reader_output.end_pos + 1
+        ]
+
+        if not return_dict:
+            return reader_output, predicted_answer_ids
+
+        return RealmForOpenQAOutput(
+            reader_output=reader_output,
+            predicted_answer_ids=predicted_answer_ids,
+        )
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `xs_transformers-1.0.0/xs_transformers/training_args_tf.py` & `xs_transformers-1.0.1/xs_transformers/models/layoutlmv3/configuration_layoutlmv3.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,296 +1,303 @@
-# Copyright 2020 The HuggingFace Team. All rights reserved.
+# coding=utf-8
+# Copyright 2022 Microsoft Research and The HuggingFace Inc. team. All rights reserved.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+""" LayoutLMv3 model configuration"""
 
-import warnings
-from dataclasses import dataclass, field
-from typing import Optional, Tuple
+from collections import OrderedDict
+from typing import TYPE_CHECKING, Any, Mapping, Optional
 
-from .training_args import TrainingArguments
-from .utils import cached_property, is_tf_available, logging, tf_required
+from packaging import version
 
-logger = logging.get_logger(__name__)
+from ...configuration_utils import PretrainedConfig
+from ...onnx import OnnxConfig
+from ...onnx.utils import compute_effective_axis_dimension
+from ...utils import logging
 
-if is_tf_available():
-    import tensorflow as tf
+if TYPE_CHECKING:
+    from ...processing_utils import ProcessorMixin
+    from ...utils import TensorType
 
 
-@dataclass
-class TFTrainingArguments(TrainingArguments):
-    """
-    TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop
-    itself**.
-
-    Using [`HfArgumentParser`] we can turn this class into
-    [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the
-    command line.
-
-    Parameters:
-        output_dir (`str`):
-            The output directory where the model predictions and checkpoints will be written.
-        overwrite_output_dir (`bool`, *optional*, defaults to `False`):
-            If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`
-            points to a checkpoint directory.
-        do_train (`bool`, *optional*, defaults to `False`):
-            Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used
-            by your training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        do_eval (`bool`, *optional*):
-            Whether to run evaluation on the validation set or not. Will be set to `True` if `evaluation_strategy` is
-            different from `"no"`. This argument is not directly used by [`Trainer`], it's intended to be used by your
-            training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        do_predict (`bool`, *optional*, defaults to `False`):
-            Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's
-            intended to be used by your training/evaluation scripts instead. See the [example
-            scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.
-        evaluation_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"no"`):
-            The evaluation strategy to adopt during training. Possible values are:
-
-                - `"no"`: No evaluation is done during training.
-                - `"steps"`: Evaluation is done (and logged) every `eval_steps`.
-                - `"epoch"`: Evaluation is done at the end of each epoch.
-
-        per_device_train_batch_size (`int`, *optional*, defaults to 8):
-            The batch size per GPU/TPU core/CPU for training.
-        per_device_eval_batch_size (`int`, *optional*, defaults to 8):
-            The batch size per GPU/TPU core/CPU for evaluation.
-        gradient_accumulation_steps: (`int`, *optional*, defaults to 1):
-            Number of updates steps to accumulate the gradients for, before performing a backward/update pass.
-
-            <Tip warning={true}>
-
-            When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,
-            evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.
-
-            </Tip>
-
-        learning_rate (`float`, *optional*, defaults to 5e-5):
-            The initial learning rate for Adam.
-        weight_decay (`float`, *optional*, defaults to 0):
-            The weight decay to apply (if not zero).
-        adam_beta1 (`float`, *optional*, defaults to 0.9):
-            The beta1 hyperparameter for the Adam optimizer.
-        adam_beta2 (`float`, *optional*, defaults to 0.999):
-            The beta2 hyperparameter for the Adam optimizer.
-        adam_epsilon (`float`, *optional*, defaults to 1e-8):
-            The epsilon hyperparameter for the Adam optimizer.
-        max_grad_norm (`float`, *optional*, defaults to 1.0):
-            Maximum gradient norm (for gradient clipping).
-        num_train_epochs(`float`, *optional*, defaults to 3.0):
-            Total number of training epochs to perform.
-        max_steps (`int`, *optional*, defaults to -1):
-            If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.
-        warmup_ratio (`float`, *optional*, defaults to 0.0):
-            Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.
-        warmup_steps (`int`, *optional*, defaults to 0):
-            Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.
-        logging_dir (`str`, *optional*):
-            [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to
-            *runs/**CURRENT_DATETIME_HOSTNAME***.
-        logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
-            The logging strategy to adopt during training. Possible values are:
-
-                - `"no"`: No logging is done during training.
-                - `"epoch"`: Logging is done at the end of each epoch.
-                - `"steps"`: Logging is done every `logging_steps`.
-
-        logging_first_step (`bool`, *optional*, defaults to `False`):
-            Whether to log and evaluate the first `global_step` or not.
-        logging_steps (`int`, *optional*, defaults to 500):
-            Number of update steps between two logs if `logging_strategy="steps"`.
-        save_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `"steps"`):
-            The checkpoint save strategy to adopt during training. Possible values are:
-
-                - `"no"`: No save is done during training.
-                - `"epoch"`: Save is done at the end of each epoch.
-                - `"steps"`: Save is done every `save_steps`.
-
-        save_steps (`int`, *optional*, defaults to 500):
-            Number of updates steps before two checkpoint saves if `save_strategy="steps"`.
-        save_total_limit (`int`, *optional*):
-            If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in
-            `output_dir`.
-        no_cuda (`bool`, *optional*, defaults to `False`):
-            Whether to not use CUDA even when it is available or not.
-        seed (`int`, *optional*, defaults to 42):
-            Random seed that will be set at the beginning of training.
-        fp16 (`bool`, *optional*, defaults to `False`):
-            Whether to use 16-bit (mixed) precision training (through NVIDIA Apex) instead of 32-bit training.
-        fp16_opt_level (`str`, *optional*, defaults to 'O1'):
-            For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on
-            the [Apex documentation](https://nvidia.github.io/apex/amp).
-        local_rank (`int`, *optional*, defaults to -1):
-            During distributed training, the rank of the process.
-        tpu_num_cores (`int`, *optional*):
-            When training on TPU, the number of TPU cores (automatically passed by launcher script).
-        debug (`bool`, *optional*, defaults to `False`):
-            Whether to activate the trace to record computation graphs and profiling information or not.
-        dataloader_drop_last (`bool`, *optional*, defaults to `False`):
-            Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)
-            or not.
-        eval_steps (`int`, *optional*, defaults to 1000):
-            Number of update steps before two evaluations.
-        past_index (`int`, *optional*, defaults to -1):
-            Some models like [TransformerXL](../model_doc/transformerxl) or :doc*XLNet <../model_doc/xlnet>* can make
-            use of the past hidden states for their predictions. If this argument is set to a positive int, the
-            `Trainer` will use the corresponding output (usually index 2) as the past state and feed it to the model at
-            the next training step under the keyword argument `mems`.
-        tpu_name (`str`, *optional*):
-            The name of the TPU the process is running on.
-        tpu_zone (`str`, *optional*):
-            The zone of the TPU the process is running on. If not specified, we will attempt to automatically detect
-            from metadata.
-        gcp_project (`str`, *optional*):
-            Google Cloud Project name for the Cloud TPU-enabled project. If not specified, we will attempt to
-            automatically detect from metadata.
-        run_name (`str`, *optional*):
-            A descriptor for the run. Notably used for wandb logging.
-        xla (`bool`, *optional*):
-            Whether to activate the XLA compilation or not.
-    """
-
-    framework = "tf"
-    tpu_name: Optional[str] = field(
-        default=None,
-        metadata={"help": "Name of TPU"},
-    )
-
-    tpu_zone: Optional[str] = field(
-        default=None,
-        metadata={"help": "Zone of TPU"},
-    )
-
-    gcp_project: Optional[str] = field(
-        default=None,
-        metadata={"help": "Name of Cloud TPU-enabled project"},
-    )
-
-    poly_power: float = field(
-        default=1.0,
-        metadata={"help": "Power for the Polynomial decay LR scheduler."},
-    )
-
-    xla: bool = field(
-        default=False,
-        metadata={"help": "Whether to activate the XLA compilation or not"},
-    )
-
-    @cached_property
-    @tf_required
-    def _setup_strategy(self) -> Tuple["tf.distribute.Strategy", int]:
-        logger.info("Tensorflow: setting up strategy")
-
-        gpus = tf.config.list_physical_devices("GPU")
-
-        # Set to float16 at first
-        if self.fp16:
-            tf.keras.mixed_precision.set_global_policy("mixed_float16")
+logger = logging.get_logger(__name__)
 
-        if self.no_cuda:
-            strategy = tf.distribute.OneDeviceStrategy(device="/cpu:0")
-        else:
-            try:
-                if self.tpu_name:
-                    tpu = tf.distribute.cluster_resolver.TPUClusterResolver(
-                        self.tpu_name, zone=self.tpu_zone, project=self.gcp_project
-                    )
-                else:
-                    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
-            except ValueError:
-                if self.tpu_name:
-                    raise RuntimeError(f"Couldn't connect to TPU {self.tpu_name}!")
-                else:
-                    tpu = None
-
-            if tpu:
-                # Set to bfloat16 in case of TPU
-                if self.fp16:
-                    tf.keras.mixed_precision.set_global_policy("mixed_bfloat16")
-
-                tf.config.experimental_connect_to_cluster(tpu)
-                tf.tpu.experimental.initialize_tpu_system(tpu)
-
-                strategy = tf.distribute.TPUStrategy(tpu)
-
-            elif len(gpus) == 0:
-                strategy = tf.distribute.OneDeviceStrategy(device="/cpu:0")
-            elif len(gpus) == 1:
-                strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")
-            elif len(gpus) > 1:
-                # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`
-                strategy = tf.distribute.MirroredStrategy()
-            else:
-                raise ValueError(
-                    "Cannot find the proper strategy, please check your environment properties."
-                )
+LAYOUTLMV3_PRETRAINED_CONFIG_ARCHIVE_MAP = {
+    "microsoft/layoutlmv3-base": "https://huggingface.co/microsoft/layoutlmv3-base/resolve/main/config.json",
+}
+
+
+class LayoutLMv3Config(PretrainedConfig):
+    r"""
+    This is the configuration class to store the configuration of a [`LayoutLMv3Model`]. It is used to instantiate an
+    LayoutLMv3 model according to the specified arguments, defining the model architecture. Instantiating a
+    configuration with the defaults will yield a similar configuration to that of the LayoutLMv3
+    [microsoft/layoutlmv3-base](https://huggingface.co/microsoft/layoutlmv3-base) architecture.
+
+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the
+    documentation from [`PretrainedConfig`] for more information.
+
+    Args:
+        vocab_size (`int`, *optional*, defaults to 50265):
+            Vocabulary size of the LayoutLMv3 model. Defines the number of different tokens that can be represented by
+            the `inputs_ids` passed when calling [`LayoutLMv3Model`].
+        hidden_size (`int`, *optional*, defaults to 768):
+            Dimension of the encoder layers and the pooler layer.
+        num_hidden_layers (`int`, *optional*, defaults to 12):
+            Number of hidden layers in the Transformer encoder.
+        num_attention_heads (`int`, *optional*, defaults to 12):
+            Number of attention heads for each attention layer in the Transformer encoder.
+        intermediate_size (`int`, *optional*, defaults to 3072):
+            Dimension of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
+        hidden_act (`str` or `function`, *optional*, defaults to `"gelu"`):
+            The non-linear activation function (function or string) in the encoder and pooler. If string, `"gelu"`,
+            `"relu"`, `"selu"` and `"gelu_new"` are supported.
+        hidden_dropout_prob (`float`, *optional*, defaults to 0.1):
+            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.1):
+            The dropout ratio for the attention probabilities.
+        max_position_embeddings (`int`, *optional*, defaults to 512):
+            The maximum sequence length that this model might ever be used with. Typically set this to something large
+            just in case (e.g., 512 or 1024 or 2048).
+        type_vocab_size (`int`, *optional*, defaults to 2):
+            The vocabulary size of the `token_type_ids` passed when calling [`LayoutLMv3Model`].
+        initializer_range (`float`, *optional*, defaults to 0.02):
+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.
+        layer_norm_eps (`float`, *optional*, defaults to 1e-5):
+            The epsilon used by the layer normalization layers.
+        max_2d_position_embeddings (`int`, *optional*, defaults to 1024):
+            The maximum value that the 2D position embedding might ever be used with. Typically set this to something
+            large just in case (e.g., 1024).
+        coordinate_size (`int`, *optional*, defaults to `128`):
+            Dimension of the coordinate embeddings.
+        shape_size (`int`, *optional*, defaults to `128`):
+            Dimension of the width and height embeddings.
+        has_relative_attention_bias (`bool`, *optional*, defaults to `True`):
+            Whether or not to use a relative attention bias in the self-attention mechanism.
+        rel_pos_bins (`int`, *optional*, defaults to 32):
+            The number of relative position bins to be used in the self-attention mechanism.
+        max_rel_pos (`int`, *optional*, defaults to 128):
+            The maximum number of relative positions to be used in the self-attention mechanism.
+        max_rel_2d_pos (`int`, *optional*, defaults to 256):
+            The maximum number of relative 2D positions in the self-attention mechanism.
+        rel_2d_pos_bins (`int`, *optional*, defaults to 64):
+            The number of 2D relative position bins in the self-attention mechanism.
+        has_spatial_attention_bias (`bool`, *optional*, defaults to `True`):
+            Whether or not to use a spatial attention bias in the self-attention mechanism.
+        visual_embed (`bool`, *optional*, defaults to `True`):
+            Whether or not to add patch embeddings.
+        input_size (`int`, *optional*, defaults to `224`):
+            The size (resolution) of the images.
+        num_channels (`int`, *optional*, defaults to `3`):
+            The number of channels of the images.
+        patch_size (`int`, *optional*, defaults to `16`)
+            The size (resolution) of the patches.
+        classifier_dropout (`float`, *optional*):
+            The dropout ratio for the classification head.
+
+    Example:
+
+    ```python
+    >>> from transformers import LayoutLMv3Config, LayoutLMv3Model
+
+    >>> # Initializing a LayoutLMv3 microsoft/layoutlmv3-base style configuration
+    >>> configuration = LayoutLMv3Config()
+
+    >>> # Initializing a model (with random weights) from the microsoft/layoutlmv3-base style configuration
+    >>> model = LayoutLMv3Model(configuration)
+
+    >>> # Accessing the model configuration
+    >>> configuration = model.config
+    ```"""
+    model_type = "layoutlmv3"
+
+    def __init__(
+        self,
+        vocab_size=50265,
+        hidden_size=768,
+        num_hidden_layers=12,
+        num_attention_heads=12,
+        intermediate_size=3072,
+        hidden_act="gelu",
+        hidden_dropout_prob=0.1,
+        attention_probs_dropout_prob=0.1,
+        max_position_embeddings=512,
+        type_vocab_size=2,
+        initializer_range=0.02,
+        layer_norm_eps=1e-5,
+        pad_token_id=1,
+        bos_token_id=0,
+        eos_token_id=2,
+        max_2d_position_embeddings=1024,
+        coordinate_size=128,
+        shape_size=128,
+        has_relative_attention_bias=True,
+        rel_pos_bins=32,
+        max_rel_pos=128,
+        rel_2d_pos_bins=64,
+        max_rel_2d_pos=256,
+        has_spatial_attention_bias=True,
+        text_embed=True,
+        visual_embed=True,
+        input_size=224,
+        num_channels=3,
+        patch_size=16,
+        classifier_dropout=None,
+        **kwargs
+    ):
+        super().__init__(
+            vocab_size=vocab_size,
+            hidden_size=hidden_size,
+            num_hidden_layers=num_hidden_layers,
+            num_attention_heads=num_attention_heads,
+            intermediate_size=intermediate_size,
+            hidden_act=hidden_act,
+            hidden_dropout_prob=hidden_dropout_prob,
+            attention_probs_dropout_prob=attention_probs_dropout_prob,
+            max_position_embeddings=max_position_embeddings,
+            type_vocab_size=type_vocab_size,
+            initializer_range=initializer_range,
+            layer_norm_eps=layer_norm_eps,
+            pad_token_id=pad_token_id,
+            bos_token_id=bos_token_id,
+            eos_token_id=eos_token_id,
+            **kwargs,
+        )
+        self.max_2d_position_embeddings = max_2d_position_embeddings
+        self.coordinate_size = coordinate_size
+        self.shape_size = shape_size
+        self.has_relative_attention_bias = has_relative_attention_bias
+        self.rel_pos_bins = rel_pos_bins
+        self.max_rel_pos = max_rel_pos
+        self.has_spatial_attention_bias = has_spatial_attention_bias
+        self.rel_2d_pos_bins = rel_2d_pos_bins
+        self.max_rel_2d_pos = max_rel_2d_pos
+        self.text_embed = text_embed
+        self.visual_embed = visual_embed
+        self.input_size = input_size
+        self.num_channels = num_channels
+        self.patch_size = patch_size
+        self.classifier_dropout = classifier_dropout
 
-        return strategy
+
+class LayoutLMv3OnnxConfig(OnnxConfig):
+    torch_onnx_minimum_version = version.parse("1.12")
 
     @property
-    @tf_required
-    def strategy(self) -> "tf.distribute.Strategy":
-        """
-        The strategy used for distributed training.
-        """
-        return self._setup_strategy
+    def inputs(self) -> Mapping[str, Mapping[int, str]]:
+        # The order of inputs is different for question answering and sequence classification
+        if self.task in ["question-answering", "sequence-classification"]:
+            return OrderedDict(
+                [
+                    ("input_ids", {0: "batch", 1: "sequence"}),
+                    ("attention_mask", {0: "batch", 1: "sequence"}),
+                    ("bbox", {0: "batch", 1: "sequence"}),
+                    (
+                        "pixel_values",
+                        {0: "batch", 1: "num_channels", 2: "height", 3: "width"},
+                    ),
+                ]
+            )
+        else:
+            return OrderedDict(
+                [
+                    ("input_ids", {0: "batch", 1: "sequence"}),
+                    ("bbox", {0: "batch", 1: "sequence"}),
+                    ("attention_mask", {0: "batch", 1: "sequence"}),
+                    ("pixel_values", {0: "batch", 1: "num_channels"}),
+                ]
+            )
 
     @property
-    @tf_required
-    def n_replicas(self) -> int:
-        """
-        The number of replicas (CPUs, GPUs or TPU cores) used in this training.
-        """
-        return self._setup_strategy.num_replicas_in_sync
+    def atol_for_validation(self) -> float:
+        return 1e-5
 
     @property
-    def train_batch_size(self) -> int:
+    def default_onnx_opset(self) -> int:
+        return 12
+
+    def generate_dummy_inputs(
+        self,
+        processor: "ProcessorMixin",
+        batch_size: int = -1,
+        seq_length: int = -1,
+        is_pair: bool = False,
+        framework: Optional["TensorType"] = None,
+        num_channels: int = 3,
+        image_width: int = 40,
+        image_height: int = 40,
+    ) -> Mapping[str, Any]:
         """
-        The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).
+        Generate inputs to provide to the ONNX exporter for the specific framework
+
+        Args:
+            processor ([`ProcessorMixin`]):
+                The processor associated with this model configuration.
+            batch_size (`int`, *optional*, defaults to -1):
+                The batch size to export the model for (-1 means dynamic axis).
+            seq_length (`int`, *optional*, defaults to -1):
+                The sequence length to export the model for (-1 means dynamic axis).
+            is_pair (`bool`, *optional*, defaults to `False`):
+                Indicate if the input is a pair (sentence 1, sentence 2).
+            framework (`TensorType`, *optional*, defaults to `None`):
+                The framework (PyTorch or TensorFlow) that the processor will generate tensors for.
+            num_channels (`int`, *optional*, defaults to 3):
+                The number of channels of the generated images.
+            image_width (`int`, *optional*, defaults to 40):
+                The width of the generated images.
+            image_height (`int`, *optional*, defaults to 40):
+                The height of the generated images.
+
+        Returns:
+            Mapping[str, Any]: holding the kwargs to provide to the model's forward function
         """
-        if self.per_gpu_train_batch_size:
-            logger.warning(
-                "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future "
-                "version. Using `--per_device_train_batch_size` is preferred."
-            )
-        per_device_batch_size = (
-            self.per_gpu_train_batch_size or self.per_device_train_batch_size
+
+        # A dummy image is used so OCR should not be applied
+        setattr(processor.feature_extractor, "apply_ocr", False)
+
+        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX
+        batch_size = compute_effective_axis_dimension(
+            batch_size,
+            fixed_dimension=OnnxConfig.default_fixed_batch,
+            num_token_to_add=0,
+        )
+        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX
+        token_to_add = processor.tokenizer.num_special_tokens_to_add(is_pair)
+        seq_length = compute_effective_axis_dimension(
+            seq_length,
+            fixed_dimension=OnnxConfig.default_fixed_sequence,
+            num_token_to_add=token_to_add,
+        )
+        # Generate dummy inputs according to compute batch and sequence
+        dummy_text = [
+            [" ".join([processor.tokenizer.unk_token]) * seq_length]
+        ] * batch_size
+
+        # Generate dummy bounding boxes
+        dummy_bboxes = [[[48, 84, 73, 128]]] * batch_size
+
+        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX
+        # batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)
+        dummy_image = self._generate_dummy_images(
+            batch_size, num_channels, image_height, image_width
         )
-        return per_device_batch_size * self.n_replicas
 
-    @property
-    def eval_batch_size(self) -> int:
-        """
-        The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).
-        """
-        if self.per_gpu_eval_batch_size:
-            logger.warning(
-                "Using deprecated `--per_gpu_eval_batch_size` argument which will be removed in a future "
-                "version. Using `--per_device_eval_batch_size` is preferred."
+        inputs = dict(
+            processor(
+                dummy_image,
+                text=dummy_text,
+                boxes=dummy_bboxes,
+                return_tensors=framework,
             )
-        per_device_batch_size = (
-            self.per_gpu_eval_batch_size or self.per_device_eval_batch_size
         )
-        return per_device_batch_size * self.n_replicas
 
-    @property
-    @tf_required
-    def n_gpu(self) -> int:
-        """
-        The number of replicas (CPUs, GPUs or TPU cores) used in this training.
-        """
-        warnings.warn(
-            "The n_gpu argument is deprecated and will be removed in a future version, use n_replicas instead.",
-            FutureWarning,
-        )
-        return self._setup_strategy.num_replicas_in_sync
+        return inputs
```

