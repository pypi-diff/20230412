# Comparing `tmp/deep_training-0.1.2-py3-none-any.whl.zip` & `tmp/deep_training-0.1.2.post0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,10 +1,10 @@
-Zip file size: 244429 bytes, number of entries: 145
+Zip file size: 244519 bytes, number of entries: 145
 -rw-rw-rw-  2.0 fat       47 b- defN 23-Mar-22 07:43 deep_training/__init__.py
--rw-rw-rw-  2.0 fat      874 b- defN 23-Apr-11 07:03 deep_training/setup.py
+-rw-rw-rw-  2.0 fat      903 b- defN 23-Apr-12 08:36 deep_training/setup.py
 -rw-rw-rw-  2.0 fat       55 b- defN 22-Dec-09 05:30 deep_training/cv/__init__.py
 -rw-rw-rw-  2.0 fat      195 b- defN 23-Jan-29 01:07 deep_training/data_helper/__init__.py
 -rw-rw-rw-  2.0 fat    29088 b- defN 23-Apr-06 04:11 deep_training/data_helper/data_helper.py
 -rw-rw-rw-  2.0 fat     4926 b- defN 23-Feb-17 02:41 deep_training/data_helper/data_module.py
 -rw-rw-rw-  2.0 fat     1383 b- defN 23-Jan-29 01:07 deep_training/data_helper/data_writer.py
 -rw-rw-rw-  2.0 fat    12552 b- defN 23-Apr-11 05:55 deep_training/data_helper/training_args.py
 -rw-rw-rw-  2.0 fat       70 b- defN 22-Dec-13 03:17 deep_training/nlp/__init__.py
@@ -18,16 +18,16 @@
 -rw-rw-rw-  2.0 fat     1220 b- defN 22-Jul-21 00:57 deep_training/nlp/layers/prefix_encoder.py
 -rw-rw-rw-  2.0 fat     7259 b- defN 22-Dec-14 02:36 deep_training/nlp/layers/seq_pointer.py
 -rw-rw-rw-  2.0 fat     3550 b- defN 22-Dec-15 00:57 deep_training/nlp/layers/w2ner.py
 -rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/__init__.py
 -rw-rw-rw-  2.0 fat    15095 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/layers.py
 -rw-rw-rw-  2.0 fat     1819 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v1/utils.py
 -rw-rw-rw-  2.0 fat       72 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/__init__.py
--rw-rw-rw-  2.0 fat    15485 b- defN 23-Apr-11 04:41 deep_training/nlp/layers/lora_v2/adalora.py
--rw-rw-rw-  2.0 fat     8585 b- defN 23-Apr-11 04:40 deep_training/nlp/layers/lora_v2/layers.py
+-rw-rw-rw-  2.0 fat    15465 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/adalora.py
+-rw-rw-rw-  2.0 fat     8565 b- defN 23-Apr-12 00:34 deep_training/nlp/layers/lora_v2/layers.py
 -rw-rw-rw-  2.0 fat     9106 b- defN 23-Apr-11 00:30 deep_training/nlp/layers/lora_v2/utils.py
 -rw-rw-rw-  2.0 fat     3662 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchAllTripletLoss.py
 -rw-rw-rw-  2.0 fat     3880 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py
 -rw-rw-rw-  2.0 fat     8358 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchHardTripletLoss.py
 -rw-rw-rw-  2.0 fat     4552 b- defN 22-Nov-16 07:15 deep_training/nlp/losses/BatchSemiHardTripletLoss.py
 -rw-rw-rw-  2.0 fat     2255 b- defN 22-Nov-18 01:05 deep_training/nlp/losses/ContrastiveLoss.py
 -rw-rw-rw-  2.0 fat     4573 b- defN 22-Nov-16 07:04 deep_training/nlp/losses/ContrastiveTensionLoss.py
@@ -102,19 +102,19 @@
 -rw-rw-rw-  2.0 fat    15150 b- defN 23-Apr-03 00:32 deep_training/nlp/models/chatglm/quantization.py
 -rw-rw-rw-  2.0 fat    16460 b- defN 23-Apr-10 00:42 deep_training/nlp/models/chatglm/tokenization.py
 -rw-rw-rw-  2.0 fat    34123 b- defN 23-Mar-13 06:18 deep_training/nlp/models/laMDA/__init__.py
 -rw-rw-rw-  2.0 fat     5981 b- defN 23-Mar-13 06:15 deep_training/nlp/models/laMDA/configuration.py
 -rw-rw-rw-  2.0 fat      181 b- defN 23-Apr-11 07:19 deep_training/nlp/models/lora/__init__.py
 -rw-rw-rw-  2.0 fat      123 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/__init__.py
 -rw-rw-rw-  2.0 fat     7054 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/configuration.py
--rw-rw-rw-  2.0 fat    13385 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v1/lora_wrapper.py
+-rw-rw-rw-  2.0 fat    13576 b- defN 23-Apr-12 08:35 deep_training/nlp/models/lora/v1/lora_wrapper.py
 -rw-rw-rw-  2.0 fat      206 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/__init__.py
 -rw-rw-rw-  2.0 fat    13112 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/adalora_model.py
 -rw-rw-rw-  2.0 fat    11281 b- defN 23-Apr-11 06:27 deep_training/nlp/models/lora/v2/configuration.py
--rw-rw-rw-  2.0 fat    11165 b- defN 23-Apr-11 04:35 deep_training/nlp/models/lora/v2/lora_model.py
+-rw-rw-rw-  2.0 fat    11332 b- defN 23-Apr-12 08:32 deep_training/nlp/models/lora/v2/lora_model.py
 -rw-rw-rw-  2.0 fat    10265 b- defN 23-Apr-11 01:58 deep_training/nlp/models/lora/v2/lora_wrapper.py
 -rw-rw-rw-  2.0 fat     4889 b- defN 23-Apr-11 00:30 deep_training/nlp/models/lora/v2/save_and_load.py
 -rw-rw-rw-  2.0 fat      102 b- defN 22-Nov-22 08:00 deep_training/nlp/models/splinker/__init__.py
 -rw-rw-rw-  2.0 fat     2851 b- defN 22-Dec-22 08:14 deep_training/nlp/models/splinker/splinker.py
 -rw-rw-rw-  2.0 fat    14478 b- defN 23-Feb-11 09:07 deep_training/nlp/models/t5decoder/__init__.py
 -rw-rw-rw-  2.0 fat     6646 b- defN 23-Feb-09 00:28 deep_training/nlp/models/t5encoder/__init__.py
 -rw-rw-rw-  2.0 fat       56 b- defN 22-Dec-14 08:02 deep_training/nlp/optimizer/__init__.py
@@ -136,12 +136,12 @@
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:25 deep_training/tfnlp/scheduler/__init__.py
 -rw-rw-rw-  2.0 fat       53 b- defN 23-Mar-07 01:26 deep_training/tfnlp/utils/__init__.py
 -rw-rw-rw-  2.0 fat       55 b- defN 23-Mar-07 01:20 deep_training/utils/__init__.py
 -rw-rw-rw-  2.0 fat     1941 b- defN 23-Mar-07 01:20 deep_training/utils/distributed.py
 -rw-rw-rw-  2.0 fat     1724 b- defN 23-Mar-07 01:22 deep_training/utils/func.py
 -rw-rw-rw-  2.0 fat     5117 b- defN 23-Feb-21 09:01 deep_training/utils/maskedlm.py
 -rw-rw-rw-  2.0 fat     7469 b- defN 23-Mar-20 00:27 deep_training/utils/trainer.py
--rw-rw-rw-  2.0 fat      623 b- defN 23-Apr-11 07:24 deep_training-0.1.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-11 07:24 deep_training-0.1.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-11 07:24 deep_training-0.1.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    13986 b- defN 23-Apr-11 07:24 deep_training-0.1.2.dist-info/RECORD
-145 files, 833408 bytes uncompressed, 221765 bytes compressed:  73.4%
+-rw-rw-rw-  2.0 fat      629 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    14010 b- defN 23-Apr-12 08:40 deep_training-0.1.2.post0.dist-info/RECORD
+145 files, 833785 bytes uncompressed, 221807 bytes compressed:  73.4%
```

## zipnote {}

```diff
@@ -417,20 +417,20 @@
 
 Filename: deep_training/utils/maskedlm.py
 Comment: 
 
 Filename: deep_training/utils/trainer.py
 Comment: 
 
-Filename: deep_training-0.1.2.dist-info/METADATA
+Filename: deep_training-0.1.2.post0.dist-info/METADATA
 Comment: 
 
-Filename: deep_training-0.1.2.dist-info/WHEEL
+Filename: deep_training-0.1.2.post0.dist-info/WHEEL
 Comment: 
 
-Filename: deep_training-0.1.2.dist-info/top_level.txt
+Filename: deep_training-0.1.2.post0.dist-info/top_level.txt
 Comment: 
 
-Filename: deep_training-0.1.2.dist-info/RECORD
+Filename: deep_training-0.1.2.post0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deep_training/setup.py

```diff
@@ -1,25 +1,26 @@
 #! -*- coding: utf-8 -*-
 
 from setuptools import setup, find_packages
 
 ignore = ['test','tests']
 setup(
     name='deep_training',
-    version='0.1.2',
+    version='0.1.2@post0',
     description='an easy training architecture',
     long_description='torch_training: https://github.com/ssbuild/deep_training.git',
     license='Apache License 2.0',
     url='https://github.com/ssbuild/deep_training',
     author='ssbuild',
     author_email='9727464@qq.com',
     install_requires=['pytorch-lightning>=2',
                       'fastdatasets>=0.9.6 , <= 1',
                       'tfrecords >= 0.2.4 , <= 1',
                       'sentencepiece',
                       'numpy',
                       'transformers >= 4.22',
                       'seqmetric','scipy',
                       'scikit-learn',
-                      'tqdm','six'],
+                      'tqdm',
+                      'six'],
     packages=[p for p in find_packages() if p not in ignore]
 )
```

## deep_training/nlp/layers/lora_v2/adalora.py

```diff
@@ -26,17 +26,15 @@
 
     def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=None):
         self.r[adapter_name] = r
         self.lora_alpha[adapter_name] = lora_alpha
         if lora_dropout > 0.0:
             lora_dropout_layer = nn.Dropout(p=lora_dropout)
         else:
-
-            def lora_dropout_layer(x):
-                return x
+            lora_dropout_layer = nn.Identity()
 
         self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
         # Actual trainable parameters
         # Right singular vectors
         self.lora_A.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(r, self.in_features,dtype=dtype))}))
         # Singular values
         self.lora_E.update(nn.ParameterDict({adapter_name: nn.Parameter(torch.zeros(r, 1,dtype=dtype))}))
```

## deep_training/nlp/layers/lora_v2/layers.py

```diff
@@ -56,17 +56,15 @@
 
     def update_layer(self, adapter_name, r, lora_alpha, lora_dropout, init_lora_weights,dtype=None):
         self.r[adapter_name] = r
         self.lora_alpha[adapter_name] = lora_alpha
         if lora_dropout > 0.0:
             lora_dropout_layer = nn.Dropout(p=lora_dropout)
         else:
-
-            def lora_dropout_layer(x):
-                return x
+            lora_dropout_layer = nn.Identity()
 
         self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
         # Actual trainable parameters
         if r > 0:
             self.lora_A.update(nn.ModuleDict({adapter_name: nn.Linear(self.in_features, r, bias=False,dtype=dtype)}))
             self.lora_B.update(nn.ModuleDict({adapter_name: nn.Linear(r, self.out_features, bias=False,dtype=dtype)}))
             self.scaling[adapter_name] = lora_alpha / r
```

## deep_training/nlp/models/lora/v1/lora_wrapper.py

```diff
@@ -95,29 +95,33 @@
         self.model = model
         self._find_and_replace()
         mark_only_lora_as_trainable(self.model, self.lora_config.bias)
         self.forward = self.model.forward
 
     def _find_and_replace(self):
         loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
+        if not loaded_in_8bit:
+            if hasattr(self.model, 'model'):
+                loaded_in_8bit = getattr(self.model.model, "is_loaded_in_8bit", False)
+
         if loaded_in_8bit and not is_bnb_available():
             raise ImportError(
                 "To use Lora with 8-bit quantization, please install the `bitsandbytes` package. "
                 "You can install it with `pip install bitsandbytes`."
             )
         is_target_modules_in_base_model = False
         kwargs = {
             "r": self.lora_config.r,
             "lora_alpha": self.lora_config.lora_alpha,
             "lora_dropout": self.lora_config.lora_dropout,
             "fan_in_fan_out": self.lora_config.fan_in_fan_out,
             "merge_weights": self.lora_config.merge_weights or self.lora_config.inference_mode,
         }
 
-        if self.lora_config.target_dtype is not None:
+        if self.lora_config.target_dtype is not None and not loaded_in_8bit:
             if self.lora_config.target_dtype == 16 or self.lora_config.target_dtype == '16':
                 kwargs['dtype'] = torch.float16
             elif self.lora_config.target_dtype == 32 or self.lora_config.target_dtype == '32':
                 kwargs['dtype'] = torch.float32
             elif self.lora_config.target_dtype == 64 or self.lora_config.target_dtype == '64':
                 kwargs['dtype'] = torch.float64
             elif self.lora_config.target_dtype == 'bf16':
```

## deep_training/nlp/models/lora/v2/lora_model.py

```diff
@@ -63,14 +63,18 @@
             _freeze_adapter(self.model, adapter_name)
         else:
             mark_only_lora_as_trainable(self.model, self.peft_config[adapter_name].bias)
 
     def _find_and_replace(self, adapter_name):
         lora_config = self.peft_config[adapter_name]
         loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
+        if not loaded_in_8bit:
+            if hasattr(self.model,'model'):
+                loaded_in_8bit = getattr(self.model.model, "is_loaded_in_8bit", False)
+
         if loaded_in_8bit and not is_bnb_available():
             raise ImportError(
                 "To use Lora with 8-bit quantization, please install the `bitsandbytes` package. "
                 "You can install it with `pip install bitsandbytes`."
             )
         is_target_modules_in_base_model = False
         kwargs = {
```

## Comparing `deep_training-0.1.2.dist-info/METADATA` & `deep_training-0.1.2.post0.dist-info/METADATA`

 * *Files 18% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: deep-training
-Version: 0.1.2
+Version: 0.1.2-post0
 Summary: an easy training architecture
 Home-page: https://github.com/ssbuild/deep_training
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache License 2.0
 Platform: UNKNOWN
 Requires-Dist: pytorch-lightning (>=2)
```

## Comparing `deep_training-0.1.2.dist-info/RECORD` & `deep_training-0.1.2.post0.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 deep_training/__init__.py,sha256=bhATnUT4VEzwvA8_8IwxspnDRKf32ZgEeHYCN2E5Dd4,47
-deep_training/setup.py,sha256=Ad8W7pe3W5X66xEkiPShhmyNoSGZDIoJal-Waihh3e8,874
+deep_training/setup.py,sha256=Gz4IDp0RAgCkkuzEG3_uYR_STeSbY55JHVs6Bs5MLOU,903
 deep_training/cv/__init__.py,sha256=J-zlKxMsAfAgoO0vSAzgYJXSuMSJcJ7NKAPKeaeC3TM,55
 deep_training/data_helper/__init__.py,sha256=P8rAMalR6xNepAf-9ldGoOSsEiUtur8Px6gUpTXQhd8,195
 deep_training/data_helper/data_helper.py,sha256=wamHVXHpCIcG-COkVLRmwIjmcQ0MQy99xjago0ulkkY,29088
 deep_training/data_helper/data_module.py,sha256=cRqwzcKMpFaE2HhdbAEwrIC9KxOE0rLkeQ25VCJh2W8,4926
 deep_training/data_helper/data_writer.py,sha256=BnxUmMuR60Wawd1SH9TTEBsQuDLozxWJdmLozuO-Wv4,1383
 deep_training/data_helper/training_args.py,sha256=pxNrCoUDfHa1LoC7sEUPxT1bnY3WFMlq_7KYbmuQUOo,12552
 deep_training/nlp/__init__.py,sha256=L4_ltrwpG8mrgN1hZRKimefLHgjhRYyXVtLMFzr1grw,70
@@ -17,16 +17,16 @@
 deep_training/nlp/layers/prefix_encoder.py,sha256=y_Y4wWLnEyZJf33pQJFjWfl-AX1VS8Aejj3JXOdiRXQ,1220
 deep_training/nlp/layers/seq_pointer.py,sha256=KmwZclK9gqdluy2o-h7nd3zL3EZjjjw1L6dz0isCFI8,7259
 deep_training/nlp/layers/w2ner.py,sha256=fP7hlMHp1NTH6elNMJA-wBOER76VTouSSsKCinLsCyM,3550
 deep_training/nlp/layers/lora_v1/__init__.py,sha256=gmwJqLmiKqPfh5_VGWWr38y8lLgdPWw6JrjNVrvsLEY,72
 deep_training/nlp/layers/lora_v1/layers.py,sha256=JWz9RtqA-hLsTxyhZPBlz82aN_VaPjNoDYRhssKV1H0,15095
 deep_training/nlp/layers/lora_v1/utils.py,sha256=1ouFUmTF9IXzum97eIlrTeT6J4OAnEwIaWkZdgXMjSc,1819
 deep_training/nlp/layers/lora_v2/__init__.py,sha256=dGpWUx0v7UoVgwZY5srCDCBvt_hlI77zA6mQO3CxMaE,72
-deep_training/nlp/layers/lora_v2/adalora.py,sha256=rbH_mo1s1O9_1sbfGMY3Y7XFeJhr8B703i7lAyK5zt8,15485
-deep_training/nlp/layers/lora_v2/layers.py,sha256=-SK-sx29ssrb3gBjcZkSbY-DjVfnsmNVWH-uQtvvCG8,8585
+deep_training/nlp/layers/lora_v2/adalora.py,sha256=dOqXcfqFfs4tpiwqU52bEzO1WoX9wP2itSBysw9rQ90,15465
+deep_training/nlp/layers/lora_v2/layers.py,sha256=giRQXEolkWGhU0CsNoOMOZBXrgwBbp6_FvOf9_Cjdeg,8565
 deep_training/nlp/layers/lora_v2/utils.py,sha256=Hl3i9r0eP5lUzSqyPAXKnO0oEgDCY8lC3ZGV0scmNRs,9106
 deep_training/nlp/losses/BatchAllTripletLoss.py,sha256=_2Og7Hf3Bjd1GT55UFmbZq5QLxdcKUyv4T00loPrKUo,3662
 deep_training/nlp/losses/BatchHardSoftMarginTripletLoss.py,sha256=Caif480bvgTWAQueadlAGSODpdaxVyRaik5-3j84wwg,3880
 deep_training/nlp/losses/BatchHardTripletLoss.py,sha256=xDdaQkcr6KCehSWlasABBkjPS9D6u_H5EngmppBkpXc,8358
 deep_training/nlp/losses/BatchSemiHardTripletLoss.py,sha256=xcfR8X3zyEs7YFoAT0q78iuZ2CJPtsrRUkp1Q8v5-QA,4552
 deep_training/nlp/losses/ContrastiveLoss.py,sha256=f7ZO4BUrqsSj0_Q670B6yIGHaSjYNOxwehBMPfRSEdg,2255
 deep_training/nlp/losses/ContrastiveTensionLoss.py,sha256=MNigl0maM6IpRasB83cf2UHPm4E73-J4N3F6y-F-uxc,4573
@@ -101,19 +101,19 @@
 deep_training/nlp/models/chatglm/quantization.py,sha256=sqX_poTcYNLJLDPbCwfRllDCF0enhshjX_dw7yZa604,15150
 deep_training/nlp/models/chatglm/tokenization.py,sha256=8K8W7b2ciL8rAYlq-XEYM9p0H2NtyLn9oIUYO8zHlQo,16460
 deep_training/nlp/models/laMDA/__init__.py,sha256=fvxTQQ8jfU-msPRdC8KsGlCwzM6u8-WBmayu6gE-s0E,34123
 deep_training/nlp/models/laMDA/configuration.py,sha256=8ZvPEl1C1KUGYWw7a8XcgIgl3gWH9WXa_-ZNDqz34PE,5981
 deep_training/nlp/models/lora/__init__.py,sha256=YyYmxdwz0IdyE4QXMPWhjUNmvOccqO8kJzPblfPwGJI,181
 deep_training/nlp/models/lora/v1/__init__.py,sha256=zwGdNKqudVj7c8sMWbmZ9CnnncWXuEapAucWY-VEhLs,123
 deep_training/nlp/models/lora/v1/configuration.py,sha256=MAkg2BQCqL6XvHd2SRXsijtPToNMuN-4Hmm_02HVCmU,7054
-deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=c0mGdjE8Urop_s1i1_gLiU1igljpzVb1RUpYhJJrat4,13385
+deep_training/nlp/models/lora/v1/lora_wrapper.py,sha256=449xBRufJ8Tt4p_OLsp7TZ3v8Y3iIBdpccgReGDzUng,13576
 deep_training/nlp/models/lora/v2/__init__.py,sha256=2XorjeFlyNuH6xTXiyNO1A8A3P5acBApOjxVv3YEon0,206
 deep_training/nlp/models/lora/v2/adalora_model.py,sha256=iKfKWnW--iY2gmXkMcBv6QWJr9vu-uSll57r1UNvRrY,13112
 deep_training/nlp/models/lora/v2/configuration.py,sha256=RXTsOjRKUHGFKpSrrFywjV41kT2liEPj1C6Li-ogJpI,11281
-deep_training/nlp/models/lora/v2/lora_model.py,sha256=hYnqCaTi2E7BpicGLx5TtBM-ZBnsU-iMMHHa81T7siQ,11165
+deep_training/nlp/models/lora/v2/lora_model.py,sha256=RSOldvGoGfarFpxok8fvf_Ng-1490ZiX4s7Kgl35U1o,11332
 deep_training/nlp/models/lora/v2/lora_wrapper.py,sha256=tzDCWUiGYC81cbfNiu4ZKo3VZsftM_SF8NwH8r56BO0,10265
 deep_training/nlp/models/lora/v2/save_and_load.py,sha256=U7_ZaPm8gpg8gQhZei6UG5KvsJXDtSNZfZk1gWo6nWc,4889
 deep_training/nlp/models/splinker/__init__.py,sha256=QtgnpJa78vAq9bzfjN67NmHU3dXU6WH84jeyZoD1sBs,102
 deep_training/nlp/models/splinker/splinker.py,sha256=cAXQoPucM3ULYUhBw9z-OxPK_b9hHKEZPgMb4vFfQwc,2851
 deep_training/nlp/models/t5decoder/__init__.py,sha256=R9Op4Ysli9isootQQ2FcjhpbG13fNESlmUROu6cfGH0,14478
 deep_training/nlp/models/t5encoder/__init__.py,sha256=692ChfLf2sZWgzhBM37g1PdpmEmsU1R9RRl_uTHRET0,6646
 deep_training/nlp/optimizer/__init__.py,sha256=c4cmx9ebIdqwXBu3N9QbcNNHb32t2MV6fTK9aC2VBGQ,56
@@ -135,11 +135,11 @@
 deep_training/tfnlp/scheduler/__init__.py,sha256=69flKnae4cQQyWUDwuYE0w0iaPonvH0P_WjBd_t-IqU,53
 deep_training/tfnlp/utils/__init__.py,sha256=kAmlOWNSpQCHbtT-mAsKGQzQFoWKp2jQf3neCJ0cCRY,53
 deep_training/utils/__init__.py,sha256=JFm7m_LPsS9Oavyxn9rbWqllCmV_zBho19rISlHNX4c,55
 deep_training/utils/distributed.py,sha256=-dhvJ6YHpRxvtZ1_on50IE33fUFW3zKXBKqqK-L1HGM,1941
 deep_training/utils/func.py,sha256=1p8hiQDCyk_gQGKrF7y6Dt66k3jLXSAt2IQeJuHQEl8,1724
 deep_training/utils/maskedlm.py,sha256=o8EB2BbDdh7wdgqz9Oi6SsVr1uBWxV15qfTk2VPjWsU,5117
 deep_training/utils/trainer.py,sha256=Rit5xEC2r9MvQdDR4A5A6E4_gzNCVNmW9m55kC83O50,7469
-deep_training-0.1.2.dist-info/METADATA,sha256=2_iat8bkNEIHtf8ySktVlmNReZC7iHnRTIptVt_5-8U,623
-deep_training-0.1.2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-deep_training-0.1.2.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
-deep_training-0.1.2.dist-info/RECORD,,
+deep_training-0.1.2.post0.dist-info/METADATA,sha256=7tvOzHtlpKlshtZR5LlbKXIo8JTENv8WBjOB7oStm9w,629
+deep_training-0.1.2.post0.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+deep_training-0.1.2.post0.dist-info/top_level.txt,sha256=P4qengiW56PZRm1VvlGcseSUCmAaBCsalCviUABZtO0,14
+deep_training-0.1.2.post0.dist-info/RECORD,,
```

