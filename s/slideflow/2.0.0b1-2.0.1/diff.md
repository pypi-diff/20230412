# Comparing `tmp/slideflow-2.0.0b1-py3-none-any.whl.zip` & `tmp/slideflow-2.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,42 +1,32 @@
-Zip file size: 2198211 bytes, number of entries: 409
--rw-rw-r--  2.0 unx     1411 b- defN 23-Apr-05 13:31 slideflow/__init__.py
--rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-05 13:31 slideflow/_backend.py
--rw-rw-r--  2.0 unx      503 b- defN 23-Apr-05 13:33 slideflow/_version.py
--rw-rw-r--  2.0 unx   159218 b- defN 23-Apr-05 13:31 slideflow/dataset.py
--rw-rw-r--  2.0 unx     3733 b- defN 23-Apr-05 13:31 slideflow/errors.py
--rw-rw-r--  2.0 unx    38304 b- defN 23-Apr-05 13:31 slideflow/heatmap.py
--rw-rw-r--  2.0 unx    25639 b- defN 23-Apr-05 13:31 slideflow/mosaic.py
--rw-rw-r--  2.0 unx   166666 b- defN 23-Apr-05 13:31 slideflow/project.py
--rw-rw-r--  2.0 unx    33320 b- defN 23-Apr-05 13:31 slideflow/project_utils.py
+Zip file size: 1837018 bytes, number of entries: 359
+-rw-rw-r--  2.0 unx     1411 b- defN 23-Apr-09 06:29 slideflow/__init__.py
+-rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-09 06:29 slideflow/_backend.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-Apr-12 04:31 slideflow/_version.py
+-rw-rw-r--  2.0 unx   159660 b- defN 23-Apr-10 16:45 slideflow/dataset.py
+-rw-rw-r--  2.0 unx     3733 b- defN 23-Apr-09 06:29 slideflow/errors.py
+-rw-rw-r--  2.0 unx    38304 b- defN 23-Apr-09 06:29 slideflow/heatmap.py
+-rw-rw-r--  2.0 unx    25615 b- defN 23-Apr-09 06:29 slideflow/mosaic.py
+-rw-rw-r--  2.0 unx   168044 b- defN 23-Apr-09 06:29 slideflow/project.py
+-rw-rw-r--  2.0 unx    33320 b- defN 23-Apr-09 06:29 slideflow/project_utils.py
 -rw-rw-r--  2.0 unx      978 b- defN 22-Jul-18 12:12 slideflow/sample_actions.py
--rw-rw-r--  2.0 unx     1195 b- defN 23-Apr-05 13:31 slideflow/biscuit/__init__.py
--rw-rw-r--  2.0 unx     4281 b- defN 23-Apr-05 13:31 slideflow/biscuit/delong.py
--rw-rw-r--  2.0 unx      318 b- defN 23-Apr-05 13:31 slideflow/biscuit/errors.py
--rw-rw-r--  2.0 unx    46741 b- defN 23-Apr-05 13:31 slideflow/biscuit/experiment.py
--rw-rw-r--  2.0 unx     1260 b- defN 23-Apr-05 13:31 slideflow/biscuit/hp.py
--rw-rw-r--  2.0 unx    21292 b- defN 23-Apr-05 13:31 slideflow/biscuit/threshold.py
--rw-rw-r--  2.0 unx    17143 b- defN 23-Apr-05 13:31 slideflow/biscuit/utils.py
--rw-rw-r--  2.0 unx    26001 b- defN 23-Apr-05 13:31 slideflow/cellseg/__init__.py
+-rw-rw-r--  2.0 unx     1195 b- defN 23-Apr-09 06:29 slideflow/biscuit/__init__.py
+-rw-rw-r--  2.0 unx     4281 b- defN 23-Apr-09 06:29 slideflow/biscuit/delong.py
+-rw-rw-r--  2.0 unx      318 b- defN 23-Apr-09 06:29 slideflow/biscuit/errors.py
+-rw-rw-r--  2.0 unx    46729 b- defN 23-Apr-09 06:29 slideflow/biscuit/experiment.py
+-rw-rw-r--  2.0 unx     1260 b- defN 23-Apr-09 06:29 slideflow/biscuit/hp.py
+-rw-rw-r--  2.0 unx    21373 b- defN 23-Apr-09 06:29 slideflow/biscuit/threshold.py
+-rw-rw-r--  2.0 unx    17143 b- defN 23-Apr-09 06:29 slideflow/biscuit/utils.py
+-rw-rw-r--  2.0 unx    26315 b- defN 23-Apr-10 13:46 slideflow/cellseg/__init__.py
 -rw-rw-r--  2.0 unx     5454 b- defN 23-Mar-01 23:20 slideflow/cellseg/seg_utils.py
--rw-rw-r--  2.0 unx      444 b- defN 23-Apr-05 13:31 slideflow/clam/__init__.py
--rw-rw-r--  2.0 unx     4161 b- defN 23-Apr-05 13:27 slideflow/clam/create_attention.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Apr-05 13:27 slideflow/clam/datasets/__init__.py
--rw-rw-r--  2.0 unx    15027 b- defN 23-Apr-05 13:27 slideflow/clam/datasets/dataset_generic.py
--rw-rw-r--  2.0 unx        0 b- defN 23-Apr-05 13:27 slideflow/clam/models/__init__.py
--rw-rw-r--  2.0 unx    10137 b- defN 23-Apr-05 13:27 slideflow/clam/models/model_clam.py
--rw-rw-r--  2.0 unx     3208 b- defN 23-Apr-05 13:27 slideflow/clam/models/model_mil.py
--rw-rw-r--  2.0 unx     6008 b- defN 23-Apr-05 13:27 slideflow/clam/utils/__init__.py
--rw-rw-r--  2.0 unx    19419 b- defN 23-Apr-05 13:27 slideflow/clam/utils/core_utils.py
--rw-rw-r--  2.0 unx     3577 b- defN 23-Apr-05 13:27 slideflow/clam/utils/eval_utils.py
--rw-rw-r--  2.0 unx     1131 b- defN 23-Apr-05 13:27 slideflow/clam/utils/file_utils.py
+-rw-rw-r--  2.0 unx      444 b- defN 23-Apr-09 06:29 slideflow/clam/__init__.py
 -rw-rw-r--  2.0 unx      116 b- defN 22-Oct-06 15:16 slideflow/experimental/__init__.py
 -rw-rw-r--  2.0 unx    17224 b- defN 23-Feb-01 21:03 slideflow/experimental/embedding_search.py
 -rw-rw-r--  2.0 unx      273 b- defN 23-Feb-01 21:03 slideflow/gan/__init__.py
--rw-rw-r--  2.0 unx    24993 b- defN 23-Apr-05 13:31 slideflow/gan/interpolate.py
+-rw-rw-r--  2.0 unx    24993 b- defN 23-Apr-09 06:29 slideflow/gan/interpolate.py
 -rw-rw-r--  2.0 unx      949 b- defN 23-Feb-01 21:00 slideflow/gan/utils.py
 -rw-rw-r--  2.0 unx       45 b- defN 22-Oct-22 03:43 slideflow/gan/stylegan2/__init__.py
 -rw-rw-r--  2.0 unx     8931 b- defN 22-Dec-06 01:17 slideflow/gan/stylegan2/calc_metrics.py
 -rw-rw-r--  2.0 unx    18836 b- defN 22-Jun-25 16:07 slideflow/gan/stylegan2/dataset_tool.py
 -rw-rw-r--  2.0 unx     2803 b- defN 22-Oct-21 17:38 slideflow/gan/stylegan2/generate.py
 -rw-rw-r--  2.0 unx     6841 b- defN 22-Oct-21 17:38 slideflow/gan/stylegan2/generate_tfrecords.py
 -rw-rw-r--  2.0 unx     5497 b- defN 22-Nov-07 15:35 slideflow/gan/stylegan2/interpolate.py
@@ -95,15 +85,15 @@
 -rw-rw-r--  2.0 unx     6901 b- defN 22-Sep-27 20:15 slideflow/gan/stylegan3/train.py
 -rw-rw-r--  2.0 unx     6095 b- defN 22-Sep-27 20:16 slideflow/gan/stylegan3/visualizer.py
 -rw-rw-r--  2.0 unx       77 b- defN 22-Nov-09 23:18 slideflow/gan/stylegan3/stylegan3/__init__.py
 -rw-rw-r--  2.0 unx     6128 b- defN 22-Oct-22 00:54 slideflow/gan/stylegan3/stylegan3/generate.py
 -rw-rw-r--  2.0 unx    17302 b- defN 22-Nov-05 01:09 slideflow/gan/stylegan3/stylegan3/legacy.py
 -rw-rw-r--  2.0 unx    15354 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/train.py
 -rw-rw-r--  2.0 unx     1845 b- defN 22-Oct-22 03:51 slideflow/gan/stylegan3/stylegan3/utils.py
--rw-rw-r--  2.0 unx    13379 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/visualizer.py
+-rw-rw-r--  2.0 unx    13379 b- defN 23-Apr-08 14:23 slideflow/gan/stylegan3/stylegan3/visualizer.py
 -rw-rw-r--  2.0 unx      488 b- defN 22-Aug-03 00:08 slideflow/gan/stylegan3/stylegan3/dnnlib/__init__.py
 -rw-rw-r--  2.0 unx    17870 b- defN 22-Dec-06 01:04 slideflow/gan/stylegan3/stylegan3/dnnlib/util.py
 -rw-rw-r--  2.0 unx     3153 b- defN 22-Oct-22 02:38 slideflow/gan/stylegan3/stylegan3/embedding/__init__.py
 -rw-rw-r--  2.0 unx      448 b- defN 22-Aug-03 00:08 slideflow/gan/stylegan3/stylegan3/gui_utils/__init__.py
 -rw-rw-r--  2.0 unx    16361 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/gui_utils/gl_utils.py
 -rw-rw-r--  2.0 unx     7848 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/gui_utils/glfw_window.py
 -rw-rw-r--  2.0 unx     7648 b- defN 22-Oct-28 14:24 slideflow/gan/stylegan3/stylegan3/gui_utils/imgui_utils.py
@@ -158,254 +148,214 @@
 -rw-rw-r--  2.0 unx     9520 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/layer_widget.py
 -rw-rw-r--  2.0 unx     3566 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/performance_widget.py
 -rw-rw-r--  2.0 unx     9460 b- defN 23-Apr-05 13:33 slideflow/gan/stylegan3/stylegan3/viz/pickle_widget.py
 -rw-rw-r--  2.0 unx    24417 b- defN 22-Oct-30 14:59 slideflow/gan/stylegan3/stylegan3/viz/renderer.py
 -rw-rw-r--  2.0 unx     5573 b- defN 23-Apr-03 13:00 slideflow/gan/stylegan3/stylegan3/viz/stylemix_widget.py
 -rw-rw-r--  2.0 unx     5825 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/thumb_widget.py
 -rw-rw-r--  2.0 unx     3845 b- defN 22-Oct-21 22:00 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py
--rw-rw-r--  2.0 unx    15662 b- defN 23-Apr-05 13:31 slideflow/grad/__init__.py
+-rw-rw-r--  2.0 unx    15662 b- defN 23-Apr-09 06:29 slideflow/grad/__init__.py
 -rw-rw-r--  2.0 unx     7376 b- defN 23-Feb-01 21:03 slideflow/grad/plot_utils.py
--rw-rw-r--  2.0 unx    13457 b- defN 23-Apr-05 13:31 slideflow/io/__init__.py
+-rw-rw-r--  2.0 unx    11715 b- defN 23-Apr-09 06:29 slideflow/io/__init__.py
 -rw-rw-r--  2.0 unx    10541 b- defN 22-Jul-18 12:12 slideflow/io/gaussian.py
--rw-rw-r--  2.0 unx     9916 b- defN 23-Mar-13 02:04 slideflow/io/io_utils.py
--rw-rw-r--  2.0 unx    34511 b- defN 23-Apr-05 13:31 slideflow/io/tensorflow.py
--rw-rw-r--  2.0 unx    44556 b- defN 23-Apr-05 13:31 slideflow/io/torch.py
+-rw-rw-r--  2.0 unx     9918 b- defN 23-Apr-09 06:29 slideflow/io/io_utils.py
+-rw-rw-r--  2.0 unx    34511 b- defN 23-Apr-09 06:29 slideflow/io/tensorflow.py
+-rw-rw-r--  2.0 unx    44556 b- defN 23-Apr-09 06:29 slideflow/io/torch.py
 -rw-rw-r--  2.0 unx       68 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/__init__.py
 -rw-rw-r--  2.0 unx     8419 b- defN 23-Mar-13 02:04 slideflow/io/preservedsite/crossfolds.py
--rw-rw-r--  2.0 unx      243 b- defN 23-Apr-05 13:31 slideflow/mil/__init__.py
--rw-rw-r--  2.0 unx     6180 b- defN 23-Apr-05 13:31 slideflow/mil/_params.py
--rw-rw-r--  2.0 unx     5231 b- defN 23-Apr-05 13:31 slideflow/mil/data.py
--rw-rw-r--  2.0 unx    13754 b- defN 23-Apr-05 13:31 slideflow/mil/eval.py
--rw-rw-r--  2.0 unx     3846 b- defN 23-Apr-05 13:31 slideflow/mil/clam/__init__.py
--rw-rw-r--  2.0 unx     4334 b- defN 23-Apr-05 13:31 slideflow/mil/clam/create_attention.py
--rw-rw-r--  2.0 unx     1131 b- defN 23-Apr-05 13:31 slideflow/mil/clam/datasets/__init__.py
--rw-rw-r--  2.0 unx    14982 b- defN 23-Apr-05 13:31 slideflow/mil/clam/datasets/dataset_generic.py
--rw-rw-r--  2.0 unx        6 b- defN 23-Feb-22 15:26 slideflow/mil/clam/models/__init__.py
--rw-rw-r--  2.0 unx    12758 b- defN 23-Feb-25 01:01 slideflow/mil/clam/models/model_clam.py
--rw-rw-r--  2.0 unx     3468 b- defN 23-Feb-25 01:01 slideflow/mil/clam/models/model_mil.py
--rw-rw-r--  2.0 unx     5910 b- defN 23-Apr-05 13:31 slideflow/mil/clam/utils/__init__.py
--rw-rw-r--  2.0 unx    19783 b- defN 23-Apr-05 13:31 slideflow/mil/clam/utils/core_utils.py
--rw-rw-r--  2.0 unx     4150 b- defN 23-Apr-05 13:31 slideflow/mil/clam/utils/eval_utils.py
--rw-rw-r--  2.0 unx     1283 b- defN 23-Apr-05 13:31 slideflow/mil/clam/utils/file_utils.py
--rw-rw-r--  2.0 unx     3497 b- defN 23-Apr-05 13:31 slideflow/mil/clam/utils/loss_utils.py
--rw-rw-r--  2.0 unx       38 b- defN 23-Feb-22 15:26 slideflow/mil/marugoto/__init__.py
--rw-rw-r--  2.0 unx     5085 b- defN 23-Feb-22 15:26 slideflow/mil/marugoto/data.py
--rw-rw-r--  2.0 unx     2797 b- defN 23-Feb-22 15:26 slideflow/mil/marugoto/model.py
--rw-rw-r--  2.0 unx      185 b- defN 23-Apr-05 13:31 slideflow/mil/models/__init__.py
--rw-rw-r--  2.0 unx      378 b- defN 23-Apr-05 13:31 slideflow/mil/models/_utils.py
--rw-rw-r--  2.0 unx     3239 b- defN 23-Apr-05 13:31 slideflow/mil/models/att_mil.py
--rw-rw-r--  2.0 unx    12816 b- defN 23-Apr-05 13:31 slideflow/mil/models/clam.py
--rw-rw-r--  2.0 unx     3607 b- defN 23-Apr-05 13:31 slideflow/mil/models/mil_fc.py
--rw-rw-r--  2.0 unx     4137 b- defN 23-Apr-05 13:31 slideflow/mil/models/transmil.py
--rw-rw-r--  2.0 unx    12710 b- defN 23-Apr-05 13:31 slideflow/mil/train/__init__.py
--rw-rw-r--  2.0 unx     8237 b- defN 23-Apr-05 13:31 slideflow/mil/train/_fastai.py
--rw-rw-r--  2.0 unx     7795 b- defN 23-Apr-05 13:31 slideflow/mil/train/_legacy.py
--rw-rw-r--  2.0 unx       27 b- defN 23-Feb-22 15:26 slideflow/mil/transmil/__init__.py
--rw-rw-r--  2.0 unx     2986 b- defN 23-Feb-22 15:26 slideflow/mil/transmil/model.py
--rw-rw-r--  2.0 unx     7497 b- defN 23-Apr-05 13:31 slideflow/model/__init__.py
+-rw-rw-r--  2.0 unx      284 b- defN 23-Apr-09 06:29 slideflow/mil/__init__.py
+-rw-rw-r--  2.0 unx    14819 b- defN 23-Apr-09 06:29 slideflow/mil/_params.py
+-rw-rw-r--  2.0 unx     5231 b- defN 23-Apr-09 06:29 slideflow/mil/data.py
+-rw-rw-r--  2.0 unx    15323 b- defN 23-Apr-12 04:31 slideflow/mil/eval.py
+-rw-rw-r--  2.0 unx     3890 b- defN 23-Apr-09 06:29 slideflow/mil/clam/__init__.py
+-rw-rw-r--  2.0 unx     4334 b- defN 23-Apr-09 06:29 slideflow/mil/clam/create_attention.py
+-rw-rw-r--  2.0 unx     1131 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/__init__.py
+-rw-rw-r--  2.0 unx    14990 b- defN 23-Apr-09 06:29 slideflow/mil/clam/datasets/dataset_generic.py
+-rw-rw-r--  2.0 unx     5914 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/__init__.py
+-rw-rw-r--  2.0 unx    19782 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/core_utils.py
+-rw-rw-r--  2.0 unx     4150 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/eval_utils.py
+-rw-rw-r--  2.0 unx     1287 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/file_utils.py
+-rw-rw-r--  2.0 unx     3497 b- defN 23-Apr-09 06:29 slideflow/mil/clam/utils/loss_utils.py
+-rw-rw-r--  2.0 unx      185 b- defN 23-Apr-09 06:29 slideflow/mil/models/__init__.py
+-rw-rw-r--  2.0 unx      378 b- defN 23-Apr-09 06:29 slideflow/mil/models/_utils.py
+-rw-rw-r--  2.0 unx     3239 b- defN 23-Apr-09 06:29 slideflow/mil/models/att_mil.py
+-rw-rw-r--  2.0 unx    12816 b- defN 23-Apr-09 06:29 slideflow/mil/models/clam.py
+-rw-rw-r--  2.0 unx     3607 b- defN 23-Apr-09 06:29 slideflow/mil/models/mil_fc.py
+-rw-rw-r--  2.0 unx     4137 b- defN 23-Apr-09 06:29 slideflow/mil/models/transmil.py
+-rw-rw-r--  2.0 unx    15008 b- defN 23-Apr-12 04:31 slideflow/mil/train/__init__.py
+-rw-rw-r--  2.0 unx     8214 b- defN 23-Apr-12 04:31 slideflow/mil/train/_fastai.py
+-rw-rw-r--  2.0 unx     7795 b- defN 23-Apr-09 06:29 slideflow/mil/train/_legacy.py
+-rw-rw-r--  2.0 unx     7485 b- defN 23-Apr-09 06:29 slideflow/model/__init__.py
 -rw-rw-r--  2.0 unx     1879 b- defN 23-Mar-13 02:04 slideflow/model/adv_utils.py
--rw-rw-r--  2.0 unx    23032 b- defN 23-Apr-05 13:31 slideflow/model/base.py
--rw-rw-r--  2.0 unx    55285 b- defN 23-Apr-05 13:31 slideflow/model/features.py
--rw-rw-r--  2.0 unx   110789 b- defN 23-Apr-05 13:31 slideflow/model/tensorflow.py
+-rw-rw-r--  2.0 unx    23588 b- defN 23-Apr-09 06:29 slideflow/model/base.py
+-rw-rw-r--  2.0 unx    55187 b- defN 23-Apr-10 23:12 slideflow/model/features.py
+-rw-rw-r--  2.0 unx   110777 b- defN 23-Apr-09 06:29 slideflow/model/tensorflow.py
 -rw-rw-r--  2.0 unx    22547 b- defN 23-Apr-03 05:27 slideflow/model/tensorflow_utils.py
--rw-rw-r--  2.0 unx   102914 b- defN 23-Apr-05 13:31 slideflow/model/torch.py
--rw-rw-r--  2.0 unx    16945 b- defN 23-Apr-05 13:31 slideflow/model/torch_utils.py
--rw-rw-r--  2.0 unx      394 b- defN 23-Apr-05 13:31 slideflow/model/extractors/__init__.py
--rw-rw-r--  2.0 unx     3708 b- defN 23-Apr-05 13:31 slideflow/model/extractors/_factory.py
--rw-rw-r--  2.0 unx     4544 b- defN 23-Apr-05 13:31 slideflow/model/extractors/_factory_tensorflow.py
--rw-rw-r--  2.0 unx     5711 b- defN 23-Apr-05 13:31 slideflow/model/extractors/_factory_torch.py
--rw-rw-r--  2.0 unx     1570 b- defN 23-Apr-05 13:31 slideflow/model/extractors/_registry.py
--rw-rw-r--  2.0 unx     2646 b- defN 23-Apr-05 13:31 slideflow/model/extractors/_slide.py
--rw-rw-r--  2.0 unx    25681 b- defN 23-Apr-05 13:31 slideflow/model/extractors/ctranspath.py
--rw-rw-r--  2.0 unx    12045 b- defN 23-Apr-05 13:31 slideflow/model/extractors/retccl.py
--rw-rw-r--  2.0 unx    27079 b- defN 23-Apr-05 13:31 slideflow/norm/__init__.py
--rw-rw-r--  2.0 unx     1605 b- defN 23-Apr-05 13:31 slideflow/norm/augment.py
--rw-rw-r--  2.0 unx    13438 b- defN 23-Apr-05 13:31 slideflow/norm/macenko.py
+-rw-rw-r--  2.0 unx   103048 b- defN 23-Apr-11 21:54 slideflow/model/torch.py
+-rw-rw-r--  2.0 unx    16945 b- defN 23-Apr-09 06:29 slideflow/model/torch_utils.py
+-rw-rw-r--  2.0 unx      394 b- defN 23-Apr-09 06:29 slideflow/model/extractors/__init__.py
+-rw-rw-r--  2.0 unx     3708 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory.py
+-rw-rw-r--  2.0 unx     4544 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory_tensorflow.py
+-rw-rw-r--  2.0 unx     5711 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_factory_torch.py
+-rw-rw-r--  2.0 unx     1570 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_registry.py
+-rw-rw-r--  2.0 unx     2646 b- defN 23-Apr-09 06:29 slideflow/model/extractors/_slide.py
+-rw-rw-r--  2.0 unx    25681 b- defN 23-Apr-09 06:29 slideflow/model/extractors/ctranspath.py
+-rw-rw-r--  2.0 unx    12045 b- defN 23-Apr-09 06:29 slideflow/model/extractors/retccl.py
+-rw-rw-r--  2.0 unx    26156 b- defN 23-Apr-09 06:29 slideflow/norm/__init__.py
+-rw-rw-r--  2.0 unx     1605 b- defN 23-Apr-09 06:29 slideflow/norm/augment.py
+-rw-rw-r--  2.0 unx    13438 b- defN 23-Apr-09 06:29 slideflow/norm/macenko.py
 -rw-rw-r--  2.0 unx   177672 b- defN 22-Jul-15 00:02 slideflow/norm/norm_tile.jpg
--rw-rw-r--  2.0 unx    16964 b- defN 23-Apr-05 13:31 slideflow/norm/reinhard.py
--rw-rw-r--  2.0 unx    15314 b- defN 23-Apr-05 13:31 slideflow/norm/utils.py
--rw-rw-r--  2.0 unx     7836 b- defN 23-Apr-05 13:31 slideflow/norm/vahadane.py
--rw-rw-r--  2.0 unx    11998 b- defN 23-Apr-05 13:31 slideflow/norm/tensorflow/__init__.py
+-rw-rw-r--  2.0 unx    16964 b- defN 23-Apr-09 06:29 slideflow/norm/reinhard.py
+-rw-rw-r--  2.0 unx    15314 b- defN 23-Apr-09 06:29 slideflow/norm/utils.py
+-rw-rw-r--  2.0 unx     7836 b- defN 23-Apr-09 06:29 slideflow/norm/vahadane.py
+-rw-rw-r--  2.0 unx    11998 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/__init__.py
 -rw-rw-r--  2.0 unx     5879 b- defN 22-Jul-15 00:02 slideflow/norm/tensorflow/color.py
--rw-rw-r--  2.0 unx    20759 b- defN 23-Apr-05 13:31 slideflow/norm/tensorflow/macenko.py
--rw-rw-r--  2.0 unx    26169 b- defN 23-Apr-05 13:31 slideflow/norm/tensorflow/reinhard.py
--rw-rw-r--  2.0 unx     1685 b- defN 23-Apr-05 13:31 slideflow/norm/tensorflow/utils.py
--rw-rw-r--  2.0 unx    11313 b- defN 23-Apr-05 13:31 slideflow/norm/torch/__init__.py
+-rw-rw-r--  2.0 unx    20759 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/macenko.py
+-rw-rw-r--  2.0 unx    26169 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/reinhard.py
+-rw-rw-r--  2.0 unx     1685 b- defN 23-Apr-09 06:29 slideflow/norm/tensorflow/utils.py
+-rw-rw-r--  2.0 unx    11313 b- defN 23-Apr-09 06:29 slideflow/norm/torch/__init__.py
 -rw-rw-r--  2.0 unx     7826 b- defN 22-Aug-06 13:13 slideflow/norm/torch/color.py
--rw-rw-r--  2.0 unx    14869 b- defN 23-Apr-05 13:31 slideflow/norm/torch/macenko.py
--rw-rw-r--  2.0 unx    24997 b- defN 23-Apr-05 13:31 slideflow/norm/torch/reinhard.py
--rw-rw-r--  2.0 unx     1673 b- defN 23-Apr-05 13:31 slideflow/norm/torch/utils.py
+-rw-rw-r--  2.0 unx    14869 b- defN 23-Apr-09 06:29 slideflow/norm/torch/macenko.py
+-rw-rw-r--  2.0 unx    24997 b- defN 23-Apr-09 06:29 slideflow/norm/torch/reinhard.py
+-rw-rw-r--  2.0 unx     1673 b- defN 23-Apr-09 06:29 slideflow/norm/torch/utils.py
 -rw-rw-r--  2.0 unx      458 b- defN 23-Mar-13 02:04 slideflow/simclr/__init__.py
 -rw-rw-r--  2.0 unx        6 b- defN 23-Feb-07 02:07 slideflow/simclr/simclr/__init__.py
 -rw-rw-r--  2.0 unx    20637 b- defN 23-Feb-15 14:12 slideflow/simclr/simclr/tf2/__init__.py
 -rw-rw-r--  2.0 unx    10131 b- defN 23-Feb-06 18:28 slideflow/simclr/simclr/tf2/data.py
 -rw-rw-r--  2.0 unx    18550 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/data_util.py
 -rw-rw-r--  2.0 unx     6315 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/lars_optimizer.py
 -rw-rw-r--  2.0 unx     2997 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/metrics.py
 -rw-rw-r--  2.0 unx    12256 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/model.py
 -rw-rw-r--  2.0 unx     4983 b- defN 23-Jan-23 19:16 slideflow/simclr/simclr/tf2/objective.py
 -rw-rw-r--  2.0 unx    28397 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/resnet.py
 -rw-rw-r--  2.0 unx     6524 b- defN 23-Feb-03 23:00 slideflow/simclr/simclr/tf2/run.py
 -rw-rw-r--  2.0 unx     9517 b- defN 23-Feb-06 18:24 slideflow/simclr/simclr/tf2/utils.py
--rw-rw-r--  2.0 unx   111435 b- defN 23-Apr-05 13:31 slideflow/slide/__init__.py
--rw-rw-r--  2.0 unx    18178 b- defN 23-Apr-05 13:31 slideflow/slide/report.py
--rw-rw-r--  2.0 unx    20093 b- defN 21-Nov-18 15:42 slideflow/slide/slideflow-logo-name-small.jpg
--rw-rw-r--  2.0 unx     5515 b- defN 23-Apr-05 13:31 slideflow/slide/utils.py
+-rw-rw-r--  2.0 unx   111594 b- defN 23-Apr-11 19:17 slideflow/slide/__init__.py
+-rw-rw-r--  2.0 unx    19045 b- defN 23-Apr-09 06:29 slideflow/slide/report.py
+-rw-rw-r--  2.0 unx    30934 b- defN 23-Apr-09 06:29 slideflow/slide/slideflow-logo-name-small.jpg
+-rw-rw-r--  2.0 unx     5516 b- defN 23-Apr-10 16:45 slideflow/slide/utils.py
 -rw-rw-r--  2.0 unx      708 b- defN 23-Mar-01 23:20 slideflow/slide/backends/__init__.py
--rw-rw-r--  2.0 unx    14008 b- defN 23-Apr-05 13:31 slideflow/slide/backends/cucim.py
--rw-rw-r--  2.0 unx    22391 b- defN 23-Apr-05 13:31 slideflow/slide/backends/vips.py
+-rw-rw-r--  2.0 unx    14008 b- defN 23-Apr-09 06:29 slideflow/slide/backends/cucim.py
+-rw-rw-r--  2.0 unx    22391 b- defN 23-Apr-09 06:29 slideflow/slide/backends/vips.py
 -rw-rw-r--  2.0 unx      117 b- defN 23-Mar-13 02:04 slideflow/slide/qc/__init__.py
 -rw-rw-r--  2.0 unx     1010 b- defN 22-Nov-29 16:52 slideflow/slide/qc/deepfocus_qc.py
--rw-rw-r--  2.0 unx     4304 b- defN 23-Apr-05 13:31 slideflow/slide/qc/gaussian.py
--rw-rw-r--  2.0 unx     5232 b- defN 23-Apr-05 13:31 slideflow/slide/qc/otsu.py
+-rw-rw-r--  2.0 unx     4304 b- defN 23-Apr-09 06:29 slideflow/slide/qc/gaussian.py
+-rw-rw-r--  2.0 unx     5232 b- defN 23-Apr-09 06:29 slideflow/slide/qc/otsu.py
 -rw-rw-r--  2.0 unx     3028 b- defN 23-Mar-13 02:04 slideflow/slide/qc/saver.py
 -rw-rw-r--  2.0 unx     4676 b- defN 23-Mar-13 02:04 slideflow/slide/qc/strided_dl.py
 -rw-rw-r--  2.0 unx      390 b- defN 23-Feb-01 21:02 slideflow/stats/__init__.py
--rw-rw-r--  2.0 unx     4293 b- defN 23-Apr-05 13:31 slideflow/stats/delong.py
--rw-rw-r--  2.0 unx    35785 b- defN 23-Apr-05 13:31 slideflow/stats/metrics.py
--rw-rw-r--  2.0 unx     5757 b- defN 23-Apr-05 13:31 slideflow/stats/plot.py
--rw-rw-r--  2.0 unx    43101 b- defN 23-Apr-05 13:31 slideflow/stats/slidemap.py
--rw-rw-r--  2.0 unx     3261 b- defN 23-Apr-05 13:31 slideflow/stats/stats_utils.py
--rw-rw-r--  2.0 unx    77586 b- defN 23-Apr-05 13:31 slideflow/studio/__init__.py
--rw-rw-r--  2.0 unx     2187 b- defN 23-Apr-05 13:31 slideflow/studio/__main__.py
--rw-rw-r--  2.0 unx    19662 b- defN 23-Apr-05 13:31 slideflow/studio/_renderer.py
--rw-rw-r--  2.0 unx     3686 b- defN 23-Apr-05 13:31 slideflow/studio/utils.py
--rw-rw-r--  2.0 unx        8 b- defN 23-Apr-05 13:31 slideflow/studio/gui/__init__.py
--rw-rw-r--  2.0 unx    14387 b- defN 23-Apr-05 13:31 slideflow/studio/gui/_glfw.py
--rw-rw-r--  2.0 unx     4116 b- defN 23-Apr-05 13:31 slideflow/studio/gui/annotator.py
--rw-rw-r--  2.0 unx    11035 b- defN 23-Apr-05 13:31 slideflow/studio/gui/gl_utils.py
--rw-rw-r--  2.0 unx    10029 b- defN 23-Apr-05 13:31 slideflow/studio/gui/imgui_utils.py
--rw-rw-r--  2.0 unx    29775 b- defN 23-Apr-05 13:31 slideflow/studio/gui/logo_dark_outline.png
--rw-rw-r--  2.0 unx   179066 b- defN 23-Apr-05 13:31 slideflow/studio/gui/splash.png
--rw-rw-r--  2.0 unx     5291 b- defN 23-Apr-05 13:31 slideflow/studio/gui/text_utils.py
--rw-rw-r--  2.0 unx     2241 b- defN 23-Apr-05 13:31 slideflow/studio/gui/theme.py
--rw-rw-r--  2.0 unx     2333 b- defN 23-Apr-05 13:31 slideflow/studio/gui/toast.py
--rw-rw-r--  2.0 unx     7885 b- defN 23-Apr-05 13:31 slideflow/studio/gui/window.py
--rw-rw-r--  2.0 unx     2425 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_circle_lightning.png
--rw-rw-r--  2.0 unx     2127 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_circle_lightning_highlighted.png
--rw-rw-r--  2.0 unx     2278 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_circle_plus.png
--rw-rw-r--  2.0 unx     2017 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_circle_plus_highlighted.png
--rw-rw-r--  2.0 unx     1994 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_extensions.png
--rw-rw-r--  2.0 unx     1823 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_extensions_highlighted.png
--rw-rw-r--  2.0 unx     1297 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_floppy.png
--rw-rw-r--  2.0 unx     1291 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_floppy_highlighted.png
--rw-rw-r--  2.0 unx     1463 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_folder.png
--rw-rw-r--  2.0 unx     1440 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_folder_highlighted.png
--rw-rw-r--  2.0 unx     2776 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_gear.png
--rw-rw-r--  2.0 unx     2139 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_gear_highlighted.png
--rw-rw-r--  2.0 unx     1331 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_heatmap.png
--rw-rw-r--  2.0 unx     1271 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_heatmap_highlighted.png
--rw-rw-r--  2.0 unx     2403 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_model.png
--rw-rw-r--  2.0 unx     2109 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_model_highlighted.png
--rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_model_loaded.png
--rw-rw-r--  2.0 unx     3622 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_model_loaded_highlighted.png
--rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_mosaic.png
--rw-rw-r--  2.0 unx     1609 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_mosaic_highlighted.png
--rw-rw-r--  2.0 unx     1164 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_pencil.png
--rw-rw-r--  2.0 unx      937 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_pencil_highlighted.png
--rw-rw-r--  2.0 unx     1185 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_project.png
--rw-rw-r--  2.0 unx     1167 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_project_highlighted.png
--rw-rw-r--  2.0 unx     3058 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_segment.png
--rw-rw-r--  2.0 unx     2665 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_segment_highlighted.png
--rw-rw-r--  2.0 unx     1079 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_slide.png
--rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_slide_highlighted.png
--rw-rw-r--  2.0 unx     2704 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_stylegan.png
--rw-rw-r--  2.0 unx     2301 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/button_stylegan_highlighted.png
--rw-rw-r--  2.0 unx     3563 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_cucim.png
--rw-rw-r--  2.0 unx     2316 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_ellipsis.png
--rw-rw-r--  2.0 unx     2254 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_ellipsis_highlighted.png
--rw-rw-r--  2.0 unx     2725 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_folder.png
--rw-rw-r--  2.0 unx     4078 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_gear.png
--rw-rw-r--  2.0 unx     2533 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_lowmem.png
--rw-rw-r--  2.0 unx     3500 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_refresh.png
--rw-rw-r--  2.0 unx     3026 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_verified.png
--rw-rw-r--  2.0 unx     3560 b- defN 23-Apr-05 13:31 slideflow/studio/gui/buttons/small_button_vips.png
--rw-rw-r--  2.0 unx   194488 b- defN 23-Apr-05 13:31 slideflow/studio/gui/fonts/DroidSans-Bold.ttf
--rw-rw-r--  2.0 unx   190776 b- defN 23-Apr-05 13:31 slideflow/studio/gui/fonts/DroidSans.ttf
--rw-rw-r--  2.0 unx     7568 b- defN 23-Apr-05 13:31 slideflow/studio/gui/icons/error.png
--rw-rw-r--  2.0 unx     7286 b- defN 23-Apr-05 13:31 slideflow/studio/gui/icons/info.png
--rw-rw-r--  2.0 unx   125763 b- defN 23-Apr-05 13:31 slideflow/studio/gui/icons/logo.png
--rw-rw-r--  2.0 unx     7181 b- defN 23-Apr-05 13:31 slideflow/studio/gui/icons/success.png
--rw-rw-r--  2.0 unx     6817 b- defN 23-Apr-05 13:31 slideflow/studio/gui/icons/warn.png
--rw-rw-r--  2.0 unx      107 b- defN 23-Apr-05 13:31 slideflow/studio/gui/viewer/__init__.py
--rw-rw-r--  2.0 unx     7874 b- defN 23-Apr-05 13:31 slideflow/studio/gui/viewer/_mosaic.py
--rw-rw-r--  2.0 unx    24818 b- defN 23-Apr-05 13:31 slideflow/studio/gui/viewer/_slide.py
--rw-rw-r--  2.0 unx    14452 b- defN 23-Apr-05 13:31 slideflow/studio/gui/viewer/_viewer.py
--rw-rw-r--  2.0 unx      439 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/__init__.py
--rw-rw-r--  2.0 unx      735 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/_utils.py
--rw-rw-r--  2.0 unx     6651 b- defN 23-Mar-26 06:29 slideflow/studio/widgets/annotation.py
--rw-rw-r--  2.0 unx     4264 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/capture.py
--rw-rw-r--  2.0 unx     3817 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/extensions.py
--rw-rw-r--  2.0 unx    17013 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/heatmap.py
--rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/layer_umap.py
--rw-rw-r--  2.0 unx    24474 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/model.py
--rw-rw-r--  2.0 unx    14307 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/mosaic.py
+-rw-rw-r--  2.0 unx     4293 b- defN 23-Apr-09 06:29 slideflow/stats/delong.py
+-rw-rw-r--  2.0 unx    35785 b- defN 23-Apr-09 06:29 slideflow/stats/metrics.py
+-rw-rw-r--  2.0 unx     5757 b- defN 23-Apr-09 06:29 slideflow/stats/plot.py
+-rw-rw-r--  2.0 unx    43101 b- defN 23-Apr-09 06:29 slideflow/stats/slidemap.py
+-rw-rw-r--  2.0 unx     3261 b- defN 23-Apr-09 06:29 slideflow/stats/stats_utils.py
+-rw-rw-r--  2.0 unx    77576 b- defN 23-Apr-09 06:29 slideflow/studio/__init__.py
+-rw-rw-r--  2.0 unx     2187 b- defN 23-Apr-09 06:29 slideflow/studio/__main__.py
+-rw-rw-r--  2.0 unx    19662 b- defN 23-Apr-09 06:29 slideflow/studio/_renderer.py
+-rw-rw-r--  2.0 unx     3686 b- defN 23-Apr-09 06:29 slideflow/studio/utils.py
+-rw-rw-r--  2.0 unx        8 b- defN 23-Apr-09 06:29 slideflow/studio/gui/__init__.py
+-rw-rw-r--  2.0 unx    14387 b- defN 23-Apr-09 06:29 slideflow/studio/gui/_glfw.py
+-rw-rw-r--  2.0 unx     4116 b- defN 23-Apr-09 06:29 slideflow/studio/gui/annotator.py
+-rw-rw-r--  2.0 unx    11035 b- defN 23-Apr-09 06:29 slideflow/studio/gui/gl_utils.py
+-rw-rw-r--  2.0 unx    10029 b- defN 23-Apr-09 06:29 slideflow/studio/gui/imgui_utils.py
+-rw-rw-r--  2.0 unx    29775 b- defN 23-Apr-09 06:29 slideflow/studio/gui/logo_dark_outline.png
+-rw-rw-r--  2.0 unx   179066 b- defN 23-Apr-09 06:29 slideflow/studio/gui/splash.png
+-rw-rw-r--  2.0 unx     5291 b- defN 23-Apr-09 06:29 slideflow/studio/gui/text_utils.py
+-rw-rw-r--  2.0 unx     2241 b- defN 23-Apr-09 06:29 slideflow/studio/gui/theme.py
+-rw-rw-r--  2.0 unx     2333 b- defN 23-Apr-09 06:29 slideflow/studio/gui/toast.py
+-rw-rw-r--  2.0 unx     7885 b- defN 23-Apr-09 06:29 slideflow/studio/gui/window.py
+-rw-rw-r--  2.0 unx     2425 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_lightning.png
+-rw-rw-r--  2.0 unx     2127 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_lightning_highlighted.png
+-rw-rw-r--  2.0 unx     2278 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_plus.png
+-rw-rw-r--  2.0 unx     2017 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_circle_plus_highlighted.png
+-rw-rw-r--  2.0 unx     1994 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_extensions.png
+-rw-rw-r--  2.0 unx     1823 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_extensions_highlighted.png
+-rw-rw-r--  2.0 unx     1297 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_floppy.png
+-rw-rw-r--  2.0 unx     1291 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_floppy_highlighted.png
+-rw-rw-r--  2.0 unx     1463 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_folder.png
+-rw-rw-r--  2.0 unx     1440 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_folder_highlighted.png
+-rw-rw-r--  2.0 unx     2776 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_gear.png
+-rw-rw-r--  2.0 unx     2139 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_gear_highlighted.png
+-rw-rw-r--  2.0 unx     1331 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_heatmap.png
+-rw-rw-r--  2.0 unx     1271 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_heatmap_highlighted.png
+-rw-rw-r--  2.0 unx     2403 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model.png
+-rw-rw-r--  2.0 unx     2109 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_highlighted.png
+-rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_loaded.png
+-rw-rw-r--  2.0 unx     3622 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_model_loaded_highlighted.png
+-rw-rw-r--  2.0 unx     1662 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_mosaic.png
+-rw-rw-r--  2.0 unx     1609 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_mosaic_highlighted.png
+-rw-rw-r--  2.0 unx     1164 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_pencil.png
+-rw-rw-r--  2.0 unx      937 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_pencil_highlighted.png
+-rw-rw-r--  2.0 unx     1185 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_project.png
+-rw-rw-r--  2.0 unx     1167 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_project_highlighted.png
+-rw-rw-r--  2.0 unx     3058 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_segment.png
+-rw-rw-r--  2.0 unx     2665 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_segment_highlighted.png
+-rw-rw-r--  2.0 unx     1079 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_slide.png
+-rw-rw-r--  2.0 unx     1069 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_slide_highlighted.png
+-rw-rw-r--  2.0 unx     2704 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_stylegan.png
+-rw-rw-r--  2.0 unx     2301 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/button_stylegan_highlighted.png
+-rw-rw-r--  2.0 unx     3563 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_cucim.png
+-rw-rw-r--  2.0 unx     2316 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_ellipsis.png
+-rw-rw-r--  2.0 unx     2254 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_ellipsis_highlighted.png
+-rw-rw-r--  2.0 unx     2725 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_folder.png
+-rw-rw-r--  2.0 unx     4078 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_gear.png
+-rw-rw-r--  2.0 unx     2533 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_lowmem.png
+-rw-rw-r--  2.0 unx     3500 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_refresh.png
+-rw-rw-r--  2.0 unx     3026 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_verified.png
+-rw-rw-r--  2.0 unx     3560 b- defN 23-Apr-09 06:29 slideflow/studio/gui/buttons/small_button_vips.png
+-rw-rw-r--  2.0 unx   194488 b- defN 23-Apr-09 06:29 slideflow/studio/gui/fonts/DroidSans-Bold.ttf
+-rw-rw-r--  2.0 unx   190776 b- defN 23-Apr-09 06:29 slideflow/studio/gui/fonts/DroidSans.ttf
+-rw-rw-r--  2.0 unx     7568 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/error.png
+-rw-rw-r--  2.0 unx     7286 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/info.png
+-rw-rw-r--  2.0 unx   125763 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/logo.png
+-rw-rw-r--  2.0 unx     7181 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/success.png
+-rw-rw-r--  2.0 unx     6817 b- defN 23-Apr-09 06:29 slideflow/studio/gui/icons/warn.png
+-rw-rw-r--  2.0 unx      107 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/__init__.py
+-rw-rw-r--  2.0 unx     7874 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_mosaic.py
+-rw-rw-r--  2.0 unx    24818 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_slide.py
+-rw-rw-r--  2.0 unx    14452 b- defN 23-Apr-09 06:29 slideflow/studio/gui/viewer/_viewer.py
+-rw-rw-r--  2.0 unx      439 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/__init__.py
+-rw-rw-r--  2.0 unx      735 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/_utils.py
+-rw-rw-r--  2.0 unx     4264 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/capture.py
+-rw-rw-r--  2.0 unx     5412 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/extensions.py
+-rw-rw-r--  2.0 unx    17013 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/heatmap.py
+-rw-rw-r--  2.0 unx     3842 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/layer_umap.py
+-rw-rw-r--  2.0 unx    24474 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/model.py
+-rw-rw-r--  2.0 unx    14307 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/mosaic.py
 -rw-rw-r--  2.0 unx     2597 b- defN 23-Mar-19 15:05 slideflow/studio/widgets/mosaic_experimental.py
--rw-rw-r--  2.0 unx     4662 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/performance.py
--rw-rw-r--  2.0 unx     6437 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/picam.py
--rw-rw-r--  2.0 unx     7726 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/project.py
--rw-rw-r--  2.0 unx     5983 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/seed_map.py
--rw-rw-r--  2.0 unx    19875 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/segment.py
--rw-rw-r--  2.0 unx     2071 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/settings.py
--rw-rw-r--  2.0 unx    31074 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/slide.py
--rw-rw-r--  2.0 unx    16402 b- defN 23-Apr-05 13:31 slideflow/studio/widgets/stylegan.py
--rw-rw-r--  2.0 unx    32107 b- defN 23-Apr-03 05:28 slideflow/test/__init__.py
+-rw-rw-r--  2.0 unx     4662 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/performance.py
+-rw-rw-r--  2.0 unx     6437 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/picam.py
+-rw-rw-r--  2.0 unx     7726 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/project.py
+-rw-rw-r--  2.0 unx     5983 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/seed_map.py
+-rw-rw-r--  2.0 unx    19875 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/segment.py
+-rw-rw-r--  2.0 unx     2071 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/settings.py
+-rw-rw-r--  2.0 unx    31074 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/slide.py
+-rw-rw-r--  2.0 unx    16402 b- defN 23-Apr-09 06:29 slideflow/studio/widgets/stylegan.py
+-rw-rw-r--  2.0 unx    32309 b- defN 23-Apr-09 06:29 slideflow/test/__init__.py
 -rw-rw-r--  2.0 unx    12446 b- defN 23-Feb-01 21:02 slideflow/test/dataset_test.py
 -rw-rw-r--  2.0 unx     9560 b- defN 23-Mar-26 19:33 slideflow/test/functional.py
--rw-rw-r--  2.0 unx     8052 b- defN 23-Apr-05 13:31 slideflow/test/model_test.py
+-rw-rw-r--  2.0 unx     8052 b- defN 23-Apr-09 06:29 slideflow/test/model_test.py
 -rw-rw-r--  2.0 unx    12098 b- defN 23-Mar-01 23:20 slideflow/test/norm_test.py
--rw-rw-r--  2.0 unx     2909 b- defN 23-Apr-05 13:31 slideflow/test/slide_test.py
+-rw-rw-r--  2.0 unx     2909 b- defN 23-Apr-09 06:29 slideflow/test/slide_test.py
 -rw-rw-r--  2.0 unx    11481 b- defN 23-Mar-13 02:04 slideflow/test/stats_test.py
--rw-rw-r--  2.0 unx    11752 b- defN 23-Apr-05 13:31 slideflow/test/utils.py
+-rw-rw-r--  2.0 unx    11752 b- defN 23-Apr-09 06:29 slideflow/test/utils.py
 -rw-rw-r--  2.0 unx      891 b- defN 23-Mar-13 02:04 slideflow/tfrecord/__init__.py
 -rw-rw-r--  2.0 unx     2905 b- defN 22-Dec-02 04:52 slideflow/tfrecord/iterator_utils.py
 -rw-rw-r--  2.0 unx    15505 b- defN 23-Jan-29 22:26 slideflow/tfrecord/reader.py
 -rw-rw-r--  2.0 unx     5637 b- defN 22-Jul-18 12:12 slideflow/tfrecord/writer.py
 -rw-rw-r--  2.0 unx      179 b- defN 22-Jul-18 12:12 slideflow/tfrecord/tools/__init__.py
 -rw-rw-r--  2.0 unx      310 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/__init__.py
 -rw-rw-r--  2.0 unx     7857 b- defN 22-Jul-18 12:12 slideflow/tfrecord/torch/dataset.py
--rw-rw-r--  2.0 unx    40040 b- defN 23-Apr-05 13:31 slideflow/util/__init__.py
+-rw-rw-r--  2.0 unx    41104 b- defN 23-Apr-09 06:29 slideflow/util/__init__.py
 -rw-rw-r--  2.0 unx      738 b- defN 22-Jul-15 00:02 slideflow/util/colors.py
 -rw-rw-r--  2.0 unx    17912 b- defN 22-Jul-18 12:12 slideflow/util/example_pb2.py
 -rw-rw-r--  2.0 unx     4468 b- defN 23-Feb-01 21:02 slideflow/util/log_utils.py
 -rw-rw-r--  2.0 unx     4381 b- defN 22-Jul-18 12:12 slideflow/util/neptune_utils.py
--rw-rw-r--  2.0 unx    20061 b- defN 23-Apr-05 13:31 slideflow/util/smac_utils.py
--rw-rw-r--  2.0 unx     6490 b- defN 23-Apr-05 13:31 slideflow/util/tfrecord2idx.py
--rw-rw-r--  2.0 unx    64602 b- defN 23-Apr-05 13:27 slideflow/workbench/__init__.py
--rw-rw-r--  2.0 unx     2750 b- defN 23-Apr-05 13:27 slideflow/workbench/__main__.py
--rw-rw-r--  2.0 unx     4329 b- defN 23-Apr-05 13:27 slideflow/workbench/capture_widget.py
--rw-rw-r--  2.0 unx    19460 b- defN 23-Apr-05 13:27 slideflow/workbench/heatmap_widget.py
--rw-rw-r--  2.0 unx     3847 b- defN 23-Apr-05 13:27 slideflow/workbench/layer_umap_widget.py
--rw-rw-r--  2.0 unx    15653 b- defN 23-Apr-05 13:27 slideflow/workbench/model_widget.py
--rw-rw-r--  2.0 unx    11488 b- defN 23-Feb-24 18:16 slideflow/workbench/mosaic_widget.py
--rw-rw-r--  2.0 unx     3982 b- defN 23-Apr-05 13:27 slideflow/workbench/performance_widget.py
--rw-rw-r--  2.0 unx     6465 b- defN 23-Apr-05 13:27 slideflow/workbench/picam_widget.py
--rw-rw-r--  2.0 unx     4617 b- defN 23-Apr-05 13:27 slideflow/workbench/project_widget.py
--rw-rw-r--  2.0 unx     6045 b- defN 23-Apr-05 13:27 slideflow/workbench/seed_map_widget.py
--rw-rw-r--  2.0 unx    19721 b- defN 23-Apr-05 13:27 slideflow/workbench/segment_widget.py
--rw-rw-r--  2.0 unx    12956 b- defN 23-Apr-05 13:27 slideflow/workbench/slide_renderer.py
--rw-rw-r--  2.0 unx    26409 b- defN 23-Apr-05 13:27 slideflow/workbench/slide_widget.py
--rw-rw-r--  2.0 unx     3582 b- defN 23-Apr-05 13:27 slideflow/workbench/utils.py
--rw-rw-r--  2.0 unx   194488 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/DroidSans-Bold.ttf
--rw-rw-r--  2.0 unx   190776 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/DroidSans.ttf
--rw-rw-r--  2.0 unx        8 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/__init__.py
--rw-rw-r--  2.0 unx    17670 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/gl_utils.py
--rw-rw-r--  2.0 unx     9432 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/glfw_window.py
--rw-rw-r--  2.0 unx    11859 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/imgui_utils.py
--rw-rw-r--  2.0 unx    14815 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/imgui_window.py
--rw-rw-r--  2.0 unx     7577 b- defN 23-Feb-05 05:27 slideflow/workbench/gui_utils/mosaic_viewer.py
--rw-rw-r--  2.0 unx     6043 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/text_utils.py
--rw-rw-r--  2.0 unx    14450 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/viewer.py
--rw-rw-r--  2.0 unx    21988 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/wsi_utils.py
--rw-rw-r--  2.0 unx     7568 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/icons/error.png
--rw-rw-r--  2.0 unx     7286 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/icons/info.png
--rw-rw-r--  2.0 unx    37355 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/icons/logo.png
--rw-rw-r--  2.0 unx     7181 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/icons/success.png
--rw-rw-r--  2.0 unx     6817 b- defN 23-Apr-05 13:27 slideflow/workbench/gui_utils/icons/warn.png
--rwxrwxr-x  2.0 unx    14329 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.data/scripts/slideflow-studio
--rwxrwxr-x  2.0 unx    13814 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.data/scripts/slideflow-studio.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.dist-info/LICENSE
--rw-rw-r--  2.0 unx    12341 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.dist-info/WHEEL
--rw-rw-r--  2.0 unx       10 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    40410 b- defN 23-Apr-05 13:33 slideflow-2.0.0b1.dist-info/RECORD
-409 files, 5597348 bytes uncompressed, 2132959 bytes compressed:  61.9%
+-rw-rw-r--  2.0 unx    20061 b- defN 23-Apr-09 06:29 slideflow/util/smac_utils.py
+-rw-rw-r--  2.0 unx     7920 b- defN 23-Apr-12 04:31 slideflow/util/tfrecord2idx.py
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-12 04:31 slideflow-2.0.1.data/scripts/slideflow-studio
+-rwxrwxr-x  2.0 unx    14085 b- defN 23-Apr-10 13:18 slideflow-2.0.1.data/scripts/slideflow-studio.py
+-rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-12 04:31 slideflow-2.0.1.dist-info/LICENSE
+-rw-rw-r--  2.0 unx    12998 b- defN 23-Apr-12 04:31 slideflow-2.0.1.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-Apr-12 04:31 slideflow-2.0.1.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       10 b- defN 23-Apr-12 04:31 slideflow-2.0.1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    35648 b- defN 23-Apr-12 04:31 slideflow-2.0.1.dist-info/RECORD
+359 files, 4764377 bytes uncompressed, 1779382 bytes compressed:  62.7%
```

## zipnote {}

```diff
@@ -54,44 +54,14 @@
 
 Filename: slideflow/cellseg/seg_utils.py
 Comment: 
 
 Filename: slideflow/clam/__init__.py
 Comment: 
 
-Filename: slideflow/clam/create_attention.py
-Comment: 
-
-Filename: slideflow/clam/datasets/__init__.py
-Comment: 
-
-Filename: slideflow/clam/datasets/dataset_generic.py
-Comment: 
-
-Filename: slideflow/clam/models/__init__.py
-Comment: 
-
-Filename: slideflow/clam/models/model_clam.py
-Comment: 
-
-Filename: slideflow/clam/models/model_mil.py
-Comment: 
-
-Filename: slideflow/clam/utils/__init__.py
-Comment: 
-
-Filename: slideflow/clam/utils/core_utils.py
-Comment: 
-
-Filename: slideflow/clam/utils/eval_utils.py
-Comment: 
-
-Filename: slideflow/clam/utils/file_utils.py
-Comment: 
-
 Filename: slideflow/experimental/__init__.py
 Comment: 
 
 Filename: slideflow/experimental/embedding_search.py
 Comment: 
 
 Filename: slideflow/gan/__init__.py
@@ -534,23 +504,14 @@
 
 Filename: slideflow/mil/clam/datasets/__init__.py
 Comment: 
 
 Filename: slideflow/mil/clam/datasets/dataset_generic.py
 Comment: 
 
-Filename: slideflow/mil/clam/models/__init__.py
-Comment: 
-
-Filename: slideflow/mil/clam/models/model_clam.py
-Comment: 
-
-Filename: slideflow/mil/clam/models/model_mil.py
-Comment: 
-
 Filename: slideflow/mil/clam/utils/__init__.py
 Comment: 
 
 Filename: slideflow/mil/clam/utils/core_utils.py
 Comment: 
 
 Filename: slideflow/mil/clam/utils/eval_utils.py
@@ -558,23 +519,14 @@
 
 Filename: slideflow/mil/clam/utils/file_utils.py
 Comment: 
 
 Filename: slideflow/mil/clam/utils/loss_utils.py
 Comment: 
 
-Filename: slideflow/mil/marugoto/__init__.py
-Comment: 
-
-Filename: slideflow/mil/marugoto/data.py
-Comment: 
-
-Filename: slideflow/mil/marugoto/model.py
-Comment: 
-
 Filename: slideflow/mil/models/__init__.py
 Comment: 
 
 Filename: slideflow/mil/models/_utils.py
 Comment: 
 
 Filename: slideflow/mil/models/att_mil.py
@@ -594,20 +546,14 @@
 
 Filename: slideflow/mil/train/_fastai.py
 Comment: 
 
 Filename: slideflow/mil/train/_legacy.py
 Comment: 
 
-Filename: slideflow/mil/transmil/__init__.py
-Comment: 
-
-Filename: slideflow/mil/transmil/model.py
-Comment: 
-
 Filename: slideflow/model/__init__.py
 Comment: 
 
 Filename: slideflow/model/adv_utils.py
 Comment: 
 
 Filename: slideflow/model/base.py
@@ -993,17 +939,14 @@
 
 Filename: slideflow/studio/widgets/__init__.py
 Comment: 
 
 Filename: slideflow/studio/widgets/_utils.py
 Comment: 
 
-Filename: slideflow/studio/widgets/annotation.py
-Comment: 
-
 Filename: slideflow/studio/widgets/capture.py
 Comment: 
 
 Filename: slideflow/studio/widgets/extensions.py
 Comment: 
 
 Filename: slideflow/studio/widgets/heatmap.py
@@ -1107,122 +1050,29 @@
 
 Filename: slideflow/util/smac_utils.py
 Comment: 
 
 Filename: slideflow/util/tfrecord2idx.py
 Comment: 
 
-Filename: slideflow/workbench/__init__.py
-Comment: 
-
-Filename: slideflow/workbench/__main__.py
-Comment: 
-
-Filename: slideflow/workbench/capture_widget.py
-Comment: 
-
-Filename: slideflow/workbench/heatmap_widget.py
-Comment: 
-
-Filename: slideflow/workbench/layer_umap_widget.py
-Comment: 
-
-Filename: slideflow/workbench/model_widget.py
-Comment: 
-
-Filename: slideflow/workbench/mosaic_widget.py
-Comment: 
-
-Filename: slideflow/workbench/performance_widget.py
-Comment: 
-
-Filename: slideflow/workbench/picam_widget.py
-Comment: 
-
-Filename: slideflow/workbench/project_widget.py
-Comment: 
-
-Filename: slideflow/workbench/seed_map_widget.py
-Comment: 
-
-Filename: slideflow/workbench/segment_widget.py
-Comment: 
-
-Filename: slideflow/workbench/slide_renderer.py
-Comment: 
-
-Filename: slideflow/workbench/slide_widget.py
-Comment: 
-
-Filename: slideflow/workbench/utils.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/DroidSans-Bold.ttf
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/DroidSans.ttf
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/__init__.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/gl_utils.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/glfw_window.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/imgui_utils.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/imgui_window.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/mosaic_viewer.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/text_utils.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/viewer.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/wsi_utils.py
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/icons/error.png
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/icons/info.png
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/icons/logo.png
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/icons/success.png
-Comment: 
-
-Filename: slideflow/workbench/gui_utils/icons/warn.png
-Comment: 
-
-Filename: slideflow-2.0.0b1.data/scripts/slideflow-studio
+Filename: slideflow-2.0.1.data/scripts/slideflow-studio
 Comment: 
 
-Filename: slideflow-2.0.0b1.data/scripts/slideflow-studio.py
+Filename: slideflow-2.0.1.data/scripts/slideflow-studio.py
 Comment: 
 
-Filename: slideflow-2.0.0b1.dist-info/LICENSE
+Filename: slideflow-2.0.1.dist-info/LICENSE
 Comment: 
 
-Filename: slideflow-2.0.0b1.dist-info/METADATA
+Filename: slideflow-2.0.1.dist-info/METADATA
 Comment: 
 
-Filename: slideflow-2.0.0b1.dist-info/WHEEL
+Filename: slideflow-2.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: slideflow-2.0.0b1.dist-info/top_level.txt
+Filename: slideflow-2.0.1.dist-info/top_level.txt
 Comment: 
 
-Filename: slideflow-2.0.0b1.dist-info/RECORD
+Filename: slideflow-2.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## slideflow/_version.py

```diff
@@ -4,18 +4,18 @@
 # unpacked source archive. Distribution tarballs contain a pre-generated copy
 # of this file.
 
 import json
 
 version_json = '''
 {
- "date": "2023-04-05T08:20:31-0500",
+ "date": "2023-04-11T23:17:03-0500",
  "dirty": false,
  "error": null,
- "full-revisionid": "6f56a6aa2a6da6260b08bac9d0808f713f9bd129",
- "version": "2.0.0-beta1"
+ "full-revisionid": "3495a878316e9e6ba4525c3a225827817f5198e9",
+ "version": "2.0.1"
 }
 '''  # END VERSION_JSON
 
 
 def get_versions():
     return json.loads(version_json)
```

## slideflow/dataset.py

```diff
@@ -1274,17 +1274,18 @@
                 for tfr in manifest
             }
         return ret
 
     def convert_xml_rois(self):
         """Convert ImageScope XML ROI files to QuPath format CSV ROI files."""
         n_converted = 0
+        xml_list = []
         for source in self.sources:
             if self._roi_set(source):
-                xml_list += glob(join(self.sources[source]['roi'], "*.xml"))
+                xml_list = glob(join(self.sources[source]['roi'], "*.xml"))
                 if len(xml_list) == 0:
                     raise errors.DatasetError(
                         'No XML files found. Check dataset configuration.'
                     )
                 for xml in xml_list:
                     try:
                         sf.slide.utils.xml_to_csv(xml)
@@ -2261,15 +2262,22 @@
                 )
             else:
                 if slide not in sf.util.EMPTY:
                     result[slide] = patient
         return result
 
     def pt_files(self, path, warn_missing=True):
-        """Return list of *.pt files with slide names in this dataset."""
+        """Return list of \*.pt files with slide names in this dataset.
+
+        Args:
+            path (str): Directory to search for \*.pt files.
+            warn_missing (bool): Raise a warning if any slides in this dataset
+                do not have a \*.pt file.
+
+        """
         slides = self.slides()
         bags = np.array([
             join(path, f) for f in os.listdir(path)
             if f.endswith('.pt') and path_to_name(f) in slides
         ])
         if (len(bags) != len(slides)) and warn_missing:
             log.warning(f"Bags missing for {len(slides) - len(bags)} slides.")
@@ -2301,15 +2309,15 @@
 
         """
         tfr = self.find_tfrecord(slide=slide)
         if tfr is None:
             raise errors.TFRecordsError(
                 f"Could not find associated TFRecord for slide '{slide}'"
             )
-        return sf.io.read_tfrecord_by_location(tfr, loc, decode=decode)
+        return sf.io.get_tfrecord_by_location(tfr, loc, decode=decode)
 
     def remove_filter(self, **kwargs: Any) -> "Dataset":
         """Remove a specific filter from the active filters.
 
         Keyword Args:
             filters (list of str): Filter keys. Will remove filters with
                 these keys.
@@ -3185,15 +3193,19 @@
             for i, record in enumerate(dataset):
                 if i > 9:
                     break
                 image_raw_data = parser(record)[0]
                 if normalizer:
                     image_raw_data = normalizer.jpeg_to_jpeg(image_raw_data)
                 sample_tiles += [image_raw_data]
-            reports += [SlideReport(sample_tiles, tfr)]
+            reports += [SlideReport(sample_tiles,
+                                    tfr,
+                                    tile_px=self.tile_px,
+                                    tile_um=self.tile_um,
+                                    ignore_thumb_errors=True)]
 
         # Generate and save PDF
         log.info('Generating PDF (this may take some time)...')
         pdf_report = ExtractionReport(reports, title='TFRecord Report')
         timestring = datetime.now().strftime('%Y%m%d-%H%M%S')
         if exists(dest) and isdir(dest):
             filename = join(dest, f'tfrecord_report-{timestring}.pdf')
```

## slideflow/mosaic.py

```diff
@@ -13,15 +13,15 @@
 import cv2
 import numpy as np
 from rich.progress import track
 
 import slideflow as sf
 from slideflow import errors
 from slideflow.stats import SlideMap, get_centroid_index
-from slideflow.util import log, tfrecord2idx
+from slideflow.util import log
 from slideflow.stats import get_centroid_index
 
 if TYPE_CHECKING:
     from slideflow.norm import StainNormalizer
 
 # -----------------------------------------------------------------------------
 
@@ -29,15 +29,15 @@
     if args is None:
         return None, None, None, None
     point_index, x, y, display_size, alpha, image = args
     if not point_index:
         return None, None, None, None
     if isinstance(image, tuple):
         tfr, tfr_idx = image
-        image = tfrecord2idx.get_record_by_index(tfr, tfr_idx)['image_raw']
+        image = sf.io.get_tfrecord_by_index(tfr, tfr_idx)['image_raw']
     if image is None:
         return point_index, None, None, None
     if sf.model.is_tensorflow_tensor(image):
         image = image.numpy()
     image = decode_image(image, **decode_kwargs)
     extent = [
         x - display_size/2,
@@ -291,15 +291,15 @@
         point = self.points.loc[index]
         if 'tfr_index' in point:
             tfr = self._get_tfrecords_from_slide(point.slide)
             tfr_idx = point.tfr_index
             if not tfr:
                 log.error(f"TFRecord {tfr} not found in slide_map")
                 return None
-            image = tfrecord2idx.get_record_by_index(tfr, tfr_idx)['image_raw']
+            image = sf.io.get_tfrecord_by_index(tfr, tfr_idx)['image_raw']
         else:
             image = self.images[index]
         return image
 
     def _get_tfrecords_from_slide(self, slide: str) -> Optional[str]:
         """Using the internal list of TFRecord paths, returns the path to a
         TFRecord for a given corresponding slide."""
```

## slideflow/project.py

```diff
@@ -1427,28 +1427,28 @@
 
     def evaluate_mil(
         self,
         model: str,
         outcomes: Union[str, List[str]],
         dataset: Dataset,
         bags: Union[str, List[str]],
-        config: Optional["mil.TrainerConfig"] = None,
+        config: Optional["mil._TrainerConfig"] = None,
         **kwargs
     ):
         r"""Evaluate a multi-instance learning model.
 
         Saves results for the evaluation in the ``mil_eval`` project folder,
         including predictions (parquet format), attention (Numpy format for
         each slide), and attention heatmaps (if ``attention_heatmaps=True``).
 
         Logs classifier metrics (AUROC and AP) to the console.
 
         Args:
-            config (:class:`slideflow.mil.TrainerConfig): Training
-                configuration, as obtained by
+            config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+                Training configuration, as obtained by
                 :func:`slideflow.mil.mil_config()`.
             train_dataset (:class:`slideflow.Dataset`): Training dataset.
             val_dataset (:class:`slideflow.Dataset`): Validation dataset.
             outcomes (str): Outcome column (annotation header) from which to
                 derive category labels.
             bags (str): Either a path to directory with \*.pt files, or a list
                 of paths to individual \*.pt files. Each file should contain
@@ -1457,14 +1457,23 @@
 
         Keyword args:
             exp_label (str): Experiment label, used for naming the subdirectory
                 in the ``{project root}/mil`` folder, where training history
                 and the model will be saved.
             attention_heatmaps (bool): Calculate and save attention heatmaps.
                 Defaults to False.
+            interpolation (str, optional): Interpolation strategy for smoothing
+                attention heatmaps. Defaults to 'bicubic'.
+            cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+                valid matplotlib colormap. Defaults to 'inferno'.
+            norm (str, optional): Normalization strategy for assigning heatmap
+                values to colors. Either 'two_slope', or any other valid value
+                for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+                If 'two_slope', normalizes values less than 0 and greater than 0
+                separately. Defaults to None.
 
         """
         from .mil import eval_mil
 
         return eval_mil(
             model,
             dataset=dataset,
@@ -3422,15 +3431,15 @@
         """Train an ensemble of model(s).
 
         Trains models using a given set of parameters and outcomes by calling
         the train function ``n_ensembles`` of times.
 
         Args:
             outcomes (str or list(str)): Outcome label annotation header(s).
-            params (:class:`slideflow.model.ModelParams`, list or dict):
+            params (:class:`slideflow.ModelParams`, list or dict):
                 Model parameters for training. May provide one `ModelParams`,
                 a list, or dict mapping model names to params. If multiple
                 params are provided, will train an hyper deep ensemble models
                 for them, otherwise a deep ensemble model.
 
         Keyword Args:
             n_ensembles (int, optional): Total models needed in the ensemble.
@@ -3461,15 +3470,15 @@
                     "Keyword argument 'n_ensembles' is required if 'params' is"
                     " not a list of ModelParams."
                 )
         elif isinstance(params, list):
             hyper_deep = True
             if not all([isinstance(hp, ModelParams) for hp in params]):
                 raise errors.ModelParamsError(
-                    'If params is a list, items must be sf.model.ModelParams'
+                    'If params is a list, items must be sf.ModelParams'
                 )
             hp_list = params
             n_ensembles = len(hp_list)
         elif isinstance(params, dict):
             hyper_deep = True
             if not all([isinstance(hp, str) for hp in params.keys()]):
                 raise errors.ModelParamsError(
@@ -3676,28 +3685,28 @@
             outdir=join(self.root, 'clam'),
             splits=join(self.root, splits),
             **kwargs
         )
 
     def train_mil(
         self,
-        config: "mil.TrainerConfig",
+        config: "mil._TrainerConfig",
         train_dataset: Dataset,
         val_dataset: Dataset,
         outcomes: Union[str, List[str]],
         bags: Union[str, List[str]],
         *,
         exp_label: Optional[str] = None,
         **kwargs
     ):
         r"""Train a multi-instance learning model.
 
         Args:
-            config (:class:`slideflow.mil.TrainerConfig): Training
-                configuration, as obtained by
+            config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+                Training configuration, as obtained by
                 :func:`slideflow.mil.mil_config()`.
             train_dataset (:class:`slideflow.Dataset`): Training dataset.
             val_dataset (:class:`slideflow.Dataset`): Validation dataset.
             outcomes (str): Outcome column (annotation header) from which to
                 derive category labels.
             bags (str): Either a path to directory with \*.pt files, or a list
                 of paths to individual \*.pt files. Each file should contain
@@ -3706,14 +3715,23 @@
 
         Keyword args:
             exp_label (str): Experiment label, used for naming the subdirectory
                 in the ``{project root}/mil`` folder, where training history
                 and the model will be saved.
             attention_heatmaps (bool): Calculate and save attention heatmaps
                 on the validation dataset. Defaults to False.
+            interpolation (str, optional): Interpolation strategy for smoothing
+                attention heatmaps. Defaults to 'bicubic'.
+            cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+                valid matplotlib colormap. Defaults to 'inferno'.
+            norm (str, optional): Normalization strategy for assigning heatmap
+                values to colors. Either 'two_slope', or any other valid value
+                for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+                If 'two_slope', normalizes values less than 0 and greater than 0
+                separately. Defaults to None.
 
         """
         from .mil import train_mil
 
         return train_mil(
             config,
             train_dataset,
```

## slideflow/biscuit/experiment.py

```diff
@@ -78,16 +78,15 @@
                 raise ValueError(f"Unrecognized value for eval_project: {eval_projects}")
 
         self.outcome = outcome
         self.outcome1 = outcome1
         self.outcome2 = outcome2
         self.outdir = outdir
 
-    @staticmethod
-    def add(path, label, out1, out2, order='f', order_col='order', gan=0):
+    def add(self, path, label, out1, out2, order='f', order_col='order', gan=0):
         """Adds a sample size experiment to the given project annotations file.
 
         Args:
             path (str): Path to project annotations file.
             label (str): Experimental label.
             out1 (int): Number of lung adenocarcinomas (LUAD) to include in the
                 experiment.
```

## slideflow/biscuit/threshold.py

```diff
@@ -2,15 +2,14 @@
 import matplotlib.pyplot as plt
 import numpy as np
 import pandas as pd
 import seaborn as sns
 
 from sklearn import metrics
 from sklearn.exceptions import UndefinedMetricWarning
-from skmisc.loess import loess
 from slideflow.util import log
 
 from . import errors, utils
 
 
 def plot_uncertainty(df, kind, threshold=None, title=None):
     """Plots figure of tile or slide-level predictions vs. uncertainty.
@@ -23,21 +22,26 @@
         threshold (float, optional): Uncertainty threshold.
             Defaults to None.
         title (str, optional): Title for plots. Defaults to None.
 
     Returns:
         None
     """
+    try:
+        from skmisc.loess import loess
+    except ImportError:
+        raise ImportError(
+            "Uncertainty plots with loess estimation require scikit-misc, "
+            "which is not installed."
+        )
 
     # Subsample tile-level predictions
     if kind == 'tile':
         df = df.sample(n=1000)
 
-    #print(f"Saving uncertainty plot (threshold={threshold})")
-    #df.to_csv('uncertainty_plot_raw.csv')
     f, axes = plt.subplots(1, 3)
     f.set_size_inches(15, 5)
     palette = sns.color_palette("Set2")
     tf_pal = {True: palette[0], False: palette[1]}
 
     # Left figure - KDE -------------------------------------------------------
     kde = sns.kdeplot(
@@ -179,15 +183,15 @@
 
 def process_group_predictions(df, pred_thresh, level):
     '''From a given dataframe of tile-level predictions, calculate group-level
     predictions and uncertainty.'''
 
     if any(c not in df.columns for c in ('y_true', 'y_pred', 'uncertainty')):
         raise ValueError('Missing columns. Expected y_true, y_pred, uncertainty.'
-                         f'Got: {grouped.columns}')
+                         f'Got: {df.columns}')
 
     # Calculate group-level predictions
     log.debug(f'Calculating {level}-level means from {len(df)} predictions')
     levels = [l for l in pd.unique(df[level]) if l is not np.nan]
     reduced_df = df[[level, 'y_pred', 'y_true', 'uncertainty']]
     grouped = reduced_df.groupby(level, as_index=False).mean()
     yp = np.array([
```

## slideflow/cellseg/__init__.py

```diff
@@ -125,30 +125,37 @@
 
         Args:
             scale (float): ROI scale (roi size / WSI base dimension size).
             annpolys (list(``shapely.geometry.Polygon``)): List of ROI
                 polygons, as available in ``slideflow.WSI.annPolys``.
 
         """
-        roi_seg_scale = scale / self.wsi_ratio
-        scaled_polys = [
-            sa.scale(
-                poly,
-                xfact=roi_seg_scale,
-                yfact=roi_seg_scale,
-                origin=(0, 0)
-            ) for poly in annpolys
-        ]
-        roi_seg_mask = rasterio.features.rasterize(
-            scaled_polys,
-            out_shape=self.masks.shape,
-            all_touched=False
-        ).astype(bool)
-        self.masks *= roi_seg_mask
-        self.calculate_centroids(force=True)
+        if self.wsi_ratio is not None and len(annpolys):
+            roi_seg_scale = scale / self.wsi_ratio
+            scaled_polys = [
+                sa.scale(
+                    poly,
+                    xfact=roi_seg_scale,
+                    yfact=roi_seg_scale,
+                    origin=(0, 0)
+                ) for poly in annpolys
+            ]
+            roi_seg_mask = rasterio.features.rasterize(
+                scaled_polys,
+                out_shape=self.masks.shape,
+                all_touched=False
+            ).astype(bool)
+            self.masks *= roi_seg_mask
+            self.calculate_centroids(force=True)
+        elif self.wsi_ratio is None:
+            log.warning("Unable to apply ROIs; WSI dimensions not set.")
+            return
+        else:
+            # No ROIs to apply
+            return
 
     def centroids(self, wsi_dim: bool = False) -> np.ndarray:
         """Calculate and return mask centroids.
 
         Args:
             wsi_dim (bool): Convert centroids from mask space to WSI space.
                 Requires that ``wsi_dim`` was provided during initialization.
```

## slideflow/io/__init__.py

```diff
@@ -9,14 +9,15 @@
 from random import shuffle
 from typing import Any, Dict, Optional, Tuple, Union, List
 
 import slideflow as sf
 from slideflow import errors
 from slideflow.io.io_utils import detect_tfrecord_format, convert_dtype
 from slideflow.util import log
+from slideflow.util.tfrecord2idx import get_tfrecord_by_index, get_tfrecord_length
 from rich.progress import Progress
 
 # --- Backend-specific imports and configuration ------------------------------
 
 if sf.backend() == 'tensorflow':
     from slideflow.io.tensorflow import get_tfrecord_parser  # noqa F401
     from slideflow.io.tensorflow import read_and_return_record  # noqa F401
@@ -41,22 +42,16 @@
 def update_manifest_at_dir(
     directory: str,
     force_update: bool = False
 ) -> Optional[Union[str, Dict]]:
     """Log number of tiles in each TFRecord file present in the given
     directory and all subdirectories, saving manifest to file within
     the parent directory.
-    """
-
-    if sf.backend() == 'tensorflow':
-        import tensorflow as tf
-        dataloss_errors = [tf.errors.DataLossError, errors.TFRecordsError]
-    else:
-        dataloss_errors = [errors.TFRecordsError]
 
+    """
     manifest_path = join(directory, "manifest.json")
     if not exists(manifest_path):
         manifest = {}
     else:
         manifest = sf.util.load_json(manifest_path)
     prior_manifest = copy.deepcopy(manifest)
     try:
@@ -76,16 +71,16 @@
         tfr = join(directory, rel_tfr)
         if ((not force_update)
            and (rel_tfr in manifest)
            and ('total' in manifest[rel_tfr])):
             return None
         rel_tfr_manifest = {rel_tfr: {}}
         try:
-            total = read_tfrecord_length(tfr)
-        except dataloss_errors:
+            total = get_tfrecord_length(tfr)
+        except (errors.TFRecordsError, OSError):
             log.error(f"Corrupt or incomplete TFRecord at {tfr}; removing")
             os.remove(tfr)
             return None
         if not total:
             log.error(f"Empty TFRecord at {tfr}; removing")
             os.remove(tfr)
             return None
@@ -109,50 +104,15 @@
     # Write manifest file
     if (manifest != prior_manifest) or (manifest == {}):
         sf.util.write_json(manifest, manifest_path)
     pool.close()
     return manifest
 
 
-def get_tfrecord_by_index(
-    tfrecord: str,
-    index: int,
-    decode: bool = True
-) -> Any:
-    '''Reads and returns an individual record from a tfrecord by index,
-    including slide name and processed image data.
-    '''
-    if type(index) != int:
-        try:
-            index = int(index)
-        except ValueError:
-            raise IndexError(f"index must be an integer, not {type(index)} "
-                             f"(provided {index}).")
-
-    dataset = TFRecordDataset(tfrecord)
-    parser = get_tfrecord_parser(
-        tfrecord,
-        ('slide', 'image_raw'),
-        decode_images=decode
-    )
-    total = 0
-    for i, record in enumerate(dataset):
-        total += 1
-        if i == index:
-            return parser(record)  # type: ignore
-        else:
-            continue
-    log.error(
-        f"Unable to find record: index {index} in {sf.util.green(tfrecord)}"
-        f" ({total} total records)"
-    )
-    return False, False
-
-
-def read_tfrecord_by_location(
+def get_tfrecord_by_location(
     tfrecord: str,
     location: Tuple[int, int],
     decode: bool = True
 ) -> Any:
     '''Reads and returns an individual record from a tfrecord by index,
     including slide name and processed image data.
 
@@ -346,49 +306,26 @@
             os.makedirs(dest_folder)
         tile_filename = f"tile{i}.{img_type}"
         image_string = open(join(dest_folder, tile_filename), 'wb')
         image_string.write(image_raw)
         image_string.close()
 
 
-def read_tfrecord_length(tfrecord: str) -> int:
-    """Returns number of records stored in the given tfrecord file."""
-    infile = open(tfrecord, "rb")
-    num_records = 0
-    while True:
-        infile.tell()
-        try:
-            byte_len = infile.read(8)
-            if len(byte_len) == 0:
-                break
-            infile.read(4)
-            proto_len = struct.unpack("q", byte_len)[0]
-            infile.read(proto_len)
-            infile.read(4)
-            num_records += 1
-        except Exception:
-            log.error(f"Failed to parse TFRecord at {tfrecord}")
-            infile.close()
-            return 0
-    infile.close()
-    return num_records
-
-
 def get_locations_from_tfrecord(
     filename: str,
     as_dict: bool = True
 ) -> Union[
     Dict[int, Tuple[int, int]],
     List[Tuple[int, int]],
 ]:
     '''Returns dictionary mapping indices to tile locations (X, Y)'''
     out_dict = {}
     out_list = []
-    for i in range(sf.util.get_tfrecord_length(filename)):
-        record = sf.util.get_record_by_index(filename, i)
+    for i in range(sf.io.get_tfrecord_length(filename)):
+        record = sf.io.get_tfrecord_by_index(filename, i)
         loc_x = record['loc_x']
         loc_y = record['loc_y']
         if as_dict:
             out_dict.update({i: (loc_x, loc_y)})
         else:
             out_list.append((loc_x, loc_y))
     return out_dict if as_dict else out_list
@@ -396,9 +333,9 @@
 
 def tfrecord_has_locations(
     filename: str,
     check_x: int = True,
     check_y: bool = False
 ) -> bool:
     """Check if a given TFRecord has location information stored."""
-    record = sf.util.get_record_by_index(filename, 0)
+    record = sf.io.get_tfrecord_by_index(filename, 0)
     return (((not check_x) or 'loc_x' in record ) and ((not check_y) or 'loc_y' in record ))
```

## slideflow/io/io_utils.py

```diff
@@ -65,15 +65,15 @@
         A tuple containing
 
             list(str): List of detected features.
 
             str: Image file type (png/jpeg)
     '''
     try:
-        record = tfrecord2idx.get_record_by_index(tfr, index=0)
+        record = tfrecord2idx.get_tfrecord_by_index(tfr, index=0)
     except errors.EmptyTFRecordsError:
         log.debug(f"Unable to detect format for {tfr}; file empty.")
         return None, None
     img_type = imghdr.what('', record['image_raw'])
     return list(record.keys()), img_type
```

## slideflow/mil/__init__.py

```diff
@@ -1,6 +1,7 @@
 from .train import train_mil, train_clam, train_fastai, build_fastai_learner
 from .eval import eval_mil
 from .train._legacy import legacy_train_clam
 from ._params import (
-    mil_config, TrainerConfig, TrainerConfigFastAI, TrainerConfigCLAM
+    mil_config, _TrainerConfig, TrainerConfigFastAI, TrainerConfigCLAM,
+    ModelConfigCLAM, ModelConfigFastAI
 )
```

## slideflow/mil/_params.py

```diff
@@ -1,15 +1,37 @@
-"""Model and trainer configuration classes for MIL models."""
+"""Model and trainer configuration for MIL models."""
 
 from torch import nn
 from typing import Optional, Union, Callable
 from slideflow.mil.models import Attention_MIL
 
 
 def mil_config(model: Union[str, Callable], trainer: str = 'fastai', **kwargs):
+    """Create a multiple-instance learning (MIL) training configuration.
+
+    All models by default are trained with the FastAI trainer. However, CLAM
+    models can also be trained using the original, legacy CLAM trainer. This
+    deprecated trainer has been kept for backwards compatibility; the
+    FastAI trainer is preferred to all models, including CLAM.
+
+    Args:
+        model (str, Callable): Either the name of a model, or a custom torch
+            module. Valid model names include ``"clam_sb"``, ``"clam_mb"``,
+            ``"mil_fc"``, ``"mil_fc_mc"``, ``"attention_mil"``, and
+            ``"transmil"``.
+        trainer (str): Type of MIL trainer to use. Either 'fastai' or 'clam'.
+            All models (including CLAM) can be trained with 'fastai'.
+            The deprecated, legacy 'clam' trainer is only available for CLAM
+            models, and has been kept for backwards compatibility.
+            Defaults to 'fastai' (preferred).
+        **kwargs: All additional keyword arguments are passed to either
+            :class:`slideflow.mil.TrainerConfigCLAM` for CLAM models, or
+            :class:`slideflow.mil.TrainerConfigFastAI` for all other models.
+
+    """
     if model not in ModelConfigCLAM.valid_models and trainer == 'clam':
         raise ValueError(f"Model {model} incompatible with trainer {trainer}")
     if trainer == 'fastai':
         return TrainerConfigFastAI(model=model, **kwargs)
     elif trainer == 'clam':
         return TrainerConfigCLAM(model=model, **kwargs)
     else:
@@ -22,103 +44,173 @@
         pass
 
     def to_dict(self):
         return {k:v for k,v in vars(self).items()
                 if k not in ('self', 'model_fn', 'loss_fn') and not k.startswith('_')}
 
 
-class TrainerConfig(DictConfig):
+class _TrainerConfig(DictConfig):
 
     def __init__(self, *args, **kwargs):
+        """Multiple-instance learning (MIL) training configuration.
+
+        This configuration should not be created directly, but rather should
+        be created through :func:`slideflow.mil.mil_config`, which will create
+        and prepare an appropriate trainer configuration.
+
+        """
         self.model_config = None
         super().__init__(*args, **kwargs)
 
     def __str__(self):
         out = f"{self.__class__.__name__}("
         for p, val in self.to_dict().items():
             if p != 'model_config':
                 out += '\n  {}={!r}'.format(p, val)
         out += '\n)'
         return out
 
     @property
     def model_fn(self):
+        """MIL model architecture (class/module)."""
         return self.model_config.model_fn
 
     @property
     def loss_fn(self):
+        """MIL loss function."""
         return self.model_config.loss_fn
 
     def to_dict(self):
+        """Converts this training configuration to a dictionary."""
         d = super().to_dict()
         if self.model_config is None:
             return d
         else:
             d.update(self.model_config.to_dict())
             del d['model_config']
             return d
 
     def json_dump(self):
+        """Converts this training configuration to a JSON-compatible dict."""
         return dict(
             trainer=('fastai' if isinstance(self, TrainerConfigFastAI) else 'clam'),
             params=self.to_dict()
         )
 
 # -----------------------------------------------------------------------------
 
-class TrainerConfigFastAI(TrainerConfig):
+class TrainerConfigFastAI(_TrainerConfig):
     def __init__(
         self,
         model: Union[str, Callable] = 'attention_mil',
+        *,
         lr: Optional[float] = None,
-        lr_max: Optional[float] = None,
         wd: float = 1e-5,
         bag_size: int = 512,
         fit_one_cycle: bool = True,
         epochs: int = 32,
         batch_size: int = 64,
         **kwargs
     ):
+        r"""Training configuration for FastAI MIL models.
+
+        This configuration should not be created directly, but rather should
+        be created through :func:`slideflow.mil.mil_config`, which will create
+        and prepare an appropriate trainer configuration.
+
+        Args:
+            model (str, Callable): Either the name of a model, or a custom torch
+                module. Valid model names include ``"clam_sb"``, ``"clam_mb"``,
+                ``"mil_fc"``, ``"mil_fc_mc"``, ``"attention_mil"``, and
+                ``"transmil"``.
+
+        Keyword args:
+            lr (float, optional): Learning rate. If ``fit_one_cycle=True``,
+                this is the maximum learning rate. If None, uses the Leslie
+                Smith `LR Range test <https://arxiv.org/abs/1506.01186>`_ to
+                find an optimal learning rate. Defaults to None.
+            wd (float): Weight decay. Only used if ``fit_one_cycle=False``.
+                Defaults to 1e-5.
+            bag_size (int): Bag size. Defaults to 512.
+            fit_one_cycle (bool): Use `1cycle <https://sgugger.github.io/the-1cycle-policy.html>`_
+                learning rate schedule. Defaults to True.
+            epochs (int): Maximum number of epochs. Defaults to 32.
+            batch_size (int): Batch size. Defaults to 64.
+            **kwargs: All additional keyword arguments are passed to either
+                :class:`slideflow.mil.ModelConfigCLAM` for CLAM models, or
+                :class:`slideflow.mil.ModelConfigFastAI` for all other models.
+
+        """
         self.lr = lr
-        self.lr_max = lr_max
         self.wd = wd
         self.bag_size = bag_size
         self.fit_one_cycle = fit_one_cycle
         self.epochs = epochs
         self.batch_size = batch_size
         if model in ModelConfigCLAM.valid_models:
             self.model_config = ModelConfigCLAM(model=model, **kwargs)
         else:
             self.model_config = ModelConfigFastAI(model=model, **kwargs)
 
 
-class TrainerConfigCLAM(TrainerConfig):
+class TrainerConfigCLAM(_TrainerConfig):
     def __init__(
         self,
-        num_splits=1,
-        k=3,
-        k_start=-1,
-        k_end=-1,
-        max_epochs=20,
-        lr=1e-4,
-        reg=1e-5,
-        label_frac=1,
-        weighted_sample=False,
-        log_data=False,
-        testing=False,
-        early_stopping=False,
-        subtyping=False,
-        seed=1,
-        results_dir=None,
-        n_classes=None,
+        *,
+        num_splits: int = 1,   # Unused; kept for backwards compatibility
+        k: int = 3,
+        k_start: int = -1,
+        k_end: int = -1,
+        max_epochs: int = 20,
+        lr: float = 1e-4,
+        reg: float = 1e-5,
+        label_frac: float = 1,
+        weighted_sample: bool = False,
+        log_data: bool = False,
+        testing: bool = False,
+        early_stopping: bool = False,
+        subtyping: bool = False,
+        seed: int = 1,
+        results_dir: Optional[str] = None,  # Unused; kept for compatibility
+        n_classes: Optional[int] = None,
         split_dir=None,
         data_root_dir=None,
         micro_average=False,
         **kwargs
     ):
+        """Training configuration for the legacy CLAM trainer.
+
+        This configures the legacy CLAM trainer. The FastAI trainer is
+        preferred for all models, including CLAM.
+
+        The configuration options for the legacy CLAM trainer are identical to
+        the options in the `original CLAM paper <https://arxiv.org/abs/2004.09666>`_.
+
+        Keyword args:
+            k (int): Number of cross-fold splits. Defaults to 3.
+            k_start (int): Starting cross-fold. Defaults to first cross-fold.
+            k_end (int): Ending cross-fold. Defaults to ending after last
+                cross-fold is done.
+            max_epochs (int): Number of epochs to train. Defaults to 20.
+            lr (float): Learning rate. Defaults to 1e-4.
+            reg (float): Weight decay. Defaults to 1e-5.
+            weighted_sample (bool): Equally sample from all outcome classes.
+                Defaults to False.
+            log_data (bool): Log to tensorboard. Defaults to False.
+            early_stopping (bool): Stop the training if validation loss doesn't
+                improve after 5 epochs. Will not trigger early stopping
+                until epoch 50. Defaults to False.
+            subtyping (bool): Whether this is a subtyping problem.
+                Defaults to False.
+            seed (int): Set the random seed. Defaults to 1.
+            n_classes (int): Number of outcome classes. Defaults to None.
+            micro_average (bool): Use micro averaging when calculate AUROC.
+            **kwargs: All additional keyword arguments are passed to
+                :class:`slideflow.mil.ModelConfigCLAM`.
+        """
         for argname, argval in dict(locals()).items():
             if argname != 'kwargs':
                 setattr(self, argname, argval)
         self.model_config = ModelConfigCLAM(**kwargs)
 
     def _to_clam_args(self):
         """Convert into CLAM_Args format (legacy support)."""
@@ -135,24 +227,102 @@
 
 class ModelConfigCLAM(DictConfig):
 
     valid_models = ['clam_sb', 'clam_mb', 'mil_fc_mc', 'mil_fc']
 
     def __init__(
         self,
-        bag_loss='ce',
-        bag_weight=0.7,
-        model='clam_sb',
-        model_size='small',
-        dropout=False,
-        opt='adam',
-        inst_loss=None,
-        no_inst_cluster=False,
-        B=8,
+        *,
+        model: str = 'clam_sb',
+        model_size: str = 'small',
+        bag_loss: str = 'ce',
+        bag_weight: float = 0.7,
+        dropout: bool = False,
+        opt: str = 'adam',
+        inst_loss: str = 'ce',
+        no_inst_cluster: bool = False,
+        B: int = 8,
     ):
+        """Model configuration for CLAM models.
+
+        These configuration options are identical to the options in the
+        `original CLAM paper <https://arxiv.org/abs/2004.09666>`_.
+
+        Keyword args:
+            model (str): Model. Either ``'clam_sb'``, ``'clam_mb'``,
+                ``'mil_fc'``, or ``'mil_fc_mc'``. Defaults to ``'clam_sb'``.
+            model_size (str): Size of the model. Available sizes include:
+
+                ``clam_sb``
+
+                .. list-table::
+                    :header-rows: 0
+
+                    * - small
+                      - [1024, 512, 256]
+                    * - big
+                      - [1024, 512, 384]
+                    * - multiscale
+                      - [2048, 512, 256]
+                    * - xception
+                      - [2048, 256, 128]
+                    * - xception_multi
+                      - [1880, 128, 64]
+                    * - xception_3800
+                      - [3800, 512, 256]
+
+                ``clam_mb``
+
+                .. list-table::
+                    :header-rows: 0
+
+                    * - small
+                      - [1024, 512, 256]
+                    * - big
+                      - [1024, 512, 384]
+                    * - multiscale
+                      - [2048, 512, 256]
+
+                ``mil_fc``
+
+                .. list-table::
+                    :header-rows: 0
+
+                    * - small
+                      - [1024, 512]
+
+                ``mil_fc_mc``
+
+                .. list-table::
+                    :header-rows: 0
+
+                    * - small
+                      - [1024, 512]
+
+            bag_loss (str): Primary loss function. Either 'ce' or 'svm'.
+                If 'ce', the model loss function is a cross entropy loss.
+                If 'svm', the model loss is topk.SmoothTop1SVM.
+                Defaults to 'ce'.
+            bag_weight (float): Weight of the bag loss. The total loss is
+                defined0 as ``W * loss + (1 - W) * instance_loss``, where
+                ``W`` is the bag weight. Defaults to 0.7
+            dropout (bool): Add dropout (p=0.25) after the attention layers.
+                Defaults to False.
+            opt (str): Optimizer. Either 'adam' (Adam optimizer) or 'sgd'
+                (Stochastic Gradient Descent). Defaults to 'adam'.
+            inst_loss (str): Instance loss function. Either 'ce' or 'svm'.
+                If 'ce', the instance loss is a cross entropy loss.
+                If 'svm', the loss is topk.SmoothTop1SVM.
+                Defaults to 'ce'.
+            no_inst_cluster (bool): Disable instance-level clustering.
+                Defaults to False.
+            B (int): Number of positive/negative patches to sample for
+                instance-level training. Defaults to 8.
+
+        """
         for argname, argval in dict(locals()).items():
             setattr(self, argname, argval)
 
     @property
     def model_fn(self):
         from .models import CLAM_MB, CLAM_SB, MIL_fc_mc, MIL_fc
         model_dict = {
@@ -175,16 +345,32 @@
 class ModelConfigFastAI(DictConfig):
 
     valid_models = ['attention_mil', 'transmil']
 
     def __init__(
         self,
         model: Union[str, Callable] = 'attention_mil',
+        *,
         use_lens: Optional[bool] = None
     ) -> None:
+        """Model configuration for a non-CLAM MIL model.
+
+        Args:
+            model (str, Callable): Either the name of a model, or a custom torch
+                module. Valid model names include ``"attention_mil"`` and
+                ``"transmil"``. Defaults to 'attention_mil'.
+
+        Keyword args:
+            use_lens (bool, optional): Whether the model expects a second
+                argument to its ``.forward()`` function, an array with the
+                bag size for each slide. If None, will default to True for
+                ``'attention_mil'`` models and False otherwise.
+                Defaults to None.
+
+        """
         self.model = model
         if use_lens is None and (model == 'attention_mil' or model is Attention_MIL):
             self.use_lens = True
         elif use_lens is None:
             self.use_lens = False
         else:
             self.use_lens = use_lens
```

## slideflow/mil/eval.py

```diff
@@ -7,25 +7,25 @@
 from rich.progress import Progress
 from os.path import join, exists, isdir, dirname
 from typing import Union, List, Optional, Callable, Tuple, Any
 from slideflow import Dataset, log, errors
 from slideflow.util import path_to_name
 from slideflow.stats.metrics import ClassifierMetrics
 from ._params import (
-    TrainerConfig, ModelConfigCLAM, TrainerConfigCLAM
+    _TrainerConfig, ModelConfigCLAM, TrainerConfigCLAM
 )
 
 # -----------------------------------------------------------------------------
 
 def eval_mil(
     weights: str,
     dataset: Dataset,
     outcomes: Union[str, List[str]],
     bags: Union[str, List[str]],
-    config: Optional[TrainerConfig] = None,
+    config: Optional[_TrainerConfig] = None,
     *,
     outdir: str = 'mil',
     attention_heatmaps: bool = False,
     **heatmap_kwargs
 ) -> pd.DataFrame:
     """Evaluate a multi-instance learning model.
 
@@ -38,23 +38,32 @@
     Args:
         weights (str): Path to model weights to load.
         dataset (sf.Dataset): Dataset to evaluation.
         outcomes (str, list(str)): Outcomes.
         bags (str, list(str)): Path to bags, or list of bag file paths.
             Each bag should contain PyTorch array of features from all tiles in
             a slide, with the shape ``(n_tiles, n_features)``.
-        config (TrainerConfig): Configuration for building model.
-            If ``weights`` is a path to a model directory, will attempt to
-            read ``mil_params.json`` from this location and auto-load
-            saved configuration. Defaults to None.
+        config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+            Configuration for building model. If ``weights`` is a path to a
+            model directory, will attempt to read ``mil_params.json`` from this
+            location and load saved configuration. Defaults to None.
 
     Keyword arguments:
         outdir (str): Path at which to save results.
         attention_heatmaps (bool): Generate attention heatmaps for slides.
             Defaults to False.
+        interpolation (str, optional): Interpolation strategy for smoothing
+            attention heatmaps. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
 
     """
     import torch
 
     if isinstance(config, TrainerConfigCLAM):
         raise NotImplementedError
 
@@ -79,15 +88,15 @@
         bags = dataset.pt_files(bags)
     else:
         bags = np.array([b for b in bags if path_to_name(b) in slides])
 
     # Handle the case where some bags are missing.
     if len(bags) != len(slides):
         slides = [path_to_name(b) for b in bags]
-    
+
     y_true = np.array([labels[s] for s in slides])
 
     # Detect feature size from bags
     n_features = torch.load(bags[0]).shape[-1]
     n_out = len(unique)
 
     log.info(f"Building model {config.model_fn.__name__}")
@@ -164,26 +173,27 @@
     return df
 
 # -----------------------------------------------------------------------------
 
 
 def predict_from_model(
     model: Callable,
-    config: TrainerConfig,
+    config: _TrainerConfig,
     dataset: "sf.Dataset",
     outcomes: Union[str, List[str]],
     bags: Union[str, np.ndarray, List[str]],
     *,
     attention: bool = False
 ) -> Union[pd.DataFrame, Tuple[pd.DataFrame, List[np.ndarray]]]:
     """Generate predictions from a model.
 
     Args:
         model (torch.nn.Module): Model from which to generate predictions.
-        config (TrainerConfig): Configuration for the MIL model.
+        config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+            Configuration for the MIL model.
         dataset (sf.Dataset): Dataset from which to generation predictions.
         outcomes (str, list(str)): Outcomes.
         bags (str, list(str)): Path to bags, or list of bag file paths.
             Each bag should contain PyTorch array of features from all tiles in
             a slide, with the shape ``(n_tiles, n_features)``.
 
     Returns:
@@ -243,37 +253,54 @@
         dataset (sf.Dataset): Dataset.
         bags (str, list(str)): List of bag file paths.
             Each bag should contain PyTorch array of features from all tiles in
             a slide, with the shape ``(n_tiles, n_features)``.
         attention (list(np.ndarray)): Attention scores for each slide.
             Length of ``attention`` should equal the length of ``bags``.
 
+    Keyword args:
+        interpolation (str, optional): Interpolation strategy for smoothing
+            heatmap. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
+
+
     """
     assert len(bags) == len(attention)
     if not exists(outdir):
         os.makedirs(outdir)
     pb = Progress(transient=True)
     task = pb.add_task('Generating heatmaps...', total=len(bags))
     pb.start()
     with sf.util.cleanup_progress(pb):
         for i, bag in enumerate(bags):
             pb.advance(task)
             slidename = sf.util.path_to_name(bag)
             slide_path = dataset.find_slide(slide=slidename)
             locations_file = join(dirname(bag), f'{slidename}.index.npz')
+            npy_loc_file = locations_file[:-1] + 'y'
             if slide_path is None:
                 log.info(f"Unable to find slide {slidename}")
                 continue
-            if not exists(locations_file):
+            if exists(locations_file):
+                locations = np.load(locations_file)['arr_0']
+            elif exists(npy_loc_file):
+                locations = np.load(npy_loc_file)
+            else:
                 log.info(
                     f"Unable to find locations index file for {slidename}"
                 )
                 continue
             sf.util.location_heatmap(
-                locations=np.load(locations_file)['arr_0'],
+                locations=locations,
                 values=attention[i],
                 slide=slide_path,
                 tile_px=dataset.tile_px,
                 tile_um=dataset.tile_um,
                 outdir=outdir,
                 **kwargs
             )
```

## slideflow/mil/clam/__init__.py

```diff
@@ -12,15 +12,15 @@
 from .utils.file_utils import save_pkl
 
 # -----------------------------------------------------------------------------
 
 class CLAM_Args:
     def __init__(
         self,
-        num_splits=1,
+        num_splits=1,  # Unused; kept for backwards compatibility
         k=3,
         k_start=-1,
         k_end=-1,
         max_epochs=20,
         lr=1e-4,
         reg=1e-5,
         label_frac=1,
```

## slideflow/mil/clam/datasets/dataset_generic.py

```diff
@@ -1,11 +1,10 @@
 """Modification of https://github.com/mahmoodlab/CLAM"""
 
 import os
-import h5py
 import numpy as np
 import pandas as pd
 import torch
 
 from typing import Union
 from scipy import stats
 from torch.utils.data import Dataset
@@ -341,14 +340,15 @@
         self.data_dir = data_dir
         self.use_h5 = False
 
     def load_from_h5(self, toggle):
         self.use_h5 = toggle
 
     def __getitem__(self, idx):
+        import h5py
         slide_id = self.slide_data['slide'][idx]
         label = self.slide_data['label'][idx]
         if type(self.data_dir) == dict:
             source = self.slide_data['source'][idx]
             data_dir = self.data_dir[source]
         else:
             data_dir = self.data_dir
```

## slideflow/mil/clam/utils/__init__.py

```diff
@@ -35,15 +35,15 @@
 def collate_features(batch):
     img = torch.cat([item[0] for item in batch], dim = 0)
     coords = np.vstack([item[1] for item in batch])
     return [img, coords]
 
 
 def get_simple_loader(dataset, batch_size=1, num_workers=1):
-    kwargs = {'num_workers': 4, 'pin_memory': False, 'num_workers': num_workers} if device.type == "cuda" else {}
+    kwargs = {'num_workers': 4, 'pin_memory': False, 'num_workers': num_workers} if torch.cuda.is_available() else {}
     loader = DataLoader(dataset, batch_size=batch_size, sampler = sampler.SequentialSampler(dataset), collate_fn = collate_MIL, **kwargs)
     return loader
 
 def get_split_loader(split_dataset, training = False, testing = False, weighted = False):
     """
         return either the validation loader or training loader
     """
```

## slideflow/mil/clam/utils/core_utils.py

```diff
@@ -54,15 +54,15 @@
 
 class EarlyStopping:
     """Early stops the training if validation loss doesn't improve after a given patience."""
     def __init__(self, patience=5, stop_epoch=50, verbose=False):
         """
         Args:
             patience (int): How long to wait after last time validation loss improved.
-                            Default: 20
+                            Default: 5
             stop_epoch (int): Earliest epoch possible for stopping
             verbose (bool): If True, prints a message for each validation loss improvement.
                             Default: False
         """
         self.patience = patience
         self.stop_epoch = stop_epoch
         self.verbose = verbose
```

## slideflow/mil/clam/utils/file_utils.py

```diff
@@ -1,11 +1,10 @@
 """Modification of https://github.com/mahmoodlab/CLAM"""
 
 import pickle
-import h5py
 
 
 def save_pkl(filename, save_object):
 	writer = open(filename,'wb')
 	pickle.dump(save_object, writer)
 	writer.close()
 
@@ -14,14 +13,15 @@
 	loader = open(filename,'rb')
 	file = pickle.load(loader)
 	loader.close()
 	return file
 
 
 def save_hdf5(output_path, asset_dict, attr_dict= None, mode='a'):
+    import h5py
     file = h5py.File(output_path, mode)
     for key, val in asset_dict.items():
         data_shape = val.shape
         if key not in file:
             data_type = val.dtype
             chunk_shape = (1, ) + data_shape[1:]
             maxshape = (None, ) + data_shape[1:]
```

## slideflow/mil/train/__init__.py

```diff
@@ -7,37 +7,38 @@
 from typing import Union, List, Optional, TYPE_CHECKING
 from slideflow import Dataset, log
 from slideflow.util import path_to_name
 from os.path import join
 
 from ..eval import predict_from_model, generate_attention_heatmaps
 from .._params import (
-    TrainerConfig, TrainerConfigCLAM, TrainerConfigFastAI
+    _TrainerConfig, TrainerConfigCLAM, TrainerConfigFastAI
 )
 
 if TYPE_CHECKING:
     from fastai.learner import Learner
 
 # -----------------------------------------------------------------------------
 
 def train_mil(
-    config: TrainerConfig,
+    config: _TrainerConfig,
     train_dataset: Dataset,
     val_dataset: Optional[Dataset],
     outcomes: Union[str, List[str]],
     bags: Union[str, List[str]],
     *,
     outdir: str = 'mil',
     exp_label: Optional[str] = None,
     **kwargs
 ):
-    """Train a multi-instance learning model.
+    """Train a multiple-instance learning (MIL) model.
 
     Args:
-        config (``TrainerConfig``): Trainer and model configuration.
+        config (:class:`slideflow.mil.TrainerConfigFastAI` or :class:`slideflow.mil.TrainerConfigCLAM`):
+            Trainer and model configuration.
         train_dataset (:class:`slideflow.Dataset`): Training dataset.
         val_dataset (:class:`slideflow.Dataset`): Validation dataset.
         outcomes (str): Outcome column (annotation header) from which to
             derive category labels.
         bags (str): Either a path to directory with \*.pt files, or a list
             of paths to individual \*.pt files. Each file should contain
             exported feature vectors, with each file containing all tile
@@ -46,14 +47,24 @@
     Keyword args:
         outdir (str): Directory in which to save model and results.
         exp_label (str): Experiment label, used for naming the subdirectory
             in the ``{project root}/mil`` folder, where training history
             and the model will be saved.
         attention_heatmaps (bool): Generate attention heatmaps for slides.
             Defaults to False.
+        interpolation (str, optional): Interpolation strategy for smoothing
+            attention heatmaps. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
+
     """
     log.info("Training FastAI MIL model with config:")
     log.info(f"{config}")
     if isinstance(config, TrainerConfigFastAI):
         train_fn = train_fastai
     elif isinstance(config, TrainerConfigCLAM):
         train_fn = train_clam
@@ -94,15 +105,16 @@
     config: TrainerConfigCLAM,
     train_dataset: Dataset,
     val_dataset: Dataset,
     outcomes: Union[str, List[str]],
     bags: Union[str, List[str]],
     *,
     outdir: str = 'mil',
-    attention_heatmaps: bool = False
+    attention_heatmaps: bool = False,
+    **heatmap_kwargs
 ) -> None:
     """Train a CLAM model from layer activations exported with
     :meth:`slideflow.project.generate_features_for_clam`.
 
     See :ref:`clam_mil` for more information.
 
     Args:
@@ -114,20 +126,29 @@
             of paths to individual \*.pt files. Each file should contain
             exported feature vectors, with each file containing all tile
             features for one patient.
 
     Keyword args:
         outdir (str): Directory in which to save model and results.
         exp_label (str): Experiment label, used for naming the subdirectory
-            in the ``{project root}/mil`` folder, where training history
+            in the ``outdir`` folder, where training history
             and the model will be saved.
         clam_args (optional): Namespace with clam arguments, as provided
             by :func:`slideflow.clam.get_args`.
         attention_heatmaps (bool): Generate attention heatmaps for slides.
             Defaults to False.
+        interpolation (str, optional): Interpolation strategy for smoothing
+            attention heatmaps. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
 
     Returns:
         None
 
     """
     import slideflow.clam as clam
     from slideflow.clam import CLAM_Dataset
@@ -216,15 +237,16 @@
 
     if attention and attention_heatmaps:
         assert len(val_bags) == len(attention)
         generate_attention_heatmaps(
             outdir=join(outdir, 'heatmaps'),
             dataset=val_dataset,
             bags=val_bags,
-            attention=attention
+            attention=attention,
+            **heatmap_kwargs
         )
 
 # -----------------------------------------------------------------------------
 
 def build_fastai_learner(
     config: TrainerConfigFastAI,
     train_dataset: Dataset,
@@ -245,17 +267,18 @@
             of paths to individual \*.pt files. Each file should contain
             exported feature vectors, with each file containing all tile
             features for one patient.
 
     Keyword args:
         outdir (str): Directory in which to save model and results.
         exp_label (str): Experiment label, used for naming the subdirectory
-            in the ``{project root}/mil`` folder, where training history
+            in the ``outdir`` folder, where training history
             and the model will be saved.
-        lr_max (float): Maximum learning rate.
+        lr (float): Learning rate, or maximum learning rate if
+            ``fit_one_cycle=True``.
         epochs (int): Maximum epochs.
 
     Returns:
         fastai.learner.Learner
     """
     from . import _fastai
 
@@ -278,14 +301,17 @@
     train_slides = train_dataset.slides()
     train_idx = np.array([i for i, bag in enumerate(bags)
                             if path_to_name(bag) in train_slides])
     val_slides = val_dataset.slides()
     val_idx = np.array([i for i, bag in enumerate(bags)
                             if path_to_name(bag) in val_slides])
 
+    log.info("Training dataset: {} bags (from {} slides)".format(len(train_idx), len(train_slides)))
+    log.info("Validation dataset: {} bags (from {} slides)".format(len(val_idx), len(val_slides)))
+
     # Build FastAI Learner
     learner = _fastai.build_learner(
         config,
         bags=bags,
         targets=targets,
         train_idx=train_idx,
         val_idx=val_idx,
@@ -300,14 +326,15 @@
     train_dataset: Dataset,
     val_dataset: Dataset,
     outcomes: Union[str, List[str]],
     bags: Union[str, List[str]],
     *,
     outdir: str = 'mil',
     attention_heatmaps: bool = False,
+    **heatmap_kwargs
 ) -> None:
     """Train an aMIL model using FastAI.
 
     Args:
         train_dataset (:class:`slideflow.Dataset`): Training dataset.
         val_dataset (:class:`slideflow.Dataset`): Validation dataset.
         outcomes (str): Outcome column (annotation header) from which to
@@ -318,18 +345,28 @@
             features for one patient.
 
     Keyword args:
         outdir (str): Directory in which to save model and results.
         exp_label (str): Experiment label, used for naming the subdirectory
             in the ``{project root}/mil`` folder, where training history
             and the model will be saved.
-        lr_max (float): Maximum learning rate.
+        lr (float): Learning rate, or maximum learning rate if
+            ``fit_one_cycle=True``.
         epochs (int): Maximum epochs.
         attention_heatmaps (bool): Generate attention heatmaps for slides.
             Defaults to False.
+        interpolation (str, optional): Interpolation strategy for smoothing
+            attention heatmaps. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
 
     Returns:
         fastai.learner.Learner
     """
     from . import _fastai
 
     # Prepare bags.
@@ -369,11 +406,12 @@
 
     # Attention heatmaps.
     if attention and attention_heatmaps:
         generate_attention_heatmaps(
             outdir=join(outdir, 'heatmaps'),
             dataset=val_dataset,
             bags=val_bags,
-            attention=attention
+            attention=attention,
+            **heatmap_kwargs
         )
 
     return learner
```

## slideflow/mil/train/_fastai.py

```diff
@@ -28,20 +28,20 @@
     cbs = [
         SaveModelCallback(fname=f"best_valid"),
         CSVLogger(),
     ]
     if callbacks:
         cbs += callbacks
     if config.fit_one_cycle:
-        if config.lr_max is None:
-            lr_max = learner.lr_find().valley
-            log.info(f"Using auto-detected learning rate: {lr_max}")
+        if config.lr is None:
+            lr = learner.lr_find().valley
+            log.info(f"Using auto-detected learning rate: {lr}")
         else:
-            lr_max = config.lr_max
-        learner.fit_one_cycle(n_epoch=config.epochs, lr_max=lr_max, cbs=cbs)
+            lr = config.lr
+        learner.fit_one_cycle(n_epoch=config.epochs, lr_max=lr, cbs=cbs)
     else:
         if config.lr is None:
             lr = learner.lr_find().valley
             log.info(f"Using auto-detected learning rate: {lr}")
         else:
             lr = config.lr
         learner.fit(n_epoch=config.epochs, lr=lr, wd=config.wd, cbs=cbs)
@@ -197,15 +197,15 @@
         use_lens=config.model_config.use_lens
     )
     train_dl = DataLoader(
         train_dataset,
         batch_size=config.batch_size,
         shuffle=True,
         num_workers=1,
-        drop_last=True,
+        drop_last=False,
         device=device
     )
     val_dataset = build_dataset(
         bags[val_idx],
         targets[val_idx],
         encoder=encoder,
         bag_size=None,
```

## slideflow/model/__init__.py

```diff
@@ -85,19 +85,19 @@
 
 def build_trainer(
     hp: "ModelParams",
     outdir: str,
     labels: Dict[str, Any],
     **kwargs
 ) -> Trainer:
-    """From the given :class:`slideflow.model.ModelParams` object, returns
+    """From the given :class:`slideflow.ModelParams` object, returns
     the appropriate instance of :class:`slideflow.model.Trainer`.
 
     Args:
-        hp (:class:`slideflow.model.ModelParams`): ModelParams object.
+        hp (:class:`slideflow.ModelParams`): ModelParams object.
         outdir (str): Path for event logs and checkpoints.
         labels (dict): Dict mapping slide names to outcome labels (int or
             float format).
 
     Keyword Args:
         slide_input (dict): Dict mapping slide names to additional
             slide-level input, concatenated after post-conv.
```

## slideflow/model/base.py

```diff
@@ -65,59 +65,80 @@
         backend (Tensorflow or PyTorch), which can be viewed with
         :func:`slideflow.backend`. While most model parameters are
         cross-compatible between Tensorflow and PyTorch, some parameters are
         unique to a backend, so this object should be configured in the same
         backend that the model will be trained in.
 
         Args:
-            tile_px (int, optional): Tile width in pixels. Defaults to 299.
-            tile_um (int or str, optional): Tile width in microns (int) or
+            tile_px (int): Tile width in pixels. Defaults to 299.
+            tile_um (int or str): Tile width in microns (int) or
                 magnification (str, e.g. "20x"). Defaults to 302.
-            epochs (int, optional): Number of epochs to train the full model. Defaults to 3.
-            toplayer_epochs (int, optional): Number of epochs to only train the fully-connected layers. Defaults to 0.
-            model (str, optional): Base model architecture name. Defaults to 'xception'.
-            pooling (str, optional): Post-convolution pooling. 'max', 'avg', or 'none'. Defaults to 'max'.
-            loss (str, optional): Loss function. Defaults to 'sparse_categorical_crossentropy'.
-            learning_rate (float, optional): Learning rate. Defaults to 0.0001.
-            learning_rate_decay (int, optional): Learning rate decay rate. Defaults to 0.
-            learning_rate_decay_steps (int, optional): Learning rate decay steps. Defaults to 100000.
-            batch_size (int, optional): Batch size. Defaults to 16.
-            hidden_layers (int, optional): Number of fully-connected hidden layers after core model. Defaults to 0.
-            hidden_layer_width (int, optional): Width of fully-connected hidden layers. Defaults to 500.
-            optimizer (str, optional): Name of optimizer. Defaults to 'Adam'.
-            early_stop (bool, optional): Use early stopping. Defaults to False.
-            early_stop_patience (int, optional): Patience for early stopping, in epochs. Defaults to 0.
-            early_stop_method (str, optional): Metric to monitor for early stopping. Defaults to 'loss'.
+            epochs (int): Number of epochs to train the full model. Defaults to 3.
+            toplayer_epochs (int): Number of epochs to only train the fully-connected layers. Defaults to 0.
+            model (str): Base model architecture name. Defaults to 'xception'.
+            pooling (str): Post-convolution pooling. 'max', 'avg', or 'none'. Defaults to 'max'.
+            loss (str): Loss function. Defaults to 'sparse_categorical_crossentropy'.
+            learning_rate (float): Learning rate. Defaults to 0.0001.
+            learning_rate_decay (int): Learning rate decay rate. Defaults to 0.
+            learning_rate_decay_steps (int): Learning rate decay steps. Defaults to 100000.
+            batch_size (int): Batch size. Defaults to 16.
+            hidden_layers (int): Number of fully-connected hidden layers after core model. Defaults to 0.
+            hidden_layer_width (int): Width of fully-connected hidden layers. Defaults to 500.
+            optimizer (str): Name of optimizer. Defaults to 'Adam'.
+            early_stop (bool): Use early stopping. Defaults to False.
+            early_stop_patience (int): Patience for early stopping, in epochs. Defaults to 0.
+            early_stop_method (str): Metric to monitor for early stopping. Defaults to 'loss'.
             manual_early_stop_epoch (int, optional): Manually override early stopping to occur at this epoch/batch.
                 Defaults to None.
             manual_early_stop_batch (int, optional): Manually override early stopping to occur at this epoch/batch.
                 Defaults to None.
-            training_balance ([type], optional): Type of batch-level balancing to use during training.
+            training_balance (str, optional): Type of batch-level balancing to use during training.
                 Options include 'tile', 'category', 'patient', 'slide', and None. Defaults to 'category' if a
                 categorical loss is provided, and 'patient' if a linear loss is provided.
-            validation_balance ([type], optional): Type of batch-level balancing to use during validation.
+            validation_balance (str, optional): Type of batch-level balancing to use during validation.
                 Options include 'tile', 'category', 'patient', 'slide', and None. Defaults to 'none'.
-            trainable_layers (int, optional): Number of layers which are traininable. If 0, trains all layers.
+            trainable_layers (int): Number of layers which are traininable. If 0, trains all layers.
                 Defaults to 0.
             l1 (int, optional): L1 regularization weight. Defaults to 0.
             l2 (int, optional): L2 regularization weight. Defaults to 0.
             l1_dense (int, optional): L1 regularization weight for Dense layers. Defaults to the value of l1.
             l2_dense (int, optional): L2 regularization weight for Dense layers. Defaults to the value of l2.
             dropout (int, optional): Post-convolution dropout rate. Defaults to 0.
             uq (bool, optional): Use uncertainty quantification with dropout. Requires dropout > 0. Defaults to False.
-            augment (str): Image augmentations to perform. String containing characters designating augmentations.
-                'x' indicates random x-flipping, 'y' y-flipping, 'r' rotating, and 'j' JPEG compression/decompression
-                at random quality levels. Passing either 'xyrj' or True will use all augmentations.
+            augment (str, optional): Image augmentations to perform. Characters in the string designate augmentations.
+                Combine these characters to define the augmentation pipeline. For example, 'xyrj' will perform x-flip,
+                y-flip, rotation, and JPEG compression. True will use all augmentations. Defaults to 'xyrj'.
+
+                .. list-table::
+                    :header-rows: 1
+                    :widths: 10 90
+
+                    * - Character
+                      - Augmentation
+                    * - x
+                      - Random x-flipping
+                    * - y
+                      - Random y-flipping
+                    * - r
+                      - Random cardinal rotation
+                    * - j
+                      - Random JPEG compression (10% chance to JPEG compress with quality between 50-100%)
+                    * - b
+                      - Random Guassian blur (50% chance to blur with sigma between 0.5 - 2.0)
+                    * - s
+                      - Stain augmentation (must be using stain normalization)
+
+
             normalizer (str, optional): Normalization strategy to use on image tiles. Defaults to None.
             normalizer_source (str, optional): Path to normalizer source image. Defaults to None.
                 If None but using a normalizer, will use an internal tile for normalization.
                 Internal default tile can be found at slideflow.slide.norm_tile.jpg
-            include_top (bool, optional): Include post-convolution fully-connected layers from the core model. Defaults
+            include_top (bool): Include post-convolution fully-connected layers from the core model. Defaults
                 to True. include_top=False is not currently compatible with the PyTorch backend.
-            drop_images (bool, optional): Drop images, using only other slide-level features as input.
+            drop_images (bool): Drop images, using only other slide-level features as input.
                 Defaults to False.
         """
         if isinstance(tile_um, str):
             sf.util.assert_is_mag(tile_um)
             tile_um = tile_um.lower()
 
         self.tile_px = tile_px
```

## slideflow/model/features.py

```diff
@@ -1002,19 +1002,15 @@
                              description=f"Feature {f}"):
                 for tfr in self.tfrecords:
                     if sf.util.path_to_name(tfr) == g['slide']:
                         tfr_dir = tfr
                 if not tfr_dir:
                     log.warning("TFRecord location not found for "
                                 f"slide {g['slide']}")
-                slide, image = sf.io.get_tfrecord_by_index(
-                    tfr_dir,
-                    g['index'],
-                    decode=False
-                )
+                slide, image = sf.io.get_tfrecord_by_index(tfr_dir, g['index'])
                 tile_filename = (f"{i}-tfrecord{g['slide']}-{g['index']}"
                                  + f"-{g['val']:.2f}.jpg")
                 image_string = open(join(outdir, str(f), tile_filename), 'wb')
                 image_string.write(image.numpy())
                 image_string.close()
 
     # --- Deprecated functions ----------------------------------------------------
@@ -1385,15 +1381,15 @@
                 features, preds, unc, slides, loc = self._process_out(
                     model_out, batch_slides, batch_loc
                 )
 
                 for d, slide in enumerate(slides):
                     if self.layers:
                         activations[slide].append(features[d])
-                    if self.include_preds and predictions is not None:
+                    if self.include_preds and preds is not None:
                         predictions[slide].append(preds[d])
                     if self.uq and self.include_uncertainty:
                         uncertainty[slide].append(unc[d])
                     if loc is not None:
                         locations[slide].append(loc[d])
 
         batch_proc_thread = threading.Thread(target=batch_worker, daemon=True)
```

## slideflow/model/tensorflow.py

```diff
@@ -1079,15 +1079,15 @@
         load_method: str = 'weights',
         custom_objects: Optional[Dict[str, Any]] = None,
     ) -> None:
 
         """Sets base configuration, preparing model inputs and outputs.
 
         Args:
-            hp (:class:`slideflow.model.ModelParams`): ModelParams object.
+            hp (:class:`slideflow.ModelParams`): ModelParams object.
             outdir (str): Path for event logs and checkpoints.
             labels (dict): Dict mapping slide names to outcome labels (int or
                 float format).
             slide_input (dict): Dict mapping slide names to additional
                 slide-level input, concatenated after post-conv.
             name (str, optional): Optional name describing the model, used for
                 model saving. Defaults to 'Trainer'.
@@ -2177,15 +2177,15 @@
         self.layers = layers
         self.path = path
         if path is not None:
             self._model = load(self.path, method=load_method)  # type: ignore
             config = sf.util.get_model_config(path)
             if 'img_format' in config:
                 self.img_format = config['img_format']
-            self.hp = sf.model.ModelParams()
+            self.hp = sf.ModelParams()
             self.hp.load_dict(config['hp'])
             self.wsi_normalizer = self.hp.get_normalizer()
             if 'norm_fit' in config and config['norm_fit'] is not None:
                 if self.wsi_normalizer is None:
                     log.warn('norm_fit found in model config file, but model '
                              'params does not use a normalizer. Ignoring.')
                 else:
```

## slideflow/model/torch.py

```diff
@@ -221,15 +221,15 @@
         'mobilenet_v3_large': torchvision.models.mobilenet_v3_large,
         'wide_resnet50_2': torchvision.models.wide_resnet50_2,
         'mnasnet': torchvision.models.mnasnet1_0,
         'xception': torch_utils.xception,
         'nasnet_large': torch_utils.nasnetalarge
     }
 
-    def __init__(self, loss: str = 'CrossEntropy', **kwargs) -> None:
+    def __init__(self, *, loss: str = 'CrossEntropy', **kwargs) -> None:
         self.OptDict = {
             'Adadelta': torch.optim.Adadelta,
             'Adagrad': torch.optim.Adagrad,
             'Adam': torch.optim.Adam,
             'AdamW': torch.optim.AdamW,
             'SparseAdam': torch.optim.SparseAdam,
             'Adamax': torch.optim.Adamax,
@@ -353,15 +353,17 @@
                 if param.kind == param.POSITIONAL_OR_KEYWORD
             ]
             call_kw = {}
             if 'image_size' in model_kw:
                 call_kw.update(dict(image_size=self.tile_px))
             if (version.parse(torchvision.__version__) >= version.parse("0.13")
                and not self.model.startswith('timm_')):
-                call_kw.update(dict(weights=pretrain))  # type: ignore
+                # New Torchvision API
+                w = 'DEFAULT' if pretrain == 'imagenet' else pretrain
+                call_kw.update(dict(weights=w))  # type: ignore
             else:
                 call_kw.update(dict(pretrained=pretrain))  # type: ignore
             _model = model_fn(**call_kw)
 
         # Add final layers to models
         hidden_layers = [
             self.hidden_layer_width
@@ -429,15 +431,15 @@
         neptune_workspace: Optional[str] = None,
         load_method: str = 'weights',
         custom_objects: Optional[Dict[str, Any]] = None,
     ):
         """Sets base configuration, preparing model inputs and outputs.
 
         Args:
-            hp (:class:`slideflow.model.ModelParams`): ModelParams object.
+            hp (:class:`slideflow.ModelParams`): ModelParams object.
             outdir (str): Destination for event logs and checkpoints.
             labels (dict): Dict mapping slide names to outcome labels (int or
                 float format).
             slide_input (dict): Dict mapping slide names to additional
                 slide-level input, concatenated after post-conv.
             name (str, optional): Optional name describing the model, used for
                 model saving. Defaults to None.
@@ -844,14 +846,15 @@
         elif (self.normalizer
               and 'norm_fit' in self.config
               and self.config['norm_fit'] is not None):
             log.debug("Detecting normalizer fit from model config")
             self.normalizer.set_fit(**self.config['norm_fit'])
 
     def _has_gpu_normalizer(self) -> bool:
+        import slideflow.norm.torch
         return (isinstance(self.normalizer, sf.norm.torch.TorchStainNormalizer)
                 and self.normalizer.device != "cpu")
 
     def _labels_to_device(
         self,
         labels: Union[Dict[Any, Tensor], Tensor],
         device: torch.device
```

## slideflow/norm/__init__.py

```diff
@@ -1,30 +1,8 @@
-"""H&E stain normalization and augmentation tools.
-
-Overview
---------
-
-The main normalizer interface, :class:`slideflow.norm.StainNormalizer`, offers
-efficient numpy implementations for the Macenko, Reinhard, Reinhard-Fast,
-Reinhard (masked), and Vahadane H&E stain normalization algorithms, as well
-as an HSV colorspace stain augmentation method. This normalizer can convert
-images to and from Tensors, numpy arrays, and raw JPEG/PNG images.
-
-In addition to these numpy implementations, PyTorch-native and Tensorflow-native
-implementations are also provided, which offer performance improvements
-and/or vectorized application. The native normalizers are found in
-``slideflow.norm.tensorflow`` and ``slideflow.norm.torch``, respectively.
-
-The Vahadane normalizer has two numpy implementations available: SPAMS
-(``vahadane_spams``) and sklearn (``vahadane_sklearn``). By default,
-the SPAMS implementation will be used if unspecified (``method='vahadane'``).
-
-Use :func:`slideflow.norm.autoselect` to get the fastest available normalizer
-for a given method and active backend (Tensorflow/PyTorch).
-"""
+"""H&E stain normalization and augmentation tools."""
 
 from __future__ import absolute_import
 
 import os
 import sys
 import multiprocessing as mp
 from io import BytesIO
@@ -193,16 +171,18 @@
         batch_size: int = 64,
         num_threads: Union[str, int] = 'auto',
         **kwargs,
     ) -> "StainNormalizer":
         """Fit the normalizer to a target image or dataset of images.
 
         Args:
-            arg1: (Dataset, np.ndarray, str): Target to fit. May be a numpy
-                image array (uint8), path to an image, or a Slideflow Dataset.
+            arg1: (Dataset, np.ndarray, str): Target to fit. May be a str,
+                numpy image array (uint8), path to an image, or a Slideflow
+                Dataset. If this is a string, will fit to the corresponding
+                preset fit (either 'v1', 'v2', or 'v3').
                 If a Dataset is provided, will average fit values across
                 all images in the dataset.
             batch_size (int, optional): Batch size during fitting, if fitting
                 to dataset. Defaults to 64.
             num_threads (Union[str, int], optional): Number of threads to use
                 during fitting, if fitting to a dataset. Defaults to 'auto'.
         """
@@ -611,15 +591,15 @@
         white pixels (255) will be masked.
 
         This function is a context manager used for temporarily setting the
         image context. For example:
 
         .. code-block:: python
 
-            with normalizer.image_context(slide):
+            with normalizer.context(slide):
                 normalizer.transform(target)
 
         If a slide (``sf.WSI``) is used for context, any existing QC filters
         and regions of interest will be used to mask out background as white
         pixels, and the masked thumbnail will be used for creating the
         normalizer context. If no QC has been applied to the slide and the
         slide does not have any Regions of Interest, then both otsu's
```

## slideflow/slide/__init__.py

```diff
@@ -258,15 +258,15 @@
             raise errors.SlideLoadError(
                 f"{self.name}: unsupported filetype '{self.filetype}'"
             )
 
         # Collect basic slide information
         try:
             self.mpp = float(self.slide.mpp)
-        except KeyError:
+        except Exception as e:
             raise errors.SlideLoadError(
                 f"Slide [green]{self.name}[/] missing MPP ({OPS_MPP_X})"
             )
 
         # Calculate downsample by magnification
         if isinstance(tile_um, str):
             sf.util.assert_is_mag(tile_um)
@@ -838,21 +838,25 @@
             if gs_fractions:
                 df_dict.update({'gs_fraction': pd.Series(gs_fractions, dtype=float)})
             report_data = dict(
                 blur_burden=self.blur_burden,
                 num_tiles=len(locations),
                 qc_mask=self.qc_mask,
                 locations=pd.DataFrame(df_dict),
-                num_rois=(0 if self.roi_method == 'ignore' else len(self.rois))
+                num_rois=(0 if self.roi_method == 'ignore' else len(self.rois)),
+                tile_px=self.tile_px,
+                tile_um=self.tile_um,
             )
             slide_report = SlideReport(
                 sample_tiles,
                 self.slide.path,
                 data=report_data,
                 thumb_coords=locations,
+                tile_px=self.tile_px,
+                tile_um=self.tile_um,
             )
             return slide_report
         else:
             log.debug("Skipping slide report")
             return None
 
     def preview(self, rois: bool = True, **kwargs) -> Optional[Image.Image]:
```

## slideflow/slide/report.py

```diff
@@ -3,20 +3,21 @@
 from __future__ import absolute_import, division, print_function
 
 import io
 import os
 import tempfile
 import pandas as pd
 import numpy as np
+import cv2
 from fpdf import FPDF
 from PIL import Image
 from datetime import datetime
 from os.path import join, exists
 from types import SimpleNamespace
-from typing import Any, Dict, List, Optional, TYPE_CHECKING
+from typing import Any, Dict, List, Optional, Union, TYPE_CHECKING
 
 import slideflow as sf
 from slideflow.util import log, path_to_name  # noqa F401
 
 if TYPE_CHECKING:
     import pandas as pd
 
@@ -36,19 +37,22 @@
     example images of extracted tiles.
     '''
 
     def __init__(
         self,
         images: List[bytes],
         path: str,
+        tile_px: int,
+        tile_um: Union[int, str],
         *,
         thumb: Optional[Image.Image] = None,
         thumb_coords: Optional[np.ndarray] = None,
-        data: Dict[str, Any] = None,
-        compress: bool = True
+        data: Optional[Dict[str, Any]] = None,
+        compress: bool = True,
+        ignore_thumb_errors: bool = False
     ) -> None:
         """Creates a slide report summarizing tile extraction, with some example
         extracted images.
 
         Args:
             images (list(str)): List of JPEG image strings (example tiles).
             path (str): Path to slide.
@@ -56,44 +60,52 @@
                 metadata. Expected keys may include 'blur_burden', 'num_tiles',
                 'locations', and 'qc_mask'. Defaults to None.
             compress (bool, optional): Compresses images to reduce image sizes.
                 Defaults to True.
             thumb (PIL.Image): Thumbnail of slide. Defaults to None.
             thumb_coords (np.ndarray): Array of (x, y) tile extraction
                 coordinates, for display on the thumbnail. Defaults to None.
+            ignore_thumb_errors (bool): Ignore errors raised when attempting
+                to create a slide thumbnail.
 
 
         """
         self.data = data
         self.path = path
-        self.has_rois = 'num_rois' in data and data['num_rois'] > 0
+        self.tile_px = tile_px
+        self.tile_um = tile_um
+        if data is not None:
+            self.has_rois = 'num_rois' in data and data['num_rois'] > 0
+        else:
+            self.has_rois = False
         self.timestamp = str(datetime.now())
 
         # Thumbnail
+        self.ignore_thumb_errors = ignore_thumb_errors
         self.thumb_coords = thumb_coords
         if thumb is not None:
             self._thumb = Image.fromarray(np.array(thumb)[:, :, 0:3])
         else:
             self._thumb = None
 
         if not compress:
             self.images = images  # type: List[bytes]
         else:
             self.images = [self._compress(img) for img in images]
 
     @property
     def thumb(self):
         if self._thumb is None:
-            wsi = sf.WSI(self.path, 500, 500, verbose=False)
-            self._thumb = wsi.thumb(
-                coords=self.thumb_coords,
-                rois=self.has_rois,
-                low_res=True
-            )
-            self._thumb = Image.fromarray(np.array(self._thumb)[:, :, 0:3])
+            try:
+                self.calc_thumb()
+            except Exception:
+                if self.ignore_thumb_errors:
+                    return None
+                else:
+                    raise
         return self._thumb
 
     @property
     def blur_burden(self) -> Optional[float]:
         """Metric defined as the proportion of non-background slide
         with high blur. Only calculated if both Otsu and Blur QC is used.
 
@@ -164,17 +176,35 @@
         if self.data is None:
             return None
         if 'qc_mask' in self.data:
             return self.data['qc_mask']
         else:
             return None
 
+    def calc_thumb(self) -> None:
+        wsi = sf.WSI(
+            self.path,
+            tile_px=self.tile_px,
+            tile_um=self.tile_um,
+            verbose=False
+        )
+        self._thumb = wsi.thumb(
+            coords=self.thumb_coords,
+            rois=self.has_rois,
+            low_res=True,
+            width=512
+        )
+        self._thumb = Image.fromarray(np.array(self._thumb)[:, :, 0:3])
+
     def _compress(self, img: bytes) -> bytes:
         with io.BytesIO() as output:
-            Image.open(io.BytesIO(img)).save(output, format="JPEG", quality=75)
+            img = Image.open(io.BytesIO(img))
+            if img.height > 256:
+                img = Image.fromarray(cv2.resize(np.array(img), [256, 256]))
+            img.save(output, format="JPEG", quality=75)
             return output.getvalue()
 
     def image_row(self) -> Optional[bytes]:
         '''Merges images into a single row of images'''
         if not self.images:
             return None
         pil_images = [Image.open(io.BytesIO(i)) for i in self.images]
@@ -261,14 +291,25 @@
         self.bb_threshold = bb_threshold
         self.reports = reports
         self.meta = meta
         pdf = ExtractionPDF(title=title)
         pdf.alias_nb_pages()
         pdf.add_page()
 
+        # Render thumbnails, if a multiprocesing pool is provided.
+        if pool is not None:
+            log.debug("Rendering thumbnails with pool.")
+            thumbnails = pool.map(render_thumbnail, reports)
+            log.debug("Rendering tile images with pool.")
+            image_rows = pool.map(render_image_row, reports)
+            log.debug("Report render complete.")
+        else:
+            thumbnails = [r.thumb for r in reports]
+            image_rows = [r.image_row() for r in reports]
+
         if meta is not None and hasattr(meta, 'ws_frac'):
             n_tiles = np.array([r.num_tiles for r in reports if r is not None])
             bb = np.array([r.blur_burden for r in reports if r is not None])
             bb_names = [r.path for r in reports if r is not None]
             self.warn_txt = ''
             for slide, b in zip(bb_names, bb):
                 if b is not None and b > self.bb_threshold:
@@ -326,25 +367,14 @@
             for m in (meta.gs_frac, meta.gs_thresh, meta.ws_frac,
                       meta.ws_thresh, meta.normalizer, meta.img_format,
                       sf.slide_backend()):
                 pdf.cell(75)
                 pdf.cell(20, 4, str(m), ln=1)
             pdf.ln(20)
 
-            # Render thumbnails, if a multiprocesing pool is provided.
-            if pool is not None:
-                log.debug("Rendering thumbnails with pool.")
-                thumbnails = pool.map(render_thumbnail, reports)
-                log.debug("Rendering tile images with pool.")
-                image_rows = pool.map(render_image_row, reports)
-                log.debug("Report render complete.")
-            else:
-                thumbnails = [r.thumb for r in reports]
-                image_rows = [r.image_row() for r in reports]
-
             # Save thumbnail first
             pdf.set_font('Arial', 'B', 7)
             n_images = 0
             log.debug("Rendering PDF pages with thumbnails.")
             for i, report in enumerate(reports):
                 if report is None:
                     continue
```

## slideflow/slide/slideflow-logo-name-small.jpg

### Image content

```diff
@@ -1,8 +1,8 @@
                                                             
-  . . :88  . %%. . X %; . .  S. .  . 8 .  @. .  . . .  . .  
-    t@X@8. 8..t:t 88.%;    .8X   .   @8. .;   .      .    . 
- . .88X8S. .S@   .8@ S8.88: SX.8S 8tt  ;8:; S8.t :88.t8X 88 
-.@8@%  @;;  S8 X% 88.88S@ . 8X;8%%.S 88  ;:@X.  @@@X.8:%t%S 
- .X@: .8t; .    S 88 88tX  .8X:%%ttt 88 .;;@S.  %X  %X: t@ .
-  .88@8XX.  t:t8% .@ :8 @8;:t8 8X .S %% . @ .8.t8: 88X 88X  
-    ;... .   . . .. ...  . .:   .  .   .     .           .  
+  .  .t@. . :88. ;@ :  . . S%; . .  ..%  8 . .  . . .  . .  
+   .:8SS     @X .;;  X   .:XS   .:   ;;  :   .;      .     .
+   ;@X88   8 X   ;X tS t X t8 %  . X888:t: ; ;:88 @% .8t 8@ 
+ t%@t;%8S.  %X.8.;; tt @@  S@ 8 S%88 tt  : ;    8@. . S8:   
+ t8S. :8S:   ..@%;S ;t 8X  S@ 88ttt;.S%  : .   .88:8%::8%;  
+ .;88%X:: .%;:X8.;S ;S::XX:tt:t@t :8. 8  @..8% 8;  t . :..  
+   .:...    . . . . .;     .:   .           .    .  .  ..  .
```

### Image metadata

```diff
@@ -1,24 +1,20 @@
 Image format: JPEG
-File size: 20093B
-Height: 143
+File size: 30934B
+Height: 142
 Width: 600
-Orientation: TopLeft
+Orientation: Undefined
 Compression type: JPEG
-Compression quality: 90
+Compression quality: 100
 Colorspace: sRGB
 Channels: srgb
 Depth: 8
 Interlace mode: None
 Rendering intent: Perceptual
-X resolution: 0
-Y resolution: 0
-Resolution units: Undefined
+X resolution: 300
+Y resolution: 300
+Resolution units: PixelsPerInch
 Transparency channel enabled: False
 Gamma: 0.454545
-Number of unique colors: 2525
+Number of unique colors: 805
 Comment: 
-EXIF data: exif:ExifOffset=68
-exif:Orientation=1
-exif:PixelXDimension=600
-exif:PixelYDimension=143
-exif:Software=Shotwell 0.30.10
+EXIF data:
```

## slideflow/slide/utils.py

```diff
@@ -133,15 +133,15 @@
         str: Path to new CSV file.
 
     Raises:
         slideflow.errors.ROIError: If the XML could not be converted.
     """
     tree = ET.parse(path)
     root = tree.getroot()
-    new_csv_file = path[:-4] + 'csv'
+    new_csv_file = path[:-4] + '.csv'
     required_attributes = ['.//Region', './/Vertex']
     if not all(root.findall(a) for a in required_attributes):
         raise errors.ROIError(
             f"No ROIs found in the XML file {path}. Check that the XML "
             "file attributes are named correctly named in ImageScope "
             "format with 'Region' and 'Vertex' tags."
         )
```

## slideflow/studio/__init__.py

```diff
@@ -504,15 +504,15 @@
                         w.view_menu_options()
 
                 imgui.end_menu()
 
             # --- Help --------------------------------------------------------
             if imgui.begin_menu('Help', True):
                 if imgui.menu_item('Get Started')[1]:
-                    webbrowser.open('https://slideflow.dev/slideflow_studio')
+                    webbrowser.open('https://slideflow.dev/studio')
                 if imgui.menu_item('Documentation')[1]:
                     webbrowser.open('https://slideflow.dev')
 
                 # Widgets with "Help" menu.
                 for w in self.widgets:
                     if hasattr(w, 'help_menu_options'):
                         imgui.separator()
```

## slideflow/studio/widgets/extensions.py

```diff
@@ -1,9 +1,10 @@
 import numpy as np
 import imgui
+import textwrap
 from PIL import Image
 from os.path import join, dirname, abspath
 
 from ..gui import imgui_utils, gl_utils
 
 #----------------------------------------------------------------------------
 
@@ -12,14 +13,15 @@
     tag = 'extensions'
     description = 'Extensions'
     icon = join(dirname(abspath(__file__)), '..', 'gui', 'buttons', 'button_extensions.png')
     icon_highlighted = join(dirname(abspath(__file__)), '..', 'gui', 'buttons', 'button_extensions_highlighted.png')
 
     def __init__(self, viz):
         self.viz                = viz
+        self._show_err_popup    = False
 
         self.stylegan = any([w.tag == 'stylegan' for w in viz.widgets])
         self.mosaic = any([w.tag == 'mosaic' for w in viz.widgets])
         self.segment = any([w.tag == 'segment' for w in viz.widgets])
 
         _off_path = join(dirname(abspath(__file__)), '..', 'gui', 'buttons', 'small_button_verified.png')
         self._official_tex      = gl_utils.Texture(
@@ -67,14 +69,35 @@
         else:
             imgui.text('')
         imgui.same_line(imgui.get_content_region_max()[0] - viz.font_size - viz.spacing * 1.5)
         result = imgui.checkbox(f'##{title}_checkbox', check_value)
         imgui.end_child()
         return result
 
+    def show_extension_error(self, message):
+        self._show_err_popup = True
+        self._err_msg = message
+
+    def draw_error_popup(self):
+        """Show an error message that an extension failed to load."""
+        wrapped = textwrap.wrap(self._err_msg, width=45)
+        lh = imgui.get_text_line_height_with_spacing()
+        window_size = (self.viz.font_size * 18, lh * len(wrapped) + self.viz.font_size * 4)
+        self.viz.center_next_window(*window_size)
+        imgui.set_next_window_size(*window_size)
+        _, opened = imgui.begin('Error loading extension', closable=True, flags=imgui.WINDOW_NO_RESIZE)
+        if not opened:
+            self._show_err_popup = False
+
+        for line in wrapped:
+            imgui.text(line)
+
+        if self.viz.sidebar.full_button("OK", width=-1):
+            self._show_err_popup = False
+        imgui.end()
 
     @imgui_utils.scoped_by_object_id
     def __call__(self, show=True):
         viz = self.viz
 
         if show:
             viz.header("Extensions")
@@ -92,18 +115,35 @@
             _c1, self.stylegan = self.extension_checkbox(
                 'StyleGAN',
                 description='Generate images with StyleGAN.',
                 check_value=self.stylegan,
                 official=True
             )
             if _c1:
-                self.update_stylegan()
+                try:
+                    self.update_stylegan()
+                except Exception as e:
+                    self.show_extension_error(str(e))
+                    self.stylegan = False
             imgui.separator()
 
             _c3, self.segment = self.extension_checkbox(
                 'Cell Segmentation',
                 description='Segment cells with Cellpose.',
                 check_value=self.segment,
                 official=True
             )
             if _c3:
-                self.update_segment()
+                try:
+                    self.update_segment()
+                except ImportError as e:
+                    self.show_extension_error(
+                        'Cellpose is not installed. Cellpose can be installed '
+                        'with "pip install cellpose"'
+                    )
+                    self.segment = False
+                except Exception as e:
+                    self.show_extension_error(str(e))
+                    self.segment = False
+
+        if self._show_err_popup:
+            self.draw_error_popup()
```

## slideflow/test/__init__.py

```diff
@@ -193,15 +193,15 @@
                 normalizer=normalizer,
                 label='TEST',
                 uq=uq,
                 filename='sweep.json'
             )
 
         # Create single hyperparameter combination
-        hp = sf.model.ModelParams(
+        hp = sf.ModelParams(
             tile_px=self.tile_px,
             tile_um=1208,
             epochs=1,
             toplayer_epochs=0,
             model="mobilenet_v2",
             pooling='max',
             loss=loss,
@@ -729,51 +729,55 @@
                 sf.test.functional.wsi_prediction_tester,
                 project=self.project,
                 model=model
             )
             if not passed:
                 test.fail()
 
-    def test_clam(self) -> None:
-        """Test the CLAM submodule."""
+    def test_mil(self) -> None:
+        """Test the MIL submodule."""
 
         assert self.project is not None
         model = self._get_model('category1-manual_hp-TEST-HPSweep0-kfold1')
         assert exists(model), "Model has not yet been trained."
 
         try:
             skip_test = False
             import torch  # noqa F401
         except ImportError:
-            log.warning("Unable to import pytorch, skipping CLAM test")
+            log.warning("Unable to import pytorch, skipping MIL test")
             skip_test = True
 
-        with TaskWrapper("Testing CLAM feature export...") as test:
+        with TaskWrapper("Testing MIL feature export...") as test:
             if skip_test:
                 test.skip()
             else:
                 passed = process_isolate(
                     sf.test.functional.clam_feature_generator_tester,
                     project=self.project,
                     model=model
                 )
                 if not passed:
                     test.fail()
 
-        with TaskWrapper("Testing CLAM training...") as test:
+        with TaskWrapper("Testing MIL training...") as test:
             if skip_test:
                 test.skip()
             else:
                 try:
                     dataset = self.project.dataset(self.tile_px, 1208)
-                    self.project.train_clam(
-                        'TEST_CLAM',
-                        join(self.project.root, 'clam'),
-                        'category1',
-                        dataset
+                    train_dts, val_dts = dataset.split(val_fraction=0.3)
+                    import slideflow.mil
+                    config = sf.mil.mil_config('clam_sb')
+                    self.project.train_mil(
+                        config,
+                        train_dts,
+                        val_dts,
+                        outcomes='category1',
+                        bags=join(self.project.root, 'clam'),
                     )
                 except Exception as e:
                     log.error(traceback.format_exc())
                     test.fail()
 
     def test(
         self,
@@ -783,15 +787,15 @@
         train: bool = True,
         normalizer: bool = True,
         evaluate: bool = True,
         predict: bool = True,
         heatmap: bool = True,
         activations: bool = True,
         predict_wsi: bool = True,
-        clam: bool = True,
+        mil: bool = True,
         slide_threads: Optional[int] = None
     ) -> None:
         """Perform and report results of all available testing."""
 
         start = time.time()
         if unit:
             self.unittests()
@@ -813,16 +817,16 @@
                 self.test_prediction()
             if heatmap:
                 self.test_heatmap()
             if activations:
                 self.test_activations_and_mosaic()
             if predict_wsi:
                 self.test_predict_wsi()
-            if clam:
-                self.test_clam()
+            if mil:
+                self.test_mil()
         end = time.time()
         m, s = divmod(end-start, 60)
         print(f'Tests complete. Time: {int(m)} min, {s:.2f} sec')
 
     def unittests(self) -> None:
         """Run unit tests."""
```

## slideflow/util/__init__.py

```diff
@@ -27,15 +27,14 @@
 from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union
 
 import numpy as np
 import slideflow as sf
 from slideflow import errors
 from . import example_pb2, log_utils
 from .colors import *  # noqa F403,F401 - Here for compatibility
-from .tfrecord2idx import get_record_by_index, get_tfrecord_length
 from .smac_utils import (broad_search_space, shallow_search_space,
                          create_search_space)
 
 tf_available = importlib.util.find_spec('tensorflow')
 torch_available = importlib.util.find_spec('torch')
 
 # Enable color sequences on Windows
@@ -903,18 +902,42 @@
 def location_heatmap(
     locations: np.ndarray,
     values: np.ndarray,
     slide: str,
     tile_px: int,
     tile_um: Union[int, str],
     outdir: str,
+    *,
     interpolation: Optional[str] = 'bicubic',
     cmap: str = 'inferno',
-    norm: str = 'two_slope',
+    norm: Optional[str] = None,
 ) -> Dict[str, Dict[str, float]]:
+    """Generate a heatmap for a slide.
+
+    Args:
+        locations (np.ndarray): Array of shape ``(n_tiles, 2)`` containing x, y
+            coordinates for all image tiles.
+        values (np.ndarray): Array of shape ``(n_tiles,)`` containing heatmap
+            values for each tile.
+        slide (str): Path to corresponding slide.
+        tile_px (int): Tile pixel size.
+        tile_um (int, str): Tile micron or magnification size.
+        outdir (str): Directory in which to save heatmap.
+
+    Keyword args:
+        interpolation (str, optional): Interpolation strategy for smoothing
+            heatmap. Defaults to 'bicubic'.
+        cmap (str, optional): Matplotlib colormap for heatmap. Can be any
+            valid matplotlib colormap. Defaults to 'inferno'.
+        norm (str, optional): Normalization strategy for assigning heatmap
+            values to colors. Either 'two_slope', or any other valid value
+            for the ``norm`` argument of ``matplotlib.pyplot.imshow``.
+            If 'two_slope', normalizes values less than 0 and greater than 0
+            separately. Defaults to None.
+    """
 
     import matplotlib.pyplot as plt
     import matplotlib.colors as mcol
 
     slide_name = sf.util.path_to_name(slide)
     log.info(f'Generating heatmap for [green]{slide}[/]...')
     wsi = sf.slide.WSI(slide, tile_px, tile_um, verbose=False)
```

## slideflow/util/tfrecord2idx.py

```diff
@@ -7,25 +7,29 @@
 import sys
 import numpy as np
 import slideflow as sf
 from typing import Optional, Dict
 from os.path import dirname, join, exists
 from slideflow import errors
 
+
 TYPENAME_MAPPING = {
     "byte": "bytes_list",
     "float": "float_list",
     "int": "int64_list"
 }
+
 FEATURE_DESCRIPTION = {
-        'image_raw': 'byte',
-        'slide': 'byte',
-        'loc_x': 'int',
-        'loc_y': 'int'
-    }
+    'image_raw': 'byte',
+    'slide': 'byte',
+    'loc_x': 'int',
+    'loc_y': 'int'
+}
+
+# -----------------------------------------------------------------------------
 
 def create_index(tfrecord_file: str, index_file: str) -> None:
     """Create index from the tfrecords file.
 
     Stores starting location (byte) and length (in bytes) of each
     serialized record.
 
@@ -51,51 +55,97 @@
             infile.read(proto_len)
             infile.read(4)
             out_array += [[cur, infile.tell() - cur]]
         except Exception:
             print("Failed to parse TFRecord.")
             break
     infile.close()
-    np.savez(index_file, np.array(out_array))
+    out_array = np.array(out_array)
+    if 'SF_ALLOW_ZIP' in os.environ and os.environ['SF_ALLOW_ZIP'] == '0':
+        np.save(index_file + '.npy', out_array)
+    else:
+        np.savez(index_file, out_array)
 
 
 def find_index(tfrecord: str) -> Optional[str]:
     name = sf.util.path_to_name(tfrecord)
     if exists(join(dirname(tfrecord), name+'.index')):
         return join(dirname(tfrecord), name+'.index')
     elif exists(join(dirname(tfrecord), name+'.index.npz')):
         return join(dirname(tfrecord), name+'.index.npz')
+    elif exists(join(dirname(tfrecord), name+'.index.npy')):
+        return join(dirname(tfrecord), name+'.index.npy')
     else:
         return None
 
 
 def load_index(tfrecord: str) -> Optional[np.ndarray]:
     index_path = find_index(tfrecord)
     if index_path is None:
         raise OSError(f"Could not find index path for TFRecord {tfrecord}")
     if os.stat(index_path).st_size == 0:
         return None
     elif index_path.endswith('npz'):
         return np.load(index_path)['arr_0']
+    elif index_path.endswith('npy'):
+        return np.load(index_path)
     else:
         return np.loadtxt(index_path, dtype=np.int64)
 
 
 def get_tfrecord_length(tfrecord: str) -> int:
+    """Return the number of records in a TFRecord file.
+
+    Uses an index file if available, otherwise iterates through
+    the file to find the total record length.
+
+    Args:
+        tfrecord (str): Path to TFRecord.
+
+    Returns:
+        int: Number of records.
+
+    """
     index_path = find_index(tfrecord)
     if index_path is None:
-        raise OSError(f"Could not find index path for TFRecord {tfrecord}")
+        return read_tfrecord_length(tfrecord)
     if os.stat(index_path).st_size == 0:
         return 0
     else:
-        idx = load_index(tfrecord)
-        return idx.shape[0]
+        index_array = load_index(tfrecord)
+        if index_array is None:
+            return 0
+        else:
+            return index_array.shape[0]
+
+
+def read_tfrecord_length(tfrecord: str) -> int:
+    """Returns number of records stored in the given tfrecord file."""
+    infile = open(tfrecord, "rb")
+    num_records = 0
+    while True:
+        infile.tell()
+        try:
+            byte_len = infile.read(8)
+            if len(byte_len) == 0:
+                break
+            infile.read(4)
+            proto_len = struct.unpack("q", byte_len)[0]
+            infile.read(proto_len)
+            infile.read(4)
+            num_records += 1
+        except Exception:
+            sf.log.error(f"Failed to parse TFRecord at {tfrecord}")
+            infile.close()
+            return 0
+    infile.close()
+    return num_records
 
 
-def get_record_by_index(
+def get_tfrecord_by_index(
     tfrecord: str,
     index: int,
     compression_type: Optional[str] = None,
 ) -> Dict:
     """Read a specific record in a TFRecord file.
 
     Args:
```

## Comparing `slideflow-2.0.0b1.data/scripts/slideflow-studio` & `slideflow-2.0.1.data/scripts/slideflow-studio`

 * *Files 2% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 from os.path import dirname, realpath, join
 from PIL import Image, ImageFont
 from contextlib import contextmanager
 from functools import lru_cache
 from os.path import join, dirname
 
-__version__ = "1.6.0"
+__version__ = "2.0.0"
 
 # -----------------------------------------------------------------------------
 
 @click.command()
 @click.argument('slide', metavar='PATH', required=False)
 @click.option('--model', '-m', help='Classifier network for categorical predictions.', metavar='PATH')
 @click.option('--project', '-p', help='Slideflow project.', metavar='PATH')
@@ -50,38 +50,28 @@
 
     if low_memory is None:
         low_memory = False
 
     # Load widgets
     widgets = Studio.get_default_widgets()
     if stylegan:
-        from slideflow.studio import stylegan_widgets
-        from slideflow.studio.seed_map_widget import SeedMapWidget
-        from slideflow.gan.stylegan3.stylegan3.viz.renderer import Renderer as GANRenderer
-        widgets += stylegan_widgets(advanced=advanced)
-        widgets += [SeedMapWidget]
+        from slideflow.studio.widgets.stylegan import StyleGANWidget
+        widgets += [StyleGANWidget]
 
     if picam:
         from slideflow.studio.widgets.picam import PicamWidget
         widgets += [PicamWidget]
 
     if cellpose:
         from slideflow.studio.widgets.segment import SegmentWidget
         widgets += [SegmentWidget]
 
-    viz = Studio(low_memory=low_memory, widgets=widgets, skip_tk_init=True)
+    viz = Studio(low_memory=low_memory, widgets=widgets)
     viz.project_widget.search_dirs += [dirname(realpath(__file__))]
 
-    # --- StyleGAN3 -----------------------------------------------------------
-    if stylegan:
-        viz.add_to_render_pipeline(GANRenderer(), name='stylegan')
-        if advanced:
-            viz._pane_w_div = 45
-    # -------------------------------------------------------------------------
-
     # Load model.
     if model is not None:
         viz.load_model(model)
 
     if project is not None:
         viz.load_project(project)
 
@@ -91,16 +81,30 @@
 
     # Run.
     viz.run()
 
 
 #----------------------------------------------------------------------------
 
+
+
 def import_with_splash():
 
+    _imported = False
+
+    def _import_sildeflow():
+        nonlocal _imported
+        import slideflow.studio
+        _imported = True
+
+    # Start the import thread
+    _thread = threading.Thread(target=_import_sildeflow)
+    _thread.start()
+
+    # Send Tk to the background (used for future file dialogs)
     from tkinter import Tk
     Tk().withdraw()
 
     # Load image
     sf_root = pkgutil.get_loader('slideflow').get_filename()
     splash_path = join(dirname(sf_root), 'studio', 'gui', 'splash.png')
     icon_path = join(dirname(sf_root), 'studio', 'gui', 'icons', 'logo.png')
@@ -157,27 +161,26 @@
         gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)
 
         if _tex_bg is None:
             _tex_bg = Texture(image=img, bilinear=False)
         if _tex_icon is None:
             _tex_icon = Texture(image=icon, bilinear=True)
         if _version_text is None:
-            _version_text = text_texture(__version__, size=28)
+            _version_text = text_texture(__version__, size=22)
         _tex_bg.draw(pos=0, zoom=1, align=0.5, rint=True, anchor='topleft')
         _tex_icon.draw(pos=(width//2, int(height * 0.3)), zoom=0.25, align=0.5, rint=True, anchor='center')
         _version_text.draw(pos=(width//2, int(height * 0.7)), zoom=1, align=0.5, rint=True, anchor='center')
 
-        if _first_frame:
+        if not _first_frame:
             glfw.show_window(window)
             _first_frame = False
 
         glfw.swap_buffers(window)
 
-        if not _first_frame:
-            import slideflow.studio
+        if _imported:
             glfw.destroy_window(window)
             glfw.default_window_hints()
             break
 
     glfw.terminate()
 
 # -----------------------------------------------------------------------------
```

## Comparing `slideflow-2.0.0b1.data/scripts/slideflow-studio.py` & `slideflow-2.0.1.data/scripts/slideflow-studio.py`

 * *Files 2% similar despite different names*

```diff
@@ -81,16 +81,30 @@
 
     # Run.
     viz.run()
 
 
 #----------------------------------------------------------------------------
 
+
+
 def import_with_splash():
 
+    _imported = False
+
+    def _import_sildeflow():
+        nonlocal _imported
+        import slideflow.studio
+        _imported = True
+
+    # Start the import thread
+    _thread = threading.Thread(target=_import_sildeflow)
+    _thread.start()
+
+    # Send Tk to the background (used for future file dialogs)
     from tkinter import Tk
     Tk().withdraw()
 
     # Load image
     sf_root = pkgutil.get_loader('slideflow').get_filename()
     splash_path = join(dirname(sf_root), 'studio', 'gui', 'splash.png')
     icon_path = join(dirname(sf_root), 'studio', 'gui', 'icons', 'logo.png')
@@ -152,22 +166,21 @@
             _tex_icon = Texture(image=icon, bilinear=True)
         if _version_text is None:
             _version_text = text_texture(__version__, size=22)
         _tex_bg.draw(pos=0, zoom=1, align=0.5, rint=True, anchor='topleft')
         _tex_icon.draw(pos=(width//2, int(height * 0.3)), zoom=0.25, align=0.5, rint=True, anchor='center')
         _version_text.draw(pos=(width//2, int(height * 0.7)), zoom=1, align=0.5, rint=True, anchor='center')
 
-        if _first_frame:
+        if not _first_frame:
             glfw.show_window(window)
             _first_frame = False
 
         glfw.swap_buffers(window)
 
-        if not _first_frame:
-            import slideflow.studio
+        if _imported:
             glfw.destroy_window(window)
             glfw.default_window_hints()
             break
 
     glfw.terminate()
 
 # -----------------------------------------------------------------------------
```

## Comparing `slideflow-2.0.0b1.dist-info/LICENSE` & `slideflow-2.0.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `slideflow-2.0.0b1.dist-info/METADATA` & `slideflow-2.0.1.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: slideflow
-Version: 2.0.0b1
+Version: 2.0.1
 Summary: Deep learning tools for digital histology
 Home-page: https://github.com/jamesdolezal/slideflow
 Author: James Dolezal
 Author-email: james.dolezal@uchospitals.edu
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
 Classifier: Operating System :: OS Independent
@@ -18,25 +18,24 @@
 Requires-Dist: imageio
 Requires-Dist: opencv-python-headless
 Requires-Dist: shapely
 Requires-Dist: umap-learn
 Requires-Dist: seaborn (<0.12)
 Requires-Dist: pandas
 Requires-Dist: pyvips
-Requires-Dist: fpdf
+Requires-Dist: fpdf2
 Requires-Dist: lifelines
 Requires-Dist: scikit-image
-Requires-Dist: scikit-misc
 Requires-Dist: tqdm
 Requires-Dist: click
 Requires-Dist: protobuf (<=3.20.2)
 Requires-Dist: tensorboard
 Requires-Dist: crc32c
 Requires-Dist: h5py
-Requires-Dist: numpy (<1.22)
+Requires-Dist: numpy
 Requires-Dist: tabulate
 Requires-Dist: rasterio
 Requires-Dist: smac (==1.4.0)
 Requires-Dist: ConfigSpace
 Requires-Dist: pyarrow
 Requires-Dist: ninja
 Requires-Dist: rich
@@ -53,71 +52,63 @@
 Requires-Dist: cellpose ; extra == 'all'
 Requires-Dist: cucim ; extra == 'all'
 Requires-Dist: sphinx ; extra == 'all'
 Requires-Dist: sphinx-markdown-tables ; extra == 'all'
 Requires-Dist: sphinxcontrib-video ; extra == 'all'
 Requires-Dist: torch ; extra == 'all'
 Requires-Dist: torchvision ; extra == 'all'
+Requires-Dist: fastai ; extra == 'all'
 Requires-Dist: pretrainedmodels ; extra == 'all'
-Requires-Dist: tensorflow (<2.10,>=2.7) ; extra == 'all'
-Requires-Dist: tensorflow-probability (<0.18) ; extra == 'all'
+Requires-Dist: tensorflow (<2.12,>=2.7) ; extra == 'all'
+Requires-Dist: tensorflow-probability (<0.20) ; extra == 'all'
 Requires-Dist: tensorflow-datasets ; extra == 'all'
 Provides-Extra: cellpose
 Requires-Dist: cellpose ; extra == 'cellpose'
 Provides-Extra: cucim
 Requires-Dist: cucim ; extra == 'cucim'
 Provides-Extra: dev
 Requires-Dist: sphinx ; extra == 'dev'
 Requires-Dist: sphinx-markdown-tables ; extra == 'dev'
 Requires-Dist: sphinxcontrib-video ; extra == 'dev'
 Provides-Extra: tf
-Requires-Dist: tensorflow (<2.10,>=2.7) ; extra == 'tf'
-Requires-Dist: tensorflow-probability (<0.18) ; extra == 'tf'
+Requires-Dist: tensorflow (<2.12,>=2.7) ; extra == 'tf'
+Requires-Dist: tensorflow-probability (<0.20) ; extra == 'tf'
 Requires-Dist: tensorflow-datasets ; extra == 'tf'
 Provides-Extra: torch
 Requires-Dist: torch ; extra == 'torch'
 Requires-Dist: torchvision ; extra == 'torch'
 Requires-Dist: pretrainedmodels ; extra == 'torch'
 Requires-Dist: cellpose ; extra == 'torch'
+Requires-Dist: fastai ; extra == 'torch'
 
 ![slideflow logo](https://github.com/jamesdolezal/slideflow/raw/master/docs-source/pytorch_sphinx_theme/images/slideflow-banner.png)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5703792.svg)](https://doi.org/10.5281/zenodo.5703792)
 [![Python application](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml/badge.svg?branch=master)](https://github.com/jamesdolezal/slideflow/actions/workflows/python-app.yml)
 [![PyPI version](https://badge.fury.io/py/slideflow.svg)](https://badge.fury.io/py/slideflow)
 
+[ArXiv](https://arxiv.org/abs/2304.04142) | [Docs](https://slideflow.dev) | [Slideflow Studio](https://slideflow.dev/studio/) | [Cite](#reference)
+
 Slideflow is a deep learning library for digital pathology that provides a unified API for building, training, and testing models using Tensorflow or PyTorch.
 
-Slideflow includes tools for **whole-slide image processing**, **customizable deep learning model training** with dozens of supported architectures, **multi-instance learning**, **self-supervised learning**, **explainability tools** (including heatmaps, mosaic maps, GANs, and saliency maps), **analysis of layer activations**, **uncertainty quantification**, and more.
+Slideflow includes tools for **[whole-slide image processing](https://slideflow.dev/slide_processing)**, **customizable deep learning [model training](https://slideflow.dev/training)** with dozens of supported architectures, **[multi-instance learning](https://slideflow.dev/mil)**, **[self-supervised learning](https://slideflow.dev/ssl)**, **[cell segmentation](https://slideflow.dev/cellseg)**, **explainability tools** (including [heatmaps](https://slideflow.dev/evaluation/#heatmaps), [mosaic maps](https://slideflow.dev/posthoc/#mosaic-maps), [GANs](https://slideflow.dev/stylegan/), and [saliency maps](https://slideflow.dev/saliency/)), **analysis of [layer activations](https://slideflow.dev/posthoc/)**, **[uncertainty quantification](https://slideflow.dev/uq/)**, and more.
 
-A variety of fast, optimized whole-slide image processing tools are included, including background filtering, blur/artifact detection, [stain normalization](https://slideflow.dev/norm.html), and efficient storage in `*.tfrecords` format. Model training is easy and highly configurable, with an straightforward API for training custom architectures. Slideflow can be used as an image processing backend for external training loops, serving an optimized `tf.data.Dataset` or `torch.utils.data.DataLoader` to read and process slide images and perform real-time stain normalization.
+A variety of fast, optimized whole-slide image processing tools are included, including background filtering, blur/artifact detection, [stain normalization](https://slideflow.dev/norm), and efficient storage in `*.tfrecords` format. Model training is easy and highly configurable, with an straightforward API for training custom architectures. Slideflow can be used as an image processing backend for external training loops, serving an optimized `tf.data.Dataset` or `torch.utils.data.DataLoader` to read and process slide images and perform real-time stain normalization.
 
 Full documentation with example tutorials can be found at [slideflow.dev](https://www.slideflow.dev/).
 
-![studio preview](https://github.com/jamesdolezal/slideflow/raw/master/docs-source/source/studio_preview.png)
+![Studio preview](https://slideflow.dev/_images/studio_saliency.jpg)
 *Slideflow Studio: a visualization tool for interacting with models and whole-slide images.*
 
-Slideflow has been used by:
-
-- [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020
-- [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020
-- [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021
-- [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022
-- [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022
-- [Partin et al](https://arxiv.org/abs/2204.11678) [arXiv], 2022
-- [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) [abstract], 2022
-- [Howard et al](https://www.biorxiv.org/content/10.1101/2022.07.07.499039v1) [bioRxiv], 2022
-- [Dolezal et al](https://arxiv.org/abs/2211.06522) [arXiv], 2022
-
 ## Requirements
 - Python >= 3.7 (<3.10 if using [cuCIM](https://docs.rapids.ai/api/cucim/stable/))
-- [Tensorflow](https://www.tensorflow.org/) 2.5-2.9 _or_ [PyTorch](https://pytorch.org/) 1.9-1.12
+- [Tensorflow](https://www.tensorflow.org/) 2.5-2.11 _or_ [PyTorch](https://pytorch.org/) 1.9-2.0
+  - GAN and MIL functions require PyTorch <1.13
 
 ### Optional
 - [Libvips](https://libvips.github.io/libvips/) >= 8.9 (alternative slide reader, adds support for *.scn, *.mrxs, *.ndpi, *.vms, and *.vmu files).
-- [QuPath](https://qupath.github.io/) (for pathologist ROIs)
 - Linear solver (for preserved-site cross-validation)
   - [CPLEX](https://www.ibm.com/docs/en/icos/12.10.0?topic=v12100-installing-cplex-optimization-studio) 20.1.0 with [Python API](https://www.ibm.com/docs/en/icos/12.10.0?topic=cplex-setting-up-python-api)
   - _or_ [Pyomo](http://www.pyomo.org/installation) with [Bonmin](https://anaconda.org/conda-forge/coinbonmin) solver
 
 
 ## Installation
 Slideflow can be installed with PyPI, as a Docker container, or run from source.
@@ -254,30 +245,40 @@
 )
 ```
 
 ## Evaluation, heatmaps, mosaic maps, and more
 
 Slideflow includes a host of additional tools, including model [evaluation and prediction](https://slideflow.dev/evaluation), [heatmaps](https://slideflow.dev/evaluation#heatmaps), analysis of [layer activations](https://slideflow.dev/posthoc), [mosaic maps](https://slideflow.dev/posthoc#mosaic-maps), and more. See our [full documentation](https://slideflow.dev) for more details and tutorials.
 
+## Publications
+
+Slideflow has been used by:
+
+- [Dolezal et al](https://www.nature.com/articles/s41379-020-00724-3), _Modern Pathology_, 2020
+- [Rosenberg et al](https://ascopubs.org/doi/10.1200/JCO.2020.38.15_suppl.e23529), _Journal of Clinical Oncology_ [abstract], 2020
+- [Howard et al](https://www.nature.com/articles/s41467-021-24698-1), _Nature Communications_, 2021
+- [Dolezal et al](https://www.nature.com/articles/s41467-022-34025-x) _Nature Communications_, 2022
+- [Storozuk et al](https://www.nature.com/articles/s41379-022-01039-1.pdf), _Modern Pathology_ [abstract], 2022
+- [Partin et al](https://doi.org/10.3389/fmed.2023.1058919) _Front Med_, 2022
+- [Dolezal et al](https://ascopubs.org/doi/abs/10.1200/JCO.2022.40.16_suppl.8549) [abstract], 2022
+- [Howard et al](https://www.biorxiv.org/content/10.1101/2022.07.07.499039v1) [bioRxiv], 2022
+- [Dolezal et al](https://arxiv.org/abs/2211.06522) [arXiv], 2022
+- [Hieromnimon et al](https://doi.org/10.1101/2023.03.22.533810) [bioRxiv], 2023
+
 ## License
 This code is made available under the GPLv3 License and is available for non-commercial academic purposes.
 
 ## Reference
 If you find our work useful for your research, or if you use parts of this code, please consider citing as follows:
 
-James Dolezal, Sara Kochanny, & Frederick Howard. (2022). Slideflow: A Unified Deep Learning Pipeline for Digital Histology (1.5.0). Zenodo. https://doi.org/10.5281/zenodo.5703792
+Dolezal, J. M., Kochanny, S., Dyer, E., *et al*. Slideflow: Deep Learning for Digital Histopathology with Real-Time Whole-Slide Visualization. ArXiv [q-Bio.QM] (2023). http://arxiv.org/abs/2304.04142
 
 ```
-@software{james_dolezal_2022_5703792,
-  author       = {James Dolezal and
-                  Sara Kochanny and
-                  Frederick Howard},
-  title        = {{Slideflow: A Unified Deep Learning Pipeline for
-                   Digital Histology}},
-  month        = oct,
-  year         = 2022,
-  publisher    = {Zenodo},
-  version      = {1.5.0},
-  doi          = {10.5281/zenodo.5703792},
-  url          = {https://doi.org/10.5281/zenodo.5703792}
+@misc{dolezal2023slideflow,
+      title={Slideflow: Deep Learning for Digital Histopathology with Real-Time Whole-Slide Visualization}, 
+      author={James M. Dolezal and Sara Kochanny and Emma Dyer and Andrew Srisuwananukorn and Matteo Sacco and Frederick M. Howard and Anran Li and Prajval Mohan and Alexander T. Pearson},
+      year={2023},
+      eprint={2304.04142},
+      archivePrefix={arXiv},
+      primaryClass={q-bio.QM}
 }
 ```
```

## Comparing `slideflow-2.0.0b1.dist-info/RECORD` & `slideflow-2.0.1.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,37 +1,27 @@
 slideflow/__init__.py,sha256=Y463ep8Pwe89TUUVaqmFAbKmtQVjjDEEZHIlQ8unTE0,1411
 slideflow/_backend.py,sha256=Yi7LUgiYUUVlGdi6DBYe2raev3LLdYdtR6PtHozFORU,1662
-slideflow/_version.py,sha256=5QOI1oJlQNogkbF7XxdjIOvkApcg8DmZInl371K3Px4,503
-slideflow/dataset.py,sha256=_Wirc76THEvheHZP8qJvTIg2HdoDCCgW4qJiIKXBWt0,159218
+slideflow/_version.py,sha256=ZavQGjTygEdfjVnzZH8ZrXFwt_GLP6KP7q3xeTiG-GA,497
+slideflow/dataset.py,sha256=bXKwfFq4sLmIYxl7kESIkfW3Uzr2g4oSG4RiSnFjvB0,159660
 slideflow/errors.py,sha256=ueMKAc73Xz1OoSzibSt_qewJ29Lkk8KjkOkaNMw5ks0,3733
 slideflow/heatmap.py,sha256=7p6RenakRi01yo6DGH5mIUV9MfLQ66Q1iBPV6gnDV0Q,38304
-slideflow/mosaic.py,sha256=VWmbmaTPOZBBxAJFro1kHNRnREm1yQqnlYbh9XONb0k,25639
-slideflow/project.py,sha256=SoXvYdrb8wx9GwT7lZgZ9ZXqpY56WdigFA0oj_leZnw,166666
+slideflow/mosaic.py,sha256=GjceACaGgxo2UKqa4AA0RSNaiGTMvEgW0ceFPUpjkrA,25615
+slideflow/project.py,sha256=do9n8psKyemUyWQ7nY1yl_ofI1JtZDB8kYrjIjga8YQ,168044
 slideflow/project_utils.py,sha256=YzjnuM3faru-lqQau7rBhuCyXGSGIXpSAmkR3hzCAio,33320
 slideflow/sample_actions.py,sha256=khhho6m0GzAXUD9licPFKlq8oxqY3hdP1qg1B3CTOMY,978
 slideflow/biscuit/__init__.py,sha256=iSjHWIB2GZloqeMxPhNSpb90gBagYm_hX7xut8rKf48,1195
 slideflow/biscuit/delong.py,sha256=3zc0ctK1ZVHaeyFfxOzkKU8doNlzbfPiNkeA18moCs0,4281
 slideflow/biscuit/errors.py,sha256=nAGXgZnJT6GAUs0zw6tNZ3jCfPM_7U3rcu1roSWe1zA,318
-slideflow/biscuit/experiment.py,sha256=6ovEAcULHOqwr0JOm7JSfj9_aXJIphc8DWVx8KF7Fl4,46741
+slideflow/biscuit/experiment.py,sha256=SNrkbYkrOlX_-Pf1LY7cuG2GcX8UYvrhbVhcPkOr5Gs,46729
 slideflow/biscuit/hp.py,sha256=bSuup8s4G9mxpSZ0Wy1ADlpmsNfbBsXzv1dmspAOPGU,1260
-slideflow/biscuit/threshold.py,sha256=t2wk3C4oxu9iT1-wIl3UtWEG_moLW33fR8AgDjYDCtU,21292
+slideflow/biscuit/threshold.py,sha256=k7Of9UwlU4HJjsAGcAwepoanNptFql1bdVhjFfVtE5Q,21373
 slideflow/biscuit/utils.py,sha256=q0lUNv2PFTSbL3Q-1ewqMcq_E8pcwXH9HaBNgpr3dzE,17143
-slideflow/cellseg/__init__.py,sha256=PUv4m7dC_iUwBB89SKVvgMFOgYIq73Re3x2ks7VSCsI,26001
+slideflow/cellseg/__init__.py,sha256=WxBuRrPa5XAPXtTwymnL6QBS-nkG_71LdrZS0AUt7SE,26315
 slideflow/cellseg/seg_utils.py,sha256=NGOTmdmaU2-bbisuo7pwruY0uUl14kT-BXiko1nYdng,5454
 slideflow/clam/__init__.py,sha256=3BiTqyeuQ3pfqDpm4WzyEXlwmhqV19VzNMgutT1gr3E,444
-slideflow/clam/create_attention.py,sha256=3J1J9a9lxl2-dprpspVoaAja6PZlKRzXFyQFmzap6B0,4161
-slideflow/clam/datasets/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-slideflow/clam/datasets/dataset_generic.py,sha256=xonGjljeaklhjZvJWWxPlmBz6lZw3X2tj8WfMIOmYH4,15027
-slideflow/clam/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-slideflow/clam/models/model_clam.py,sha256=vW19CZWt5DzX_tkf8hO5OhYdUiqocKWEkpIIVsgIV6c,10137
-slideflow/clam/models/model_mil.py,sha256=ZhxgsXq8HeCsLNqzTnp6jEOKu6l7Pf4t1u2BHw02TJA,3208
-slideflow/clam/utils/__init__.py,sha256=_5HTg4O9JYF8tv8rkeqGZ0tRvLHq0drkkGzunfzrlvk,6008
-slideflow/clam/utils/core_utils.py,sha256=TxAlRK5SegmYoBq1A-3mGG1MGZswPOgw0KJRjGYrqGQ,19419
-slideflow/clam/utils/eval_utils.py,sha256=3PxG5oXOOL1GHDQDDfUGVAwjYXq9gLaaejQCNLnWi0E,3577
-slideflow/clam/utils/file_utils.py,sha256=trD4XCCbw_diNyyQ94Xdqx1LiVidhTjKjkU5yyUJ5BE,1131
 slideflow/experimental/__init__.py,sha256=XlV9ei1aGs6mm7IpfqzptPrm46WnqNOPSccdQWlhFdA,116
 slideflow/experimental/embedding_search.py,sha256=KnBkLdB6MSod8prKzkCDdOC-aq6PXCk50yJwg9JOViQ,17224
 slideflow/gan/__init__.py,sha256=PBAaQH7NByvQh6BUNk6RzppSOZ37mg6fSH1GBf6aC9A,273
 slideflow/gan/interpolate.py,sha256=ISqxULHO93ngyYrZKO2vF-DwWLCoWHg9FXjyf_g0fDI,24993
 slideflow/gan/utils.py,sha256=S6MpJTf5mK8WLiJBYHHXL_nc-BDvIgqOCBTDvUaOfiw,949
 slideflow/gan/stylegan2/__init__.py,sha256=vOW9PS4ArDZ1k15GT6kRZghBueB8N2tWHQsxHMeKfbI,45
 slideflow/gan/stylegan2/calc_metrics.py,sha256=gfyDM36IKpaAGvPFh7uhft0dFCghDN-aEjGIhU6faBo,8931
@@ -159,68 +149,60 @@
 slideflow/gan/stylegan3/stylegan3/viz/pickle_widget.py,sha256=mI8Les69cJkHkmC6gbtQyYTLyuRfIVLrj71TjgFGq80,9460
 slideflow/gan/stylegan3/stylegan3/viz/renderer.py,sha256=OaSwrKXsRWxvkl95ytUgK1YAO4MQ70E5YePV-R-rk0I,24417
 slideflow/gan/stylegan3/stylegan3/viz/stylemix_widget.py,sha256=MoZm72ZSLHz2iacQNeGrkjhmEXy6jnnxnFNzKBbcWoQ,5573
 slideflow/gan/stylegan3/stylegan3/viz/thumb_widget.py,sha256=Q3CDEvVzWEmNNhv0QvRWsovHNZX-My8gHcBJ8eJKUuo,5825
 slideflow/gan/stylegan3/stylegan3/viz/trunc_noise_widget.py,sha256=j8Is6WXvSx9iOgQdKfw2zC_V7wPPX3xBjak0gUegTH8,3845
 slideflow/grad/__init__.py,sha256=_cBjPmYSKKW6urH2xZ6llP-gthQoKCsgxfx0Lksq7cs,15662
 slideflow/grad/plot_utils.py,sha256=vkUAan14mAY3UoMCw5oqtgfiifqWcSw5BPYuD9yDIE4,7376
-slideflow/io/__init__.py,sha256=-LuHv8P21CBWvg6M_26qEwSYsREBrMnwmAdADEbYEqU,13457
+slideflow/io/__init__.py,sha256=xnHhQDCnsX3cNzz2YvGtob4cG_mo6B6z7JFXcHSNFF0,11715
 slideflow/io/gaussian.py,sha256=7mhQeBcEQmpKqq_55WST6VE67jQwFf2uq6u7ZVM99PM,10541
-slideflow/io/io_utils.py,sha256=az-nqLzunRSdgVsnAFRTJz7Icl4dfNoQrbD5xy2DlwE,9916
+slideflow/io/io_utils.py,sha256=8oneye-50sEu9eZpaCzy2IdkrCGBd3WKlVSA4s8mW9A,9918
 slideflow/io/tensorflow.py,sha256=R52b44ZkKdUUoXHGDh02v28pV1fJ07EJ1LhmYKQQRi0,34511
 slideflow/io/torch.py,sha256=OXUtAVqEC45CXsXREMmD7EVl2aCAC90P9zVWpGtYXWI,44556
 slideflow/io/preservedsite/__init__.py,sha256=9chqMmsn_iKvFwnTxZtVr4Pn08dyOeEi4YpgKhrf1Kw,68
 slideflow/io/preservedsite/crossfolds.py,sha256=UD4e0JqgZJeYl9lfrFHkCKhePJHGbsB20ZK6CA3g7bU,8419
-slideflow/mil/__init__.py,sha256=a55CIZKOyyIRkp29603xTv3CuX15dfumiYgiyWgi5G0,243
-slideflow/mil/_params.py,sha256=rGXnGIPvKbQhEKFyNj8DQwxivBDZAswPjlQdIZi0KNs,6180
+slideflow/mil/__init__.py,sha256=H_nvcxN4HLMS_rnghdaBTqeaG6gb-7sJctm0UVz38Fc,284
+slideflow/mil/_params.py,sha256=sCpjitfJxCcsx7iTnOHqD2cLXJe0Z-qxzcldUTYWgBI,14819
 slideflow/mil/data.py,sha256=rtVJwoYz7Vo-EZpNJvLDNVbh0z13dult4EXEavt8cbU,5231
-slideflow/mil/eval.py,sha256=9QPXo0EWikyhGvzQvr2gsbO6Vp0ywhWl8EuxdYIZS0w,13754
-slideflow/mil/clam/__init__.py,sha256=YEXcMLbkZuybr5yHulz_kMAVF-Z91kxnoCzkl5s1sWU,3846
+slideflow/mil/eval.py,sha256=iA0iqKZc4wC7Z6YNt5qSwUkDSSylWEWyYkMCjhPwWcA,15323
+slideflow/mil/clam/__init__.py,sha256=XFJYtb4FK5xQm7DqdmW3gBnG8IHYEjh3S1pzwFcbDKE,3890
 slideflow/mil/clam/create_attention.py,sha256=oGjZEfcT0zRqMhB8XITtFXHdw5Y9lNT9ZeiOcjntFtE,4334
 slideflow/mil/clam/datasets/__init__.py,sha256=lrqpfKa8fvcZHBzpmdJWt1uikjTNV8ZKqqpMIoRJkh4,1131
-slideflow/mil/clam/datasets/dataset_generic.py,sha256=PViQqpMdxjOOJo8Plq5pB9f7Nrb1r8RuTvTMjzLMrHE,14982
-slideflow/mil/clam/models/__init__.py,sha256=M1HPoNrZnhd3zOIQyYO1YXGBeA22ZXmi6BeSccg5774,6
-slideflow/mil/clam/models/model_clam.py,sha256=_6ruazQh1KYCLQ5ByReOERtxjhCok0XvWsolKYlbgUs,12758
-slideflow/mil/clam/models/model_mil.py,sha256=3BgYMBQQ3627_xlILq-R6dEFsXYFfXFcznfVuMC-ryA,3468
-slideflow/mil/clam/utils/__init__.py,sha256=d92ro95l8MA0YwT1ijNnzEgrdgeQKLa7UYH6WObVkbU,5910
-slideflow/mil/clam/utils/core_utils.py,sha256=_cj_lgOf_4X1brLjoA37SXHHHZd5PYR5rwIfpiakTWg,19783
+slideflow/mil/clam/datasets/dataset_generic.py,sha256=Ulj-XQigidFsdx_DAoJ-RTQnY3jZucDtZH3YJ95Sw4g,14990
+slideflow/mil/clam/utils/__init__.py,sha256=EbG-maRA3elGYWTGMzVZbQfRqf2p9I8ojMHHJkb1MgU,5914
+slideflow/mil/clam/utils/core_utils.py,sha256=hWgMxfLv4gRWwyBzS0vhDRGJ4IF_AUz6IA6SedbII7U,19782
 slideflow/mil/clam/utils/eval_utils.py,sha256=zFFeNXOP-FhUg2HrYJxG_k5BUFwso1tJo_-kpIxAmWY,4150
-slideflow/mil/clam/utils/file_utils.py,sha256=npJTuzGlsSqJWZ70Fcdp_CzzlEiOYkblalJOjf5Ll34,1283
+slideflow/mil/clam/utils/file_utils.py,sha256=uj7pGXXTKrB4MeLc2IGHO3C6UZ9980H10-nppd4gOFc,1287
 slideflow/mil/clam/utils/loss_utils.py,sha256=crw4fumbNZxcKWYCVBKMSZS6Ig3Cfr_YWfCZpC2rU3A,3497
-slideflow/mil/marugoto/__init__.py,sha256=AzX_BGT5mJkGnJvjBBXC6OkSAp2r33tDnDBgKWQHafk,38
-slideflow/mil/marugoto/data.py,sha256=tL4YRpIEeYgmJ94MK9bJC2GPoI7VEEJECR9wURKlMcY,5085
-slideflow/mil/marugoto/model.py,sha256=ZSUKoa_LLeU1lQyhnqhU6Opcow6pC8wEOiUHhSsdNFI,2797
 slideflow/mil/models/__init__.py,sha256=KbHLo_IiQOJ6wkOarvVur2kvJY2L38xmccgAYJTJOnk,185
 slideflow/mil/models/_utils.py,sha256=OzeCy0V1sY4wzcEI-4XgWa6vZ_knIH8lr5_JvosPQJM,378
 slideflow/mil/models/att_mil.py,sha256=Tor-RDpdQ1OmXo3pd_7wTT3-qZZRxVtIC3EQxsDROPg,3239
 slideflow/mil/models/clam.py,sha256=7XiRuX4dnkaevi_0WYaaTky98xwkmCO9jtnsXgtfuy0,12816
 slideflow/mil/models/mil_fc.py,sha256=xZ7t5P--bzbnJscs5iTUwxkyStbgkDnXzLB2c862HmE,3607
 slideflow/mil/models/transmil.py,sha256=CpVxNifuytkGE7oRUcc2nFWJQHWo51xGXHtn8BIvk7A,4137
-slideflow/mil/train/__init__.py,sha256=njgIGNA78T2HTdmUVfaY8ChOlpVnNWX5ucAuWVShK7g,12710
-slideflow/mil/train/_fastai.py,sha256=u8g-2MsojCo3bypjhVfwObOLj056yKs0EaucwafGTeI,8237
+slideflow/mil/train/__init__.py,sha256=SNILpMx3nYeBFYmO2Y_PoJAiC0LgdGKRAKD_pE052ec,15008
+slideflow/mil/train/_fastai.py,sha256=Af8wZL5gOaoKYq8helUE5SpfuxfcdZ8XxB24p4XXFTA,8214
 slideflow/mil/train/_legacy.py,sha256=v1s5Gqc46BZggwz7dcNyLuqVIM9lIYgUfNqTfH6d90A,7795
-slideflow/mil/transmil/__init__.py,sha256=_wVEaWFbQccvvmAuvCiNTxowsEWIbQIR0Njhzktddbs,27
-slideflow/mil/transmil/model.py,sha256=7XpYbT_BsYw6-y3Rskit-O7Gw2H6ui1ySmjp0RZ9etw,2986
-slideflow/model/__init__.py,sha256=71fbXSO371Lch3_i53lt0Hl7jUnw2wxyo1Miw3PqcBk,7497
+slideflow/model/__init__.py,sha256=oJpmQVOsQiDUhI7RqD7WC3n5dKxkrOSA5Emkfgs-Vf0,7485
 slideflow/model/adv_utils.py,sha256=qt25QlPxEQk7vJtqc82DBlkw9KgSf8KkJvpC0ATzF7s,1879
-slideflow/model/base.py,sha256=jHjsvDWe-HNpdOYed4VNG7WV2uVXl_nby7X1hsMr_eI,23032
-slideflow/model/features.py,sha256=GosGZvQHvaYbmI_0Ckeuf1ey7kWUXiXQqsX64Nb7Kj0,55285
-slideflow/model/tensorflow.py,sha256=hwaosKd57CNpBQf1GQCFaKpqfKzDFN6G-tz5t7Mgjds,110789
+slideflow/model/base.py,sha256=0tOogk4fN9FMkrJCK5K0lgh3_vVI6KQIrfgMKBPQtg4,23588
+slideflow/model/features.py,sha256=xCVmqJMzEpgICPgooccA64N-T_WFAiaJIVoyqY6r3cE,55187
+slideflow/model/tensorflow.py,sha256=qfhD2GAYr_Dgcs59Cuy2RkdILAVE6-geRg3iUicX51E,110777
 slideflow/model/tensorflow_utils.py,sha256=5HYrVE8Ph3pSXjMfSRi7vmJIC_8mD3pQd28LLj1YAEY,22547
-slideflow/model/torch.py,sha256=Nqc2eM-jZd5bUZf5G3wnGmvxkBt_a43YEBgSaX-MytU,102914
+slideflow/model/torch.py,sha256=rL7YpjxSja5-1QolWnDwJCFdtdFgOqvT5vT5goX-l88,103048
 slideflow/model/torch_utils.py,sha256=Zudk3htcyZ1wnvZRX8z8xJsnwkS8M8n63vdqBQXaf54,16945
 slideflow/model/extractors/__init__.py,sha256=MEt448aADc_WUGzTSGF5IvW4Neu2atSiAFAyxzc-4uI,394
 slideflow/model/extractors/_factory.py,sha256=CXR4fa08GLM8Q4b_S5qIO1bmtzRX1VDtYrU7PjPFUHI,3708
 slideflow/model/extractors/_factory_tensorflow.py,sha256=zpvP7rOYQEvbuZbSlFY_L7iV0dub53kHS8fAcYNnaHs,4544
 slideflow/model/extractors/_factory_torch.py,sha256=-5kOiu91eBWq_9erNSTIj7DiUYg4edVKl5SPns0_9JQ,5711
 slideflow/model/extractors/_registry.py,sha256=BBudKPe2sOGvnNZFiuFJhjcKhHZdf2gRe0hheTN8ZKo,1570
 slideflow/model/extractors/_slide.py,sha256=nC0OzgF8_11f5ql0oujeVymM8X8DJ6HU71C4ts-Q9s8,2646
 slideflow/model/extractors/ctranspath.py,sha256=fvxFcJFnd0E5KRRlUgQleXwRNfEMpnsncLLzQKRlgrw,25681
 slideflow/model/extractors/retccl.py,sha256=Qm6QGtpCS2Wc7S4EOvxzO9kUbLpJOpBvgMO1lKZI3CE,12045
-slideflow/norm/__init__.py,sha256=HRsLywWayJHpbTqLdmEwEU8PHM_UTkYEIAU6IumPPH4,27079
+slideflow/norm/__init__.py,sha256=W2OuXm74-QNXb1u3idY3XKMjOCV4E9uVLOBLLPjQ_xM,26156
 slideflow/norm/augment.py,sha256=k1uWWhhnwtapzHuq2G8_7eKJ92f-tNOrLh811vQXGxw,1605
 slideflow/norm/macenko.py,sha256=eu58kIhs8FK_6xiJr8rlzFOxxlPR1lJ2GMnAPG7Pl-Y,13438
 slideflow/norm/norm_tile.jpg,sha256=Y3wWM8FDrsfYdiMhReDBC90Rf7Io5s8Ga0UJx4oBGQk,177672
 slideflow/norm/reinhard.py,sha256=cYI--gXzLY-xpqPURpJKF8YsPIXcY0tAQ3MgVJacgwM,16964
 slideflow/norm/utils.py,sha256=dA9JzmAhz1tnzKoCBAgbTPX6iLNhfm6phd9Ng5xWrz0,15314
 slideflow/norm/vahadane.py,sha256=YIRyky8TCVqT_H95wijSbwgOPoOMhH9bApKABypLP30,7836
 slideflow/norm/tensorflow/__init__.py,sha256=EKJBEUXsly7J1j_Z5byskYVpQJzMXUXsOWMpJjrq3lM,11998
@@ -241,18 +223,18 @@
 slideflow/simclr/simclr/tf2/lars_optimizer.py,sha256=yJVRWbEDJsiVCzZnP-6_fS9VA7v36txA3DAEbVds59U,6315
 slideflow/simclr/simclr/tf2/metrics.py,sha256=RUtmvVamcnS-8_ZXLE2XdALsLgNgVPOUNl17hKPk13Y,2997
 slideflow/simclr/simclr/tf2/model.py,sha256=3p7YnZeZS09XOkPbVJ19dl77h7w3DNuf4c6rd6gqlqA,12256
 slideflow/simclr/simclr/tf2/objective.py,sha256=v5V1UzGCaSzT1i6xDe4UOaUIOhQyZ9sPDlrEP_2XYQ8,4983
 slideflow/simclr/simclr/tf2/resnet.py,sha256=BJgzhGO3TudNubzAAsx3jl8wsUW6DJ8EzivN3qlV9wY,28397
 slideflow/simclr/simclr/tf2/run.py,sha256=8Ej3YtqrKgpqlh0sjyqbUmKvLPGIRLz7XYi44dpvPE0,6524
 slideflow/simclr/simclr/tf2/utils.py,sha256=BEiKTWmuJ0XbZfG15413xPs_KnNmRg5uYL0QZEzgDYo,9517
-slideflow/slide/__init__.py,sha256=37RuR3M5LnS7UVDKWOPDewOK7ZiF4A6mu2cMRTFS1_g,111435
-slideflow/slide/report.py,sha256=Ss9gAZcF2THWe3eJQk59YY6Ql_IJl7C_v4Zu4z22n7k,18178
-slideflow/slide/slideflow-logo-name-small.jpg,sha256=woOYhxjLMDXGTjPvTSFYGNRcZ-GYXQuh0ZHQJW_jrto,20093
-slideflow/slide/utils.py,sha256=_QJlaM1amEhh5mO4VW2agLJ_hLmWoQe0SrMJOINDxKo,5515
+slideflow/slide/__init__.py,sha256=r2ip2MuMLjfjwOMmSTY2bKQ-wLkX6aY5VgXYEyKArqs,111594
+slideflow/slide/report.py,sha256=A0GEqPN5MisFk9gWuAsFq0qRBuUd1NccZgnHM7UyNHU,19045
+slideflow/slide/slideflow-logo-name-small.jpg,sha256=C9-2QV_cZkmn_FqzvorF5GvrkD__VE-7NSKME58fTR8,30934
+slideflow/slide/utils.py,sha256=5NmVU1LdFdBciwJLg_Sh9xMU4DgXEVKEB-XV7jBf2lc,5516
 slideflow/slide/backends/__init__.py,sha256=9BjTu2AaVW8zySEvLrZ-RQ21ET9kqSajYV8YkCMpAzU,708
 slideflow/slide/backends/cucim.py,sha256=dcQzgy8BzktiOY2v8t_EJn35X7OG4orC1RMWPv-qRZ4,14008
 slideflow/slide/backends/vips.py,sha256=VbBwd4S8q2-YyXl_56Nb12E8XW5-u8gWu43jXo6YN9A,22391
 slideflow/slide/qc/__init__.py,sha256=ePMfd6ZlrWdxYiRkS6gsCIMoGfS3hDPuqIyjX7kQT30,117
 slideflow/slide/qc/deepfocus_qc.py,sha256=TuHOVLYPWUD_8hYS0OFzmLvgDNAoxxuykeEcb4_xuKc,1010
 slideflow/slide/qc/gaussian.py,sha256=JGBXw0A2mDx34EZepK6A8DT22DppkZIj8P3HZcM6QEU,4304
 slideflow/slide/qc/otsu.py,sha256=l1ipll7WH8XSno5jCNf6Jdct6cVmwjiKC7IXD_L1UKA,5232
@@ -260,15 +242,15 @@
 slideflow/slide/qc/strided_dl.py,sha256=rQqqovzbjFQrPp8ZvOjewQqbL1DMZGzfEc68NrPybhA,4676
 slideflow/stats/__init__.py,sha256=PoUMiNzTv2ra_wvkFgwz7ZjJRu0G_aD2tebhEc01BKw,390
 slideflow/stats/delong.py,sha256=C6QaDckNp9FDmwjPkoHDv8qTaT_Yaipa-ky-43tDBtM,4293
 slideflow/stats/metrics.py,sha256=h0cR4Yuh1FziuL0RZNGrNlDn69c-Q6ASL2aFTqiRqMw,35785
 slideflow/stats/plot.py,sha256=qnyOA--h_SmGR0uyGGRXVQhk-Z3wRMM38__leADAtrQ,5757
 slideflow/stats/slidemap.py,sha256=vXS39-CyQ-3WA_ILUQs99GcBTtKjWyUjw2ebVlyMpuQ,43101
 slideflow/stats/stats_utils.py,sha256=jk7vmXMk_cGUrMwT0vGc1JSwnNCXj97LoWS7IzN8G6k,3261
-slideflow/studio/__init__.py,sha256=L0mitLY2OXcjmMGEfe965avRAxKsUQJCQmFkgjqzPks,77586
+slideflow/studio/__init__.py,sha256=pwtCiDS_IMebIAytNu8p1XH3OgY_YJRMWz443mE_P1w,77576
 slideflow/studio/__main__.py,sha256=mjuK2AaJaTFqapCI3ZG26_-OkQTSsH4Xyvvt1GLBl8w,2187
 slideflow/studio/_renderer.py,sha256=oPASsmG9hztqFcstTh7MXZdxkriWTWWluQDDI39-XdU,19662
 slideflow/studio/utils.py,sha256=uP3mb3Xq6UgPVdRaJ0365KhTR4-rQQWOrqzQEw14lX4,3686
 slideflow/studio/gui/__init__.py,sha256=9_8wL9Scv8_Cs8HJyJHGvx1vwXErsuvlsAqNZLcJQR0,8
 slideflow/studio/gui/_glfw.py,sha256=D1CberJgfOn0gG9A3SvoLC0HRKvjvnoG0Mky_h45Ljk,14387
 slideflow/studio/gui/annotator.py,sha256=VUqx2cUw754W5JmAyuuw-or7oAm_nW-_R0HgDIqS7fw,4116
 slideflow/studio/gui/gl_utils.py,sha256=HAdhcChvrthkOl5c8DGfP04-G4nahCM7Ukdq188xvzI,11035
@@ -327,83 +309,51 @@
 slideflow/studio/gui/icons/warn.png,sha256=mAVKiVI85LF10roGtrrOf43DE5gpR4XORYh3z62iACU,6817
 slideflow/studio/gui/viewer/__init__.py,sha256=9wiXSrhD8Raz5AVGdCBrF-Fykn1sNqkqGgjfXtRwNuU,107
 slideflow/studio/gui/viewer/_mosaic.py,sha256=dKDyGt12-f9sju5g0plSJUoOiP4uYAH2emiK3vQFUb0,7874
 slideflow/studio/gui/viewer/_slide.py,sha256=64C67XjagXAQMul4XLN237q2EWGElP4Nt_kjM2dwBGg,24818
 slideflow/studio/gui/viewer/_viewer.py,sha256=M_aVUL7tZRyOI2OhI3xiRUVpQe3kunoLAt5kfRMxZSM,14452
 slideflow/studio/widgets/__init__.py,sha256=M8gSPhj8W4FThgSm5kMF1GuAffF1PkauBfU5xtwAmxI,439
 slideflow/studio/widgets/_utils.py,sha256=jI9Ah-aO4SPN8CdBB0pWV2-bxifLSW6DpGK106938js,735
-slideflow/studio/widgets/annotation.py,sha256=TBTpzFmWD3GcZiJ2C4qX7o8-GHoeVDaSWTWiBHc2Lus,6651
 slideflow/studio/widgets/capture.py,sha256=MtXvouNKrMcxfmHVN4xBv1ODK-clnC3-QzH8waowHmc,4264
-slideflow/studio/widgets/extensions.py,sha256=qeIXWRoKKUXIymLo9dVpUkxtvF8DCNNQzunFQaHgyCc,3817
+slideflow/studio/widgets/extensions.py,sha256=V7u_T1t5IxiqRfiza9ru5UgvPBUruvLI7dXmTBwlnm8,5412
 slideflow/studio/widgets/heatmap.py,sha256=SWkACMxN5Strm3DwOLvEe14thVwQFa9D5cOAGJNYGKc,17013
 slideflow/studio/widgets/layer_umap.py,sha256=Dol3NIxJge6Ff2e80Ydjvfh0xc8CBMb34DmM-n1lPmg,3842
 slideflow/studio/widgets/model.py,sha256=T50CTM8jt1e_lNrmh4zx7XOQdTh9S6RApmnJp1Gh7fw,24474
 slideflow/studio/widgets/mosaic.py,sha256=u1a_HvZ20PWuzdQ1EVAWIP9HOUE85m42OvLDY0i4pKk,14307
 slideflow/studio/widgets/mosaic_experimental.py,sha256=_Z3huwfE3uJeP3-Fs_PwAvJ00T8ekykeWBvTH2AXN4k,2597
 slideflow/studio/widgets/performance.py,sha256=kTk3fuS6M5mQ5K4pmKZvx5sVJC070IHImfPTCQbxNec,4662
 slideflow/studio/widgets/picam.py,sha256=K_z4pt1-VvU0HkWfyO0NsOEqne4utfr8blZjpxoIvRI,6437
 slideflow/studio/widgets/project.py,sha256=naeSkPx8X_euzt2DBU0LLcI7Lod_824ygcf5m6QPrpQ,7726
 slideflow/studio/widgets/seed_map.py,sha256=b4ftLvj5ySevm9zKjhcKN-HOQSYlza1JKEhknw83aa4,5983
 slideflow/studio/widgets/segment.py,sha256=ocfTzY3fansXaQoPGFUxvVBg2nbzbH8KzRtUpzjQ9-E,19875
 slideflow/studio/widgets/settings.py,sha256=L6etk5UsWrzLxa_5c7S70ABTllviiFAKPZFLw4fFE7o,2071
 slideflow/studio/widgets/slide.py,sha256=rPsEjfW3aKpGOHMmkpjqUnOQO-d7ufmoiJ3n5tEDutY,31074
 slideflow/studio/widgets/stylegan.py,sha256=wV9mYOpEr11jRQd2e-yNHrbyShyjGaUsIKAvNmvjY4k,16402
-slideflow/test/__init__.py,sha256=SkIKXL2Ua1pGxhdK3a7CIBdgJE3MmBYxLhbHSjlTntE,32107
+slideflow/test/__init__.py,sha256=5IsYXrynGmJDMLmjueGyMzBVbifbmEBAzSEGf_HRhw4,32309
 slideflow/test/dataset_test.py,sha256=XJOx1T-J5Rcsjwx54kLY6-GuOQEgUtO_-a6sW9oXVvE,12446
 slideflow/test/functional.py,sha256=V6vs6N-ty49oan2KbM80nT4EmMis9-mQ3UZbVuH-woI,9560
 slideflow/test/model_test.py,sha256=rLiz27NDLOukTbTEPUL88a-aPeSe2ePgScatV4VM7_s,8052
 slideflow/test/norm_test.py,sha256=au7oXuHg-LzfX84nJw6-mSEKa4R46JL4eu9j7hW_r4U,12098
 slideflow/test/slide_test.py,sha256=P-cmg5JpL2LgpyrP61lF5YMpZ9vqmR6sSuJjNGFhIso,2909
 slideflow/test/stats_test.py,sha256=_n_9JHZZGSReWCR3usfid1uiFxGJ363UyFL0ah9U7f8,11481
 slideflow/test/utils.py,sha256=Qxmvs8FAxnVLOKjqstQIsE6DKZORPsvpRMVKsZmwMbQ,11752
 slideflow/tfrecord/__init__.py,sha256=YsF14xnyOnYgR2CCaj6zl0_YSeF7z3CqxZ0yO11dll0,891
 slideflow/tfrecord/iterator_utils.py,sha256=IFRjADsWLpfN65GSZp_1F2CEcgPRw77FszXHNcSegvs,2905
 slideflow/tfrecord/reader.py,sha256=vGf6X-5JRMeUQN1gDDvfXJ131QbHEHhK16bq8CDN_Bw,15505
 slideflow/tfrecord/writer.py,sha256=47JbvdMp9xcx3RgacsIXZ0MlDm2bjREIYWG-XlcFzpQ,5637
 slideflow/tfrecord/tools/__init__.py,sha256=3082iuyLMnQ8Gc2WFwulW1sSJwBqIJPnBNoMT-VuW-o,179
 slideflow/tfrecord/torch/__init__.py,sha256=zl9XVfdCnojlhGDx0mwtqCbjMyj1ypROP-Ut2J_M7eQ,310
 slideflow/tfrecord/torch/dataset.py,sha256=Vc9q1vHG0xTtMu-tsi-kX1c9wM95BNWb9r5CWzh8B0w,7857
-slideflow/util/__init__.py,sha256=t5_iK4a2pT3kXhXLdeMVICiED_MoN8TV3buxQjNN3RU,40040
+slideflow/util/__init__.py,sha256=xaoOXySQBk5xPlk7arNDV1ZAp4ohvvh0dsBw6KNLiOg,41104
 slideflow/util/colors.py,sha256=KOfggJhBNe3uHYa4MNQYdJnOb3A-Nyl1lxIMQ6Sqivc,738
 slideflow/util/example_pb2.py,sha256=oU4oQBXz89HAyrehrDeK-uJYqL7eNznjd8y3YDwn0dY,17912
 slideflow/util/log_utils.py,sha256=kyfTOCmSJW7gIW_1nHXiyPydN-jyO8YEBM3wiEb5OUc,4468
 slideflow/util/neptune_utils.py,sha256=rus5wDerStaFQalTbF9VaEbi6OelhkpkUHmwt0ggejQ,4381
 slideflow/util/smac_utils.py,sha256=T_-b1Wv0V2fQVX-FGlnhX1i4bu69imixlbAJd2aNJgs,20061
-slideflow/util/tfrecord2idx.py,sha256=1ZrAEiFviuypOL03y1R7n0RWsOmEv9hMgh4RiEllR-g,6490
-slideflow/workbench/__init__.py,sha256=S8vEyeAxgc5p6gKB0Ez2rB0lW48EIqFn12ZpOCHU61o,64602
-slideflow/workbench/__main__.py,sha256=pPuOxeBN6r4MaOjCYbjtAbNgT_EQdUQ6KUjk5CTmw4o,2750
-slideflow/workbench/capture_widget.py,sha256=F2NCZgfflAt5aboWhLCo-X5yh1DgZA53aAIvtOYHLF0,4329
-slideflow/workbench/heatmap_widget.py,sha256=VyhUi-aneWs2ydRkf7oPUmRCSdDrpEBlH2pORY057d0,19460
-slideflow/workbench/layer_umap_widget.py,sha256=N9wlF4QQZy3mblzRuFY4JJ6064hLVVPdLau5Ab8ZVYM,3847
-slideflow/workbench/model_widget.py,sha256=1ilQvmoXpE9bfflAyAnmnAPzmJQgUpVcUQnHvOg2KYA,15653
-slideflow/workbench/mosaic_widget.py,sha256=kzRuiuj7isOyisEWTSdkMBOBBL-HS8uMOATOjhBhsyA,11488
-slideflow/workbench/performance_widget.py,sha256=4TfYe6UUlqAWRYNjv0aJyxKbATi7_KGdbLfkrmeE8RQ,3982
-slideflow/workbench/picam_widget.py,sha256=P9EN60gxEINNBaWPLfWoFKm20aMfQ72lzwthCcoayAg,6465
-slideflow/workbench/project_widget.py,sha256=fjjaC01bNohA0nW0tkwWkQTUecOR-yniX1Sw0kiB0mY,4617
-slideflow/workbench/seed_map_widget.py,sha256=kh01QYOSuMoDzNz2dyoMbWw_CnA3BXL1aSFLOhY_w90,6045
-slideflow/workbench/segment_widget.py,sha256=aa4j64MYd63mZF-Lc9qq0WF0fmJWAxfnrS-NCSAzlco,19721
-slideflow/workbench/slide_renderer.py,sha256=-nr4umTOBorF5buhq45PLOXHyNudE3lmKOC-m5auWYg,12956
-slideflow/workbench/slide_widget.py,sha256=YmKBUDR0EGYLOEF6MvNsfA1_vM_ZawyGhT7LNuA2q20,26409
-slideflow/workbench/utils.py,sha256=lA-gVvl_Qw94qg4SWAbMt2ynb6Gn9ntJvqmesB3EN9Y,3582
-slideflow/workbench/gui_utils/DroidSans-Bold.ttf,sha256=L1KaPmDAB5edldKXlMNmBpQhf7iCQp-zORnSJF_paek,194488
-slideflow/workbench/gui_utils/DroidSans.ttf,sha256=9RuIlF9MGyNvRLjVWi0wQxaGkSfpUkjENcI_HkFCp9s,190776
-slideflow/workbench/gui_utils/__init__.py,sha256=9_8wL9Scv8_Cs8HJyJHGvx1vwXErsuvlsAqNZLcJQR0,8
-slideflow/workbench/gui_utils/gl_utils.py,sha256=v59VwD0yTnIioH5atJ6tTEe0L_LYgiSWIwslCMz6u38,17670
-slideflow/workbench/gui_utils/glfw_window.py,sha256=8zBLC2FXAXDBnbYKUrF7nrXrlkeBOOZL5I1s5Wfkz1Y,9432
-slideflow/workbench/gui_utils/imgui_utils.py,sha256=fr1TvLBO3kXoeI75-JpnzZ-A2jG5nnCjCC9vaeOx8rc,11859
-slideflow/workbench/gui_utils/imgui_window.py,sha256=nYoPhaU-AoHh71cTJ601BjAJHyWSGQKtCA0X9NACa2Q,14815
-slideflow/workbench/gui_utils/mosaic_viewer.py,sha256=Dg342fcdp1V5KkH9Me9taBpwfPsVZElPfVQaodAV2uo,7577
-slideflow/workbench/gui_utils/text_utils.py,sha256=8u_NIOU10NA2MpvSVtrmh9e-Kq2ECfcQY1e9OI1q8Qs,6043
-slideflow/workbench/gui_utils/viewer.py,sha256=PNncjLLVzcECLfrLgEgiosPbWIecV2_Jp9dE6UP4b8s,14450
-slideflow/workbench/gui_utils/wsi_utils.py,sha256=7AC_y2A1jsTWC5DTLl-kMHdLPA-Syx94Ql7p0oTJ2-E,21988
-slideflow/workbench/gui_utils/icons/error.png,sha256=XQOSYuK4Gipt36VpGPGYse7HbnPnxoqi5oLKxmNvFeU,7568
-slideflow/workbench/gui_utils/icons/info.png,sha256=vqiqW4eXPfcrj_m7FyKajAq8r3YI3AkDGzhUp0__jG8,7286
-slideflow/workbench/gui_utils/icons/logo.png,sha256=--xn2jp0GtkzQIIptiQAip_n3LqIMTetlll67GTM784,37355
-slideflow/workbench/gui_utils/icons/success.png,sha256=BxXVANTnqknCN79rZabK-gBap_3YrCeCvar8Fxxr0Jc,7181
-slideflow/workbench/gui_utils/icons/warn.png,sha256=mAVKiVI85LF10roGtrrOf43DE5gpR4XORYh3z62iACU,6817
-slideflow-2.0.0b1.data/scripts/slideflow-studio,sha256=EKOvjx1aRWAFjXKTUKPhYbYkuepGBmGwi1outdQGMQY,14329
-slideflow-2.0.0b1.data/scripts/slideflow-studio.py,sha256=XiVSnUMHfSbaqoDOFmkiYP_WbQo8FSdTmd0h-JyP2No,13814
-slideflow-2.0.0b1.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-slideflow-2.0.0b1.dist-info/METADATA,sha256=FurSHLTnyv8p_nxsdkrZqIquucdpNhpXSmP-P_gdzSE,12341
-slideflow-2.0.0b1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-slideflow-2.0.0b1.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
-slideflow-2.0.0b1.dist-info/RECORD,,
+slideflow/util/tfrecord2idx.py,sha256=k3QHnE5T1o_qPHT-mRKBomfyPha9Ud_zsS-zKA_EKlY,7920
+slideflow-2.0.1.data/scripts/slideflow-studio,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
+slideflow-2.0.1.data/scripts/slideflow-studio.py,sha256=dYg_ktDH_I74oIs8Sj4l56M-03frzO4KisE7WeXAofI,14085
+slideflow-2.0.1.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+slideflow-2.0.1.dist-info/METADATA,sha256=icziIiXMBAeCIuv_kO0y6-7km7X9eXlMbY1x4G6X9zs,12998
+slideflow-2.0.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+slideflow-2.0.1.dist-info/top_level.txt,sha256=FRikcoh3_TcsLUYbI4LowF3nrEDWcGLaAyNgsi9Lu9M,10
+slideflow-2.0.1.dist-info/RECORD,,
```

