# Comparing `tmp/RegScale_CLI-4.24.1-py3-none-any.whl.zip` & `tmp/RegScale_CLI-4.25.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,84 +1,85 @@
-Zip file size: 195128 bytes, number of entries: 82
--rw-r--r--  2.0 unx    14494 b- defN 23-Apr-06 16:27 regscale.py
--rw-r--r--  2.0 unx       47 b- defN 23-Apr-06 16:27 app/__init__.py
--rw-r--r--  2.0 unx       73 b- defN 23-Apr-06 16:27 app/_version.py
--rw-r--r--  2.0 unx    14024 b- defN 23-Apr-06 16:27 app/api.py
--rw-r--r--  2.0 unx    12991 b- defN 23-Apr-06 16:27 app/application.py
--rw-r--r--  2.0 unx     1191 b- defN 23-Apr-06 16:27 app/logz.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 app/commercial/__init__.py
--rw-r--r--  2.0 unx    16432 b- defN 23-Apr-06 16:27 app/commercial/ad.py
--rw-r--r--  2.0 unx    46852 b- defN 23-Apr-06 16:27 app/commercial/defender.py
--rw-r--r--  2.0 unx     8101 b- defN 23-Apr-06 16:27 app/commercial/jira.py
--rw-r--r--  2.0 unx    27597 b- defN 23-Apr-06 16:27 app/commercial/okta.py
--rw-r--r--  2.0 unx    11726 b- defN 23-Apr-06 16:27 app/commercial/servicenow.py
--rw-r--r--  2.0 unx    22914 b- defN 23-Apr-06 16:27 app/commercial/tenable.py
--rw-r--r--  2.0 unx    56612 b- defN 23-Apr-06 16:27 app/commercial/wiz.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 app/internal/__init__.py
--rw-r--r--  2.0 unx    27853 b- defN 23-Apr-06 16:27 app/internal/admin_actions.py
--rw-r--r--  2.0 unx    31774 b- defN 23-Apr-06 16:27 app/internal/assessments_editor.py
--rw-r--r--  2.0 unx    16287 b- defN 23-Apr-06 16:27 app/internal/comparison.py
--rw-r--r--  2.0 unx    15286 b- defN 23-Apr-06 16:27 app/internal/control_editor.py
--rw-r--r--  2.0 unx     5912 b- defN 23-Apr-06 16:27 app/internal/encrypt.py
--rw-r--r--  2.0 unx    40841 b- defN 23-Apr-06 16:27 app/internal/evidence.py
--rw-r--r--  2.0 unx     2330 b- defN 23-Apr-06 16:27 app/internal/healthcheck.py
--rw-r--r--  2.0 unx     6394 b- defN 23-Apr-06 16:27 app/internal/login.py
--rw-r--r--  2.0 unx     9419 b- defN 23-Apr-06 16:27 app/internal/migrations.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 app/public/__init__.py
--rw-r--r--  2.0 unx    18801 b- defN 23-Apr-06 16:27 app/public/cisa.py
--rw-r--r--  2.0 unx    10226 b- defN 23-Apr-06 16:27 app/public/emass.py
--rw-r--r--  2.0 unx    64892 b- defN 23-Apr-06 16:27 app/public/fedramp.py
--rw-r--r--  2.0 unx     7932 b- defN 23-Apr-06 16:27 app/public/nist_catalog.py
--rw-r--r--  2.0 unx    71195 b- defN 23-Apr-06 16:27 app/public/oscal.py
--rw-r--r--  2.0 unx     6183 b- defN 23-Apr-06 16:27 app/public/otx.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 app/utils/__init__.py
--rw-r--r--  2.0 unx    19729 b- defN 23-Apr-06 16:27 app/utils/app_utils.py
--rw-r--r--  2.0 unx    10417 b- defN 23-Apr-06 16:27 app/utils/regscale_utils.py
--rw-r--r--  2.0 unx     1846 b- defN 23-Apr-06 16:27 app/utils/threadhandler.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 exceptions/__init__.py
--rw-r--r--  2.0 unx      195 b- defN 23-Apr-06 16:27 exceptions/license_exception.py
--rw-r--r--  2.0 unx       79 b- defN 23-Apr-06 16:27 models/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 models/app_models/__init__.py
--rw-r--r--  2.0 unx     4112 b- defN 23-Apr-06 16:27 models/app_models/click.py
--rw-r--r--  2.0 unx    14069 b- defN 23-Apr-06 16:27 models/app_models/control_editor.py
--rw-r--r--  2.0 unx      889 b- defN 23-Apr-06 16:27 models/app_models/pipeline.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 models/integration_models/__init__.py
--rw-r--r--  2.0 unx     7858 b- defN 23-Apr-06 16:27 models/integration_models/azure_alerts.py
--rw-r--r--  2.0 unx      872 b- defN 23-Apr-06 16:27 models/integration_models/recommendations.py
--rw-r--r--  2.0 unx     8319 b- defN 23-Apr-06 16:27 models/integration_models/tenable.py
--rw-r--r--  2.0 unx     1833 b- defN 23-Apr-06 16:27 models/integration_models/wiz.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 models/regscale_models/__init__.py
--rw-r--r--  2.0 unx     6688 b- defN 23-Apr-06 16:27 models/regscale_models/assessment.py
--rw-r--r--  2.0 unx     5224 b- defN 23-Apr-06 16:27 models/regscale_models/asset.py
--rw-r--r--  2.0 unx     1237 b- defN 23-Apr-06 16:27 models/regscale_models/components.py
--rw-r--r--  2.0 unx     7831 b- defN 23-Apr-06 16:27 models/regscale_models/control_implementation.py
--rw-r--r--  2.0 unx     2850 b- defN 23-Apr-06 16:27 models/regscale_models/interconnects.py
--rw-r--r--  2.0 unx     4256 b- defN 23-Apr-06 16:27 models/regscale_models/issue.py
--rw-r--r--  2.0 unx     4891 b- defN 23-Apr-06 16:27 models/regscale_models/modules.py
--rw-r--r--  2.0 unx     2353 b- defN 23-Apr-06 16:27 models/regscale_models/ports_protocols.py
--rw-r--r--  2.0 unx     2415 b- defN 23-Apr-06 16:27 models/regscale_models/requirements.py
--rw-r--r--  2.0 unx     5886 b- defN 23-Apr-06 16:27 models/regscale_models/securityplans.py
--rw-r--r--  2.0 unx     1426 b- defN 23-Apr-06 16:27 models/regscale_models/threat.py
--rw-r--r--  2.0 unx     2313 b- defN 23-Apr-06 16:27 models/regscale_models/user.py
--rw-r--r--  2.0 unx        0 b- defN 23-Apr-06 16:27 tests/__init__.py
--rw-r--r--  2.0 unx     1648 b- defN 23-Apr-06 16:27 tests/conftest.py
--rw-r--r--  2.0 unx     1334 b- defN 23-Apr-06 16:27 tests/test_app_utils.py
--rw-r--r--  2.0 unx     1930 b- defN 23-Apr-06 16:27 tests/test_assessments_editor.py
--rw-r--r--  2.0 unx     1687 b- defN 23-Apr-06 16:27 tests/test_cisa.py
--rw-r--r--  2.0 unx     1747 b- defN 23-Apr-06 16:27 tests/test_control_editor.py
--rw-r--r--  2.0 unx     1425 b- defN 23-Apr-06 16:27 tests/test_dependabot.py
--rw-r--r--  2.0 unx     2811 b- defN 23-Apr-06 16:27 tests/test_emass.py
--rw-r--r--  2.0 unx    14035 b- defN 23-Apr-06 16:27 tests/test_evidence.py
--rw-r--r--  2.0 unx     1186 b- defN 23-Apr-06 16:27 tests/test_login.py
--rw-r--r--  2.0 unx     1416 b- defN 23-Apr-06 16:27 tests/test_npm_audit.py
--rw-r--r--  2.0 unx     9248 b- defN 23-Apr-06 16:27 tests/test_oscal.py
--rw-r--r--  2.0 unx      866 b- defN 23-Apr-06 16:27 tests/test_snow.py
--rw-r--r--  2.0 unx     1245 b- defN 23-Apr-06 16:27 tests/test_sonarcloud.py
--rw-r--r--  2.0 unx     1666 b- defN 23-Apr-06 16:27 tests/test_tenable.py
--rw-r--r--  2.0 unx     1254 b- defN 23-Apr-06 16:27 tests/test_update_regscale_config.py
--rw-r--r--  2.0 unx     1076 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/LICENSE
--rw-r--r--  2.0 unx     6195 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       43 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       37 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6814 b- defN 23-Apr-06 16:27 RegScale_CLI-4.24.1.dist-info/RECORD
-82 files, 743722 bytes uncompressed, 184444 bytes compressed:  75.2%
+Zip file size: 201432 bytes, number of entries: 83
+-rw-r--r--  2.0 unx    14547 b- defN 23-Apr-12 13:37 regscale.py
+-rw-r--r--  2.0 unx       47 b- defN 23-Apr-12 13:37 app/__init__.py
+-rw-r--r--  2.0 unx       73 b- defN 23-Apr-12 13:37 app/_version.py
+-rw-r--r--  2.0 unx    14197 b- defN 23-Apr-12 13:37 app/api.py
+-rw-r--r--  2.0 unx    13397 b- defN 23-Apr-12 13:37 app/application.py
+-rw-r--r--  2.0 unx     1287 b- defN 23-Apr-12 13:37 app/logz.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 app/commercial/__init__.py
+-rw-r--r--  2.0 unx    16359 b- defN 23-Apr-12 13:37 app/commercial/ad.py
+-rw-r--r--  2.0 unx    45796 b- defN 23-Apr-12 13:37 app/commercial/defender.py
+-rw-r--r--  2.0 unx     8007 b- defN 23-Apr-12 13:37 app/commercial/jira.py
+-rw-r--r--  2.0 unx    27240 b- defN 23-Apr-12 13:37 app/commercial/okta.py
+-rw-r--r--  2.0 unx    27892 b- defN 23-Apr-12 13:37 app/commercial/qualys.py
+-rw-r--r--  2.0 unx    11689 b- defN 23-Apr-12 13:37 app/commercial/servicenow.py
+-rw-r--r--  2.0 unx    22634 b- defN 23-Apr-12 13:37 app/commercial/tenable.py
+-rw-r--r--  2.0 unx    56457 b- defN 23-Apr-12 13:37 app/commercial/wiz.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 app/internal/__init__.py
+-rw-r--r--  2.0 unx    26998 b- defN 23-Apr-12 13:37 app/internal/admin_actions.py
+-rw-r--r--  2.0 unx    31042 b- defN 23-Apr-12 13:37 app/internal/assessments_editor.py
+-rw-r--r--  2.0 unx    16114 b- defN 23-Apr-12 13:37 app/internal/comparison.py
+-rw-r--r--  2.0 unx    15277 b- defN 23-Apr-12 13:37 app/internal/control_editor.py
+-rw-r--r--  2.0 unx     5912 b- defN 23-Apr-12 13:37 app/internal/encrypt.py
+-rw-r--r--  2.0 unx    39915 b- defN 23-Apr-12 13:37 app/internal/evidence.py
+-rw-r--r--  2.0 unx     2325 b- defN 23-Apr-12 13:37 app/internal/healthcheck.py
+-rw-r--r--  2.0 unx     6397 b- defN 23-Apr-12 13:37 app/internal/login.py
+-rw-r--r--  2.0 unx     9419 b- defN 23-Apr-12 13:37 app/internal/migrations.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 app/public/__init__.py
+-rw-r--r--  2.0 unx    18601 b- defN 23-Apr-12 13:37 app/public/cisa.py
+-rw-r--r--  2.0 unx    10193 b- defN 23-Apr-12 13:37 app/public/emass.py
+-rw-r--r--  2.0 unx    63918 b- defN 23-Apr-12 13:37 app/public/fedramp.py
+-rw-r--r--  2.0 unx     7946 b- defN 23-Apr-12 13:37 app/public/nist_catalog.py
+-rw-r--r--  2.0 unx    71111 b- defN 23-Apr-12 13:37 app/public/oscal.py
+-rw-r--r--  2.0 unx     6081 b- defN 23-Apr-12 13:37 app/public/otx.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 app/utils/__init__.py
+-rw-r--r--  2.0 unx    19408 b- defN 23-Apr-12 13:37 app/utils/app_utils.py
+-rw-r--r--  2.0 unx    10375 b- defN 23-Apr-12 13:37 app/utils/regscale_utils.py
+-rw-r--r--  2.0 unx     1591 b- defN 23-Apr-12 13:37 app/utils/threadhandler.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 exceptions/__init__.py
+-rw-r--r--  2.0 unx      195 b- defN 23-Apr-12 13:37 exceptions/license_exception.py
+-rw-r--r--  2.0 unx       79 b- defN 23-Apr-12 13:37 models/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 models/app_models/__init__.py
+-rw-r--r--  2.0 unx     3997 b- defN 23-Apr-12 13:37 models/app_models/click.py
+-rw-r--r--  2.0 unx    13863 b- defN 23-Apr-12 13:37 models/app_models/control_editor.py
+-rw-r--r--  2.0 unx      889 b- defN 23-Apr-12 13:37 models/app_models/pipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 models/integration_models/__init__.py
+-rw-r--r--  2.0 unx     7858 b- defN 23-Apr-12 13:37 models/integration_models/azure_alerts.py
+-rw-r--r--  2.0 unx      872 b- defN 23-Apr-12 13:37 models/integration_models/recommendations.py
+-rw-r--r--  2.0 unx     8319 b- defN 23-Apr-12 13:37 models/integration_models/tenable.py
+-rw-r--r--  2.0 unx     1833 b- defN 23-Apr-12 13:37 models/integration_models/wiz.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 models/regscale_models/__init__.py
+-rw-r--r--  2.0 unx     6688 b- defN 23-Apr-12 13:37 models/regscale_models/assessment.py
+-rw-r--r--  2.0 unx     5433 b- defN 23-Apr-12 13:37 models/regscale_models/asset.py
+-rw-r--r--  2.0 unx     1237 b- defN 23-Apr-12 13:37 models/regscale_models/components.py
+-rw-r--r--  2.0 unx     7805 b- defN 23-Apr-12 13:37 models/regscale_models/control_implementation.py
+-rw-r--r--  2.0 unx     2850 b- defN 23-Apr-12 13:37 models/regscale_models/interconnects.py
+-rw-r--r--  2.0 unx     4175 b- defN 23-Apr-12 13:37 models/regscale_models/issue.py
+-rw-r--r--  2.0 unx     4878 b- defN 23-Apr-12 13:37 models/regscale_models/modules.py
+-rw-r--r--  2.0 unx     2353 b- defN 23-Apr-12 13:37 models/regscale_models/ports_protocols.py
+-rw-r--r--  2.0 unx     2415 b- defN 23-Apr-12 13:37 models/regscale_models/requirements.py
+-rw-r--r--  2.0 unx     5886 b- defN 23-Apr-12 13:37 models/regscale_models/securityplans.py
+-rw-r--r--  2.0 unx     1398 b- defN 23-Apr-12 13:37 models/regscale_models/threat.py
+-rw-r--r--  2.0 unx     2242 b- defN 23-Apr-12 13:37 models/regscale_models/user.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Apr-12 13:37 tests/__init__.py
+-rw-r--r--  2.0 unx     1818 b- defN 23-Apr-12 13:37 tests/conftest.py
+-rw-r--r--  2.0 unx     1334 b- defN 23-Apr-12 13:37 tests/test_app_utils.py
+-rw-r--r--  2.0 unx     1930 b- defN 23-Apr-12 13:37 tests/test_assessments_editor.py
+-rw-r--r--  2.0 unx     1687 b- defN 23-Apr-12 13:37 tests/test_cisa.py
+-rw-r--r--  2.0 unx     1747 b- defN 23-Apr-12 13:37 tests/test_control_editor.py
+-rw-r--r--  2.0 unx     1425 b- defN 23-Apr-12 13:37 tests/test_dependabot.py
+-rw-r--r--  2.0 unx     2811 b- defN 23-Apr-12 13:37 tests/test_emass.py
+-rw-r--r--  2.0 unx    14035 b- defN 23-Apr-12 13:37 tests/test_evidence.py
+-rw-r--r--  2.0 unx     1186 b- defN 23-Apr-12 13:37 tests/test_login.py
+-rw-r--r--  2.0 unx     1416 b- defN 23-Apr-12 13:37 tests/test_npm_audit.py
+-rw-r--r--  2.0 unx     9248 b- defN 23-Apr-12 13:37 tests/test_oscal.py
+-rw-r--r--  2.0 unx      866 b- defN 23-Apr-12 13:37 tests/test_snow.py
+-rw-r--r--  2.0 unx     1245 b- defN 23-Apr-12 13:37 tests/test_sonarcloud.py
+-rw-r--r--  2.0 unx     1666 b- defN 23-Apr-12 13:37 tests/test_tenable.py
+-rw-r--r--  2.0 unx     1254 b- defN 23-Apr-12 13:37 tests/test_update_regscale_config.py
+-rw-r--r--  2.0 unx     1076 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6330 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       43 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       37 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     6896 b- defN 23-Apr-12 13:37 RegScale_CLI-4.25.0.dist-info/RECORD
+83 files, 765652 bytes uncompressed, 190624 bytes compressed:  75.1%
```

## zipnote {}

```diff
@@ -27,14 +27,17 @@
 
 Filename: app/commercial/jira.py
 Comment: 
 
 Filename: app/commercial/okta.py
 Comment: 
 
+Filename: app/commercial/qualys.py
+Comment: 
+
 Filename: app/commercial/servicenow.py
 Comment: 
 
 Filename: app/commercial/tenable.py
 Comment: 
 
 Filename: app/commercial/wiz.py
@@ -222,26 +225,26 @@
 
 Filename: tests/test_tenable.py
 Comment: 
 
 Filename: tests/test_update_regscale_config.py
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/LICENSE
+Filename: RegScale_CLI-4.25.0.dist-info/LICENSE
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/METADATA
+Filename: RegScale_CLI-4.25.0.dist-info/METADATA
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/WHEEL
+Filename: RegScale_CLI-4.25.0.dist-info/WHEEL
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/entry_points.txt
+Filename: RegScale_CLI-4.25.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/top_level.txt
+Filename: RegScale_CLI-4.25.0.dist-info/top_level.txt
 Comment: 
 
-Filename: RegScale_CLI-4.24.1.dist-info/RECORD
+Filename: RegScale_CLI-4.25.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## regscale.py

```diff
@@ -1,19 +1,19 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ Main script for starting RegScale CLI application """
 
 # standard python imports
 import os
 import sys
+from getpass import getpass
+from urllib.parse import urlparse
 
 import click
-from getpass import getpass
 from rich.console import Console
-from urllib.parse import urlparse
 
 # Fitz library requires this static directory in the PWD.
 if not os.path.exists("./static"):
     os.makedirs("./static")
 
 ############################################################
 # Versioning
@@ -30,31 +30,32 @@
 ############################################################
 # Commercial Integrations
 ############################################################
 from app.commercial.ad import ad
 from app.commercial.defender import defender
 from app.commercial.jira import jira
 from app.commercial.okta import okta
+from app.commercial.qualys import qualys
 from app.commercial.servicenow import servicenow
 from app.commercial.tenable import tenable
 from app.commercial.wiz import wiz
 
 ############################################################
 # Internal Integrations
 ############################################################
 import app.internal.healthcheck as hc
 import app.internal.login as lg
 
 from app.internal.admin_actions import actions
 from app.internal.comparison import compare
-from app.internal.control_editor import control_editor
 from app.internal.assessments_editor import assessments
 from app.internal.encrypt import JH0847, IOA21H98, YO9322
 from app.internal.evidence import evidence
 from app.internal.migrations import migrations
+from app.internal.control_editor import control_editor
 
 ############################################################
 # Public Integrations
 ############################################################
 from app.public.cisa import cisa
 from app.public.emass import emass
 from app.public.fedramp import fedramp
@@ -86,22 +87,22 @@
     """Provides information about the CLI and its current version."""
     bannerv2()
     about_display()
 
 
 def about_display():
     """Provides information about the CLI and its current version."""
-    console.print("[red]RegScale[/red] CLI Version: " + __version__)
+    console.print(f"[red]RegScale[/red] CLI Version: {__version__}")
     console.print("Author: J. Travis Howerton (thowerton@regscale.com)")
     console.print("Copyright: RegScale Incorporated")
     console.print("Website: https://www.regscale.com")
     console.print("Read the CLI Docs: https://regscale.readme.io/docs/overview")
     java = app.get_java()
     matches = ["not found", "internal or external"]
-    if not any(x in java for x in matches):
+    if all(x not in java for x in matches):
         console.print(f"Java: {java}")
     console.print(
         "\n[red]DISCLAIMER: RegScale does not conduct any form of security scanning for data imported by the customer. "
         + "It is the customer's responsibility to ensure that data imported into the platform using "
         + "the Command Line Interface meets industry standard, minimum security screening requirements. "
         + "RegScale has no liability for failing to scan any such data or for any data imported by "
         + "the customer that fails to meet such requirements.[red]\n"
@@ -214,17 +215,15 @@
     required=True,
 )
 def config(param, val):
     """Updates init.yaml config parameter with value"""
     # check if key provided exists in init.yaml or the app.template before adding it
     if param in app.config or param in app.template:
         # check the datatype provided vs what is expected
-        if isinstance(val, type(app.config[param])) or isinstance(
-            val, type(app.template[param])
-        ):
+        if isinstance(val, (type(app.config[param]), type(app.template[param]))):
             # update init file from login
             result_msg = update_regscale_config(param, val, app=app)
             # print the result
             logger.info(result_msg)
         else:
             # try to convert val entry to an int
             try:
@@ -421,14 +420,17 @@
 
 # add FedRAMP support
 cli.add_command(fedramp)
 
 # add Reminder Functionality
 cli.add_command(actions)
 
+# add Qualys Functionality
+cli.add_command(qualys)
+
 # add ServiceNow support
 cli.add_command(servicenow)
 
 # add Tenable support
 cli.add_command(tenable)
 
 # add Wiz support
@@ -436,14 +438,13 @@
 
 # add Control Editor Feature
 cli.add_command(control_editor)
 
 # add Assessments Editor Feature
 cli.add_command(assessments)
 
-
 # add Alienvault OTX integration
 cli.add_command(alienvault)
 
 # start function for the CLI
 if __name__ == "__main__":
     cli()
```

## app/_version.py

```diff
@@ -1,4 +1,4 @@
 #!/usr/bin/env python3
 # standard python imports
 
-__version__ = "4.24.1"
+__version__ = "4.25.0"
```

## app/api.py

```diff
@@ -54,21 +54,21 @@
                 pool_block=True,
             ),
         )
         self.session = r_session
         self.auth = None
 
     def get(
-        self, url: str, headers: dict = None, params: Tuple = None
+        self, url: str, headers: dict = None, params=None
     ) -> requests.models.Response:
         """
         Get Request for API
         :param str url: URL for API call
         :param dict headers: headers for the api get call, defaults to None
-        :param Tuple params: any parameters for the API call, defaults to None
+        :param params: any parameters for the API call, defaults to None
         :return: Requests response
         :rtype: requests.models.Response
         """
         url = normalize_url(url)
         if self.auth:
             self.session.auth = self.auth
         if headers is None:
@@ -110,26 +110,28 @@
                 "accept": self.accept,
             }
         return self.session.delete(url=normalize_url(url), headers=headers)
 
     def post(
         self,
         url: str,
-        json: Optional[Union[dict, str, list]],
         headers: dict = None,
+        json: Optional[Union[dict, str, list]] = None,
         data: dict = None,
         files: list = None,
+        params=None,
     ) -> requests.models.Response:
         """
         Post data to API
         :param str url: URL for the API call
         :param dict headers: Headers for the API call, defaults to None
         :param dict | str | list json: Dictionary of data for the API call, defaults to None
         :param dict data: Dictionary of data for the API call, defaults to None
         :param list files: Files to post during API call, defaults to None
+        :param params: any parameters for the API call, defaults to None
         :return: API response
         :rtype: requests.models.Response
         """
         if self.auth:
             self.session.auth = self.auth
         if headers is None:
             try:
@@ -145,53 +147,58 @@
                 )
         if not json and data:
             response = self.session.post(
                 url=normalize_url(url),
                 headers=headers,
                 data=data,
                 files=files,
+                params=params,
                 timeout=self.timeout,
             )
         else:
             response = self.session.post(
                 url=normalize_url(url),
                 headers=headers,
                 json=json,
                 files=files,
+                params=params,
                 timeout=self.timeout,
             )
         self.logger.debug("URL: %s, headers: %s, data: %s", url, headers, json)
         return response
 
     def put(
-        self, url: str, headers: dict = None, json: dict = None
+        self, url: str, headers: dict = None, json: dict = None, params=None
     ) -> requests.models.Response:
         """
         Update data via API call
         :param str url: URL for the API call
         :param dict headers: Headers for the API call, defaults to None
         :param dict json: Dictionary of data for the API call, defaults to None
+        :param params: any parameters for the API call, defaults to None
         :return: API response
         :rtype: requests.models.Response
         """
         if self.auth:
             self.session.auth = self.auth
         if headers is None:
             headers = {
                 "Authorization": self.config["token"],
             }
         response = self.session.put(
             url=normalize_url(url),
             headers=headers,
             json=json,
+            params=params,
             timeout=self.timeout,
         )
         self.logger.debug(response.text)
         return response
 
+    # FIXME - this would also be simplified by the creation of a Query class/function
     def graph(
         self, query: str, url: str = None, headers: dict = None, res_data: dict = None
     ) -> dict:
         """
         Execute GraphQL query and handles pagination before returning the data to the API call
         :param str query: the GraphQL query to execute
         :param str url: URL for the API call, defaults to None
@@ -210,15 +217,15 @@
         if headers is None:
             headers = {
                 "Authorization": self.config["token"],
                 "Accept": self.accept,
                 "Content-Type": self.content_type,
             }
         # check the query if skip was provided, if not add it for pagination
-        if query.find("skip") == -1:
+        if "skip" not in query:
             query = query.replace("(", "(skip: 0\n")
         # set the url for the query
         url = normalize_url(f'{self.config["domain"]}/graphql' if url is None else url)
 
         # make the API call
         response = self.session.post(
             url=normalize_url(url),
@@ -241,15 +248,15 @@
                         and not pagination_flag
                     ):
                         # set pagination_flag to true
                         pagination_flag = True
 
                         # find the location of the old skip in the query and parse the int after it
                         old_skip = re.search(r"skip: (\d+)", query)
-                        old_skip = old_skip.group(1)
+                        old_skip = old_skip[1]
 
                         # set the new value of the skip using old + # of items returned
                         new_skip = int(old_skip) + len(response_data[key]["items"])
 
                         # replace the old skip value with the new skip value that was calculated
                         query = re.sub(r"skip: [0-9]+", f"skip: {new_skip}", query)
                         # if no previous pagination, break this loop
@@ -262,20 +269,17 @@
             self.logger.debug(response)
             return response_data
         except KeyError as err:
             self.logger.error("No items were returned from %s!\n%s", url, err)
             self.logger.debug(response)
             return response_data
         # check if already called for recursion
-        if res_data:
-            # set data to pagination data
-            data = res_data
-        else:
-            # set data to most recent API call
-            data = response_data
+        # res_data: set data to pagination data
+        # response_data: most recent API call
+        data = res_data or response_data
         if pagination_flag:
             # recall the function with the new query and extend the data with the results
             response_data = self.graph(
                 url=url, headers=headers, query=query, res_data=data
             )
             # set the data to the pagination data
             data = response_data
@@ -301,14 +305,15 @@
         :param dict config: Config for the API, defaults to None
         :param str message: Message to display in console, defaults to "Working"
         :raises: General Exception if response status code != 200
         :return: None
         """
         if headers is None and config:
             headers = {"Accept": "application/json", "Authorization": config["token"]}
+
         if json_list and len(json_list) > 0:
             with Progress(transient=False) as progress:
                 task = progress.add_task(message, total=len(json_list))
                 with concurrent.futures.ThreadPoolExecutor(
                     max_workers=self.config["maxThreads"]
                 ) as executor:
                     if method.lower() == "post":
@@ -318,24 +323,24 @@
                                 json_list,
                             )
                         )
                     if method.lower() == "put":
                         result_futures = list(
                             map(
                                 lambda x: executor.submit(
-                                    self.put, url + f"/{x['id']}", headers, x
+                                    self.put, f"{url}/{x['id']}", headers, x
                                 ),
                                 json_list,
                             )
                         )
                     if method.lower() == "delete":
                         result_futures = list(
                             map(
                                 lambda x: executor.submit(
-                                    self.delete, url + f"/{x['id']}", headers
+                                    self.delete, f"{url}/{x['id']}", headers
                                 ),
                                 json_list,
                             )
                         )
                     for future in concurrent.futures.as_completed(result_futures):
                         try:
                             if future.result().status_code != 200:
@@ -350,19 +355,14 @@
 def normalize_url(url: str) -> str:
     """
     Function to remove extra slashes and trailing slash from a given URL
     :param str url: URL string normalize
     :return: A normalized URL
     :rtype: str
     """
-    url = str(url)
     segments = url.split("/")
-    correct_segments = []
-    for segment in segments:
-        if segment != "":
-            correct_segments.append(segment)
+    correct_segments = [segment for segment in segments if segment != ""]
     first_segment = str(correct_segments[0])
-    if first_segment.find("http") == -1:
+    if "http" not in first_segment:
         correct_segments = ["http:"] + correct_segments
-    correct_segments[0] = correct_segments[0] + "/"
-    normalized_url = "/".join(correct_segments)
-    return normalized_url
+    correct_segments[0] = f"{correct_segments[0]}/"
+    return "/".join(correct_segments)
```

## app/application.py

```diff
@@ -63,15 +63,15 @@
     RegScale CLI configuration class
     """
 
     def __init__(self):
         """
         Initialize application
         """
-
+        # FIXME - move this to a template or config file
         template = {
             "adAccessToken": "<createdProgrammatically>",
             "adAuthUrl": "https://login.microsoftonline.com/",
             "adClientId": "<myClientIdGoesHere>",
             "adClientSecret": "<mySecretGoesHere>",
             "adGraphUrl": "https://graph.microsoft.com/.default",
             "adTenantId": "<myTenantIdGoesHere>",
@@ -88,28 +88,36 @@
             "cisaKev": "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json",
             "dependabotId": "<myGithubUserIdGoesHere>",
             "dependabotOwner": "<myGithubRepoOwnerGoesHere>",
             "dependabotRepo": "<myGithubRepoNameGoesHere>",
             "dependabotToken": "<myGithubPersonalAccessTokenGoesHere>",
             "domain": "https://regscale.yourcompany.com/",
             "evidenceFolder": "./evidence",
+            "passScore": 80,
+            "failScore": 30,
             "githubDomain": "api.github.com",
             "issues": {
                 "defender365": {
                     "high": 30,
                     "low": 365,
                     "moderate": 90,
                     "status": "Open",
                 },
                 "defenderCloud": {
                     "high": 30,
                     "low": 365,
                     "moderate": 90,
                     "status": "Open",
                 },
+                "qualys": {
+                    "high": 30,
+                    "moderate": 90,
+                    "low": 365,
+                    "status": "Open",
+                },
                 "tenable": {
                     "critical": 3,
                     "high": 5,
                     "moderate": 30,
                     "status": "Draft",
                 },
                 "wiz": {
@@ -125,14 +133,17 @@
             "jiraUserName": "<jiraUserName>",
             "maxThreads": 1000,
             "oktaApiToken": "Can be a SSWS token from Okta or created programmatically",
             "oktaClientId": "<oktaClientIdGoesHere>",
             "oktaUrl": "<oktaUrlGoesHere>",
             "oscalLocation": "/opt/OSCAL",
             "pwshPath": "/opt/microsoft/powershell/7/pwsh",
+            "qualysUrl": "",
+            "qualysUserName": "<qualysUserName>",
+            "qualysPassword": "<qualysPassword>",
             "snowPassword": "<snowPassword>",
             "snowUrl": "<mySnowUrl>",
             "snowUserName": "<snowUserName>",
             "sonarToken": "<mySonarToken>",
             "tenableAccessKey": "<tenableAccessKeyGoesHere>",
             "tenableSecretKey": "<tenableSecretKeyGoesHere>",
             "tenableUrl": "https://sc.tenalab.online",
@@ -212,15 +223,15 @@
         :return: configuration
         :raises: TypeError if unable to generate config file
         :rtype: dict
         """
         config = None
         try:
             env = self._get_env()
-            file_config = self._get_conf() if self._get_conf() else {}
+            file_config = self._get_conf() or {}
             self.logger.debug("file_config: %s", file_config)
             # Merge
             if self.templated is False:
                 config = {**file_config, **env}
             else:
                 config = {**env, **file_config}
```

## app/logz.py

```diff
@@ -2,27 +2,28 @@
 # -*- coding: utf-8 -*-
 """ Rich Logging """
 
 # standard python imports
 import logging
 import os
 import tempfile
+from typing import Optional
 from logging.handlers import TimedRotatingFileHandler
 
 from gql.transport.requests import log as requests_logger
 import click
 from rich.logging import RichHandler
 from rich.traceback import install
 
 import exceptions.license_exception
 
 install(suppress=[click, exceptions])
 
 
-def create_logger():
+def create_logger(propagate: Optional[bool] = None):
     """
     Create a logger for use in all cases
     :return: logger object
     """
     requests_logger.setLevel("WARNING")  # Suppress GQL default verbose logging
     loglevel = os.environ.get("LOGLEVEL", "INFO").upper()
     rich_handler = RichHandler(rich_tracebacks=True, markup=True)
@@ -35,9 +36,10 @@
     logging.basicConfig(
         level=loglevel,
         format="%(asctime)s [%(levelname)-5.5s]  %(message)s",
         datefmt="[%Y/%m/%d %H:%M;%S]",
         handlers=[rich_handler, file_handler],
     )
     logger = logging.getLogger("rich")
-    # logger.propagate = False
+    if propagate is not None:
+        logger.propagate = propagate
     return logger
```

## app/commercial/ad.py

```diff
@@ -173,23 +173,21 @@
             f"Unable to retrieve group information from Azure Active Directory.\n{ex}"
         )
     # loop through the groups and log the results
     if "value" in groups_data:
         for g in groups_data["value"]:
             logger.info("GROUP: " + g["displayName"])
         logger.info("%s total group(s) retrieved.", len(groups_data["value"]))
-    else:
-        # error handling (log error)
-        if "error" in groups_data:
-            try:
-                error_and_exit(
-                    f'{groups_data["error"]["code"]}: {groups_data["error"]["message"]}'
-                )
-            except Exception as ex:
-                error_and_exit(f"Unknown Error! {ex}\nData: {groups_data}")
+    elif "error" in groups_data:
+        try:
+            error_and_exit(
+                f'{groups_data["error"]["code"]}: {groups_data["error"]["message"]}'
+            )
+        except Exception as ex:
+            error_and_exit(f"Unknown Error! {ex}\nData: {groups_data}")
 
     # verify artifacts directory exists
     check_file_path("artifacts")
 
     # save group data to a json file
     save_data_to(
         file_name=f"artifacts{sep}RegScale-AD-groups",
```

## app/commercial/defender.py

```diff
@@ -1,16 +1,18 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ RegScale Microsoft Defender recommendations integration"""
+
 # standard python imports
 from datetime import datetime, timedelta
 from json import JSONDecodeError
 from typing import Tuple
 
 import click
+import requests
 from rich.console import Console
 
 from app.api import Api
 from app.internal.login import is_valid
 from app.logz import create_logger
 from app.utils.app_utils import (
     check_license,
@@ -85,47 +87,43 @@
     try:
         # load the config from YAML
         config = app.load_config()
     except FileNotFoundError:
         error_and_exit("No init.yaml file or permission error when opening file.")
     # check if RegScale token is valid:
     if is_valid(app=app):
-        # create variable to store mapped alerts from Microsoft Defender 365
-        defender_alerts = []
         url = "https://api.securitycenter.microsoft.com/api/alerts?$top=10000&$filter=status+ne+'Resolved'"
 
         # check the azure token, get a new one if needed
         token = check_token(api=api, app=app, system="365", url=url)
 
         # set headers for the data
         headers = {"Content-Type": "application/json", "Authorization": token}
 
         # get Microsoft Defender recommendations
         alerts = get_items_from_azure(
             api=api,
             headers=headers,
             url=url,
         )
-        # add the records into the alerts for the threads to use
-        for alert in alerts:
-            defender_alerts.append(Recommendations(id=alert["id"], data=alert))
+        defender_alerts = [
+            Recommendations(id=alert["id"], data=alert) for alert in alerts
+        ]
         logger.info("Found %s Microsoft Defender 365 alert(s).", len(defender_alerts))
 
         # get all issues from RegScale where the defenderId field is populated
         issues = get_issues_by_integration_field(api=api, field="defenderAlertId")
 
-        # iterate through the RegScale issues and map them to Recommendations data class
-        regscale_issues = []
-        for issue in issues:
-            regscale_issues.append(Recommendations(id=issue["id"], data=issue))
-
+        regscale_issues = [
+            Recommendations(id=issue["id"], data=issue) for issue in issues
+        ]
         # create progress bars for each threaded task
         with job_progress:
             # see if there are any issues with defender id populated
-            if len(regscale_issues) > 0:
+            if regscale_issues:
                 logger.info(
                     "%s RegScale issue(s) will be analyzed.",
                     len(regscale_issues),
                 )
                 # create progress bar and analyze the RegScale issues
                 analyze_regscale_issues = job_progress.add_task(
                     f"[#f8b737]Analyzing {len(regscale_issues)} RegScale issue(s)...",
@@ -146,15 +144,15 @@
                     thread_count=len(regscale_issues),
                 )
             else:
                 logger.info("No issues from RegScale need to be analyzed.")
             # compare defender 365 recommendations and RegScale issues
             # while removing duplicates, updating existing RegScale Issues,
             # and adding new unique recommendations to unique_recs global variable
-            if len(defender_alerts) > 0 and len(regscale_issues) >= 0:
+            if defender_alerts and regscale_issues:
                 logger.info(
                     "Comparing %s Microsoft Defender 365 alert(s) and %s RegScale issue(s).",
                     len(defender_alerts),
                     len(regscale_issues),
                 )
                 compare_issues_and_recs = job_progress.add_task(
                     f"[#ef5d23]Comparing {len(defender_alerts)} Microsoft Defender 365 alert(s) and "
@@ -196,15 +194,14 @@
         if (len(unique_recs) + len(updated) + len(closed)) == 0:
             console.print("[green]No changes required for existing RegScale issue(s)!")
         else:
             console.print(
                 f"[red]{len(unique_recs)} issue(s) created, {len(updated)} issue(s)"
                 + f" updated and {len(closed)} issue(s) were closed in RegScale."
             )
-    # Notify user the RegScale token needs to be updated
     else:
         error_and_exit(
             "Login Invalid RegScale Credentials, please login for a new token."
         )
 
 
 @defender.command(name="sync_365_recommendations")
@@ -219,50 +216,45 @@
     try:
         # load the config from YAML
         config = app.load_config()
     except FileNotFoundError:
         error_and_exit("No init.yaml file or permission error when opening file.")
     # check if RegScale token is valid:
     if is_valid(app=app):
-        # create variable to store mapped recommendations from Microsoft Defender 365
-        defender_recs = []
-
         url = "https://api.securitycenter.microsoft.com/api/recommendations?$top=10000"
 
         # check the azure token, get a new one if needed
         token = check_token(api=api, app=app, system="365", url=url)
 
         # set headers for the data
         headers = {"Content-Type": "application/json", "Authorization": token}
 
         # get Microsoft Defender recommendations
         recommendations = get_items_from_azure(
             api=api,
             headers=headers,
             url=url,
         )
-        # add the records into the recommendations for the threads to use
-        for rec in recommendations:
-            defender_recs.append(Recommendations(id=rec["id"], data=rec))
+        defender_recs = [
+            Recommendations(id=rec["id"], data=rec) for rec in recommendations
+        ]
         logger.info(
             "Found %s Microsoft Defender 365 recommendation(s).", len(defender_recs)
         )
 
         # get all issues from RegScale where the defenderId field is populated
         issues = get_issues_by_integration_field(api=api, field="defenderId")
 
-        # iterate through the RegScale issues and map them to Recommendations data class
-        regscale_issues = []
-        for issue in issues:
-            regscale_issues.append(Recommendations(id=issue["id"], data=issue))
-
+        regscale_issues = [
+            Recommendations(id=issue["id"], data=issue) for issue in issues
+        ]
         # create progress bars for each threaded task
         with job_progress:
             # see if there are any issues with defender id populated
-            if len(regscale_issues) > 0:
+            if regscale_issues:
                 logger.info(
                     "%s RegScale issue(s) will be analyzed.",
                     len(regscale_issues),
                 )
                 # create progress bar and analyze the RegScale issues
                 analyze_regscale_issues = job_progress.add_task(
                     f"[#f8b737]Analyzing {len(regscale_issues)} RegScale issue(s)...",
@@ -283,15 +275,15 @@
                     thread_count=len(regscale_issues),
                 )
             else:
                 logger.info("No issues from RegScale need to be analyzed.")
             # compare defender 365 recommendations and RegScale issues
             # while removing duplicates, updating existing RegScale Issues,
             # and adding new unique recommendations to unique_recs global variable
-            if len(defender_recs) > 0 and len(regscale_issues) >= 0:
+            if defender_recs:
                 logger.info(
                     "Comparing %s Microsoft Defender 365 recommendation(s) and %s RegScale issue(s).",
                     len(defender_recs),
                     len(regscale_issues),
                 )
                 compare_issues_and_recs = job_progress.add_task(
                     f"[#ef5d23]Comparing {len(defender_recs)} Microsoft Defender 365 recommendation(s) and "
@@ -333,15 +325,14 @@
         if (len(unique_recs) + len(updated) + len(closed)) == 0:
             console.print("[green]No changes required for existing RegScale issue(s)!")
         else:
             console.print(
                 f"[red]{len(unique_recs)} issue(s) created, {len(updated)} issue(s)"
                 + f" updated and {len(closed)} issue(s) were closed in RegScale."
             )
-    # Notify user the RegScale token needs to be updated
     else:
         error_and_exit(
             "Login Invalid RegScale Credentials, please login for a new token."
         )
 
 
 @defender.command(name="sync_cloud_alerts")
@@ -356,54 +347,47 @@
     try:
         # load the config from YAML
         config = app.load_config()
     except FileNotFoundError:
         error_and_exit("No init.yaml file or permission error when opening file.")
     # check if RegScale token is valid:
     if is_valid(app=app):
-        # create variable to store mapped recommendations from Microsoft Defender 365
-        defender_alerts = []
-
-        url = (
-            f'https://management.azure.com/subscriptions/{api.config["azureCloudSubscriptionId"]}/'
-            + "providers/Microsoft.Security/alerts?api-version=2022-01-01"
-        )
+        url = f'https://management.azure.com/subscriptions/{api.config["azureCloudSubscriptionId"]}/providers/Microsoft.Security/alerts?api-version=2022-01-01'
 
         # check the azure token, get a new one if needed
         token = check_token(api=api, app=app, system="cloud", url=url)
 
         # set headers for the data
         headers = {"Content-Type": "application/json", "Authorization": token}
 
         # get Microsoft Defender recommendations
         alerts = get_items_from_azure(
             api=api,
             headers=headers,
             url=url,
         )
-        # add the records into the global recommendations for the threads to use
-        for alert in alerts:
-            if alert["properties"]["status"] in ["Active", "In Progress"]:
-                defender_alerts.append(Recommendations(id=alert["name"], data=alert))
+        defender_alerts = [
+            Recommendations(id=alert["name"], data=alert)
+            for alert in alerts
+            if alert["properties"]["status"] in ["Active", "In Progress"]
+        ]
         logger.info(
             "Found %s Microsoft Defender for Cloud alert(s).", len(defender_alerts)
         )
 
         # get all issues from RegScale where the defenderCloudId field is populated
         issues = get_issues_by_integration_field(api=api, field="defenderCloudId")
 
-        # iterate through the RegScale issues and map them to Recommendations data class
-        regscale_issues = []
-        for issue in issues:
-            regscale_issues.append(Recommendations(id=issue["id"], data=issue))
-
+        regscale_issues = [
+            Recommendations(id=issue["id"], data=issue) for issue in issues
+        ]
         # create progress bars for each threaded task
         with job_progress:
             # see if there are any issues with defender id populated
-            if len(regscale_issues) > 0:
+            if regscale_issues:
                 logger.info(
                     "%s RegScale issue(s) will be analyzed.",
                     len(regscale_issues),
                 )
                 # create progress bar and analyze the RegScale issues
                 analyze_regscale_issues = job_progress.add_task(
                     f"[#f8b737]Analyzing {len(regscale_issues)} RegScale issue(s)...",
@@ -424,15 +408,15 @@
                     thread_count=len(regscale_issues),
                 )
             else:
                 logger.info("No issues from RegScale need to be analyzed.")
             # compare defender for cloud alerts and RegScale issues
             # while removing duplicates, updating existing RegScale Issues,
             # and adding new unique recommendations to unique_recs global variable
-            if len(defender_alerts) > 0 and len(regscale_issues) >= 0:
+            if defender_alerts and regscale_issues:
                 logger.info(
                     "Comparing %s Microsoft Defender for Cloud alert(s) and %s RegScale issue(s).",
                     len(defender_alerts),
                     len(regscale_issues),
                 )
                 compare_issues_and_recs = job_progress.add_task(
                     f"[#ef5d23]Comparing {len(defender_alerts)} Microsoft Defender for Cloud alert(s) and "
@@ -474,15 +458,14 @@
         if (len(unique_recs) + len(updated) + len(closed)) == 0:
             console.print("[green]No changes required for existing RegScale issue(s)!")
         else:
             console.print(
                 f"[red]{len(unique_recs)} issue(s) created, {len(updated)} issue(s)"
                 + f" updated and {len(closed)} issue(s) were closed in RegScale."
             )
-    # Notify user the RegScale token needs to be updated
     else:
         error_and_exit(
             "Login Invalid RegScale Credentials, please login for a new token."
         )
 
 
 def check_token(api, app, system: str, url: str) -> str:
@@ -574,38 +557,53 @@
     # get the data
     response = api.post(
         url=url,
         headers={"Content-Type": "application/x-www-form-urlencoded"},
         data=data,
     )
     try:
-        # try to read the response and parse the token
-        res = response.json()
-        token = res["access_token"]
-
-        # add the token to init.yaml
-        app.config[key] = "Bearer " + token
-
-        # write the changes back to file
-        app.save_config(app.config)
-
-        # notify the user we were successful
-        logger.info(
-            f"Azure {system.title()} Login Successful! Init.yaml file was updated with the new access token."
-        )
-        # return the token string
-        return app.config[key]
+        return parse_and_save_token(response, app, key, system)
     except KeyError as ex:
         # notify user we weren't able to get a token and exit
         error_and_exit(f"Didn't receive token from Azure.\n{ex}\n{response.text}")
     except JSONDecodeError as ex:
         # notify user we weren't able to get a token and exit
         error_and_exit(f"Unable to authenticate with Azure.\n{ex}\n{response.text}")
 
 
+def parse_and_save_token(
+    response: requests.Response, app, key: str, system: str
+) -> str:
+    """
+    Function to parse the token from the response and save it to init.yaml
+    :param requests.Response response: Response from API call
+    :param app: Application object
+    :param str key: Key to use for init.yaml token update
+    :param str system: Which system to check JWT for, either Defender 365 or Defender for Cloud
+    :return: JWT from Azure for the provided system
+    :rtype: str
+    """
+    # try to read the response and parse the token
+    res = response.json()
+    token = res["access_token"]
+
+    # add the token to init.yaml
+    app.config[key] = f"Bearer {token}"
+
+    # write the changes back to file
+    app.save_config(app.config)
+
+    # notify the user we were successful
+    logger.info(
+        f"Azure {system.title()} Login Successful! Init.yaml file was updated with the new access token."
+    )
+    # return the token string
+    return app.config[key]
+
+
 def get_items_from_azure(api, headers: dict, url: str) -> list:
     """
     Function to get data from Microsoft Defender returns the data as a list while handling pagination
     :param api: API object
     :param dict headers: Headers used for API call
     :param url: URL to use for the API call
     :raises: JSONDecodeError if API response cannot be converted to a json object
@@ -661,24 +659,19 @@
             score = 5
         elif score.lower() == "high":
             score = 9
         else:
             score = 0
     if score >= 7:
         days = config["issues"][key]["high"]
-        due_date = datetime.strptime(today, "%m/%d/%y") + timedelta(days=days)
     elif 4 <= score < 7:
         days = config["issues"][key]["moderate"]
-        due_date = datetime.strptime(today, "%m/%d/%y") + timedelta(days=days)
-    elif score < 4:
-        days = config["issues"][key]["low"]
-        due_date = datetime.strptime(today, "%m/%d/%y") + timedelta(days=days)
     else:
-        # if null or not an integer use 45 days
-        due_date = datetime.strptime(today, "%m/%d/%y") + timedelta(days=45)
+        days = config["issues"][key]["low"]
+    due_date = datetime.strptime(today, "%m/%d/%y") + timedelta(days=days)
     return due_date.strftime("%Y-%m-%dT%H:%M:%S")
 
 
 def format_description(rec: dict, tenant_id: str) -> str:
     """
     Function to format the provided dictionary into an HTML table
     :param dict rec: Recommendation from Microsoft Defender
@@ -714,18 +707,15 @@
             if isinstance(value[0], dict):
                 data = ""
                 for item in value:
                     for key, value in item.items():
                         data += f"</br>{key}: {value}"
                 payload[uncamel_case(key)] = data
             elif isinstance(value[0], list):
-                data = ""
-                # concatenate the list items
-                for item in value:
-                    data += "</br>".join(item)
+                data = "".join("</br>".join(item) for item in value)
                 payload[uncamel_case(key)] = data
             else:
                 payload[uncamel_case(key)] = "</br>".join(value)
         elif key.lower() not in skip_keys and "entities" not in key.lower():
             # make sure it isn't a list with no values
             if not isinstance(value, list):
                 # uncamel_case the key
@@ -909,17 +899,17 @@
         if rec_type in ["defender365Rec", "defender365Alert"]:
             message = "via Microsoft 365 Defender"
         elif rec_type == "defenderCloud":
             message = "via Microsoft Defender for Cloud"
         else:
             message = "via Microsoft Defender"
         issue["dateCompleted"] = get_current_datetime("%Y-%m-%dT%H:%M:%S")
-        issue["description"] += "<p>No longer recommended {} as of {}</p>".format(
-            message, get_current_datetime("%b %d,%Y")
-        )
+        issue[
+            "description"
+        ] += f'<p>No longer recommended {message} as of {get_current_datetime("%b %d,%Y")}</p>'
         closed.append(issue)
     else:
         issue["dateCompleted"] = ""
         updated.append(issue)
 
     # use the api to change the status of the given issue
     api.put(url=f'{config["domain"]}/api/issues/{issue["id"]}', json=issue)
```

## app/commercial/jira.py

```diff
@@ -122,26 +122,24 @@
         start = start_pointer * page_size
         fetch_jira_issues = jira_client.search_issues(
             f"project={jira_project}",
             fields="key,summary,description,status",
             startAt=start,
             maxResults=page_size,
         )
-        # verify all Jira issues were retrieved
         if len(jira_issues) == fetch_jira_issues.total:
             break
-        else:
-            start_pointer += 1
-            # append new records to jira_issues
-            jira_issues.extend(fetch_jira_issues)
-            logger.info(
-                "%s/%s Jira issue(s) retrieved.",
-                len(jira_issues),
-                fetch_jira_issues.total,
-            )
+        start_pointer += 1
+        # append new records to jira_issues
+        jira_issues.extend(fetch_jira_issues)
+        logger.info(
+            "%s/%s Jira issue(s) retrieved.",
+            len(jira_issues),
+            fetch_jira_issues.total,
+        )
     logger.info("%s issue(s) retrieved from Jira.", len(jira_issues))
     # loop through each RegScale issue
     new_issue_counter = 0
     # cannot thread this process due to jira api limitations
     for issue in issues_data:
         # see if Jira ticket already exists
         if "jiraId" not in issue or issue["jiraId"] == "":
```

## app/commercial/okta.py

```diff
@@ -1,23 +1,24 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ RegScale Okta integration"""
 
 # standard python imports
+import json
 import time
 from datetime import datetime, timedelta
 from json import JSONDecodeError
 from pathlib import Path
 from typing import Tuple
 
 import click
+import jwcrypto.jwk as jwk
+import python_jwt
 import requests
 
-from jwt import JWT, jwk_from_dict
-
 from app.api import Api
 from app.internal.login import is_valid
 from app.logz import create_logger
 from app.utils.app_utils import (
     check_file_path,
     check_license,
     create_progress_object,
@@ -366,22 +367,15 @@
         # get all the roles for the user
         user_roles = get_okta_data(
             api=api,
             task=task,
             url=f"{api.config['oktaUrl']}/api/v1/users/{user['id']}/roles",
             headers=headers,
         )
-        # create list variable to store the concatenated user list
-        roles = []
-
-        # iterate through the user's roles and add them to the roles list
-        for role in user_roles:
-            # append to roles list
-            roles.append(role["label"])
-
+        roles = [role["label"] for role in user_roles]
         # add concatenated user roles to their entry in all_users
         user["roles"] = ", ".join(roles)
 
         # add user to global admin_user list if, admin is in their concatenated role list
         admin_users.append(user) if any(
             "admin" in val.lower() for val in roles
         ) else None
@@ -409,19 +403,15 @@
 
     # create task for analyzing user's data
     analyze_login = job_progress.add_task(
         f"[#ef5d23]Analyzing {len(user_list)} user(s) data...", total=len(user_list)
     )
     # iterate through all users and check user's with the provided criteria
     for user in user_list:
-        # get the user's data with the provided key
-        data_filter = user[key]
-
-        # see if the data_filter exists
-        if data_filter:
+        if data_filter := user[key]:
             # verify comparing filter_value date against a string
             if isinstance(filter_value, datetime) and isinstance(data_filter, str):
                 # get today's date
                 today = datetime.now()
                 # try to convert it
                 try:
                     data_filter = datetime.strptime(
@@ -443,15 +433,14 @@
                     filtered_users.append(user)
                 elif (
                     user_type == "new"
                     and filter_value.date() <= data_filter.date() <= today.date()
                 ):
                     # add user to filtered users list
                     filtered_users.append(user)
-        # data filter is null or empty
         elif data_filter is None:
             filtered_users.append(user)
         # update analyzing user task
         job_progress.update(analyze_login, advance=1)
     # notify user of how many inactive users we found
     logger.info("Found %s %s user(s) in Okta.", len(filtered_users), user_type)
 
@@ -692,33 +681,32 @@
     :param app: Application object
     :raises: JSONDecodeError if unable to convert API response to a JSON object
     :return: JWT token for future requests
     :rtype: str
     """
     okta_token = ""
 
-    # set up instance for JWT encoding and decoding
-    instance = JWT()
-
     # get the Okta private JWK
-    private_key = jwk_from_dict(config["oktaSecretKey"])
+    jwk_token = jwk.JWK.from_json(json.dumps(config["oktaSecretKey"]))
 
     # get the url from config without any trailing /
     url = config["oktaUrl"].strip("/") + "/oauth2/v1/token"
 
     # set the payload for the to be signed JWT while setting the signed JWT to expire in 10 minutes
     payload_data = {
         "aud": url,
         "iss": config["oktaClientId"],
         "sub": config["oktaClientId"],
         "exp": int(time.time()) + 600,
     }
 
     # create a signed JWT
-    token = instance.encode(payload_data, private_key, alg="RS256")
+    token = python_jwt.generate_jwt(
+        payload_data, jwk_token, "RS256", datetime.timedelta(minutes=5)
+    )
 
     # set the headers for the API call
     headers = {
         "Accept": "application/json",
         "Content-Type": "application/x-www-form-urlencoded",
     }
```

## app/commercial/servicenow.py

```diff
@@ -147,15 +147,15 @@
                     + " - "
                     + str_issue_url,
                     "state": "New",
                     "urgency": snow_incident_type,
                 }
 
                 # create a SNOW incident
-                incident_url = snow_url + "api/now/table/incident"
+                incident_url = f"{snow_url}api/now/table/incident"
                 snow_header = {
                     "Content-Type": "application/json",
                     "Accept": "application/json",
                 }
                 try:
                     # intNew += 1
                     response = snow_api.post(
@@ -220,26 +220,26 @@
     app = Application()
     reg_api = Api(app)
     data = []
     # get secrets
     snow_url = app.config["snowUrl"]
     snow_user = app.config["snowUserName"]
     snow_pwd = app.config["snowPassword"]
-    incident_url = snow_url + "api/now/table/incident"
+    incident_url = f"{snow_url}api/now/table/incident"
     snow_api = deepcopy(
         reg_api
     )  # no need to instantiate a new config, just copy object
     snow_api.auth = (snow_user, snow_pwd)
     offset = 0
     limit = 500
     query = "&sysparm_query=GOTO123TEXTQUERY321=regscale"
     result, offset = query_incidents(
         api=snow_api, incident_url=incident_url, offset=offset, limit=limit, query=query
     )
-    data = data + result
+    data += result
     while len(result) > 0:
         result, offset = query_incidents(
             api=snow_api,
             incident_url=incident_url,
             offset=offset,
             limit=limit,
             query=query,
@@ -267,16 +267,15 @@
             regscale_response = api.get(
                 url=config["domain"] + f"/api/issues/findByServiceNowId/{sys_id}"
             )
             if regscale_response.raise_for_status():
                 logger.warning("Cannot find RegScale issue with a incident %s.", sys_id)
             else:
                 logger.debug("Processing ServiceNow Issue # %s", sys_id)
-                work_item = dat["work_notes"]
-                if work_item:
+                if work_item := dat["work_notes"]:
                     issue = regscale_response.json()[0]
                     if work_item not in issue["description"]:
                         logger.info(
                             "Updating work item for RegScale issue # %s and ServiceNow incident # %s.",
                             issue["id"],
                             sys_id,
                         )
@@ -313,14 +312,14 @@
     :param str incident_url: URL for ServiceNow incidents
     :param int offset: Used in URL for ServiceNow API call
     :param int limit: Used in URL for ServiceNow API call
     :param str query: Query string for ServiceNow API call
     :return: Tuple[Result data from API call, offset integer provided]
     :rtype: Tuple[list, int]
     """
-    offset_param = f"&sysparm_offset={str(offset)}"
-    url = incident_url + f"?sysparm_limit={limit}{offset_param}{query}"
+    offset_param = f"&sysparm_offset={offset}"
+    url = f"{incident_url}?sysparm_limit={limit}{offset_param}{query}"
     logger.debug(url)
     result = api.get(url=url).json()["result"]
     offset += limit
     logger.debug(len(result))
     return result, offset
```

## app/commercial/tenable.py

```diff
@@ -3,15 +3,14 @@
 """ Tenable integration for RegScale CLI """
 
 # standard python imports
 import collections
 import dataclasses
 import os
 import re
-import sys
 import warnings
 from dataclasses import asdict
 from datetime import datetime, timedelta
 from pathlib import Path
 from typing import Tuple
 
 import click
@@ -27,14 +26,15 @@
 from app._version import __version__
 from app.api import Api
 from app.application import Application
 from app.logz import create_logger
 from app.utils.app_utils import (
     check_file_path,
     epoch_to_datetime,
+    error_and_exit,
     get_current_datetime,
     save_data_to,
     validate_mac_address,
 )
 from app.utils.regscale_utils import lookup_reg_assets_by_parent
 from models.app_models.click import file_types, save_output_to
 from models.integration_models.tenable import TenableAsset
@@ -150,23 +150,23 @@
     return tsc.queries.list()
 
 
 @tenable.command(name="query_vuln")
 @click.option(
     "--query_id",
     type=click.INT,
-    help="Enter Tenable query ID.",
+    help="Tenable query ID to retrieve via API",
     prompt="Enter Tenable query ID",
     required=True,
 )
 @click.option(
     "--regscale_ssp_id",
     type=click.INT,
-    help="Enter RegScale SSP ID.",
-    prompt="Enter RegScale SSP ID",
+    help="The ID number from RegScale of the System Security Plan",
+    prompt="Enter RegScale System Security Plan ID",
     required=True,
 )
 @click.option(
     "--create_issue_from_recommendation",
     type=click.BOOL,
     help="Create Issue in RegScale from Vulnerability in RegScale.",
     default=False,
@@ -229,24 +229,20 @@
     )
 
     tenable_data = fetch_vulns(query_id=query_id, regscale_ssp_id=ssp_id)
     tenable_vulns = tenable_data[0]
     tenable_df = tenable_data[1]
 
     assets_to_be_inserted = list(
-        set(
-            [
-                dat
-                for dat in tenable_vulns
-                if dat.macAddress
-                not in set(
-                    asset.macAddress for asset in inner_join(reg_assets, tenable_vulns)
-                )
-            ]
-        )
+        {
+            dat
+            for dat in tenable_vulns
+            if dat.macAddress
+            not in {asset.macAddress for asset in inner_join(reg_assets, tenable_vulns)}
+        }
     )
     counts = collections.Counter(s.pluginName for s in tenable_vulns)
     update_assets = []
     insert_assets = []
     for vuln in set(tenable_vulns):  # you can list as many input dicts as you want here
         vuln.counts = dict(counts)[vuln.pluginName]
         lookup_assets = lookup_asset(reg_assets, vuln.macAddress, vuln.dnsName)
@@ -272,16 +268,16 @@
                     ):
                         update_assets.append(asdict(asset))
                 except AssertionError as aex:
                     logger.error(
                         "Asset does not have an id, unable to update!\n%s", aex
                     )
 
-    for t_asset in assets_to_be_inserted:
-        if len(assets_to_be_inserted) > 0:
+    if len(assets_to_be_inserted) > 0:
+        for t_asset in assets_to_be_inserted:
             # Do Insert
             r_asset = Asset(
                 name=t_asset.dnsName,
                 otherTrackingNumber=t_asset.pluginID,
                 parentId=ssp_id,
                 parentModule="securityplans",
                 ipAddress=t_asset.ip,
@@ -337,23 +333,19 @@
     Determine RegScale friendly OS name
     :param str os_string: String of the asset's OS
     :return: RegScale acceptable OS
     :rtype: str
     """
     linux_words = ["linux", "ubuntu", "hat", "centos", "rocky", "alma", "alpine"]
     if re.compile("|".join(linux_words), re.IGNORECASE).search(os_string):
-        os_string = "Linux"
+        return "Linux"
     elif (os_string.lower()).startswith("windows"):
-        if "server" in os_string:
-            os_string = "Windows Server"
-        else:
-            os_string = "Windows Desktop"
+        return "Windows Server" if "server" in os_string else "Windows Desktop"
     else:
-        os_string = "Other"
-    return os_string
+        return "Other"
 
 
 def get_status(asset: TenableAsset) -> str:
     """
     Convert Tenable asset status to RegScale asset status
     :param TenableAsset asset: Asset object from Tenable
     :return: RegScale status
@@ -405,43 +397,40 @@
     Trend vulnerabilities data to the console
     :param list filter: Data to use for trend graph
     :param str dns: DNS to filter data
     :param str filter_type: Type of filter to apply to data
     :param str filename: Name of the file to save as
     :return: None
     """
-    if len(filter) > 0:
-        df = pd.read_pickle(filename)
-        unique_cols = ["pluginID", "dnsName", "severity", "report_date"]
-        df = df[df[filter_type].isin(filter)]
-        df = df[df["dnsName"] == dns]
-        df = df[unique_cols]
-        df = df.drop_duplicates(subset=unique_cols)
-        if len(df) == 0:
-            logger.warning("No Rows in Dataframe, exiting")
-            sys.exit(0)
-
-        df.loc[df["severity"] == "Info", "severity_code"] = 0
-        df.loc[df["severity"] == "Low", "severity_code"] = 1
-        df.loc[df["severity"] == "Medium", "severity_code"] = 2
-        df.loc[df["severity"] == "High", "severity_code"] = 3
-        df.loc[df["severity"] == "Critical", "severity_code"] = 4
-        # Deal with linux wayland sessions
-        if (
-            "XDG_SESSION_TYPE" in os.environ
-            and os.getenv("XDG_SESSION_TYPE") == "wayland"
-        ):
-            os.environ["QT_QPA_PLATFORM"] = "wayland"
-        # plotting graph
-        for d in filter:
-            plt.plot(df["report_date"], df["severity_code"], label=d)
-        logger.info("Plotting %s rows of data.\n", len(df))
-        logger.info(df.head())
-        plt.legend()
-        plt.show(block=True)
+    if not filter:
+        return
+    df = pd.read_pickle(filename)
+    df = df[df[filter_type].isin(filter)]
+    df = df[df["dnsName"] == dns]
+    unique_cols = ["pluginID", "dnsName", "severity", "report_date"]
+    df = df[unique_cols]
+    df = df.drop_duplicates(subset=unique_cols)
+    if len(df) == 0:
+        error_and_exit("No Rows in Dataframe.")
+
+    df.loc[df["severity"] == "Info", "severity_code"] = 0
+    df.loc[df["severity"] == "Low", "severity_code"] = 1
+    df.loc[df["severity"] == "Medium", "severity_code"] = 2
+    df.loc[df["severity"] == "High", "severity_code"] = 3
+    df.loc[df["severity"] == "Critical", "severity_code"] = 4
+    # Deal with linux wayland sessions
+    if "XDG_SESSION_TYPE" in os.environ and os.getenv("XDG_SESSION_TYPE") == "wayland":
+        os.environ["QT_QPA_PLATFORM"] = "wayland"
+    # plotting graph
+    for d in filter:
+        plt.plot(df["report_date"], df["severity_code"], label=d)
+    logger.info("Plotting %s rows of data.\n", len(df))
+    logger.info(df.head())
+    plt.legend()
+    plt.show(block=True)
 
 
 def create_regscale_issue_from_vuln(regscale_ssp_id: int, df: pd.DataFrame) -> None:
     """
     Sync Tenable Vulnerabilities to RegScale issues
     :param int regscale_ssp_id: RegScale System Security Plan ID
     :param pd.Dataframe df: Pandas dataframe of Tenable data
@@ -548,28 +537,27 @@
         df["repository"] = [d.get("name") for d in df["repository"]]
         df["report_date"] = get_current_datetime(dt_format="%Y-%m-%d")
         filename = "vulnerabilities.pkl"
 
         df.drop_duplicates()
         if not Path(filename).exists():
             logger.info("Saving vulnerability data to %s.", filename)
-            df.to_pickle(filename)
         else:
             logger.info(
                 "Updating vulnerabilities.pkl with the latest data from Tenable."
             )
             old_df = pd.read_pickle(filename)
             old_df = old_df[
                 old_df["report_date"] != get_current_datetime(dt_format="%Y-%m-%d")
             ]
             try:
                 df = pd.concat([old_df, df]).drop_duplicates()
             except ValueError as vex:
                 logger.error("Pandas ValueError:%s.", vex)
-            df.to_pickle(filename)
+        df.to_pickle(filename)
         severity_arr = df.groupby(["severity", "repository"]).size().to_frame()
         console.rule("[bold red]Vulnerability Overview")
         console.print(severity_arr)
         return df
 
     except pd.errors.DataError as dex:
         logger.error(dex)
@@ -584,16 +572,18 @@
     :rtype: Tuple[list, pd.DataFrame]
     """
     tsc = gen_tsc()
     data = []
     if query_id:
         description = f"Fetching Vulnerabilities for Tenable query id: {query_id}."
         vulns = tsc.analysis.vulns(query_id=query_id)
-        for vuln in track(vulns, description=description, show_speed=False):
-            data.append(TenableAsset.from_dict(vuln))
+        data.extend(
+            TenableAsset.from_dict(vuln)
+            for vuln in track(vulns, description=description, show_speed=False)
+        )
         logger.info("Found %i vulnerabilities.", len(data))
     df = log_vulnerabilities(data, query_id=query_id, regscale_ssp_id=regscale_ssp_id)
     return data, df
 
 
 @tenable.command(name="list_tags")
 def list_tags():
@@ -618,23 +608,22 @@
 def gen_tsc() -> TenableSC:
     """
     Generate Tenable Object
     :return: Tenable client
     :rtype: TenableSC
     """
     config = Application().config
-    tsc = TenableSC(
+    return TenableSC(
         url=config["tenableUrl"],
         access_key=config["tenableAccessKey"],
         secret_key=config["tenableSecretKey"],
         vendor="RegScale, Inc.",
         product="RegScale CLI",
         build=__version__,
     )
-    return tsc
 
 
 def inner_join(reg_list: list, tenable_list: list) -> list:
     """
     Function to inner join two lists on the macAddress field and returns the assets existing in RegScale and Tenable
     :param list reg_list: List of RegScale assets
     :param list tenable_list: List of Tenable assets
```

## app/commercial/wiz.py

```diff
@@ -44,15 +44,15 @@
 from models.regscale_models.asset import Asset
 from models.regscale_models.issue import Issue
 
 # TODO:
 # Compute resources from Wiz to default.
 
 
-# Pull Wiz secret and ID from environtment and not from plaintext
+# Pull Wiz secret and ID from environment and not from plaintext
 
 
 # Private and global variables
 logger = create_logger()
 job_progress = create_progress_object()
 url_job_progress = create_progress_object()
 regscale_job_progress = create_progress_object()
@@ -155,14 +155,15 @@
     :return: tuple of token and scope
     :rtype: tuple[str, str]
     """
     logger.info("Getting a token")
     response = api.post(
         url=token_url,
         headers={"Content-Type": "application/x-www-form-urlencoded"},
+        json=None,
         data=generate_authentication_params(client_id, client_secret, token_url),
     )
     logger.debug(response.reason)
     if response.status_code == requests.codes.unauthorized:
         error_and_exit("Wiz Authentication: Unauthorized")
     if response.status_code != requests.codes.ok:
         error_and_exit(
@@ -216,50 +217,34 @@
           policies, supplychain, securityplans, components.",
     required=True,
 )
 @regscale_id(help="RegScale will create and update issues as children of this record.")
 @regscale_module()
 @click.option("--client_id", default=None, hide_input=False, required=False)
 @click.option("--client_secret", default=None, hide_input=True, required=False)
-# flake8: noqa: C901
 def inventory(
     wiz_project_id: str,
     regscale_id: int,
     regscale_module: str,
     client_id: str,
     client_secret: str,
 ) -> None:
     """Process inventory list from Wiz."""
+    wiz_authenticate(client_id, client_secret)
     app = check_license()
     api = Api(app)
-    wiz_authenticate(client_id, client_secret)
     verify_provided_module(regscale_module)
     # load the config from YAML
     config = app.config
     if not check_module_id(app, regscale_id, regscale_module):
         error_and_exit(f"Please enter a valid regscale_id for {regscale_module}.")
 
     # get secrets
     url = config["wizUrl"]
 
-    # set health check URL
-    url_assets = (
-        config["domain"]
-        + "/api/assets/getAllByParent/"
-        + str(regscale_id)
-        + "/"
-        + str(regscale_module)
-    )
-
-    # set headers
-    headers_get = {
-        "Accept": "application/json",
-        "Authorization": config["token"],
-    }
-
     # get the full list of assets
     logger.info("Fetching full asset list from RegScale.")
 
     try:
         body = """
                 query {
                     assets(take: 50, skip: 0, , where: { parentModule: {eq: "parent_module"} parentId: {
@@ -301,17 +286,17 @@
             "Artifacts directory exists.  This directly will store output files from all processing."
         )
 
     report_prefix = f"RegScale_Inventory_Report_Automated_Entities_{wiz_project_id}"
     existing_inventory_reports = [
         report for report in query_reports(app) if report_prefix in report["name"]
     ]
+    date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
     # Update existing reports
     for report in existing_inventory_reports:
-        date_format = "%Y-%m-%dT%H:%M:%S.%fZ"
         last_run = datetime.datetime.strptime(
             report["lastRun"]["runAt"], date_format
         )  # UTC
         if (
             (datetime.datetime.utcnow() - last_run).seconds
             > (app.config["wizReportAge"]) * 60
             if app.config["wizReportAge"] != 0
@@ -334,36 +319,36 @@
         wiz_report_ids,
         parent_id=regscale_id,
         parent_module=regscale_module,
     )
     new_assets = [
         asset
         for asset in wiz_assets
-        if asset["wizId"] not in set([wiz["wizId"] for wiz in existing_asset_data])
+        if asset["wizId"] not in {wiz["wizId"] for wiz in existing_asset_data}
     ]
     update_assets = []
-    existing_wiz_ids = set([wiz["wizId"] for wiz in existing_asset_data])
+    existing_wiz_ids = {wiz["wizId"] for wiz in existing_asset_data}
     for asset in existing_asset_data:
         if asset["wizId"] in existing_wiz_ids:
             regscale_id = asset["id"]
-            wiz_index = next(
+            if wiz_index := next(
                 (
                     index
                     for (index, d) in enumerate(wiz_assets)
                     if d["wizId"] == asset["wizId"]
                 ),
                 None,
-            )
-            if wiz_index:
+            ):
                 asset = wiz_assets[wiz_index]
                 asset["id"] = regscale_id
                 update_assets.append(asset)
 
     api.update_server(
-        app.config["domain"] + "/api/assets",
+        config=app.config,
+        url=app.config["domain"] + "/api/assets",
         method="put",
         message=f"[#6200ff]Updating {len(update_assets)} assets in RegScale.",
         json_list=update_assets,
     )
     new_props = []
     for asset in track(
         new_assets,
@@ -881,20 +866,22 @@
         if issue["status"] == "Closed" and not issue["dateCompleted"]:
             issue["dateCompleted"] = get_current_datetime()
         if issue["status"] == "Open":
             issue["dateCompleted"] = ""
         update_issues.append(issue)
 
     api.update_server(
+        config=app.config,
         method="post",
         url=app.config["domain"] + "/api/issues",
         json_list=new_issues,
         message=f"[#14bfc7]Inserting {len(new_issues)} issues in RegScale.",
     )
     api.update_server(
+        config=app.config,
         method="put",
         url=app.config["domain"] + "/api/issues",
         json_list=update_issues,
         message=f"[#15cfec]Updating {len(update_issues)} issues in RegScale.",
     )
 
 
@@ -1121,15 +1108,15 @@
         )
         if "errors" in wiz_issues_resp.json().keys():
             error_and_exit(
                 f'Wiz Error: {wiz_issues_resp.json()["errors"][0]["message"]}'
             )
         wiz_report_id = wiz_issues_resp.json()["data"]["createReport"]["report"]["id"]
         if "wizIssuesReportId" not in config:
-            config["wizIssuesReportId"] = {}
+            config["wizIssuesReportId"] = []
             config["wizIssuesReportId"]["report_id"] = None
             config["wizIssuesReportId"]["last_seen"] = None
             app.save_config(config)
         config["wizIssuesReportId"]["report_id"] = wiz_report_id
         config["wizIssuesReportId"]["last_seen"] = get_current_datetime()
         app.save_config(config)
     except (requests.RequestException, AttributeError) as rex:
@@ -1248,14 +1235,16 @@
 
     res = send_request(
         app,
         query=query,
         variables=variables,
         api_endpoint_url=app.config["wizUrl"],
     )
+    if "errors" in res.json().keys():
+        error_and_exit(f'Wiz Error: {res.json()["errors"][0]["message"]}')
 
     result = res.json()["data"]["reports"]["nodes"]
 
     return result
 
 
 def send_request(
```

## app/internal/admin_actions.py

```diff
@@ -129,15 +129,15 @@
             analyze_items = job_progress.add_task(
                 f"[#ef5d23]Analyzing pipeline for {len(tenant_pipeline)} user(s)...",
                 total=len(tenant_pipeline),
             )
             # convert user list into a dictionary using ID as the key for each user dictionary
             dict_users = {
                 activated_users[i]["id"]: activated_users[i]
-                for i in range(0, len(activated_users), 1)
+                for i in range(len(activated_users))
             }
             create_threads(
                 process=analyze_pipeline,
                 args=(config, analyze_items, dict_users, api),
                 thread_count=len(tenant_pipeline),
             )
             logger.info("Sending an email to %s user(s).", len(final_pipeline))
@@ -418,22 +418,17 @@
             "Assessments": {"Pipeline": res_data["assessments"]["items"]},
             "Issues": {"Pipeline": res_data["issues"]["items"]},
             "Tasks": {"Pipeline": res_data["tasks"]["items"]},
             "Data Calls": {"Pipeline": res_data["dataCalls"]["items"]},
             "Security Plans": {"Pipeline": res_data["securityPlans"]["items"]},
             "Workflow": {"Pipeline": res_data["workflowInstances"]["items"]},
         }
-
-        # create variable to see how many total objects are in the user's pipeline
-        total_tasks = 0
-
         # iterate through the user's pipeline tallying their items
-        for pipeline in pipelines.values():
-            total_tasks += len(pipeline["Pipeline"])
-
+        # create variable to see how many total objects are in the user's pipeline
+        total_tasks = sum(len(pipeline["Pipeline"]) for pipeline in pipelines.values())
         # check the total # of items in their pipeline
         if total_tasks > 0:
             # map and add the data to a global variable
             tenant_pipeline.append(
                 Pipeline(
                     email=user["email"],
                     fullName=f'{user["firstName"]} {user["lastName"]}',
@@ -507,25 +502,22 @@
                                 'style="height:40px;v-text-anchor:middle;width:60px;" arcsize="5%" '
                                 'strokecolor="#22C2DC" fillcolor="#1DC3EB"><w:anchorlock/><center'
                                 ' style="color:#ffffff;font-family:Roboto, Arial, sans-serif;font'
                                 '-size:14px;">View</center></v:roundrect><![endif]-->'
                             )
                             url += f'<a href="{href}" style="mso-hide:all;">View</a>'
 
+                            headers.append("Action")
                             if pipe.lower() == "workflow":
                                 update_dict = {"UUID": url}
                                 item = {**update_dict, **item}
-                                headers.append("Action")
                                 headers.append("ID")
                             else:
                                 # replace the UUID with the HTML url
                                 item[key] = url
-                                # append the header to the headers list
-                                headers.append("Action")
-                        # check if the item is a date field and reformat it
                         elif (
                             "ById" in key
                             or "ownerid" in key.lower()
                             or key.lower() in id_fields
                         ) and item[key]:
                             # remove ById from the key
                             new_key = key.replace("Id", "")
@@ -542,50 +534,44 @@
                                 ] = f'{users[user_id]["firstName"]} {users[user_id]["lastName"]}'
                             except KeyError:
                                 # means the user is not activated, fetch them via API
                                 user = get_user(api, user_id)
                                 item[key] = f'{user["firstName"]} {user["lastName"]}'
                             # add the updated key to the table headers
                             headers.append(new_key)
-                        # rename the Atlas Module key to Parent Module
                         elif key.lower() == "atlasmodule":
                             headers.append("Parent Module")
                         elif (
                             "date" in key.lower() or "finish" in key.lower()
                         ) and item[key]:
                             try:
                                 # convert string to a date & reformat the date to a legible string
                                 item[key] = reformat_str_date(item[key], "%b %d, %Y")
                             except ValueError:
                                 headers.append(fixed_key)
                                 continue
                             # append the Title Case header to the headers list
                             headers.append(fixed_key)
-                        # if it is the id field, make it all caps
                         elif key == "id":
                             # change the key to all uppercase
                             headers.append(key.upper())
-                        # see if the data is a string and has any html elements
                         elif isinstance(item[key], str) and "<" in item[key]:
                             # replace </br> with \n
                             text = item[key].replace("</br>", "\n")
 
                             # strip other html codes from string values
                             item[key] = re.sub("<[^<]+?>", "", text)
 
                             # append the Title Case header to headers
                             headers.append(fixed_key)
-                        # increment the current step by 1, this accounts for 0 indexing
                         elif key.lower() == "currentstep":
                             item[key] += 1
                             headers.append(fixed_key)
-                        # remove the empty list of workflow steps
                         elif key.lower() == "workflowinstancesteps":
                             del item[key]
-                        # the data doesn't have to be reformatted, append the Title Case Header
                         else:
                             headers.append(fixed_key)
                     # add it to the final pipeline for the user
                     prelim_pipeline.append(item)
                 # check to see if there is an item for the bucket before
                 # appending it to the final_pipeline for the email
                 if len(prelim_pipeline) > 0:
@@ -679,23 +665,21 @@
         if not final_pipeline[threads[i]].emailed:
             # set the emailed flag to true
             final_pipeline[threads[i]].emailed = True
 
             # iterate through all items in final_pipeline to
             # set up data tables as a html tables using pandas
             for item in final_pipeline[threads[i]].pipelines:
-                # add a header for the data table
-                tables.append(f'<h1>{item["bucket"]}</h1>')
-
-                # add data table to our tables list, format table to have no border
-                # and justify everything to the left
-                tables.append(
-                    item["items"]
-                    .to_html(justify="left", index=False)
-                    .replace('border="1"', 'border="0"')
+                tables.extend(
+                    (
+                        f'<h1>{item["bucket"]}</h1>',
+                        item["items"]
+                        .to_html(justify="left", index=False)
+                        .replace('border="1"', 'border="0"'),
+                    )
                 )
             # join all the items in tables and separate them all with a </br> tag
             tables = "</br>".join(tables)
 
             # fix any broken html tags
             tables = (
                 tables.replace("&amp;", "&")
```

## app/internal/assessments_editor.py

```diff
@@ -2,31 +2,31 @@
 # -*- coding: utf-8 -*-
 """ Module to allow user to make changes to Assessments in an excel spreadsheet for user friendly experience """
 
 # standard python imports
 import json
 import os
 import shutil
+import sys
 from pathlib import Path
 
-import sys
 import click
 import pandas as pd
 import requests
 from openpyxl import Workbook, load_workbook
 from openpyxl.styles import Protection, Font, NamedStyle
 from openpyxl.worksheet.datavalidation import DataValidation
 
 from app.api import Api
 from app.application import Application
 from app.logz import create_logger
 from app.utils.app_utils import check_file_path, error_and_exit, reformat_str_date
 from models.app_models.click import regscale_id, regscale_module
-from models.regscale_models.modules import Modules
 from models.regscale_models.assessment import Assessment
+from models.regscale_models.modules import Modules
 
 # from models.regscale_models.modules import Modules
 
 logger = create_logger()
 app = Application()
 config = app.config
 api = Api(app)
@@ -264,15 +264,15 @@
                     logger.error("Cell adjustment failed due to empty cells.")
             adjusted_width = (max_length + 2) * 1.2
             worksheet.column_dimensions[column].width = adjusted_width
 
         workbook.save(filename=os.path.join(path, "new_assessments.xlsx"))
 
     except OSError:
-        logger.error("Creation of the directory %s failed." % path)
+        logger.error(f"Creation of the directory {path} failed.")
 
     return logger.info(
         "Your excel workbook has been created. Please open the new_assessments workbook and add new assessments."
     )
 
 
 @assessments.command(name="generate")
@@ -368,17 +368,15 @@
             AssessmentType = a["assessmentType"]
             PlannedStart = reformat_str_date(a["plannedStart"])
             PlannedFinish = reformat_str_date(a["plannedFinish"])
             Status = a["status"]
             ActualFinish = (
                 reformat_str_date(a["actualFinish"]) if a["actualFinish"] else "None"
             )
-            AssessmentResult = (
-                a["assessmentResult"] if a["assessmentResult"] else "None"
-            )
+            AssessmentResult = a["assessmentResult"] or "None"
             ParentId = a["parentId"]
             ParentModule = a["parentModule"]
 
             assessments_data.append(
                 [
                     Id,
                     Title,
@@ -601,22 +599,22 @@
                 if cell.row > 1:
                     cell.style = date_style
 
         workbook.save(filename=os.path.join(path, "all_assessments.xlsx"))
 
     else:
         logger.info(
-            "Please check your selctions for RegScale Id and RegScale Module and try again."
+            "Please check your selections for RegScale Id and RegScale Module and try again."
         )
         error_and_exit(
-            "There was an error creating your workbbok for the given RegSclae Id and RegScale Module."
+            "There was an error creating your workbook for the given RegScale Id and RegScale Module."
         )
 
     return logger.info(
-        "Your data has beeen loaded into your excel workbook. Please open the all_assessments workbook and make your desired changes."
+        "Your data has been loaded into your excel workbook. Please open the all_assessments workbook and make your desired changes."
     )
 
 
 @assessments.command(name="load")
 @click.option(
     "--path",
     type=click.Path(exists=False, dir_okay=True, path_type=Path),
@@ -644,143 +642,135 @@
             columns={"name": "Organization", "id": "OrganizationId"}
         )
         accounts = pd.read_excel(new_files, sheet_name="Accounts")
         new = new.merge(accounts, how="left", on="LeadAssessor")
         new = new.merge(facilities, how="left", on="Facility")
         new = new.merge(organizations, how="left", on="Organization")
         new = new.T.to_dict()
-        new_assessments = []
-        for value in new.values():
-            new_assessments.append(
-                Assessment(
-                    leadAssessorId=value["LeadAssessorId"],
-                    title=value["Title"],
-                    assessmentType=value["AssessmentType"],
-                    plannedStart=value["PlannedStart"],
-                    plannedFinish=value["PlannedFinish"],
-                    status=value["Status"],
-                    facilityId=value["FacilityId"],
-                    orgId=value["OrganizationId"],
-                    assessmentResult=value["AssessmentResult"],
-                    actualFinish=value["ActualFinish"],
-                    parentId=value["ParentId"],
-                    parentModule=value["ParentModule"],
-                )
+        new_assessments = [
+            Assessment(
+                leadAssessorId=value["LeadAssessorId"],
+                title=value["Title"],
+                assessmentType=value["AssessmentType"],
+                plannedStart=value["PlannedStart"],
+                plannedFinish=value["PlannedFinish"],
+                status=value["Status"],
+                facilityId=value["FacilityId"],
+                orgId=value["OrganizationId"],
+                assessmentResult=value["AssessmentResult"],
+                actualFinish=value["ActualFinish"],
+                parentId=value["ParentId"],
+                parentModule=value["ParentModule"],
             )
+            for value in new.values()
+        ]
         new_load = json.dumps(new_assessments)
         try:
             api.post(
                 url=config["domain"] + "/api/assessments/batchCreate",
                 json=new_load,
             )
             logger.info(
                 "%s total assessments were added to RegScale database.",
                 str(len(new_load)),
             )
         except requests.exceptions.RequestException as ex:
             logger.error(
-                "Unable to add "
-                + str(len(new_load))
-                + " total assessments to RegScale database.",
+                f"Unable to add {len(new_load)} assessment(s) to RegScale.",
                 ex,
             )
-    else:
-        if os.path.isfile(os.path.join(path, "all_assessments")):
-            # Checking all_assessments file for differences before updating database
+    elif os.path.isfile(os.path.join(path, "all_assessments")):
+        # Checking all_assessments file for differences before updating database
 
-            df1 = load_workbook(os.path.join(path, "old_assessments.xlsx"))
-            ws1 = df1.active
-            data1 = ws1.values
-            columns1 = next(data1)[0:]
-            df1 = pd.DataFrame(data1, columns=columns1)
-
-            df2 = load_workbook(os.path.join(path, "all_assessments.xlsx"))
-            ws2 = df2.active
-            data2 = ws2.values
-            columns2 = next(data2)[0:]
-            df2 = pd.DataFrame(data2, columns=columns2)
-
-            data_frame_same = df1.equals(df2)
-            if data_frame_same:
-                logger.info("No differences detected.")
-                sys.exit(1)
-
-            else:
-                logger.info("*** WARNING *** Differences Found")
-
-            updated_files = os.path.join(path, "all_assessments.xlsx")
-            updated = pd.read_excel(updated_files)
-            updated["Facility"] = updated["Facility"].fillna("None")
-            updated["Organization"] = updated["Organization"].fillna("None")
-            updated["AssessmentResult"] = updated["AssessmentResult"].fillna("N/A")
-            facilities = pd.read_excel(updated_files, sheet_name="Facilities")
-            facilities = facilities.rename(
-                columns={"name": "Facility", "id": "FacilityId"}
-            )
-            organizations = pd.read_excel(updated_files, sheet_name="Organizations")
-            organizations = organizations.rename(
-                columns={"name": "Organization", "id": "OrganizationId"}
-            )
-            accounts = pd.read_excel(updated_files, sheet_name="Accounts")
-            updated = updated.merge(accounts, how="left", on="LeadAssessor")
-            updated = updated.merge(facilities, how="left", on="Facility")
-            updated = updated.merge(organizations, how="left", on="Organization")
-            updated = updated.T.to_dict()
-            updated_assessments = []
-            for value in updated.values():
-                updated_assessments.append(
-                    Assessment(
-                        leadAssessorId=value["LeadAssessorId"],
-                        id=value["Id"],
-                        title=value["Title"],
-                        assessmentType=value["AssessmentType"],
-                        plannedStart=value["PlannedStart"],
-                        plannedFinish=value["PlannedFinish"],
-                        status=value["Status"],
-                        facilityId=value["FacilityId"],
-                        orgId=value["OrganizationId"],
-                        assessmentResult=value["AssessmentResult"],
-                        actualFinish=value["ActualFinish"],
-                        parentId=value["ParentId"],
-                        parentModule=value["ParentModule"],
-                    )
+        df1 = load_workbook(os.path.join(path, "old_assessments.xlsx"))
+        ws1 = df1.active
+        data1 = ws1.values
+        columns1 = next(data1)[:]
+        df1 = pd.DataFrame(data1, columns=columns1)
+
+        df2 = load_workbook(os.path.join(path, "all_assessments.xlsx"))
+        ws2 = df2.active
+        data2 = ws2.values
+        columns2 = next(data2)[:]
+        df2 = pd.DataFrame(data2, columns=columns2)
+
+        if df1.equals(df2):
+            logger.info("No differences detected.")
+            sys.exit(1)
+        else:
+            logger.warning("Differences found!")
+
+        updated_files = os.path.join(path, "all_assessments.xlsx")
+        updated = pd.read_excel(updated_files)
+        updated["Facility"] = updated["Facility"].fillna("None")
+        updated["Organization"] = updated["Organization"].fillna("None")
+        updated["AssessmentResult"] = updated["AssessmentResult"].fillna("N/A")
+        facilities = pd.read_excel(updated_files, sheet_name="Facilities")
+        facilities = facilities.rename(columns={"name": "Facility", "id": "FacilityId"})
+        organizations = pd.read_excel(updated_files, sheet_name="Organizations")
+        organizations = organizations.rename(
+            columns={"name": "Organization", "id": "OrganizationId"}
+        )
+        accounts = pd.read_excel(updated_files, sheet_name="Accounts")
+        updated = updated.merge(accounts, how="left", on="LeadAssessor")
+        updated = updated.merge(facilities, how="left", on="Facility")
+        updated = updated.merge(organizations, how="left", on="Organization")
+        updated = updated.T.to_dict()
+        updated_assessments = []
+        for value in updated.values():
+            updated_assessments.append(
+                Assessment(
+                    leadAssessorId=value["LeadAssessorId"],
+                    id=value["Id"],
+                    title=value["Title"],
+                    assessmentType=value["AssessmentType"],
+                    plannedStart=value["PlannedStart"],
+                    plannedFinish=value["PlannedFinish"],
+                    status=value["Status"],
+                    facilityId=value["FacilityId"],
+                    orgId=value["OrganizationId"],
+                    assessmentResult=value["AssessmentResult"],
+                    actualFinish=value["ActualFinish"],
+                    parentId=value["ParentId"],
+                    parentModule=value["ParentModule"],
                 )
+            )
 
-                updated_load = json.dumps(updated_assessments)
-                try:
-                    api.post(
-                        url=config["domain"] + f"/api/assessments/{id}",
-                        json=updated_load,
-                    )
-                    logger.info(
-                        "Successfully updated all assessments in RegScale. Thank you!"
-                    )
-                except requests.exceptions.RequestException as ex:
-                    logger.error(
-                        "Unable to update assessments in RegScale database. \n %s", ex
-                    )
+            updated_load = json.dumps(updated_assessments)
+            try:
+                response = api.post(
+                    url=config["domain"] + f"/api/assessments/{id}",
+                    json=updated_load,
+                )
+                if response.ok:
+                    logger.info(f"Successfully updated assessment #{id} in RegScale.")
+                else:
+                    logger.warning(f"Unable to update assessment #{id} in RegScale.")
+            except requests.exceptions.RequestException as ex:
+                logger.error(
+                    "Unable to update assessments in RegScale database. \n %s", ex
+                )
     os.remove(os.path.join(path, "all_assessments.xlsx"))
     os.remove(os.path.join(path, "old_assessments.xlsx"))
     os.remove(os.path.join(path, "new_assessments.xlsx"))
     return logger.info(
         "Assessment files have been deleted. Changes made to existing files can be seen in differences.txt file. Thank you!"
     )
 
 
 def get_maximum_rows(*, sheet_object):
     """This function finds the last row containing data in a spreadsheet
     :param sheet_object: excel worksheet to be referenced
     :return: integer representing last row with data in spreadsheet
     :rtype: int
     """
-    rows = 0
-    for max_row, row in enumerate(sheet_object, 1):
-        if not all(col.value is None for col in row):
-            rows += 1
-    return rows
+    return sum(
+        any(col.value is not None for col in row)
+        for max_row, row in enumerate(sheet_object, 1)
+    )
 
 
 def get_field_names(field_name):
     """
     This function uses GraphQL to retrieve all names of a given parent table in database
     :return: pandas dataframe with facility names
     :rtype: pd.dataframe
@@ -800,40 +790,27 @@
     }
     """.replace(
         "field_name", field_name
     )
 
     field_items = api.graph(query=body)
     names = field_items[str(field_name)]["items"]
-    field_names = []
-    for i in names:
-        name = i["name"]
-        id = i["id"]
-        field_names.append([name, id])
-
+    field_names = [[i["name"], i["id"]] for i in names]
     all_names = pd.DataFrame(field_names, index=None, columns=["name", "id"])
     all_names.loc[len(all_names.index)] = ["None", "None"]
 
     return all_names
 
 
 def get_user_names():
     """This function uses API Endpoint to retrieve all user names in database
     :return: pandas dataframe with usernames
     :rtype: pd.dataframe
     """
     accounts = api.get(url=config["domain"] + "/api/accounts").json()
 
-    user_names = []
-    for item in accounts:
-        name = item["name"]
-        id = item["id"]
-        user_names.append([name, id])
-
-    all_user_names = pd.DataFrame(
+    user_names = [[item["name"], item["id"]] for item in accounts]
+    return pd.DataFrame(
         user_names,
         index=None,
         columns=["LeadAssessor", "LeadAssessorId"],
     )
-    # all_user_names.loc[len(all_user_names.index)] = ["None"]
-
-    return all_user_names
```

## app/internal/comparison.py

```diff
@@ -114,17 +114,16 @@
         if len(recent_files) == 2:
             # set the old_file and new_file accordingly
             old_file = recent_files[1]
             new_file = recent_files[0]
         else:
             # notify user we don't have two files to compare and exit application
             error_and_exit(
-                "Required 2 files to compare, but only 1 {} file found in {}!".format(
-                    most_recent_file_type, most_recent_in_file_path
-                )
+                f"Required 2 files to compare, but only 1 {most_recent_file_type}"
+                f"file found in {most_recent_in_file_path}!"
             )
     # make sure both file paths exist
     if exists(old_file) and exists(new_file):
         with job_progress:
             # check the file extensions and compare them
             old_file_type, new_file_type = get_file_type(old_file), get_file_type(
                 new_file
@@ -236,23 +235,19 @@
 
             # create new task for creating assessment in RegScale
             create_assessment = job_progress.add_task(
                 "[#21a5bb]Creating assessment in RegScale...",
                 total=1,
             )
 
-            # create a new assessment in RegScale
-            new_assessment_id = create_regscale_assessment(
+            if new_assessment_id := create_regscale_assessment(
                 url=f"{app.config['domain']}/api/assessments",
                 new_assessment=new_assessment.dict(),
                 api=api,
-            )
-
-            # verify creating the assessment was successful
-            if new_assessment_id:
+            ):
                 # mark the create_assessment task as complete
                 job_progress.update(create_assessment, advance=1)
 
                 # create new task for file uploads
                 upload_files = job_progress.add_task(
                     "[#0866b4]Uploading files to the new RegScale Assessment...",
                     total=2,
@@ -427,11 +422,11 @@
     # remove the field if requested, default is True
     if pop_flag:
         # remove the column from the dataset
         filtered_data.pop(column)
 
     # return HTML formatted data table
     return (
-        filtered_data.to_html(justify="left", index=False)
-        if not filtered_data.empty
-        else None
+        None
+        if filtered_data.empty
+        else filtered_data.to_html(justify="left", index=False)
     )
```

## app/internal/control_editor.py

```diff
@@ -98,15 +98,15 @@
                                 pageInfo {
                                     hasNextPage
                                 }
                             }
                             }""".replace(
                     "parent_module", regscale_module
                 ).replace(
-                    "parent_id", str(regscale_id)
+                    "parent_id", regscale_id
                 )
 
                 existing_implementation_data = api.graph(query=body)
 
             except requests.RequestException as ex:
                 error_and_exit(
                     "Unable to retrieve assessment list from RegScale.\n %s", ex
@@ -247,17 +247,17 @@
             for col in ["F", "G", "H", "I", "J"]:
                 for cell in worksheet[col]:
                     cell.protection = Protection(locked=False)
 
             workbook.save(filename=os.path.join(path, "all_implementations.xlsx"))
 
         except OSError:
-            logger.error("Creation of the directory %s failed." % path)
+            logger.error(f"Creation of the directory {path} failed.")
     else:
-        logger.info("Successfully created the directory %s." % path)
+        logger.info(f"Successfully created the directory {path}.")
         logger.info("All files are located within directory.")
         sys.exit()
 
     return logger.info(
         "Your data has been loaded into your excel workbook. Please open the all_implementations workbook and make your desired changes."
     )
```

## app/internal/evidence.py

```diff
@@ -1,13 +1,15 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ Integrates evidence gathering into RegScale CLI """
 
+
 # standard python imports
 import fnmatch
+import itertools
 import json
 import os
 from datetime import datetime
 from typing import Tuple
 
 import click  # type: ignore
 import fitz  # type: ignore
@@ -255,17 +257,15 @@
     # bring in score lists
     true_scores = score_data[0]
     total_scores = score_data[2]
     # set score values
     true_score = true_scores[number]
     total_score = total_scores[number]
     # calculate test score for this result and check for zero division
-    test_score = int((true_score / total_score) * 100) if int(total_score) != 0 else 0
-    # return variable for use outside of local scope
-    return test_score
+    return int((true_score / total_score) * 100) if int(total_score) != 0 else 0
 
 
 def find_signatures(file: str) -> int:
     """
     Determine if the file is digitally signed
     :param str file: file path
     :return: # of signatures found
@@ -392,34 +392,33 @@
     # create empty list to hold file modified times
     modified_times: list[dict] = []
     progress.update(task, advance=1)
     # get list of folders in parent folder
     folders_list = os.listdir(evidence_folder)
     progress.update(task, advance=1)
     # remove any child folders that start with '.'
-    remove(list_to_review=folders_list)
+    new_folders = remove(list_to_review=folders_list)
     progress.update(task, advance=1)
     # loop through directory listing
-    for folder in folders_list:
+    for folder in new_folders:
         # get list of files in each folder
         filelist = os.listdir(os.path.join(evidence_folder, folder))
         # remove any files that start with '.'
         remove(list_to_review=filelist)
         # loop through list of files in each folder
-        for filename in filelist:
-            # append the modified time for each file to the list
-            modified_times.append(
-                {
-                    "program": folder,
-                    "file": filename,
-                    "last-modified": os.path.getmtime(
-                        os.path.join(directory, folder, filename)
-                    ),
-                }
-            )
+        modified_times.extend(
+            {
+                "program": folder,
+                "file": filename,
+                "last-modified": os.path.getmtime(
+                    os.path.join(directory, folder, filename)
+                ),
+            }
+            for filename in filelist
+        )
     progress.update(task, advance=1)
     # loop through the list of timestamps
     for i, time_data in enumerate(modified_times):
         # update the last-modified value to be the count of days
         modified_times[i].update(
             {
                 "last-modified": delta(
@@ -472,23 +471,22 @@
     # create empty list to hold list of files in directory
     dir_list: list[dict] = []
     progress.update(task, advance=1)
     # build a list of all folders to iterate through
     folder_list = os.listdir(evidence_folder)
     progress.update(task, advance=1)
     # remove any folders starting with '.' from list
-    remove(folder_list)
+    new_folders_list = remove(folder_list)
     progress.update(task, advance=1)
-    for folder in folder_list:
+    for folder in new_folders_list:
         # build a list of all files contained in sub-directories
         filelist = os.listdir(evidence_folder + os.sep + folder)
         # remove folders and file names that start with a .
         remove(filelist)
-        for filename in filelist:
-            dir_list.append({"program": folder, "file": filename})
+        dir_list.extend({"program": folder, "file": filename} for filename in filelist)
     progress.update(task, advance=1)
     # return variable for use outside of local scope
     return dir_list
 
 
 def signature_assessment_results(
     directory: list[dict], r_docs: list[dict], task
@@ -603,35 +601,29 @@
     # create an empty list to hold all strings from parsed documents
     full_text: list[dict] = []
     progress.update(task, advance=1)
     # build a list of files in the folder
     folder_list = os.listdir(evidence_folder)
     progress.update(task, advance=1)
     # remove all folders that start with '.'
-    remove(folder_list)
+    removed_folders_list = remove(folder_list)
     progress.update(task, advance=1)
-    for folder in folder_list:
+    for folder in removed_folders_list:
         # create a list of files to iterate through for parsing
         file_list = os.listdir((os.path.join(evidence_folder, folder)))
         remove(file_list)
         # iterate through all files in the list
         for filename in file_list:
             # if the filename is a .docx file
             if filename.endswith(".docx"):
-                # create empty list to hold text per file
-                output: list[str] = []
                 # open the Word document to enable parsing
                 document = Document(os.path.join(evidence_folder, folder, filename))
-                # iterate through each paragraph in the document
-                for para in document.paragraphs:
-                    # append each paragraph of strings to the list
-                    output.append(para.text)
+                output: list[str] = [para.text for para in document.paragraphs]
                 # add each file and the requisite text to the dictionary to test
                 full_text.append({"program": folder, "file": filename, "text": output})
-            # if the filename is a .pdf file
             elif filename.endswith(".pdf"):
                 # create empty list to hold text per file
                 output_text_list: list[str] = []
                 # open filename with pdfplumber
                 with pdfplumber.open(filename) as pdf:
                     # set number of pages
                     pages = pdf.pages
@@ -663,38 +655,36 @@
     :return: Results of searched text in documents
     :rtype: list[dict]
     """
     # create empty list to hold assessment results
     search_list: list[dict] = []
     progress.update(task, advance=1)
     # iterate through each sentence in the required texts
-    for parsed_file in f_texts:
-        # then iterate through each parsed paragraph
-        for line in req_texts:
-            # if the required text appears in the parsed paragraph
-            if any(line in text for text in parsed_file["text"]) is True:
-                # then create a "True" entry in the empty list
-                search_list.append(
-                    {
-                        "program": parsed_file["program"],
-                        "file": parsed_file["file"],
-                        "text": line,
-                        "result": True,
-                    }
-                )
-            else:
-                # else create a "False" entry in the empty list
-                search_list.append(
-                    {
-                        "program": parsed_file["program"],
-                        "file": parsed_file["file"],
-                        "text": line,
-                        "result": False,
-                    }
-                )
+    for parsed_file, line in itertools.product(f_texts, req_texts):
+        # if the required text appears in the parsed paragraph
+        if any(line in text for text in parsed_file["text"]):
+            # then create a "True" entry in the empty list
+            search_list.append(
+                {
+                    "program": parsed_file["program"],
+                    "file": parsed_file["file"],
+                    "text": line,
+                    "result": True,
+                }
+            )
+        else:
+            # else create a "False" entry in the empty list
+            search_list.append(
+                {
+                    "program": parsed_file["program"],
+                    "file": parsed_file["file"],
+                    "text": line,
+                    "result": False,
+                }
+            )
     progress.update(task, advance=1)
     # return variable for use outside of local scope
     return search_list
 
 
 def text_assessment_results(
     searches: list[dict], r_texts: set[str], task
@@ -707,31 +697,27 @@
     :return: List of results
     :rtype: list[dict]
     """
     # create empty list to hold assessment results
     text_results: list[dict] = []
     progress.update(task, advance=1)
     # loop through text string search results
-    for result in searches:
-        # loop through required texts
-        for line in r_texts:
-            # if the text matches the required text
-            if result["text"] == line:
-                text_info = result["text"]
-                # if the result of the previous text search was true
-                if result["result"] is True:
-                    # condense results into 1 per file
-                    text_results.append(
-                        {
-                            "program": result["program"],
-                            "file": result["file"],
-                            "test": f"required-text ({text_info})",
-                            "result": result["result"],
-                        }
-                    )
+    for result, line in itertools.product(searches, r_texts):
+        # if the text matches the required text
+        if result["text"] == line and result["result"] is True:
+            text_info = result["text"]
+            # condense results into 1 per file
+            text_results.append(
+                {
+                    "program": result["program"],
+                    "file": result["file"],
+                    "test": f"required-text ({text_info})",
+                    "result": result["result"],
+                }
+            )
     # return variable for use outside of local scope
     progress.update(task, advance=1)
     # return variable for use outside of local scope
     return text_results
 
 
 def gather_test_project_data(evidence_folder: str, task) -> list[dict]:
@@ -748,22 +734,21 @@
     # test project information created in RegScale UI
     with open(
         evidence_folder + os.sep + "list.json", "r", encoding="utf-8"
     ) as json_file:
         # load json object into a readable dictionary
         lists = json.load(json_file)
         # loop through projects in the list.json
-        for i in range(len(lists["parser-list"])):
-            # append the id and program to the list of test projects
-            test_data.append(
-                {
-                    "id": lists["parser-list"][i].get("id"),
-                    "program": lists["parser-list"][i].get("folder-name"),
-                }
-            )
+        test_data.extend(
+            {
+                "id": lists["parser-list"][i].get("id"),
+                "program": lists["parser-list"][i].get("folder-name"),
+            }
+            for i in range(len(lists["parser-list"]))
+        )
     progress.update(task, advance=1)
     # create empty list to hold json response data for each project
     test_info: list[dict] = []
     # iterate through test projects and make sequential GET API calls
     for item in test_data:
         # make a GET request for each project
         response = api.get(
@@ -854,32 +839,25 @@
     :param list[dict] textres: List of text results
     :param list[dict] timeres: List of time results
     :param list[dict] sigres: List of signature results
     :param task: The task to update on the job_progress
     :return: List of assessment report for all results
     :rtype: list[dict]
     """
-    # create a list to hold all assessment results
-    assessment_report: list[dict] = []
     progress.update(task, advance=1)
-    # append all results to 1 master list
-    for result in docres:
-        assessment_report.append(result)
+    assessment_report: list[dict] = list(docres)
     progress.update(task, advance=1)
     # append all results to 1 master list
-    for result in textres:
-        assessment_report.append(result)
+    assessment_report.extend(iter(textres))
     progress.update(task, advance=1)
     # append all results to 1 master list
-    for result in timeres:
-        assessment_report.append(result)
+    assessment_report.extend(iter(timeres))
     progress.update(task, advance=1)
     # append all results to 1 master list
-    for result in sigres:
-        assessment_report.append(result)
+    assessment_report.extend(iter(sigres))
     progress.update(task, advance=1)
     # return variable for use outside of local scope
     return assessment_report
 
 
 def build_assessment_dataframe(assessments: list[dict], task) -> list[dict]:
     """
@@ -1017,14 +995,15 @@
     for i, project in enumerate(project_data):
         # call score calculation function
         test_score = calc_score(i)
         # if file name matches html output table program name
         if project_data[i]["program"] == output[i]["program"]:
             # build assessment data
             assessment_data = Assessment(
+                status="Complete",
                 leadAssessorId=app.config["userId"],
                 title="Evidence Collection Automation Assessment",
                 assessmentType="Inspection",
                 projectId=project["id"],
                 parentId=project["id"],
                 parentModule="projects",
                 assessmentReport=output[i]["html"],
@@ -1052,9 +1031,11 @@
             f'{app.config["domain"]}/api/assessments',
             json=assessment_data.dict(),
         )
         # log successful
         if update_request.status_code == 200:
             logger.info("Child assessment creation was successful.")
         else:
-            logger.error("Child assessment creation was not successful.")
+            logger.warning(
+                f"Child assessment creation was not successful.\n {update_request.status_code} {update_request.text}"
+            )
     progress.update(task, advance=1)
```

## app/internal/healthcheck.py

```diff
@@ -60,9 +60,9 @@
     else:
         logger.error("No data returned from system health check.")
         sys.exit(1)
     # process checks
     if "entries" in health_data:
         checks = health_data["entries"]
         for chk in checks:
-            logger.info("System: " + chk + ", Status: " + checks[chk]["status"])
+            logger.info(f"System: {chk}, Status: " + checks[chk]["status"])
     return health_data
```

## app/internal/login.py

```diff
@@ -1,12 +1,14 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ Module to allow user to login to RegScale """
 
+
 # standard python imports
+import contextlib
 import logging
 import sys
 from datetime import datetime
 from json import JSONDecodeError
 from ssl import SSLCertVerificationError
 from typing import Tuple
 
@@ -40,15 +42,15 @@
         raise ValueError("No domain set in the initialization file.")
     if config["domain"] == "":
         raise ValueError("The domain is blank in the initialization file.")
     # set the catalog URL for your RegScale instance
     if host is None:
         url_login = normalize_url(config["domain"] + "/api/authentication/login")
     else:
-        url_login = normalize_url(host + "/api/authentication/login")
+        url_login = normalize_url(f"{host}/api/authentication/login")
     logger.info("Logging into: %s", url_login)
 
     # create object to authenticate
     auth = {"userName": str_user, "password": str_password, "oldPassword": ""}
     logging.debug(auth)
     if auth["password"]:
         try:
@@ -137,16 +139,15 @@
                 url=f'{config["domain"]}/api/logging/filterLogs/0/0'
             )
         else:
             url_login = normalize_url(url=f"{host}/api/logging/filterLogs/0/0")
         logger.debug("config: %s", config)
         logger.debug("is_valid url: %s", url_login)
         logger.debug("is_valid headers: %s", headers)
-        response = api.get(url=url_login, headers=headers)
-        if response:
+        if response := api.get(url=url_login, headers=headers):
             if response.status_code == 200:
                 login_status = True
     except KeyError as ex:
         if str(ex).replace("'", "") == "token":
             logger.debug("Token is missing, we will generate this")
     except ConnectionError:
         logger.error(
@@ -168,15 +169,14 @@
     Verify if the application is licensed
     :param app: Application object
     :return: License status
     :rtype: bool
     """
     status = False
     api = Api(app=app)
-    try:
+    # TODO: Need to account for versions of the API with no license endpoint
+    with contextlib.suppress(requests.RequestException):
         lic = app.get_regscale_license(appl=app, api=api).json()
         license_date = datetime.strptime(lic["expirationDate"], "%Y-%m-%d")
         if lic["licenseType"] == "Enterprise" and license_date > datetime.now():
             status = True
-    except requests.RequestException:
-        pass  # TODO: Need to account for versions of the API with no license endpoint
     return status
```

## app/public/cisa.py

```diff
@@ -153,15 +153,15 @@
                 links.append((link, short_description, title))
                 logger.info("Building RegScale threat from %s.", link)
                 link = None
 
         items = len(articles)
         page += 1
         # check if max threads <= 20 to prevent IP ban from CISA
-        max_threads = app.config["maxThreads"] if app.config["maxThreads"] <= 20 else 20
+        max_threads = min(app.config["maxThreads"], 20)
         with ThreadPoolExecutor(max_workers=max_threads) as executor:
             futures = []
             for link in links:
                 logger.info("Building RegScale threat from %s.", link[0])
                 futures.append(
                     executor.submit(
                         build_threat,
@@ -202,16 +202,15 @@
             title=title,
             threatType="Specific",
             threatOwnerId=app.config["userId"],
             dateIdentified=date_created,
             targetType="Other",
             source="Open Source",
             description=short_description
-            if short_description
-            else f"""<p><a href="{detailed_link}" title="">{detailed_link}</a></p>""",
+            or f"""<p><a href="{detailed_link}" title="">{detailed_link}</a></p>""",
             vulnerabilityAnalysis="\n".join(vulnerability),
             mitigations="\n".join(mitigation),
             notes="\n".join(notes),
             dateCreated=date_created,
             status="Under Investigation",
         )
     return threat
@@ -264,15 +263,15 @@
                 nav_string = dat.text.lower()
                 try:
                     for element in [
                         ele
                         for ele in dat.next_elements
                         if ele.text.lower() != nav_string and isinstance(ele, Tag)
                     ]:
-                        if not (element.text).replace("\n", "").lower() in div_list:
+                        if (element.text).replace("\n", "").lower() not in div_list:
                             process_params()
                         else:
                             break
                 except AttributeError as ex:
                     logger.debug("AttributeError: %s", ex)
     if len(vulnerability) == 0:
         vulnerability.append("See Link for details.")
@@ -345,31 +344,29 @@
         app.save_config(config)
     response = api.get(url=cisa_url, headers={})
     try:
         response.raise_for_status()
     except exceptions.RequestException as ex:
         # Whoops it wasn't a 200
         logger.error("Error retrieving CISA KEV data: %s.", str(ex))
-    # Must have been a 200 status code
-    json_obj = response.json()
-    return json_obj
+    return response.json()
 
 
 def convert_date_string(date_str: str) -> str:
     """
     Convert the given date string for use in RegScale
     :param str date_str: date as a string
     :return: RegScale accepted datetime string format
     :rtype: str
     """
     fmt = "%Y-%m-%d"
     result_dt = datetime.strptime(
         date_str, fmt
     )  # 2022-11-03 to 2022-08-23T03:00:39.925Z
-    return result_dt.isoformat() + ".000Z"
+    return f"{result_dt.isoformat()}.000Z"
 
 
 def update_regscale(data: dict) -> None:
     """
     Update RegScale threats with the latest Known Exploited Vulnerabilities (KEV) data
     :param dict data: Threat data from CISA
     :return: None
@@ -384,15 +381,15 @@
     # reg_threats = api.get(url_threats).json()
     threats_inserted = []
     threats_updated = []
     new_threats = [
         dat for dat in data["vulnerabilities"] if dat not in matching_threats
     ]
     console.print(f"Found {len(new_threats)} new threats from CISA")
-    if len([dat for dat in data["vulnerabilities"] if dat not in matching_threats]) > 0:
+    if [dat for dat in data["vulnerabilities"] if dat not in matching_threats]:
         for rec in new_threats:
             threat = Threat(
                 uuid=Threat.xstr(None),
                 title=rec["cveID"],
                 threatType="Specific",
                 threatOwnerId=app.config["userId"],
                 dateIdentified=convert_date_string(rec["dateAdded"]),
@@ -427,18 +424,18 @@
                 threat
                 for threat in reg_threats
                 if threat["description"] == update_vuln["description"]
             ][0]
             update_vuln = merge_old(update_vuln=update_vuln, old_vuln=old_vuln)
             if old_vuln:
                 threats_updated.append(update_vuln)
-    # Update Matching Threats
-    url_threats = app.config["domain"] + "/api/threats"
     if len(threats_inserted) > 0:
         logging.getLogger("urllib3").propagate = False
+        # Update Matching Threats
+        url_threats = app.config["domain"] + "/api/threats"
         api.update_server(
             url=url_threats,
             json_list=threats_inserted,
             method="post",
             config=app.config,
             message=f"Inserting {len(threats_inserted)} threats to RegScale...",
         )
@@ -483,23 +480,19 @@
     :return: An API response based on the PUT or POST action
     :rtype: requests.Response
     """
     api = Api(app)
     config = app.config
     url_threats = config["domain"] + "/api/threats"
     headers = {"Accept": "application/json", "Authorization": config["token"]}
-    response = None
-    if not threat_id:
-        response = api.post(url=url_threats, headers=headers, json=threat)
-
-    else:
-        response = api.put(
-            url=f"{url_threats}/{threat_id}", headers=headers, json=threat
-        )
-    return response
+    return (
+        api.put(url=f"{url_threats}/{threat_id}", headers=headers, json=threat)
+        if threat_id
+        else api.post(url=url_threats, headers=headers, json=threat)
+    )
 
 
 def update_regscale_threats(
     headers: dict = None,
     json_list=None,
 ) -> None:
     """
@@ -507,18 +500,18 @@
     :param dict headers: Headers used for the RegScale API, defaults to None
     :param json_list: list of threats to be updated, defaults to None
     :return: None
     """
     logging.getLogger("urllib3").propagate = False
     app = Application()
     api = Api(app)
-    url_threats = app.config["domain"] + "/api/threats"
     if headers is None and app.config:
         headers = {"Accept": "application/json", "Authorization": app.config["token"]}
     if json_list and len(json_list) > 0:
+        url_threats = app.config["domain"] + "/api/threats"
         api.update_server(
             url=url_threats,
             method="put",
             headers=headers,
             json_list=json_list,
             message=f"Updating {len(json_list)} RegScale threats...",
         )
```

## app/public/emass.py

```diff
@@ -80,15 +80,15 @@
     raw_controls = ({},)
     formatted_controls = {}
     try:
         raw_controls = {
             val.lower(): key for key, val in file_data_dict["Control Acronym"].items()
         }
         # Remove duplicate values in dictionary
-        raw_controls = {key: val for key, val in raw_controls.items()}
+        raw_controls = dict(raw_controls)
 
         # create list of formatted controls
         formatted_controls = [
             v.lower().replace("(", ".").replace(")", "")
             for v in file_data_dict["AP Acronym"].values()
         ]
     except KeyError:
@@ -196,15 +196,15 @@
         file_data_dict=file_data_dict, file_name=file_name.name
     )
 
     # create variable to count number of rows updated
     update_counter: int = 0
 
     # create a list of all the control ids from the GraphQL query
-    regscale_control_ids = list(ctrl["control"]["controlId"] for ctrl in controls)
+    regscale_control_ids = [ctrl["control"]["controlId"] for ctrl in controls]
 
     # create comment & fill attribute for columns with missing data
     comment = Comment(
         text=f"SSP #{ssp_id} doesn't contain an assessment associated with this control.",
         author="RegScale CLI",
         height=150,
     )
```

## app/public/fedramp.py

```diff
@@ -342,18 +342,16 @@
     :return: None
     """
     parties = ssp_dict["system-security-plan"]["metadata"]["parties"]
     filtered_parties = list(filter(lambda x: x["type"] == "person", parties))
     for party in filtered_parties:
         title = [dat["value"] for dat in party["props"] if dat["name"] == "job-title"]
         phone = [dat["number"] for dat in party["telephone-numbers"]]
-        email = [dat for dat in party["email-addresses"]]
-        addresses = (
-            [dat for dat in party["addresses"]] if "addresses" in party.keys() else None
-        )
+        email = list(party["email-addresses"])
+        addresses = list(party["addresses"]) if "addresses" in party.keys() else None
         stakeholder = {
             "name": party["name"],
             "title": title[0] if title else "",
             "phone": phone[0] if phone else "",
             "email": email[0] if email else "",
             "address": addresses[0]["addr-lines"][0]
             + " "
@@ -438,17 +436,16 @@
             lastUpdatedById=config["userId"],
             createdById=config["userId"],
             status=status,
         )
         # Post Implementation
         response = post_regscale_object(api=api, config=config, obj=imp)
         imp_id = None
-        if not response.raise_for_status:
-            if response.status_code == 200:
-                imp_id = response.json()["id"]
+        if not response.raise_for_status and response.status_code == 200:
+            imp_id = response.json()["id"]
         if imp_id:
             # Add statements
             pass
 
 
 def post_regscale_object(
     api: Api, config: dict, obj: Any, endpoint="controlimplementation"
@@ -621,19 +618,19 @@
     :return: List of all document tables
     :rtype: list
     """
     tables = list(document.tables)
     for t_table in document.tables:
         for row in t_table.rows:
             for cell in row.cells:
-                for table in cell.tables:
-                    tables.append(table)
+                tables.extend(iter(cell.tables))
     return tables
 
 
+# TODO - replace these dangerous default mutables or document why they are used
 def get_xpath_data_detailed(tables, key, ident, xpath, count_array=[2, 3, 4]) -> dict:
     """
     Use Xpath to pull data from XML tables.
     :param tables: XML tables
     :param key: specific key in XML table
     :param ident:
     :param xpath: xpath of the element
@@ -641,15 +638,15 @@
     :return: Dictionary of items found
     :rtype: dict
     """
     tables = iter(tables)
     confidentiality = None
     integrity = None
     availability = None
-    for _, t_var in enumerate(tables):
+    for t_var in tables:
         if key in t_var._element.xml:
             f = StringIO(t_var._element.xml)
             tree = etree.parse(f)
             tags = tree.xpath(xpath, namespaces=namespaces)
             for p_var in tags:
                 t_tags = p_var.xpath("//w:r/w:t", namespaces=namespaces)
                 count = 0
@@ -702,20 +699,20 @@
                 # if t_tags[new_count].text[len(t_tags[new_count].text)-1] == ' ':
                 #     field = ''.join([field, t_tags[new_count+1].text])
                 #     new_count = new_count + 1
                 if element_list[count].text == field:
                     value = "".join([value, element_list[count + 1].text])
                     dat[field] = value
                     value = ""
-                    count = count + 1
+                    count += 1
                 try:
                     if element_list[count + 1].text in idents:
                         field = idents.pop(0)
                     else:
-                        if field in dat.keys():
+                        if field in dat:
                             dat[field] = "".join(
                                 [dat[field], element_list[count + 1].text]
                             )
                         count += 1
                 except IndexError:
                     logger.debug("Unable to continue, index error on row: %i.", count)
                 continue
@@ -745,22 +742,20 @@
     :param tables: XML tables
     :param key: key to sort
     :param xpath: xpath to the element
     :return: string of xpath
     :rtype: str
     """
     tables = iter(tables)
-    for _, t_enum in enumerate(tables):
+    for t_enum in tables:
         if key in t_enum._element.xml:
             f_string = StringIO(t_enum._element.xml)
             tree = etree.parse(f_string)
-            r_var = tree.xpath(xpath, namespaces=namespaces)
-            if r_var:
-                txt = r_var[0].text.split()  # Remove newlines and spaces
-                if txt:
+            if r_var := tree.xpath(xpath, namespaces=namespaces):
+                if txt := r_var[0].text.split():
                     return txt
             return None
 
 
 def get_security_objective_data(tables, key, ident):
     """
     This function currently has no usage
@@ -772,30 +767,30 @@
     tables = iter(tables)
     confidentiality = None
     integrity = None
     availability = None
     key = "Security Objective"
     xpath = "//w:tbl/w:tr"
     ident = "Confidentiality"
-    for _, t_var in enumerate(tables):
+    for t_var in tables:
         if "Security Objective" in t_var._element.xml:
             f_var = StringIO(t_var._element.xml)
             tree = etree.parse(f_var)
             tags = tree.xpath(xpath, namespaces=namespaces)
             for p_var in tags:
                 t_tags = p_var.xpath("//w:r/w:t", namespaces=namespaces)
                 count = 0
                 for t_var in t_tags:
                     if t_var.text in ident or count > 0:
                         count += 1
                         if count == 2:
                             confidentiality = t_var.text
-                        if count == 4:
+                        elif count == 4:
                             integrity = t_var.text
-                        if count == 6:
+                        elif count == 6:
                             availability = t_var.text
     return {
         "type": key,
         "confidentiality": confidentiality,
         "integrity": integrity,
         "availability": availability,
     }
@@ -804,23 +799,22 @@
 def get_information_types(tables) -> list:
     """
     Process information types
     :param tables: XML tables
     :return: List of information types
     :rtype: list
     """
-    information_types = []
-    information_types.append(
+    information_types = [
         get_xpath_data_detailed(
             tables,
             key="System Development",
             ident="C.3.5.1",
             xpath="//w:tbl/w:tr/w:tc/w:p",
         )
-    )
+    ]
     information_types.append(
         get_xpath_data_detailed(
             tables, key="System Maintenance", ident="C 3.5.3", xpath="//w:tbl/w:tr"
         )
     )
     information_types.append(
         get_xpath_data_detailed(
@@ -916,22 +910,21 @@
     key = "SP* IP Address and Interface"
     existing_interconnects = []
     dat = [table for table in table_data if key in table.keys()]
     existing_interconnect_response = api.get(
         app.config["domain"]
         + f"/api/interconnections/getAllByParent/{regscale_ssp['id']}/securityplans"
     )
-    if not existing_interconnect_response.raise_for_status():
-        if (
-            existing_interconnect_response.headers.get("content-type")
-            == "application/json; charset=utf-8"
-        ):
-            existing_interconnects = existing_interconnect_response.json()
+    if not existing_interconnect_response.raise_for_status() and (
+        existing_interconnect_response.headers.get("content-type")
+        == "application/json; charset=utf-8"
+    ):
+        existing_interconnects = existing_interconnect_response.json()
+    format = "%Y-%m-%d %H:%M:%S"
     for interconnect in dat:
-        format = "%Y-%m-%d %H:%M:%S"
         interconnection = Interconnects(
             name=interconnect[key],
             # connectionType=clean_dict['Data Direction(incoming, outgoing, or both)'],
             ao="Eaton, Bryan",
             aoId=me,
             interconnectOwner="Eaton, Bryan",
             interconnectOwnerId=me,
@@ -974,36 +967,32 @@
     key = "Ports (TCP/UDP)*"
     dat = [table for table in table_data if "Protocols" in table.keys()]
     existing_ports = []
     existing_ports_response = api.get(
         app.config["domain"]
         + f"/api/portsProtocols/getAllByParent/{ssp_id}/securityplans"
     )
-    if not existing_ports_response.raise_for_status():
-        if (
-            existing_ports_response.headers.get("content-type")
-            == "application/json; charset=utf-8"
-        ):
-            existing_ports = existing_ports_response.json()
+    if not existing_ports_response.raise_for_status() and (
+        existing_ports_response.headers.get("content-type")
+        == "application/json; charset=utf-8"
+    ):
+        existing_ports = existing_ports_response.json()
 
     for protocol in dat:
         ports_protocols = PortsProtocols(
             service=protocol["Services"],
             usedBy=protocol["Used By"],
             parentId=ssp_id,
             purpose=protocol["Purpose"],
             startPort="".join(c for c in protocol[key] if c.isdigit())
-            if "".join(c for c in protocol[key] if c.isdigit())
-            else "".join(c for c in protocol["Protocols"] if c.isdigit()),
+            or "".join(c for c in protocol["Protocols"] if c.isdigit()),
             endPort="".join(c for c in protocol[key] if c.isdigit())
-            if "".join(c for c in protocol[key] if c.isdigit())
-            else "".join(c for c in protocol["Protocols"] if c.isdigit()),
+            or "".join(c for c in protocol["Protocols"] if c.isdigit()),
             protocol="".join(c for c in protocol[key] if not c.isdigit())
-            if "".join(c for c in protocol[key] if not c.isdigit())
-            else "".join(c for c in protocol["Protocols"] if c.isdigit()),
+            or "".join(c for c in protocol["Protocols"] if c.isdigit()),
             parentModule="securityplans",
             lastUpdatedById=app.config["userId"],
             createdById=app.config["userId"],
         )
         ports_protocols.protocol = (
             ports_protocols.protocol.strip().replace("(", "").replace(")", "")
         )
@@ -1114,27 +1103,21 @@
         else:
             implementation_status = (
                 fedramp_control.implementation_status[0]
                 if fedramp_control.implementation_status
                 else "Not Implemented"
             )
         friendly_control_id = fedramp_control.number.lower()
-        match = re.search(exp, fedramp_control.number)
-        if match:
-            friendly_control_id = (
-                fedramp_control.number[0 : match.regs[1][0] - 1].strip()
-                + "."
-                + match.groups()[0]
-            ).lower()
-        control = [
+        if match := re.search(exp, fedramp_control.number):
+            friendly_control_id = f"{fedramp_control.number[:match.regs[1][0] - 1].strip()}.{match.groups()[0]}".lower()
+        if control := [
             control
             for control in mapping
             if control["controlId"].lower() == friendly_control_id
-        ]
-        if control:
+        ]:
             controlID = control[0]["controlID"]
         if len(fedramp_control.parts) > 1:
             implementation_text = "<br>".join(
                 fedramp_control.part(x).text for x in fedramp_control.parts
             )
         else:
             try:
@@ -1253,22 +1236,21 @@
     :param app: Application instance
     :param ssp_obj: SSP data from docx
     :param regscale_ssp: RegScale SSP data
     :param mapping: Profile mapping
     """
     api = Api(app)
     current_imps = get_current_implementations(app, regscale_ssp["id"])
-    ssp = [
+    if ssp := [
         ssp
         for ssp in api.get(
             url=app.config["domain"] + "/api/securityplans/getList"
         ).json()
         if ssp["title"] == regscale_ssp["systemName"]
-    ]
-    if ssp:
+    ]:
         ssp_id = ssp[0]["id"]
         existing_controls = {imp["controlID"] for imp in current_imps}
         controls_to_add = [
             control
             for control in mapping
             if control["controlID"] not in existing_controls
         ]
@@ -1385,18 +1367,15 @@
                         id_text = dat_text
                         dat["id"] = id_text
                         link_text = None
                     elif id_text:
                         title = dat_text
                         id_text = None
                     elif validate(dat_text):
-                        if title:
-                            title = " ".join([title, previous])
-                        else:
-                            title = previous
+                        title = " ".join([title, previous]) if title else previous
                         dat["title"] = title.strip()
                         link_date = e_id.text.strip()
                         dat["date"] = link_date.strip()
                         title = None
                     elif "date" in dat:
                         link_text = e_id.text.strip()
                         dat["link"] = link_text
@@ -1404,24 +1383,21 @@
                         previous = "link"
                         link_text = None
                     previous = e_id.text.strip() if len(dat) != 4 else "link"
                     if len(dat) == 4:
                         dat = {}
                     link_date = None
             titles.reverse()
-            for index, link in enumerate(table._element.xpath(".//w:hyperlink")):
+            for link in table._element.xpath(".//w:hyperlink"):
                 inner_run = link.xpath("w:r", namespaces=link.nsmap)[0]
 
                 # matches = [tit['title'] for tit in titles if tit['link'] == inner_run.text]
                 # if matches:\
                 # TODO: Improve this.
-                if titles:
-                    title = titles.pop()["title"]
-                else:
-                    title = inner_run.text
+                title = titles.pop()["title"] if titles else inner_run.text
                 # print link relationship id
                 r_id = link.get(
                     "{http://schemas.openxmlformats.org/officeDocument/2006/relationships}id"
                 )
                 # print link URL
                 attachments.append(
                     {"title": title, "link": ssp_obj.document._part.rels[r_id]._target}
@@ -1506,76 +1482,71 @@
     :param app: An application instance
     :param tables: A list of tables from the XML document.
     :param regscale_ssp: A dict of RegScale SSP data.
     :param ssp: A dict of docx SSP data.
     """
     api = Api(app)
     pocs = []
-    system_owner = get_contact_info(
-        tables, key="Owner Information", xpath="//w:tbl/w:tr"
-    )
-    pocs.append(system_owner)
-    management_poc = get_contact_info(
+    management_poc = _extracted_from_gather_stakeholders_(
         tables,
-        key="Information System Management Point of Contact",
-        xpath="//w:tbl/w:tr",
+        "Owner Information",
+        pocs,
+        "Information System Management Point of Contact",
     )
     pocs.append(management_poc)
     information = get_contact_info(
         tables,
         key="Information System Technical Point of Contact",
         xpath="//w:tbl/w:tr",
     )
-    pocs.append(information)
-    pocs.append(get_base_contact(ssp))
-
-    ao_poc = get_contact_info(
+    pocs.extend((information, get_base_contact(ssp)))
+    csp_poc = _extracted_from_gather_stakeholders_(
         tables,
-        key="AO Point of Contact",
-        xpath="//w:tbl/w:tr",
-    )
-    pocs.append(ao_poc)
-
-    csp_poc = get_contact_info(
-        tables,
-        key="CSP Name Internal ISSO (or Equivalent) Point of Contact",
-        xpath="//w:tbl/w:tr",
+        "AO Point of Contact",
+        pocs,
+        "CSP Name Internal ISSO (or Equivalent) Point of Contact",
     )
     existing_stakeholders = []
     pocs.append(csp_poc)
     existing_stakeholders_response = api.get(
         url=app.config["domain"]
         + f"/api/stakeholders/getAllByParent/{regscale_ssp['id']}/securityplans"
     )
     if existing_stakeholders_response.status_code == 200:
         existing_stakeholders = existing_stakeholders_response.json()
     pocs_inserted = []
     for poc in pocs:
-        poc = {k.lower(): v for k, v in poc.items()}  # Make this case-insensative.
+        poc = {k.lower(): v for k, v in poc.items()}  # Make this case-insensitive.
         email = ""
-        if "email" in poc.keys():
+        if "email" in poc:
             email = poc["email"]
-        if "email address" in poc.keys():
+        if "email address" in poc:
             email = poc["email address"]
         stakeholder = {
             "name": poc["name"].strip(),
-            "title": poc["title"] if "title" in poc.keys() else "",
-            "phone": poc["phone"] if "phone" in poc.keys() else "",
+            "title": poc.get("title", ""),
+            "phone": poc.get("phone", ""),
             "email": email,
-            "address": poc["address"] if "address" in poc.keys() else "",
+            "address": poc.get("address", ""),
             "parentId": regscale_ssp["id"],
             "parentModule": "securityplans",
         }
         if poc["name"].strip() not in pocs_inserted and poc["name"].strip() not in {
             guy["name"] for guy in existing_stakeholders
         }:
             post_stakeholder(api=api, config=app.config, stakeholder=stakeholder)
             pocs_inserted.append(poc["name"].strip())
 
 
+def _extracted_from_gather_stakeholders_(tables, key, pocs, arg3):
+    system_owner = get_contact_info(tables, key=key, xpath="//w:tbl/w:tr")
+    pocs.append(system_owner)
+    return get_contact_info(tables, key=arg3, xpath="//w:tbl/w:tr")
+
+
 def process_fedramp_docx(
     fedramp_file_path: click.Path, base_fedramp_profile: str
 ) -> None:
     """
     Convert a FedRAMP file to a RegScale SSP
     :param click.Path fedramp_file_path: The click file path object
     :param str base_fedramp_profile: base fedramp profile
@@ -1600,17 +1571,14 @@
         full_text,
         start_header="SYSTEM ENVIRONMENT AND INVENTORY",
         start_text="PRODUCTION ENVIRONMENT: (IMPLEMENTED)",
         end_text="Data Flow",
     )
     count = 0
     title = None
-    # version_date = document.tables[3].cell(4,0).text
-    # version_author = document.tables[3].cell(7,0).text
-    status = "Other"
     system_status = "Other"
     system_type = "Major Application"
     confidentiality = "Low"
     integrity = "Low"
     availability = "Low"
     tables = get_tables(document)
     # information_types = get_information_types(tables)
@@ -1653,28 +1621,26 @@
                 if "System Status" in row_data:
                     system_status = row_data["System Status"]
                 if "Service Provider Architecture Layers" in row_data:
                     system_type = row_data["Service Provider Architecture Layers"]
             row_data["checked"] = checked
             row_data["element"] = rem
             table_data.append(row_data)
-    if "in production" in system_status:
-        status = "Operational"
+    status = "Operational" if "in production" in system_status else "Other"
     # Links are posted to links mapped to ssp
     # post_links(app, table_data, ssp_id)
     # Parts will go in implementation fields.
     profile_name = base_fedramp_profile
     logger.info("Using the %s profile to import controls.", profile_name)
     profiles = api.get(f'{app.config["domain"]}/api/profiles/getAll').json()
     try:
         profile = None
-        profile_list = [
+        if profile_list := [
             profile for profile in profiles if profile["name"] == profile_name
-        ]
-        if profile_list:
+        ]:
             profile = profile_list.pop()
         else:
             raise ValueError(f"Unable to find profile: {profile_name}")
         profile_mapping = api.get(
             f"{app.config['domain']}/api/profileMapping/getByProfile/{profile['id']}"
         ).json()
     except (IndexError, AttributeError) as ex:
@@ -1701,17 +1667,15 @@
         status=status,
         createdById=app.config["userId"],
         lastUpdatedById=app.config["userId"],
         systemOwnerId=app.config["userId"],
         planAuthorizingOfficialId=app.config["userId"],
         planInformationSystemSecurityOfficerId=app.config["userId"],
         systemType=system_type,
-        overallCategorization=ssp.system_security_level
-        if ssp.system_security_level
-        else "Moderate",
+        overallCategorization=ssp.system_security_level or "Moderate",
         description=description,
         environment=environment,
         executiveSummary=f"Revision: {ssp.revision}",
     )
     existing_security_plans_reponse = api.get(
         url=app.config["domain"] + "/api/securityplans/getAll",
     )
```

## app/public/nist_catalog.py

```diff
@@ -104,15 +104,15 @@
         split_word = original_id.split("-")
         if len(split_word) > 1:
             # store original
             control_raw_number = split_word[1]
             # convert to a number
             control_num = float(split_word[1])
             if control_num < 10:
-                control_new_number = split_word[0] + "-0" + control_raw_number
+                control_new_number = f"{split_word[0]}-0{control_raw_number}"
             else:
                 control_new_number = original_id
             control["sortId"] = control_new_number
             sorted_controls.append(control_new_number)
 
     # output the RegScale controls
     save_data_to(
@@ -130,15 +130,15 @@
         split_word = original_id.split(".")
         if len(split_word) > 1:
             # store original
             control_raw_number = split_word[1]
             # convert to a number
             control_num = float(split_word[1])
             if control_num < 10:
-                control_new_number = split_word[0] + ".0" + control_raw_number
+                control_new_number = f"{split_word[0]}.0{control_raw_number}"
             else:
                 control_new_number = original_id
             ctrl["sortId"] = control_new_number
             second_sort.append(control_new_number)
         else:
             second_sort.append(ctrl["sortId"])
 
@@ -221,21 +221,21 @@
     Function to parse the provided control dictionary from RegScale and returns a sortId as a string
     :param dict control: A control from RegScale
     :return: string to use as a sortId
     :rtype: str
     """
     try:
         original_id = control["sortId"]
-    except KeyError:
+    except KeyError as err:
         # doesn't have a sortId so try to get the unique controlId
         try:
             original_id = control["controlId"]
             # verify original_id isn't blank
             if original_id == "":
                 # raise a KeyError to parse title of the control name
-                raise KeyError
+                raise KeyError from err
         except KeyError:
             # no controlId either so parse it from the title
             original_id = control["title"].split(" ")[0]
 
     # return the parsed sortId
     return original_id
```

## app/public/oscal.py

```diff
@@ -335,19 +335,15 @@
     :param Path file_name: Desired file name
     :return: None
     :rtype: None
     """
     # validation
     if catalog <= 0:
         error_and_exit("No catalogue provided or catalogue invalid.")
-    elif (
-        categorization.title() != "Low"
-        and categorization.title() != "Moderate"
-        and categorization.title() != "High"
-    ):
+    elif categorization.title() not in ["Low", "Moderate", "High"]:
         error_and_exit("Categorization not provided or invalid.")
     else:
         # load the catalog
         try:
             oscal = open(file_name, "r", encoding="utf-8-sig")
             oscal_data = json.load(oscal)
         except Exception as ex:
```

## app/public/otx.py

```diff
@@ -1,10 +1,12 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """AlienVault OTX RegScale integration"""
+
+
 from app.api import Api
 from app.application import Application
 from app.logz import create_logger
 from models.regscale_models.threat import Threat
 from datetime import date, datetime
 import re
 import click
@@ -21,16 +23,16 @@
 #
 # AlienVault API Documentation: https://otx.alienvault.com/api/
 #
 #####################################################################################################
 
 logger = create_logger()
 SERVER = "https://otx.alienvault.com"
-API_V1_ROOT = "{}/api/v1".format(SERVER)  # API v1 base path
-SUBSCRIBED = "{}/pulses/subscribed".format(API_V1_ROOT)  # pulse subscriptions
+API_V1_ROOT = f"{SERVER}/api/v1"
+SUBSCRIBED = f"{API_V1_ROOT}/pulses/subscribed"
 
 
 @click.group()
 def alienvault():
     """[BETA] AlienVault OTX Integration to load pulses to RegScale."""
 
 
@@ -61,15 +63,15 @@
     :param url_path: Request path (i.e. "/search/pulses")
     :param kwargs: key value pairs to be added as query parameters (i.e. limit=10, page=5)
     :return: a formatted url (i.e. "/search/pulses")
     """
     uri = url_path.format(SERVER)
     uri = uri if uri.startswith("http") else SERVER.rstrip("/") + uri
     if kwargs:
-        uri += "?" + urlencode(kwargs)
+        uri += f"?{urlencode(kwargs)}"
 
     return uri
 
 
 def walkapi_iter(
     app,
     api,
@@ -116,21 +118,16 @@
 def extract_id(descrip: str) -> str:
     """Match RegEx of Alienvault ID
 
     :param descrip: A string with the AlienVault ID
     :return: A string from the regex match
     :rtype: str
     """
-
-    result = ""
     pattern = r"(?<=AlienVault ID: ).*"
-    match = re.search(pattern, descrip)
-    if match:
-        result = match[0]
-    return result
+    return match[0] if (match := re.search(pattern, descrip)) else ""
 
 
 def post_threat(app: Application, api: Api, threat: Threat):
     """
     Post Alienvault Threat to RegScale.
     :param app: Application instance
     :param app: Api instance
```

## app/utils/app_utils.py

```diff
@@ -64,37 +64,34 @@
 def validate_mac_address(mac_address: str) -> bool:
     """
     Simple validation of a mac address input
     :param str mac_address: mac address
     :return: Whether mac address is valid or not
     :rtype: bool
     """
-    if re.match(
-        "[0-9a-f]{2}([-:]?)[0-9a-f]{2}(\\1[0-9a-f]{2}){4}$", mac_address.lower()
-    ):
-        return True
-    return False
+    return bool(
+        re.match(
+            "[0-9a-f]{2}([-:]?)[0-9a-f]{2}(\\1[0-9a-f]{2}){4}$",
+            mac_address.lower(),
+        )
+    )
 
 
 def reformat_str_date(date_str: str, dt_format: str = "%m/%d/%Y") -> str:
     """
     Function to convert a string into a datetime object and reformat it to dt_format, default format is MM/DD/YYYY
     :param str date_str: date as a string
     :param str dt_format: datetime string format, defaults to "%m/%d/%Y"
     :return: string with the provided date format
     :rtype: str
     """
     # replace the T with a space and create list of result
     date_str = date_str.replace("T", " ").split(" ")
 
-    # convert the first part of the date list into a date
-    date = datetime.strptime(date_str[0], "%Y-%m-%d").strftime(dt_format)
-
-    # return date result
-    return date
+    return datetime.strptime(date_str[0], "%Y-%m-%d").strftime(dt_format)
 
 
 def pretty_short_str(long_str: str, start_length: int, end_length: int) -> str:
     """
     Function to convert long string to shortened string
     :param str long_str: long string to shorten
     :param int start_length: number of characters to use from string start
@@ -189,22 +186,21 @@
 
 def create_progress_object() -> Progress:
     """
     Function to create and return a progress object
     :return: Progress object for live progress in console
     :rtype: Progress
     """
-    job_progress = Progress(
+    return Progress(
         "{task.description}",
         SpinnerColumn(),
         BarColumn(),
         TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
         TimeElapsedColumn(),
     )
-    return job_progress
 
 
 def get_file_type(file_name: str) -> str:
     """
     Function to get the file type of the provided file_path and returns it as a string
     :param str file_name: Path to the file
     :return: Returns string of file type
@@ -275,19 +271,18 @@
 def find_uuid_in_str(str_to_search: str) -> str:
     """
     Find a UUID in a long string
     :param str_to_search: Long string
     :return: Matching string
     :rtype: str
     """
-    dat = re.findall(
+    if dat := re.findall(
         r"[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}",
         str_to_search,
-    )
-    if dat:
+    ):
         return dat[0]
     return str_to_search
 
 
 def recursive_items(nested: dict):
     """
     Function to recursively move through a dictionary and pull out key value pairs
@@ -372,14 +367,16 @@
     :raises: General Error if unable to save json data with .write() after trying json.dump() method
     :return: None
     """
     # check the file type, so we can export the data to the correct file
     if file_type.lower() not in [".csv", ".json", ".xlsx"]:
         # notify the user an incorrect file type was provided
         error_and_exit(f"Unsupported file type provided, {file_type} is not supported.")
+    if output_log:
+        logger.info("Prepping data to be saved to %s%s", file_name, file_type)
     try:
         if file_type.lower() == ".csv":
             # convert the provided data to a pandas dataframe
             d_frame = pd.DataFrame(data)
 
             # transpose the dataset
             d_frame = d_frame.transpose()
@@ -495,33 +492,27 @@
             # iterate through the period locations
             for period in sep_locations:
                 # capitalize the character following the period
                 key = key[:period] + key[period + 1].upper() + key[period + 2 :]
 
                 # remove the @
                 key = key.replace("@", "")
-            # add the cleaned key with the original value
-            flat_dict_clean[key] = value
-        elif sep_locations == -1:
-            # no period find
-            flat_dict_clean[key] = value
-        else:
+        elif sep_locations != -1:
             # capitalize the character following the @
             key = (
                 key[:sep_locations]
                 + key[sep_locations + 1].upper()
                 + key[sep_locations + 2 :]
             )
 
             # remove the @
             key = key.replace("@", "")
 
-            # add the cleaned key with the original value
-            flat_dict_clean[key] = value
-
+        # add the cleaned key with the original value
+        flat_dict_clean[key] = value
     return flat_dict_clean
 
 
 def days_between(vuln_time: str) -> int:
     """
     Find the difference in days between 2 datetimes
     :param str vuln_time: date published
@@ -544,31 +535,26 @@
     """
     # split the string at the < signs
     split_urls = raw_string.split("<")
 
     # get the last entry
     split_url = split_urls[-1]
 
-    # remove the remaining text from the last entry
-    next_url_final = split_url[: split_url.find(">")]
-
-    # return the after id
-    return next_url_final
+    # remove the remaining text from the last entry and return it
+    return split_url[: split_url.find(">")]
 
 
 def find_keys(node, kv):
     """
     Python generator function to traverse deeply nested lists or dictionaries to
     extract values of every key found in a given node
     :param node: A string, dict or list to parse.
     :param kv: Key, Value pair
     """
     if isinstance(node, list):
         for i in node:
-            for x in find_keys(i, kv):
-                yield x
+            yield from find_keys(i, kv)
     elif isinstance(node, dict):
         if kv in node:
             yield node[kv]
         for j in node.values():
-            for x in find_keys(j, kv):
-                yield x
+            yield from find_keys(j, kv)
```

## app/utils/regscale_utils.py

```diff
@@ -77,19 +77,15 @@
     # make the api call
     file_response = api.post(
         url=f"{api.config['domain']}/api/files/file",
         headers=file_headers,
         data=data,
         files=files,
     )
-    # set the regscale_file to the json response if it was a successfull API call
-    regscale_file = file_response.json() if file_response.status_code == 200 else None
-
-    # return the regscale_file
-    return regscale_file
+    return file_response.json() if file_response.status_code == 200 else None
 
 
 def update_regscale_config(str_param: str, val: any, app: Application = None) -> str:
     """
     Update config in init.yaml
     :param str str_param: config parameter to update
     :param any val: config parameter value to update
@@ -116,21 +112,20 @@
     :param str file_name: Path to the file to upload
     :param int parent_id: RegScale parent ID
     :param str parent_module: RegScale module
     :param api: API object
     :return: Whether the file upload was successful or not
     :rtype: bool
     """
-    # first create the file in RegScale
-    regscale_file = create_regscale_file(
-        file_path=file_name, parent_id=parent_id, parent_module=parent_module, api=api
-    )
-
-    # verify the file creation was successful
-    if regscale_file:
+    if regscale_file := create_regscale_file(
+        file_path=file_name,
+        parent_id=parent_id,
+        parent_module=parent_module,
+        api=api,
+    ):
         # set up headers for file upload
         file_headers = {
             "Authorization": api.config["token"],
             "accept": "application/json, text/plain, */*",
         }
 
         # set up file_data payload with the regscale_file dictionary
@@ -168,18 +163,15 @@
     :param str url: RegScale instance URL to create the assessment
     :param dict new_assessment: API assessment payload
     :param api: API object
     :return: New RegScale assessment ID
     :rtype: int
     """
     assessment_res = api.post(url=url, json=new_assessment)
-    asset_id = (
-        assessment_res.json()["id"] if assessment_res.status_code == 200 else None
-    )
-    return asset_id
+    return assessment_res.json()["id"] if assessment_res.status_code == 200 else None
 
 
 def get_issues_by_integration_field(api, field: str) -> list:
     """
     Function to get the RegScale issues for the provided integration field that has data populated
     :param api: API Object
     :param field: Integration field to filter the RegScale issues
@@ -216,31 +208,37 @@
 def lookup_reg_assets_by_parent(api, parent_id: int, module: str) -> list:
     """
     Function to get assets from RegScale via API with the provided System Security Plan ID
     :param api: API object
     :param int parent_id: RegScale System Security Plan ID
     :param str module: RegScale module
     :raises: JSONDecodeError if API response cannot be converted to a json object
+    :raises: General Error if API response is not successful
     :return: List of data returned from RegScale API
     :rtype: list
     """
     # verify provided module
     verify_provided_module(module)
 
     config = api.config
     regscale_assets_url = (
         f"{config['domain']}/api/assets/getAllByParent/{parent_id}/{module}"
     )
     results = []
 
     response = api.get(url=regscale_assets_url)
-    try:
-        results = response.json() if response.status_code == 200 else []
-    except JSONDecodeError as ex:
-        error_and_exit(f"Unable to fetch assets from RegScale:\n{ex}")
+    if response.ok:
+        try:
+            results = response.json() if response.ok else []
+        except JSONDecodeError as ex:
+            error_and_exit(f"Unable to fetch assets from RegScale:\n{ex}")
+    else:
+        error_and_exit(
+            f"Unable to get assets from RegScale. Received:{response.status_code}\n{response.text}"
+        )
     return results
 
 
 def get_all_from_module(api, module: str) -> list[dict]:
     """
     Function to retrieve all records for the provided Module in RegScale via API
     :param api: API object
```

## app/utils/threadhandler.py

```diff
@@ -16,18 +16,15 @@
     :param process: function for the threads to execute
     :param Tuple args: args for the provided process
     :param int thread_count: # of threads needed
     :return: None
     """
     # set max threads
     max_threads = app.config["maxThreads"]
-    # check to see if we need to go over the max threads and
-    # assign allowed variable threads accordingly
-    threads = max_threads if thread_count > max_threads else thread_count
-    if threads:
+    if threads := min(thread_count, max_threads):
         # start the threads with the number of threads allowed
         with ThreadPoolExecutor(max_workers=threads) as executor:
             # iterate and start the threads that were requested
             for thread in range(threads):
                 # assign each thread the passed process and args along with the thread number
                 executor.submit(process, args, thread)
 
@@ -40,13 +37,8 @@
     :param int total_items: Total # of items to process with threads
     :return: List of items to process for the given thread
     :rtype: list
     """
     # set max threads
     max_threads = app.config["maxThreads"]
 
-    # create assigned variable list
-    assigned = []
-    for x in range(total_items):
-        if x % max_threads == thread:
-            assigned.append(x)
-    return assigned
+    return [x for x in range(total_items) if x % max_threads == thread]
```

## models/app_models/click.py

```diff
@@ -61,66 +61,65 @@
     Function to return a click.option for saving data to a directory
     :param bool exists: Whether the directory has to exist, default False
     :param bool dir_okay:  Whether to accept a directory, default True
     :param bool file_okay:  Whether a file path will be accepted, default False
     :return: click.option with the provided parameters
     :rtype: click.option
     """
-    save_output = click.option(
+    return click.option(
         "--save_output_to",
         type=click.Path(
-            exists=exists, dir_okay=dir_okay, file_okay=file_okay, path_type=Path
+            exists=exists,
+            dir_okay=dir_okay,
+            file_okay=file_okay,
+            path_type=Path,
         ),
         help="Provide the path where you would like to save the output to.",
         prompt="Enter directory for file output",
         required=True,
     )
-    return save_output
 
 
 def file_types(accepted_files: list) -> click.option:
     """
     Function to return click.option for accepted file types
     :param list accepted_files: list of file extensions
     :return: click.option with provided file list
     """
-    file_type = click.option(
+    return click.option(
         "--file_type",
         type=click.Choice(accepted_files, case_sensitive=False),
         help="Select a file type to save the output as.",
         prompt="Enter desired file type",
         required=True,
     )
-    return file_type
 
 
 def regscale_id(help: str = "Enter the desired ID # from RegScale.") -> click.option:
     """
     Function to return click.option for RegScale parent ID
     :param str help: String to display when user enters --help
     :return: click.option for RegScale parent ID
     :rtype: click.option
     """
-    regscale_parent_id = click.option(
+    return click.option(
         "--regscale_id",
         type=click.INT,
         help=help,
         prompt="Enter the RegScale Record ID",
         required=True,
     )
-    return regscale_parent_id
 
 
 def regscale_module() -> click.option:
     """
     Function to return click.option for RegScale modules
     :return: click.option for RegScale modules
     :rtype: click.option
     """
-    regscale_parent_module = click.option(
+    return click.option(
         "--regscale_module",
         type=click.STRING,
         help=f"Enter the RegScale module name.\n\n{Modules().to_str_table()}",
         prompt="Enter the RegScale Module name",
         required=True,
     )
-    return regscale_parent_module
```

## models/app_models/control_editor.py

```diff
@@ -272,59 +272,58 @@
         """
         Batch uploads updated control implementation statements to the provided RegScale parent ID
         :param str regscale_module: RegScale parent module
         :param int regscale_parent_id: RegScale parent ID
         :raises: requests.exceptions.RequestException if API call encountered an error
         :return: None
         """
-        updated_implementations = []
         reader = pd.read_excel(self.file)
-        for i in reader.iterrows():
-            updated_implementations.append(
-                {
-                    "id": i["Id"],
-                    "implementation": i["Implementation"],
-                    "policy": i["Policy"],
-                    "dateLastAssessed": None,
-                    "lastAssessmentResult": None,
-                    "controlName": i["ControlName"],
-                    "controlTitle": i["Description"],
-                    "controlId": i["ControlId"],
-                    "practiceLevel": None,
-                    "processLevel": None,
-                    "cyberFunction": None,
-                    "implementationType": None,
-                    "implementationMethod": None,
-                    "qdWellDesigned": None,
-                    "qdProcedures": None,
-                    "qdSegregation": None,
-                    "qdFlowdown": None,
-                    "qdAutomated": None,
-                    "qdOverall": None,
-                    "qiResources": None,
-                    "qiMaturity": None,
-                    "qiReporting": None,
-                    "qiVendorCompliance": None,
-                    "qiIssues": None,
-                    "qiOverall": None,
-                    "responsibility": i["Responsibility"],
-                    "parentID": regscale_parent_id,
-                    "parentModule": regscale_module,
-                    "inheritedControlId": None,
-                    "inheritedRequirementId": None,
-                    "inheritedSecurityPlanId": None,
-                    "inheritedPolicyId": None,
-                    "dateCreated": None,
-                    "lastUpdatedById": None,
-                    "dateLastUpdated": None,
-                    "weight": None,
-                    "isPublic": None,
-                    "inheritable": i["Inheritable"],
-                }
-            )
+        updated_implementations = [
+            {
+                "id": i["Id"],
+                "implementation": i["Implementation"],
+                "policy": i["Policy"],
+                "dateLastAssessed": None,
+                "lastAssessmentResult": None,
+                "controlName": i["ControlName"],
+                "controlTitle": i["Description"],
+                "controlId": i["ControlId"],
+                "practiceLevel": None,
+                "processLevel": None,
+                "cyberFunction": None,
+                "implementationType": None,
+                "implementationMethod": None,
+                "qdWellDesigned": None,
+                "qdProcedures": None,
+                "qdSegregation": None,
+                "qdFlowdown": None,
+                "qdAutomated": None,
+                "qdOverall": None,
+                "qiResources": None,
+                "qiMaturity": None,
+                "qiReporting": None,
+                "qiVendorCompliance": None,
+                "qiIssues": None,
+                "qiOverall": None,
+                "responsibility": i["Responsibility"],
+                "parentID": regscale_parent_id,
+                "parentModule": regscale_module,
+                "inheritedControlId": None,
+                "inheritedRequirementId": None,
+                "inheritedSecurityPlanId": None,
+                "inheritedPolicyId": None,
+                "dateCreated": None,
+                "lastUpdatedById": None,
+                "dateLastUpdated": None,
+                "weight": None,
+                "isPublic": None,
+                "inheritable": i["Inheritable"],
+            }
+            for i in reader.iterrows()
+        ]
         new_implementations = json.dumps(updated_implementations)
         try:
             api.post(
                 url=self.config["domain"] + "/api/controlImplementation/batchUpdate",
                 json=new_implementations,
             )
             self.logger.info(
```

## models/regscale_models/asset.py

```diff
@@ -30,14 +30,16 @@
     cpu: int = None
     ram: int = None
     diskStorage: int = None
     description: str = None
     endOfLifeDate: str = ""
     purchaseDate: str = ""
     status: str = None
+    tenableId: str = None
+    qualysId: str = None
     wizId: str = None
     wizInfo: str = None
     facilityId: int = None
     orgId: int = None
     id: int = None
     createdById: str = None
     lastUpdatedById: str = None
@@ -68,14 +70,16 @@
         _cpu = int(obj.get("cpu"))
         _ram = int(obj.get("ram"))
         _diskStorage = int(obj.get("diskStorage"))
         _description = str(obj.get("description"))
         _endOfLifeDate = str(obj.get("endOfLifeDate"))
         _purchaseDate = str(obj.get("purchaseDate"))
         _status = str(obj.get("status"))
+        _tenableId = str(obj.get("tenableId"))
+        _qualysId = str(obj.get("qualysId"))
         _wizId = str(obj.get("wizId"))
         _wizInfo = str(obj.get("wizInfo"))
         if obj.get("facilityId"):
             _facilityId = int(obj.get("facilityId"))
         else:
             _facilityId = None
         if obj.get("orgId"):
@@ -106,14 +110,16 @@
             cpu=_cpu,
             ram=_ram,
             diskStorage=_diskStorage,
             description=_description,
             endOfLifeDate=_endOfLifeDate,
             purchaseDate=_purchaseDate,
             status=_status,
+            tenableId=_tenableId,
+            qualysId=_qualysId,
             wizId=_wizId,
             wizInfo=_wizInfo,
             facilityId=_facilityId,
             orgId=_orgId,
             parentId=_parentId,
             parentModule=_parentModule,
             id=_id,
```

## models/regscale_models/control_implementation.py

```diff
@@ -82,15 +82,15 @@
 
     def dict(self) -> dict:
         """
         Create a dictionary from the Control dataclass
         :return: Dictionary of Control
         :rtype: dict
         """
-        return {k: v for k, v in asdict(self).items()}
+        return dict(asdict(self).items())
 
 
 @dataclass
 class ControlImplementation:
     """Security Control Implementation model"""
 
     parentId: int  # Required
@@ -231,8 +231,8 @@
 
     def dict(self) -> dict:
         """
         Create a dictionary from the Control Implementation dataclass
         :return: Dictionary of Control Implementation
         :rtype: dict
         """
-        return {k: v for k, v in asdict(self).items()}
+        return dict(asdict(self).items())
```

## models/regscale_models/issue.py

```diff
@@ -93,15 +93,15 @@
 
     def dict(self) -> dict:
         """
         Create a dictionary from the Asset dataclass
         :return: Dictionary of Asset
         :rtype: dict
         """
-        return {k: v for k, v in asdict(self).items()}
+        return dict(asdict(self).items())
 
     @staticmethod
     def assign_severity(value: Any = None) -> str:
         """
         Function to assign severity for an issue in RegScale using the provided value
         :param Any value: The value to analyze to determine the issue's severity, defaults to None
         :return: String of severity level for RegScale issue
@@ -110,23 +110,22 @@
         severity_levels = {
             "low": "III - Low - Other Weakness",
             "moderate": "II - Moderate - Reportable Condition",
             "high": "I - High - Significant Deficiency",
         }
         severity = "IV - Not Assigned"
         # see if the value is an int or float
-        if isinstance(value, int) or isinstance(value, float):
+        if isinstance(value, (int, float)):
             # check severity score and assign it to the appropriate RegScale severity
             if value >= 7:
                 severity = severity_levels["high"]
             elif 4 <= value < 7:
                 severity = severity_levels["moderate"]
-            elif value < 4:
+            else:
                 severity = severity_levels["low"]
-        # see if the value is a string
         elif isinstance(value, str):
             if value.lower() == "low":
                 severity = severity_levels["low"]
             elif value.lower() in ["medium", "moderate"]:
                 severity = severity_levels["moderate"]
             elif value.lower() in ["high", "critical"]:
                 severity = severity_levels["high"]
```

## models/regscale_models/modules.py

```diff
@@ -1,14 +1,14 @@
 #!/usr/bin/env python3
 # -*- coding: utf-8 -*-
 """ Class of all RegScale modules """
 
+# standard python imports
 from dataclasses import asdict, dataclass
 
-# standard python imports
 from rich.console import Console
 from rich.table import Table
 
 from app.utils.app_utils import uncamel_case
 
 
 @dataclass
@@ -61,15 +61,15 @@
 
     def dict(self) -> dict:
         """
         Create a dictionary from the Asset dataclass
         :return: Dictionary of Asset
         :rtype: dict
         """
-        return {k: v for k, v in asdict(self).items()}
+        return dict(asdict(self).items())
 
     def module_names(self) -> list:
         """
         Function to clean the keys and display them as they are in the application
         :return: list of module names
         :rtype: list
         """
```

## models/regscale_models/threat.py

```diff
@@ -48,10 +48,8 @@
     def xstr(str_eval: str) -> str:
         """
         Replaces string with None value to ""
         :param str str_eval: key to replace None value to ""
         :return: Updates provided str field to ""
         :rtype: str
         """
-        if str_eval is None:
-            return ""
-        return str(str_eval)
+        return "" if str_eval is None else str_eval
```

## models/regscale_models/user.py

```diff
@@ -23,20 +23,16 @@
         + string.ascii_uppercase
         + string.digits
         + string.punctuation
     )
 
     # randomly select characters matching the random length
     temp = random.sample(all_string_chars, length)
-
-    # join the items together into a string
-    random_password = "".join(temp)
-
-    # return random generated password
-    return random_password
+    # return a string from the temp list of samples
+    return "".join(temp)
 
 
 @dataclass
 class User:
     """User Model"""
 
     userName: str  # Required
```

## tests/conftest.py

```diff
@@ -26,28 +26,34 @@
         snowPassword: VALUE
         snowUrl: myUrl
         snowUserName: VALUE
         token: Bearer bunk_string
         userId: enter user id here
         wizAccessToken: <createdProgrammatically>
         wizAuthUrl: VALUE
-        wizClientId: VALUE
-        wizClientSecret: VALUE
         wizExcludes: VALUE
         wizScope: VALUE
         wizUrl: https://auth.wiz.io/oauth/token
         tenableAccessKey: {getenv("TENABLE_ACCESS")}
         tenableSecretKey: {getenv("TENABLE_SECRET")}
+        tenableUrl: https://sc.tenalab.online
+        qualysUserName: testuser
+        qualysPassword: MyPassword
+        qualysUrl: http://localhost:5050/
         issue:
             tenable:
               critical: 3
               high: 5
               moderate: 30
               status: Draft
-        tenableUrl: https://sc.tenalab.online
+            qualys:
+              high: 5
+              low: 3
+              moderate: 30
+              status: Draft
     """
     file_exists = exists("init.yaml")
     if not file_exists:
         with open("init.yaml", "w", encoding="utf-8") as file:
             file.write(init)
```

## Comparing `RegScale_CLI-4.24.1.dist-info/LICENSE` & `RegScale_CLI-4.25.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `RegScale_CLI-4.24.1.dist-info/METADATA` & `RegScale_CLI-4.25.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: RegScale-CLI
-Version: 4.24.1
+Version: 4.25.0
 Summary: Command Line Interface (CLI) for bulk processing/loading data into RegScale
 Home-page: https://github.com/RegScale/regscale-cli
 Author: Travis Howerton
 Author-email: thowerton@regscale.com
 License: MIT
 Platform: UNKNOWN
 Classifier: Operating System :: OS Independent
@@ -13,37 +13,39 @@
 Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.9
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: PyMuPDF (==1.21.0)
 Requires-Dist: aiohttp (==3.8.3)
 Requires-Dist: bs4
-Requires-Dist: click
-Requires-Dist: colorama
+Requires-Dist: click (==8.1.3)
+Requires-Dist: colorama (==0.4.5)
 Requires-Dist: cryptography (==39.0.1)
 Requires-Dist: docx (==0.2.4)
 Requires-Dist: fitz (==0.0.1.dev2)
+Requires-Dist: flask
 Requires-Dist: frontend
-Requires-Dist: gql
-Requires-Dist: jira
-Requires-Dist: jwt (==1.3.1)
+Requires-Dist: gql (==3.4.0)
+Requires-Dist: jira (==3.3.1)
+Requires-Dist: jwcrypto (==1.4.2)
 Requires-Dist: matplotlib
-Requires-Dist: msal
+Requires-Dist: msal (==1.18.0)
 Requires-Dist: numpy (==1.23.3)
 Requires-Dist: openpyxl
 Requires-Dist: pandas
 Requires-Dist: pdfplumber (==0.7.6)
 Requires-Dist: pre-commit
 Requires-Dist: pyTenable
 Requires-Dist: pyaml (==21.10.1)
 Requires-Dist: pytest
 Requires-Dist: python-docx
+Requires-Dist: python-jwt (==4.0.0)
 Requires-Dist: regscale-python-ssp
-Requires-Dist: requests
-Requires-Dist: rich
+Requires-Dist: requests (==2.28.1)
+Requires-Dist: rich (==12.5.1)
 Requires-Dist: setuptools
 Requires-Dist: static
 Requires-Dist: tools
 Requires-Dist: wheel
 Requires-Dist: xmltodict
 Provides-Extra: dev
 Requires-Dist: black ; extra == 'dev'
```

## Comparing `RegScale_CLI-4.24.1.dist-info/RECORD` & `RegScale_CLI-4.25.0.dist-info/RECORD`

 * *Files 19% similar despite different names*

```diff
@@ -1,82 +1,83 @@
-regscale.py,sha256=S235MNRDkfpMtk5XvURaI66gtp7J0fvIFmH3MWz2TtY,14494
+regscale.py,sha256=gWnFlYcCWnoIHk0p5pX5Yf7ECiux2qeFoES1G_3LhWg,14547
 app/__init__.py,sha256=gbUXYeGrcrqcOc70BjFtxHtBnwSqor-SlWYgZjypEOA,47
-app/_version.py,sha256=tyqE6RDb7bX8SZl192K-ktYCHAGJtzq6kdUaMFBEuRA,73
-app/api.py,sha256=rxICQ759vDtG1OaPq3wGsa3Ov2IQIJX-79-TkXRJPaI,14024
-app/application.py,sha256=KpPlof0MWYiBTPhIAiwNkyBOLXYwlzCELtqINIpL4Dk,12991
-app/logz.py,sha256=ZQDHvPjSzBpr-EWBaHmG2NH9mnA38rVp5jv1efkyxwE,1191
+app/_version.py,sha256=uEkWyTCTIko4OLjvojEvKqYA_3SQyM6CoLwk6dNKrtk,73
+app/api.py,sha256=RMqoyJNX3HhwkXBCHhjVLbW159Z8svhX2OvUolMrR5o,14197
+app/application.py,sha256=Qj9VzhImSvAoBCJQI5SMT-vVn_4Rxfsk4POQNDsh_j0,13397
+app/logz.py,sha256=p7ADwGAkz75cey8CS6XdsS--jp9WnFRZANm02ZhtFAk,1287
 app/commercial/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-app/commercial/ad.py,sha256=f-MTH90ft1tvqFa6whL14Rx5j_PqDPewcE25oYpeFBY,16432
-app/commercial/defender.py,sha256=NkDusTfc4yUd0wzsdrhly8Yn3urC3TCEKg1CzWbi9a0,46852
-app/commercial/jira.py,sha256=6MEJSQQT3NGm3mLBXrcBqgAUYmIltQvx1f-_4YTkeo0,8101
-app/commercial/okta.py,sha256=k435lO-eN4wUkKHCejU-LNK7-ui8NcYecUUnWcCuQWM,27597
-app/commercial/servicenow.py,sha256=gWs2CO9kuA06Beh6teVKg_IFZ5CZiYNfKyoIu8F81ec,11726
-app/commercial/tenable.py,sha256=02CtlDp3JRFU7luKy5xPT1SmWDmGz55RWK3EUflb5S4,22914
-app/commercial/wiz.py,sha256=m988mi-k18prb1rvNHz73I0_uEuUfu6lFylxvS6xcwc,56612
+app/commercial/ad.py,sha256=CTt7F5mNznZGHJkgEK5zSCozmP858gqJyVti1ogkh98,16359
+app/commercial/defender.py,sha256=Crt8VKp3BRZvqMz_irfZ9cOAPEMs5kXkKYmchG-q1zs,45796
+app/commercial/jira.py,sha256=bqdbbp79JN6GBL5mfAg-sHEfwWyzTlobLYPw7bCUhko,8007
+app/commercial/okta.py,sha256=P0iJ59akglkEU0EhvSILnVT799VTMkDtlQi1Tl_u3TE,27240
+app/commercial/qualys.py,sha256=J0JzKhgH073kXNdHThRxIugzFTXS0A8_CMA5OenBpGU,27892
+app/commercial/servicenow.py,sha256=nzQVHzdw02UpJDtxI850pCwipPUCGA19ijg8bi_WJUE,11689
+app/commercial/tenable.py,sha256=NeoG2BCagPD5s8A78xf8kED6Wmu076AmmrPW8bVB-Xc,22634
+app/commercial/wiz.py,sha256=P5wInsQ7HvyltQW_KccT113IkmGCRK9e_TujJi5FALk,56457
 app/internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-app/internal/admin_actions.py,sha256=mktif6dY6TlV-RhXFmvTgYQ_Apa9-S38q-xPb0hda-w,27853
-app/internal/assessments_editor.py,sha256=3iYoix2J0ZJdNkpHzC_l1ztwYztHsxzCr4668XkaGTc,31774
-app/internal/comparison.py,sha256=L4C0k6mk3MUF5Qc2wiR37y7JpSqUXR2eZDStRVvEqcs,16287
-app/internal/control_editor.py,sha256=6C3QqDk7RrbqcDAp1enFfAvPWAZgQbb-Cj2C7Kf1RVI,15286
+app/internal/admin_actions.py,sha256=Ry1x_dmM2odXmVZpBB3R3Wgs8bj3qCC4acsTA-Pel8A,26998
+app/internal/assessments_editor.py,sha256=boaP7PTNFO831WLuDP3o5cRXT6P7H6g9Wt3WJWYDE_I,31042
+app/internal/comparison.py,sha256=F4VcAUpxzBctouUVHq_9cC_VC46qkNxtDZs93Lc45tw,16114
+app/internal/control_editor.py,sha256=eTb8p_JJS7g64zj9wqMXnHZh6OynfAHYzzox7P4N6nY,15277
 app/internal/encrypt.py,sha256=UTs3mwTxackgYhr357npx6rqXz7X2gMJ9mTaQ5hBsKw,5912
-app/internal/evidence.py,sha256=GTV2MmaWXroNXwM1p5aMEgKYkNU0OqWmHg_D6q7l12o,40841
-app/internal/healthcheck.py,sha256=V5NaL0kCsEDPZ_FXxWENUfoGBexORjeE2cGUZj2IK7Q,2330
-app/internal/login.py,sha256=vvPcyxmyPCSq-iqiygeR7Wz9FkD1Pz3bUOMUlNyTbmE,6394
+app/internal/evidence.py,sha256=YcHmS7wktfZbOXlQIYsW_blYdW5UGqSiRcPBZLyEqO0,39915
+app/internal/healthcheck.py,sha256=K6NNvgKucBl1Kg5GO-XiENfISQXk331t-KzhIHSPlPE,2325
+app/internal/login.py,sha256=vk5VwrCQU18Al-nayPQaNin5lbXEx4jiQFmyizecQ08,6397
 app/internal/migrations.py,sha256=4s_QoBJPDrHI9jMvYMZVady4GbEbGMlEK41hrhsA-iw,9419
 app/public/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-app/public/cisa.py,sha256=SPY-GANKdz9oaIXXsOhzo8DmD-Ttk-wmgAnj6ySZLqk,18801
-app/public/emass.py,sha256=aqY2Cxzx7OoX81cvLaAC3lYKV1VyQMR-JxQG3rav3a0,10226
-app/public/fedramp.py,sha256=z4TAKCvVk_tGBx7CUv74zgpBXvqS4--1IrL033tRXqQ,64892
-app/public/nist_catalog.py,sha256=EpEpste_q_sKSmzDuDsb1mGm-O6i2F3dcSxGsGmXvoE,7932
-app/public/oscal.py,sha256=hfCSDPYfFKoqXs2PnGznV1mw-MYcnvsYlY2dRVKgcVU,71195
-app/public/otx.py,sha256=YzymYLlmbFcbPwosNf5lXdzxunPLsiq7ljK9NsDwm_Y,6183
+app/public/cisa.py,sha256=ON5phwcGZYqOFk-JlkVvca_K5E9H0C6EXyN1rwjaC74,18601
+app/public/emass.py,sha256=69jEKyQGJc8f8ZZAcLRta7o1FR2sqUyQksRoa6k46XA,10193
+app/public/fedramp.py,sha256=n_B4AqYk4vXHt3TIf8zgVJShEYb4gINxFwUSRFeOpSk,63918
+app/public/nist_catalog.py,sha256=MnGyJ1hNvqnfrBjtUkTIBYlV0oeQsQYpp1hZ7Y7IPZE,7946
+app/public/oscal.py,sha256=1fn5USWnq3LN-E5w0WpIlbOvFEyVY73PuYpTs5GBF9k,71111
+app/public/otx.py,sha256=Q_4BnAKpij6lGC_I71uT5ruarP4laWF20Z4zGBXo8-U,6081
 app/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-app/utils/app_utils.py,sha256=eakfN5WH6Z96bnsOvoUifEJa9z1EnnVbkJq566nsonM,19729
-app/utils/regscale_utils.py,sha256=nkNm5AR7iIh6Qo6aZ1Bx9K3kgnOdAatG1CquZ4csEac,10417
-app/utils/threadhandler.py,sha256=Yq61Df-GMlJydhKPBO3C2OyAU43p0AgZApFXFEArf78,1846
+app/utils/app_utils.py,sha256=qboD1GMvX2ma5tFggVrDxKPqNf0osOJVj-7J5mfnxhk,19408
+app/utils/regscale_utils.py,sha256=wJZ97v3Qf39SGx2LsdHgbiiwYE4LueoeH2q8ycgiZS4,10375
+app/utils/threadhandler.py,sha256=XcKXSYSQeJla1Vd5RPryt4f5yW_-P4kXIEmw_JuF3Z8,1591
 exceptions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 exceptions/license_exception.py,sha256=5lDYW1uGf7dHFKBkhzYD3FlNnI6W4BICXi24OJyOs_w,195
 models/__init__.py,sha256=GLPPGTA5Lczf4OWUnudaFTG4OV2Hqt6IjZymYO6HL04,79
 models/app_models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-models/app_models/click.py,sha256=2V0yrxAL4XND9_3TqrDqbgdTYdpZKNuaoZSgEUCtIYQ,4112
-models/app_models/control_editor.py,sha256=nDZcrvoI1oLxJYITlytJLPQjevbfoASIMxAkzCnFZIk,14069
+models/app_models/click.py,sha256=o2eu7-AZFBQtdCb1USHYmq7DKYMyh01Tx3q-oYpDhBA,3997
+models/app_models/control_editor.py,sha256=F0Ky7As96UvUBu-W_WfDoObJBfs94t-w3zcFdTslT-Q,13863
 models/app_models/pipeline.py,sha256=13BuBreZESL7SP7ajCFXy8bgaEchTnyGlaRtUMBl0bM,889
 models/integration_models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 models/integration_models/azure_alerts.py,sha256=I_jcjaLMsQpgabkdtVbalGZ23GeAH8aQCmH9iXlKFhw,7858
 models/integration_models/recommendations.py,sha256=T_zyM1G4xFsZgApVvz8CGUMYuZLzfXLgoAcXsz1xUKE,872
 models/integration_models/tenable.py,sha256=ssbs0p5VNqWBvQSwDqNchVHQOxhzblnd1F_Zjf9NAJ8,8319
 models/integration_models/wiz.py,sha256=BqHeFvHtft7Ona8Gz1p9_JctxHjdJi0KlrDJ83Ub5g4,1833
 models/regscale_models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 models/regscale_models/assessment.py,sha256=bH9Wa08CfCv1Kwq_rDgJg6xvn9IPXxlmGGuvXq9R2qo,6688
-models/regscale_models/asset.py,sha256=29n_4tz2xHmhcsx8MTsiyQN9q6Xl68M2nwKqlUkhVS8,5224
+models/regscale_models/asset.py,sha256=yQm9Wm3q6J0fZ-XsSsrNjfcnDQYkznwom2sBtl2BKEw,5433
 models/regscale_models/components.py,sha256=AcUOAey-FTUFx6DJ1-1oMHgE2H7rhKE2z8QQqjPjR5w,1237
-models/regscale_models/control_implementation.py,sha256=VHZyr-FSnpbx3rR6oFCTo3jT8Pesrm8MtmJPIw02zT4,7831
+models/regscale_models/control_implementation.py,sha256=cjKdRzMNVGQFRE94REzucDtjCmubP3nw_-Jhzha97_8,7805
 models/regscale_models/interconnects.py,sha256=Amv3CSrurRwDOaLBiqioLNWlmhAqcq6Pbsm8ekv1Yk8,2850
-models/regscale_models/issue.py,sha256=QG_y5n9U335HbsSBFT2VBtH9DBTlvMDhdtl1fo045QU,4256
-models/regscale_models/modules.py,sha256=akcS7iGrgSeo-Aw6lnlaOp3l-hTb_fIaBeam6VY0zSo,4891
+models/regscale_models/issue.py,sha256=veNXQW_hkx4_3_opiSaIE6_kuUoVR92WbB6SBCCjc9U,4175
+models/regscale_models/modules.py,sha256=Gdjv4CAOrUNeOHI7NKJ84MlpVX8hwOd-bhujJQ1pgEI,4878
 models/regscale_models/ports_protocols.py,sha256=qupVuXfeNXxMBW8EviLDgtBJc3id9YvxfcJibHpjojo,2353
 models/regscale_models/requirements.py,sha256=msCh2iZD9YujpJO5yakfRRKebd1KE_wmbQIFe-iJbHQ,2415
 models/regscale_models/securityplans.py,sha256=93qb92V2W_hOf2hOhLLw7J087LngCgzo91Zzg9gQ1S8,5886
-models/regscale_models/threat.py,sha256=Mx9OPK5ALczAgIJmc1Pn3TtQBvkJLxKiHtXt7sVJ38E,1426
-models/regscale_models/user.py,sha256=9BuwFzLY9c0jHjG4pwjnBN1874AnqpuMhGAek1kpUYA,2313
+models/regscale_models/threat.py,sha256=Qt4mhCEgcWbAQ70DUXZ0MoVVCbbKXN8MSGWChxoua7U,1398
+models/regscale_models/user.py,sha256=mm-1ajFmCFzDeHDAydcANdQMIBehfCRhjWCCoMKZXFY,2242
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-tests/conftest.py,sha256=F8oiF-kBdWATZmtjepYK-lhCAu8I-mWCE7ZkHQ4Wjt8,1648
+tests/conftest.py,sha256=7Ba9a9CreEyUJJbxXolUQnHuOt3lrI-lnsOch8N390I,1818
 tests/test_app_utils.py,sha256=CP8ZQVCVdTRKy97QICKc4_4cHWQYQPd9KZw4vivAAWY,1334
 tests/test_assessments_editor.py,sha256=JK-ap6JVnF3SABLLkKvy-OPRi8sPkcjwhbz0yjxns3Y,1930
 tests/test_cisa.py,sha256=2tpRm-hB-bYl01lif8UNwhJETFuRm_R0UQBC3SCfcwk,1687
 tests/test_control_editor.py,sha256=kad6vmaeeNmkUWp1uETRsGPDsF1TeMFjxfln2lZJPlY,1747
 tests/test_dependabot.py,sha256=tTQZnikbzxW7v5LAMucZbDHQuwnM3UttZ8KLXl3KMts,1425
 tests/test_emass.py,sha256=1N-XgKBP3IjrW76XkiulqSsEO1EOeGKsIYmmWtiE4aw,2811
 tests/test_evidence.py,sha256=B8yap4uMxhjPLm1cmIUNDGfU3GJd-VGziKpfnWCny0o,14035
 tests/test_login.py,sha256=u_LPs_H_g8ubARUKHdvPZl3flt2SHEjwFjmWuPnZYLU,1186
 tests/test_npm_audit.py,sha256=XdF0FM8telb2nzhVUc0gT4Zu7Srq3GiJheJFKSWICnk,1416
 tests/test_oscal.py,sha256=Lcm1f0hTr6Iwu5LGU-sKSC55bBsQVm2WX_zeiPBAdbA,9248
 tests/test_snow.py,sha256=tPdi-gWE_kmiv_dZV50T3vV6OB9vgbHr_jBGUi4hCaw,866
 tests/test_sonarcloud.py,sha256=Um1WqpIHRMWyPN7GF-pj263vt2YWcS2Sk7px9f1Ej-o,1245
 tests/test_tenable.py,sha256=Ik7GTnKzzXwykQH-VFsBeJPCtjqKx8LA91RCmiHNFpo,1666
 tests/test_update_regscale_config.py,sha256=GoU2ZKh7_2RYFj742_oFz8WuQx4NWL6nx2rzs4TSr40,1254
-RegScale_CLI-4.24.1.dist-info/LICENSE,sha256=ytNhYQ9Rmhj_m-EX2pPq9Ld6tH5wrqqDYg-fCf46WDU,1076
-RegScale_CLI-4.24.1.dist-info/METADATA,sha256=xujN1KcScRf19kT4Xb5Sxb7Qm-sgHuM2A9hkLPf8gV0,6195
-RegScale_CLI-4.24.1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-RegScale_CLI-4.24.1.dist-info/entry_points.txt,sha256=7K_c4hLFOPUjCZbuY5gat-C4fVj0-4aqxbwBon21Nnc,43
-RegScale_CLI-4.24.1.dist-info/top_level.txt,sha256=6YXQTQW1Iq23W68s8qehQk2zgUCqyCCr6miOwR-RiPM,37
-RegScale_CLI-4.24.1.dist-info/RECORD,,
+RegScale_CLI-4.25.0.dist-info/LICENSE,sha256=ytNhYQ9Rmhj_m-EX2pPq9Ld6tH5wrqqDYg-fCf46WDU,1076
+RegScale_CLI-4.25.0.dist-info/METADATA,sha256=UK9ujVnVmWdvLJhsfklFP9C-VJj2BGJTMMt1pzdul-w,6330
+RegScale_CLI-4.25.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+RegScale_CLI-4.25.0.dist-info/entry_points.txt,sha256=7K_c4hLFOPUjCZbuY5gat-C4fVj0-4aqxbwBon21Nnc,43
+RegScale_CLI-4.25.0.dist-info/top_level.txt,sha256=6YXQTQW1Iq23W68s8qehQk2zgUCqyCCr6miOwR-RiPM,37
+RegScale_CLI-4.25.0.dist-info/RECORD,,
```

