# Comparing `tmp/flash_attn-1.0.0.tar.gz` & `tmp/flash_attn-1.0.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "flash_attn-1.0.0.tar", last modified: Wed Apr 12 06:37:27 2023, max compression
+gzip compressed data, was "flash_attn-1.0.1.tar", last modified: Wed Apr 12 17:06:34 2023, max compression
```

## Comparing `flash_attn-1.0.0.tar` & `flash_attn-1.0.1.tar`

### file list

```diff
@@ -1,1694 +1,1699 @@
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:27.167849 flash_attn-1.0.0/
--rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.0/AUTHORS
--rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.0/LICENSE
--rw-r--r--   0 root         (0) root         (0)      251 2023-04-12 06:28:28.000000 flash_attn-1.0.0/MANIFEST.in
--rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 06:37:27.163859 flash_attn-1.0.0/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)     9529 2023-04-12 06:31:27.000000 flash_attn-1.0.0/README.md
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.522416 flash_attn-1.0.0/csrc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.831107 flash_attn-1.0.0/csrc/flash_attn/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.292243 flash_attn-1.0.0/csrc/flash_attn/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.957247 flash_attn-1.0.0/csrc/flash_attn/cutlass/cmake/
--rw-r--r--   0 root         (0) root         (0)     2023 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/cmake/nop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.893621 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:50.281335 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/00_basic_gemm/
--rw-r--r--   0 root         (0) root         (0)    14698 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:50.304969 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
--rw-r--r--   0 root         (0) root         (0)    13255 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:50.490603 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
--rw-r--r--   0 root         (0) root         (0)     7157 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:50.910665 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/
--rw-r--r--   0 root         (0) root         (0)     4478 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
--rw-r--r--   0 root         (0) root         (0)     7081 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
--rw-r--r--   0 root         (0) root         (0)     2691 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
--rw-r--r--   0 root         (0) root         (0)     5819 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
--rw-r--r--   0 root         (0) root         (0)    11415 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.009821 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/04_tile_iterator/
--rw-r--r--   0 root         (0) root         (0)     8226 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.094185 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/05_batched_gemm/
--rw-r--r--   0 root         (0) root         (0)    15161 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.320405 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
--rw-r--r--   0 root         (0) root         (0)    17570 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.419017 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18283 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.540496 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18229 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.563738 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    28127 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.590131 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/10_planar_complex/
--rw-r--r--   0 root         (0) root         (0)    21947 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.725821 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
--rw-r--r--   0 root         (0) root         (0)    23244 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:51.903862 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
--rw-r--r--   0 root         (0) root         (0)    13151 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:55.202337 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
--rw-r--r--   0 root         (0) root         (0)    26102 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    22877 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
--rw-r--r--   0 root         (0) root         (0)    28268 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
--rw-r--r--   0 root         (0) root         (0)    24493 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:55.496592 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
--rw-r--r--   0 root         (0) root         (0)    15552 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    11520 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     8756 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8759 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8712 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8762 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8787 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8793 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     8711 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7269 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7338 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7294 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7359 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7362 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7430 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
--rw-r--r--   0 root         (0) root         (0)     7627 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
--rw-r--r--   0 root         (0) root         (0)     7634 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:56.652826 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
--rw-r--r--   0 root         (0) root         (0)    16152 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    18151 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)     3973 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    26762 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
--rw-r--r--   0 root         (0) root         (0)    26775 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
--rw-r--r--   0 root         (0) root         (0)    28422 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
--rw-r--r--   0 root         (0) root         (0)    28073 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    17111 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
--rw-r--r--   0 root         (0) root         (0)    15658 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.622407 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:56.783089 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
--rw-r--r--   0 root         (0) root         (0)    10368 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.154860 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31616 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    31443 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21012 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    20494 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
--rw-r--r--   0 root         (0) root         (0)     6047 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    33788 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    33506 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    21452 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    21066 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    27144 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
--rw-r--r--   0 root         (0) root         (0)    27400 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.455355 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    18020 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.625810 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    15042 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.717549 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    27758 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.741223 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
--rw-r--r--   0 root         (0) root         (0)    12580 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.816917 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
--rw-r--r--   0 root         (0) root         (0)    14007 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:58.942489 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
--rw-r--r--   0 root         (0) root         (0)    13401 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.030177 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/20_simt_canonical/
--rw-r--r--   0 root         (0) root         (0)    12556 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.135500 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
--rw-r--r--   0 root         (0) root         (0)    17319 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.213302 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
--rw-r--r--   0 root         (0) root         (0)    21495 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.326541 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
--rw-r--r--   0 root         (0) root         (0)    27530 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.426010 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
--rw-r--r--   0 root         (0) root         (0)    50967 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.614905 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    26547 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
--rw-r--r--   0 root         (0) root         (0)    25628 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.738448 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
--rw-r--r--   0 root         (0) root         (0)    25538 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:59.881439 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
--rw-r--r--   0 root         (0) root         (0)    30446 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.138057 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
--rw-r--r--   0 root         (0) root         (0)    28159 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.229351 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
--rw-r--r--   0 root         (0) root         (0)    28403 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.343401 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
--rw-r--r--   0 root         (0) root         (0)    27329 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.420167 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/31_basic_syrk/
--rw-r--r--   0 root         (0) root         (0)    15206 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.528884 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/32_basic_trmm/
--rw-r--r--   0 root         (0) root         (0)    15907 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.624443 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
--rw-r--r--   0 root         (0) root         (0)    31803 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:00.710975 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
--rw-r--r--   0 root         (0) root         (0)    22378 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:01.133089 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
--rw-r--r--   0 root         (0) root         (0)    23114 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
--rw-r--r--   0 root         (0) root         (0)    16723 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    18713 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:01.263435 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
--rw-r--r--   0 root         (0) root         (0)    20795 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:01.715901 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
--rw-r--r--   0 root         (0) root         (0)    31111 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
--rw-r--r--   0 root         (0) root         (0)    13982 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
--rw-r--r--   0 root         (0) root         (0)    33905 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:01.804892 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
--rw-r--r--   0 root         (0) root         (0)    47455 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:02.046147 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/39_gemm_permute/
--rw-r--r--   0 root         (0) root         (0)    37896 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:03.044770 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
--rw-r--r--   0 root         (0) root         (0)    18389 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
--rw-r--r--   0 root         (0) root         (0)    11865 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
--rw-r--r--   0 root         (0) root         (0)     9885 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:03.152433 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/
--rw-r--r--   0 root         (0) root         (0)    22349 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)    22349 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9162 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
--rw-r--r--   0 root         (0) root         (0)     6768 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    34379 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6666 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    38173 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
--rw-r--r--   0 root         (0) root         (0)    39997 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:03.762939 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
--rw-r--r--   0 root         (0) root         (0)     3994 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
--rw-r--r--   0 root         (0) root         (0)     6241 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27198 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14091 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6782 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
--rw-r--r--   0 root         (0) root         (0)    13959 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
--rw-r--r--   0 root         (0) root         (0)    72984 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    10878 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:04.117644 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
--rw-r--r--   0 root         (0) root         (0)    23855 2023-03-13 04:24:25.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     3142 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64480 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)    64500 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
--rw-r--r--   0 root         (0) root         (0)     2435 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
--rw-r--r--   0 root         (0) root         (0)     9497 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
--rw-r--r--   0 root         (0) root         (0)    48655 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
--rw-r--r--   0 root         (0) root         (0)    61195 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:04.217250 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/
--rw-r--r--   0 root         (0) root         (0)     3747 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:04.240664 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
--rw-r--r--   0 root         (0) root         (0)    23901 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:04.474242 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
--rw-r--r--   0 root         (0) root         (0)    23867 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:04.823947 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.826868 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.816288 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:05.323713 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6370 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4099 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    10439 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:05.505470 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6848 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.832399 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:05.781618 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    14747 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
--rw-r--r--   0 root         (0) root         (0)    10231 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
--rw-r--r--   0 root         (0) root         (0)     3745 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:05.894838 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:05.985887 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
--rw-r--r--   0 root         (0) root         (0)    16955 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)    12669 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
--rw-r--r--   0 root         (0) root         (0)     2366 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
--rw-r--r--   0 root         (0) root         (0)    31360 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.009254 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    18422 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
--rw-r--r--   0 root         (0) root         (0)     3577 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.034997 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     5818 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.305702 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15613 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
--rw-r--r--   0 root         (0) root         (0)     7920 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    29976 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.513527 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
--rw-r--r--   0 root         (0) root         (0)    24464 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.769717 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
--rw-r--r--   0 root         (0) root         (0)    22676 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.848554 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/
--rw-r--r--   0 root         (0) root         (0)    16736 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.925596 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
--rw-r--r--   0 root         (0) root         (0)    22625 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:06.950596 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
--rw-r--r--   0 root         (0) root         (0)    18635 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:07.015085 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/60_cutlass_import/
--rw-r--r--   0 root         (0) root         (0)     2849 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:07.225459 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/common/
--rw-r--r--   0 root         (0) root         (0)     4449 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/common/helper.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.904938 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:07.252541 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/cute/tutorial/
--rw-r--r--   0 root         (0) root         (0)    14342 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.916386 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:10.432132 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/
--rw-r--r--   0 root         (0) root         (0)     3793 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:12.081505 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/
--rw-r--r--   0 root         (0) root         (0)     3538 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
--rw-r--r--   0 root         (0) root         (0)    12127 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
--rw-r--r--   0 root         (0) root         (0)     2691 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
--rw-r--r--   0 root         (0) root         (0)    14313 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
--rw-r--r--   0 root         (0) root         (0)     8511 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
--rw-r--r--   0 root         (0) root         (0)    15166 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8074 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
--rw-r--r--   0 root         (0) root         (0)    11096 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)     7040 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     4193 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
--rw-r--r--   0 root         (0) root         (0)    16554 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
--rw-r--r--   0 root         (0) root         (0)    31682 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    55577 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
--rw-r--r--   0 root         (0) root         (0)     8254 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
--rw-r--r--   0 root         (0) root         (0)    43978 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2622 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
--rw-r--r--   0 root         (0) root         (0)     3998 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
--rw-r--r--   0 root         (0) root         (0)     3656 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
--rw-r--r--   0 root         (0) root         (0)     5102 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
--rw-r--r--   0 root         (0) root         (0)     8473 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
--rw-r--r--   0 root         (0) root         (0)     5286 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
--rw-r--r--   0 root         (0) root         (0)     7746 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
--rw-r--r--   0 root         (0) root         (0)     7616 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
--rw-r--r--   0 root         (0) root         (0)    62709 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array.h
--rw-r--r--   0 root         (0) root         (0)     3662 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13128 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     6371 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/barrier.h
--rw-r--r--   0 root         (0) root         (0)    13371 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
--rw-r--r--   0 root         (0) root         (0)     6338 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/blas3.h
--rw-r--r--   0 root         (0) root         (0)     9372 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
--rw-r--r--   0 root         (0) root         (0)    19422 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/complex.h
--rw-r--r--   0 root         (0) root         (0)    47943 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/constants.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:12.395292 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/
--rw-r--r--   0 root         (0) root         (0)    22725 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)    16292 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     6664 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:12.703080 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/
--rw-r--r--   0 root         (0) root         (0)     9744 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    12078 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    10044 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:15.592357 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
--rw-r--r--   0 root         (0) root         (0)     7671 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
--rw-r--r--   0 root         (0) root         (0)    53546 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    56838 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11953 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     4690 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     4660 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    15891 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
--rw-r--r--   0 root         (0) root         (0)    28745 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    10459 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
--rw-r--r--   0 root         (0) root         (0)     9324 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    14864 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
--rw-r--r--   0 root         (0) root         (0)    11980 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    14883 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
--rw-r--r--   0 root         (0) root         (0)    19294 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
--rw-r--r--   0 root         (0) root         (0)    18048 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15430 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
--rw-r--r--   0 root         (0) root         (0)    15685 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17107 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)    16725 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:15.824424 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
--rw-r--r--   0 root         (0) root         (0)     9689 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:22.186616 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
--rw-r--r--   0 root         (0) root         (0)    15306 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    19735 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18940 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    26136 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10953 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11529 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)    11333 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    13664 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    10627 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9314 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
--rw-r--r--   0 root         (0) root         (0)     9018 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
--rw-r--r--   0 root         (0) root         (0)    10387 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    30197 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
--rw-r--r--   0 root         (0) root         (0)    11202 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    10349 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    11519 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9043 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10832 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8450 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     9569 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    11020 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15014 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     9634 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    15132 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     7945 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)     8891 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)    18249 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
--rw-r--r--   0 root         (0) root         (0)     9971 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    12024 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8821 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
--rw-r--r--   0 root         (0) root         (0)    10744 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
--rw-r--r--   0 root         (0) root         (0)     8871 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
--rw-r--r--   0 root         (0) root         (0)    10747 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
--rw-r--r--   0 root         (0) root         (0)     9899 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    20899 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
--rw-r--r--   0 root         (0) root         (0)     8921 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
--rw-r--r--   0 root         (0) root         (0)    12745 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     8097 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
--rw-r--r--   0 root         (0) root         (0)    36697 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
--rw-r--r--   0 root         (0) root         (0)    30106 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20086 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
--rw-r--r--   0 root         (0) root         (0)    12175 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    26320 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    16915 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12476 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8045 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:22.538256 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
--rw-r--r--   0 root         (0) root         (0)    12419 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
--rw-r--r--   0 root         (0) root         (0)    30655 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8772 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
--rw-r--r--   0 root         (0) root         (0)    11827 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/coord.h
--rw-r--r--   0 root         (0) root         (0)    11077 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/core_io.h
--rw-r--r--   0 root         (0) root         (0)     8697 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
--rw-r--r--   0 root         (0) root         (0)     4216 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:48.975763 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:25.657618 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    18909 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
--rw-r--r--   0 root         (0) root         (0)    11563 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     8449 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    13571 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
--rw-r--r--   0 root         (0) root         (0)    23649 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
--rw-r--r--   0 root         (0) root         (0)     9067 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
--rw-r--r--   0 root         (0) root         (0)    15195 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
--rw-r--r--   0 root         (0) root         (0)     8065 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
--rw-r--r--   0 root         (0) root         (0)     3693 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
--rw-r--r--   0 root         (0) root         (0)     8344 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
--rw-r--r--   0 root         (0) root         (0)     3058 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
--rw-r--r--   0 root         (0) root         (0)     9351 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20486 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
--rw-r--r--   0 root         (0) root         (0)    19348 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
--rw-r--r--   0 root         (0) root         (0)    12102 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
--rw-r--r--   0 root         (0) root         (0)     3688 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
--rw-r--r--   0 root         (0) root         (0)     3669 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
--rw-r--r--   0 root         (0) root         (0)     8662 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     3416 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
--rw-r--r--   0 root         (0) root         (0)     2656 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:30.602500 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)     9142 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)     3234 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7209 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13385 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
--rw-r--r--   0 root         (0) root         (0)    28290 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7129 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
--rw-r--r--   0 root         (0) root         (0)    10846 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5817 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     5763 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5947 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4409 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
--rw-r--r--   0 root         (0) root         (0)     7398 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7303 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4098 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4678 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
--rw-r--r--   0 root         (0) root         (0)    20099 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
--rw-r--r--   0 root         (0) root         (0)     8279 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
--rw-r--r--   0 root         (0) root         (0)     7455 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
--rw-r--r--   0 root         (0) root         (0)    13424 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
--rw-r--r--   0 root         (0) root         (0)    13933 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
--rw-r--r--   0 root         (0) root         (0)     7401 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)    14610 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9073 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
--rw-r--r--   0 root         (0) root         (0)    16804 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
--rw-r--r--   0 root         (0) root         (0)    52430 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    29199 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    13454 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7308 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
--rw-r--r--   0 root         (0) root         (0)    14359 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
--rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
--rw-r--r--   0 root         (0) root         (0)    19750 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
--rw-r--r--   0 root         (0) root         (0)    40870 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    18821 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
--rw-r--r--   0 root         (0) root         (0)     5636 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
--rw-r--r--   0 root         (0) root         (0)    21249 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
--rw-r--r--   0 root         (0) root         (0)    13873 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    14496 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)     9146 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
--rw-r--r--   0 root         (0) root         (0)    15534 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
--rw-r--r--   0 root         (0) root         (0)     7487 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
--rw-r--r--   0 root         (0) root         (0)    17756 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
--rw-r--r--   0 root         (0) root         (0)     7394 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:32.129187 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     7055 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7736 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5880 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)     9883 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     8924 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6045 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4864 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
--rw-r--r--   0 root         (0) root         (0)     5979 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    25658 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
--rw-r--r--   0 root         (0) root         (0)    20290 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    22922 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
--rw-r--r--   0 root         (0) root         (0)    14258 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7704 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     7485 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     3916 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)    26026 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
--rw-r--r--   0 root         (0) root         (0)    35369 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/float8.h
--rw-r--r--   0 root         (0) root         (0)     2645 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
--rw-r--r--   0 root         (0) root         (0)    12668 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/functional.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:32.194385 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:33.369206 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    17028 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
--rw-r--r--   0 root         (0) root         (0)    24413 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
--rw-r--r--   0 root         (0) root         (0)    27616 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    25202 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22367 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)    22375 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    22725 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     2591 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)    13736 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    17329 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)    20450 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    14902 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)    21594 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
--rw-r--r--   0 root         (0) root         (0)    13362 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    13968 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    14853 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     5690 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
--rw-r--r--   0 root         (0) root         (0)    18127 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)     2747 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16719 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
--rwxr-xr-x   0 root         (0) root         (0)    21050 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
--rw-r--r--   0 root         (0) root         (0)    26464 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
--rw-r--r--   0 root         (0) root         (0)    15946 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:41.653499 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
--rw-r--r--   0 root         (0) root         (0)    29360 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    37758 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
--rw-r--r--   0 root         (0) root         (0)    16130 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)    12385 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     6592 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5848 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    11104 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
--rw-r--r--   0 root         (0) root         (0)     7983 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
--rw-r--r--   0 root         (0) root         (0)     4932 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)    11951 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)     8125 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)     6457 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8086 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
--rwxr-xr-x   0 root         (0) root         (0)     5349 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
--rw-r--r--   0 root         (0) root         (0)    11560 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    20509 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)    12470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    10620 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)     9872 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
--rw-r--r--   0 root         (0) root         (0)    16990 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)     9444 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
--rwxr-xr-x   0 root         (0) root         (0)    13375 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
--rwxr-xr-x   0 root         (0) root         (0)    21830 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
--rwxr-xr-x   0 root         (0) root         (0)    10315 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    10873 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    10730 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)    10850 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    28916 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
--rw-r--r--   0 root         (0) root         (0)    13357 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     8693 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
--rw-r--r--   0 root         (0) root         (0)     8761 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
--rw-r--r--   0 root         (0) root         (0)    14687 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
--rw-r--r--   0 root         (0) root         (0)     4691 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)    15623 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)    27281 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
--rwxr-xr-x   0 root         (0) root         (0)     6144 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
--rw-r--r--   0 root         (0) root         (0)     5141 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
--rw-r--r--   0 root         (0) root         (0)    22949 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    18937 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
--rw-r--r--   0 root         (0) root         (0)     4291 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    23216 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
--rw-r--r--   0 root         (0) root         (0)    39288 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
--rw-r--r--   0 root         (0) root         (0)    47186 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
--rw-r--r--   0 root         (0) root         (0)    23605 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
--rw-r--r--   0 root         (0) root         (0)     8090 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
--rwxr-xr-x   0 root         (0) root         (0)     8979 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
--rw-r--r--   0 root         (0) root         (0)    16849 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     7148 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
--rw-r--r--   0 root         (0) root         (0)    22938 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16101 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
--rw-r--r--   0 root         (0) root         (0)     4334 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
--rw-r--r--   0 root         (0) root         (0)    24138 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    17543 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)    13586 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
--rwxr-xr-x   0 root         (0) root         (0)    23876 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    19513 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:42.332055 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     3567 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
--rw-r--r--   0 root         (0) root         (0)    15373 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
--rw-r--r--   0 root         (0) root         (0)    29987 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
--rw-r--r--   0 root         (0) root         (0)     8142 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:47.639894 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    31930 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
--rwxr-xr-x   0 root         (0) root         (0)     6979 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
--rw-r--r--   0 root         (0) root         (0)    34241 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
--rw-r--r--   0 root         (0) root         (0)     5123 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
--rw-r--r--   0 root         (0) root         (0)    57426 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
--rw-r--r--   0 root         (0) root         (0)    19257 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
--rw-r--r--   0 root         (0) root         (0)    42310 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
--rw-r--r--   0 root         (0) root         (0)   103000 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    32106 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
--rw-r--r--   0 root         (0) root         (0)    12645 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
--rw-r--r--   0 root         (0) root         (0)     7387 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    20975 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7998 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     5110 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)     4627 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     7113 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
--rw-r--r--   0 root         (0) root         (0)     6323 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)     7121 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
--rw-r--r--   0 root         (0) root         (0)     4959 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
--rw-r--r--   0 root         (0) root         (0)    65201 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
--rw-r--r--   0 root         (0) root         (0)    25495 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8509 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
--rw-r--r--   0 root         (0) root         (0)    19515 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
--rw-r--r--   0 root         (0) root         (0)    24047 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    13837 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
--rwxr-xr-x   0 root         (0) root         (0)     4726 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
--rw-r--r--   0 root         (0) root         (0)     3652 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
--rw-r--r--   0 root         (0) root         (0)     7823 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
--rw-r--r--   0 root         (0) root         (0)    27415 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
--rw-r--r--   0 root         (0) root         (0)    32894 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)    28015 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15995 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     6901 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
--rw-r--r--   0 root         (0) root         (0)    22653 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
--rw-r--r--   0 root         (0) root         (0)    14747 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
--rw-r--r--   0 root         (0) root         (0)     9864 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
--rw-r--r--   0 root         (0) root         (0)    27061 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
--rw-r--r--   0 root         (0) root         (0)     9210 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
--rw-r--r--   0 root         (0) root         (0)    25333 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
--rw-r--r--   0 root         (0) root         (0)    20473 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
--rw-r--r--   0 root         (0) root         (0)    15007 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)    26485 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:51.661875 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    20553 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     6684 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5160 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     9026 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     4053 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     4685 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)     5725 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     2619 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
--rw-r--r--   0 root         (0) root         (0)    37705 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    23132 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
--rw-r--r--   0 root         (0) root         (0)    78615 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    21205 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14589 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)     6144 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     8446 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
--rw-r--r--   0 root         (0) root         (0)     3079 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
--rw-r--r--   0 root         (0) root         (0)    59793 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11758 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    14407 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    15721 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
--rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2939 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
--rw-r--r--   0 root         (0) root         (0)     8966 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)    11017 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)   136033 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    99649 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
--rw-r--r--   0 root         (0) root         (0)    75179 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
--rw-r--r--   0 root         (0) root         (0)    13151 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
--rw-r--r--   0 root         (0) root         (0)    27101 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
--rw-r--r--   0 root         (0) root         (0)     7241 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
--rw-r--r--   0 root         (0) root         (0)    17303 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    19125 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)     4610 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
--rw-r--r--   0 root         (0) root         (0)     8728 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    23615 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/half.h
--rw-r--r--   0 root         (0) root         (0)     6893 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
--rw-r--r--   0 root         (0) root         (0)     2801 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:52.721240 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/
--rw-r--r--   0 root         (0) root         (0)     3020 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)    35369 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     9133 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
--rw-r--r--   0 root         (0) root         (0)     4696 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18295 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)    29599 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
--rw-r--r--   0 root         (0) root         (0)    33137 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
--rw-r--r--   0 root         (0) root         (0)    29336 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
--rw-r--r--   0 root         (0) root         (0)     3328 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
--rw-r--r--   0 root         (0) root         (0)   364115 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix.h
--rw-r--r--   0 root         (0) root         (0)     4991 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
--rw-r--r--   0 root         (0) root         (0)     2726 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
--rw-r--r--   0 root         (0) root         (0)    71278 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
--rw-r--r--   0 root         (0) root         (0)     3505 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
--rw-r--r--   0 root         (0) root         (0)     5492 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:52.741813 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/platform/
--rw-r--r--   0 root         (0) root         (0)    26097 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
--rw-r--r--   0 root         (0) root         (0)    15565 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
--rw-r--r--   0 root         (0) root         (0)    20900 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
--rw-r--r--   0 root         (0) root         (0)     2369 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/real.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:52.762215 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:53.487313 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
--rw-r--r--   0 root         (0) root         (0)     6823 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)     8152 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)    11579 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    11448 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:53.784871 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)     8762 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
--rw-r--r--   0 root         (0) root         (0)     7897 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
--rw-r--r--   0 root         (0) root         (0)    20685 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
--rw-r--r--   0 root         (0) root         (0)    21662 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:53.896151 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     7208 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
--rw-r--r--   0 root         (0) root         (0)     6790 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
--rw-r--r--   0 root         (0) root         (0)     2936 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
--rw-r--r--   0 root         (0) root         (0)     5929 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
--rw-r--r--   0 root         (0) root         (0)     4186 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
--rw-r--r--   0 root         (0) root         (0)    17243 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
--rw-r--r--   0 root         (0) root         (0)     8964 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)    12207 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
--rw-r--r--   0 root         (0) root         (0)    11201 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9509 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
--rw-r--r--   0 root         (0) root         (0)    10250 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    13017 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:53.983443 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/thread/
--rw-r--r--   0 root         (0) root         (0)     5931 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
--rw-r--r--   0 root         (0) root         (0)     2581 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/trace.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:54.078885 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/
--rw-r--r--   0 root         (0) root         (0)    33349 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:54.184149 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
--rw-r--r--   0 root         (0) root         (0)     3835 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
--rw-r--r--   0 root         (0) root         (0)     4309 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:57.198200 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)     6181 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44443 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    44309 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    12890 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    11097 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
--rw-r--r--   0 root         (0) root         (0)    70684 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    28232 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
--rwxr-xr-x   0 root         (0) root         (0)    10243 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
--rw-r--r--   0 root         (0) root         (0)    31412 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    62672 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    27175 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    28064 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
--rw-r--r--   0 root         (0) root         (0)    13088 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8232 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)     2638 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
--rw-r--r--   0 root         (0) root         (0)    13283 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    18623 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
--rw-r--r--   0 root         (0) root         (0)    27922 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    47789 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
--rw-r--r--   0 root         (0) root         (0)     2616 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
--rw-r--r--   0 root         (0) root         (0)    16510 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
--rw-r--r--   0 root         (0) root         (0)    15486 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
--rw-r--r--   0 root         (0) root         (0)    36050 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
--rw-r--r--   0 root         (0) root         (0)    43663 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
--rw-r--r--   0 root         (0) root         (0)     5226 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:57.260827 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
--rw-r--r--   0 root         (0) root         (0)     8828 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
--rw-r--r--   0 root         (0) root         (0)     8179 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/uint128.h
--rw-r--r--   0 root         (0) root         (0)     4543 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.077891 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:57.352559 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:35:57.480332 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/common/
--rw-r--r--   0 root         (0) root         (0)     4900 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
--rw-r--r--   0 root         (0) root         (0)     5381 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.095984 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:04.483102 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/
--rw-r--r--   0 root         (0) root         (0)    21797 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
--rw-r--r--   0 root         (0) root         (0)     5344 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5443 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9110 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8485 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5243 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5378 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12054 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9603 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5267 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5089 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    13690 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5390 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5191 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11136 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5291 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3551 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rwxr-xr-x   0 root         (0) root         (0)     8278 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    20555 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    20647 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5155 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5239 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26114 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    26210 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5111 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     5194 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5738 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5439 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7363 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3984 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    39452 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
--rw-r--r--   0 root         (0) root         (0)    14471 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4662 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26224 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
--rw-r--r--   0 root         (0) root         (0)    22092 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     5179 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5358 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5264 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3615 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7591 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10514 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5157 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5772 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23526 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
--rw-r--r--   0 root         (0) root         (0)    21512 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     5135 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5347 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3736 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5257 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12276 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
--rw-r--r--   0 root         (0) root         (0)    21643 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
--rw-r--r--   0 root         (0) root         (0)     3622 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     6560 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5256 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    17700 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
--rw-r--r--   0 root         (0) root         (0)    18451 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    22194 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     9383 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    16100 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:06.039071 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/
--rw-r--r--   0 root         (0) root         (0)     7365 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/array.cu
--rw-r--r--   0 root         (0) root         (0)     7353 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
--rw-r--r--   0 root         (0) root         (0)     6981 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/complex.cu
--rw-r--r--   0 root         (0) root         (0)     4009 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/float8.cu
--rw-r--r--   0 root         (0) root         (0)    13001 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/functional.cu
--rw-r--r--   0 root         (0) root         (0)     3553 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/half.cu
--rw-r--r--   0 root         (0) root         (0)     5295 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     8592 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
--rw-r--r--   0 root         (0) root         (0)    10684 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
--rw-r--r--   0 root         (0) root         (0)     8148 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
--rw-r--r--   0 root         (0) root         (0)     5777 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
--rw-r--r--   0 root         (0) root         (0)     6746 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
--rw-r--r--   0 root         (0) root         (0)     8885 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
--rw-r--r--   0 root         (0) root         (0)     2050 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
--rw-r--r--   0 root         (0) root         (0)     7088 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.136268 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:06.271239 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/ampere/
--rw-r--r--   0 root         (0) root         (0)     3527 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
--rw-r--r--   0 root         (0) root         (0)    14320 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:07.583965 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/
--rw-r--r--   0 root         (0) root         (0)     3332 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
--rw-r--r--   0 root         (0) root         (0)     4861 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
--rw-r--r--   0 root         (0) root         (0)     5620 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
--rw-r--r--   0 root         (0) root         (0)     7178 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
--rw-r--r--   0 root         (0) root         (0)    12569 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
--rw-r--r--   0 root         (0) root         (0)     4856 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
--rw-r--r--   0 root         (0) root         (0)     6702 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
--rw-r--r--   0 root         (0) root         (0)     6734 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
--rw-r--r--   0 root         (0) root         (0)     5914 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
--rw-r--r--   0 root         (0) root         (0)     3488 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
--rw-r--r--   0 root         (0) root         (0)     2342 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
--rw-r--r--   0 root         (0) root         (0)    13304 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:08.434043 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/
--rw-r--r--   0 root         (0) root         (0)    14365 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
--rw-r--r--   0 root         (0) root         (0)    18990 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
--rw-r--r--   0 root         (0) root         (0)    13875 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:08.556493 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/layout/
--rw-r--r--   0 root         (0) root         (0)     4544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.157631 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:08.921747 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
--rw-r--r--   0 root         (0) root         (0)    15818 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
--rw-r--r--   0 root         (0) root         (0)     6534 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
--rw-r--r--   0 root         (0) root         (0)     9964 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:11.507485 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
--rw-r--r--   0 root         (0) root         (0)    13824 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
--rw-r--r--   0 root         (0) root         (0)    27176 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
--rw-r--r--   0 root         (0) root         (0)    12061 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
--rw-r--r--   0 root         (0) root         (0)    25275 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
--rw-r--r--   0 root         (0) root         (0)    84612 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    70486 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    25293 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    13012 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
--rw-r--r--   0 root         (0) root         (0)     7743 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    19178 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
--rw-r--r--   0 root         (0) root         (0)    28433 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)    11038 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
--rw-r--r--   0 root         (0) root         (0)    11734 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:12.002198 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
--rw-r--r--   0 root         (0) root         (0)     6783 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     7275 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.190262 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:55.329309 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/
--rw-r--r--   0 root         (0) root         (0)    10189 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17899 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10164 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17984 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8915 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16447 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16575 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8317 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6714 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6747 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7895 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7930 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6516 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6549 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9016 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9053 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4628 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6165 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6124 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9634 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16357 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13189 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8845 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13583 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13464 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9571 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16239 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6140 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     9544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16417 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13075 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8775 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3539 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7965 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13273 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3648 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8608 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3645 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6096 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    18135 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13008 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8505 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11090 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6156 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6116 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11066 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    17114 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3528 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3540 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7964 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16457 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13266 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8933 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13551 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13540 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6130 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     8160 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7847 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16131 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13014 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8754 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11497 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6147 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     6107 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13518 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    13398 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7845 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16149 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6119 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7827 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16101 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9526 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7898 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    11470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     3584 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3473 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12967 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12931 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12930 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12895 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8349 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7300 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8348 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7291 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    10240 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    26146 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11339 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7346 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12397 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6859 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     7239 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8121 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16882 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17111 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12637 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8388 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    10044 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10020 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    17544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9588 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    11288 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7977 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5693 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7959 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16691 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12408 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6864 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7744 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16531 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6675 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     7752 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16484 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6663 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     4663 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4945 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     6616 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    10581 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16950 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16902 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15131 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16855 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6854 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6686 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6755 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6687 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4726 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4718 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    16715 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    12841 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     4544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13157 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
--rw-r--r--   0 root         (0) root         (0)     6028 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6031 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6064 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4909 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6088 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6037 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6040 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5382 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5406 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5402 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    13055 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13027 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5388 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     6939 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7677 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7725 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3854 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     6396 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10157 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)    10306 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)    11186 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46795 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    54085 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8318 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46687 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8411 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    46578 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40533 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    47656 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40441 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    40354 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     3513 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89517 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89304 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    89091 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    69175 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    71438 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    67796 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70056 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     7156 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     6067 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
--rw-r--r--   0 root         (0) root         (0)     9063 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
--rw-r--r--   0 root         (0) root         (0)    35894 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35813 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    35732 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    70872 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    73136 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8870 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    69488 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     8865 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    71755 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33231 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33156 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    33081 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5357 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5479 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
--rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
--rw-r--r--   0 root         (0) root         (0)     3875 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
--rw-r--r--   0 root         (0) root         (0)     3734 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)     5387 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7436 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7409 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)    17391 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    42504 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
--rw-r--r--   0 root         (0) root         (0)    22602 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    22874 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
--rw-r--r--   0 root         (0) root         (0)    42526 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)     3810 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5976 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     9313 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
--rw-r--r--   0 root         (0) root         (0)     5986 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     7268 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
--rw-r--r--   0 root         (0) root         (0)     5923 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5926 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5959 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5962 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4839 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     5983 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5932 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5935 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15203 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8623 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15104 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4777 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     8103 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8108 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8088 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8093 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8073 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8078 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8058 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8063 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15071 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8551 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14972 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5386 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5356 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5380 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5379 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12952 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5368 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7208 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5362 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7199 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7190 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4794 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4783 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4740 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    19145 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7991 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    11015 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7976 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12342 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     7961 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12321 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4786 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4775 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4993 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5017 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4987 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5011 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     5024 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4996 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     3793 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4990 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16083 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16041 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4530 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     7451 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9401 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    16027 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15985 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    20465 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
--rw-r--r--   0 root         (0) root         (0)     8264 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
--rw-r--r--   0 root         (0) root         (0)    20736 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    19479 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
--rw-r--r--   0 root         (0) root         (0)    16502 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
--rw-r--r--   0 root         (0) root         (0)    16562 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    17002 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    14698 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
--rw-r--r--   0 root         (0) root         (0)    10262 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
--rw-r--r--   0 root         (0) root         (0)     9481 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)    20898 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
--rw-r--r--   0 root         (0) root         (0)    15652 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
--rw-r--r--   0 root         (0) root         (0)     8639 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
--rw-r--r--   0 root         (0) root         (0)    15901 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
--rw-r--r--   0 root         (0) root         (0)     6124 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
--rw-r--r--   0 root         (0) root         (0)    19993 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
--rw-r--r--   0 root         (0) root         (0)    20332 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
--rw-r--r--   0 root         (0) root         (0)    17443 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
--rw-r--r--   0 root         (0) root         (0)     2626 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
--rw-r--r--   0 root         (0) root         (0)     9916 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9988 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4989 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     4992 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9762 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15614 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8733 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14089 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14444 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     4608 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    12798 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12809 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12764 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12768 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12779 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    15504 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     8673 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13989 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    14344 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:55.535876 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
--rwxr-xr-x   0 root         (0) root         (0)    46470 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
--rwxr-xr-x   0 root         (0) root         (0)    14362 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:55.845194 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/
--rw-r--r--   0 root         (0) root         (0)     4847 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)    12503 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     3109 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:56.090867 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
--rw-r--r--   0 root         (0) root         (0)     5198 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
--rw-r--r--   0 root         (0) root         (0)     7161 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
--rw-r--r--   0 root         (0) root         (0)     7124 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:36:58.609679 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25036 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
--rw-r--r--   0 root         (0) root         (0)     4345 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
--rw-r--r--   0 root         (0) root         (0)   135045 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
--rw-r--r--   0 root         (0) root         (0)     4644 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    94442 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
--rw-r--r--   0 root         (0) root         (0)    17090 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
--rw-r--r--   0 root         (0) root         (0)    13897 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14539 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    49052 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
--rw-r--r--   0 root         (0) root         (0)     8407 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
--rw-r--r--   0 root         (0) root         (0)    18705 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    78122 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    21051 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    13771 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
--rw-r--r--   0 root         (0) root         (0)    14239 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
--rw-r--r--   0 root         (0) root         (0)    29772 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12395 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
--rw-r--r--   0 root         (0) root         (0)     3502 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    12138 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
--rw-r--r--   0 root         (0) root         (0)    16308 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    12502 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:00.734122 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/
--rw-r--r--   0 root         (0) root         (0)    22128 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    10916 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
--rw-r--r--   0 root         (0) root         (0)     9873 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    18220 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
--rw-r--r--   0 root         (0) root         (0)     4920 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
--rw-r--r--   0 root         (0) root         (0)     6291 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
--rw-r--r--   0 root         (0) root         (0)     9297 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
--rw-r--r--   0 root         (0) root         (0)    37942 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
--rw-r--r--   0 root         (0) root         (0)    81659 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     9089 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
--rw-r--r--   0 root         (0) root         (0)    48928 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
--rw-r--r--   0 root         (0) root         (0)    49647 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
--rw-r--r--   0 root         (0) root         (0)    25780 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
--rw-r--r--   0 root         (0) root         (0)     7544 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
--rw-r--r--   0 root         (0) root         (0)     6487 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:01.000637 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/
--rw-r--r--   0 root         (0) root         (0)     5788 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
--rw-r--r--   0 root         (0) root         (0)     5984 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
--rw-r--r--   0 root         (0) root         (0)     7081 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.235108 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.217939 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:01.138979 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
--rw-r--r--   0 root         (0) root         (0)     2096 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.229933 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:01.261558 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
--rw-r--r--   0 root         (0) root         (0)     2915 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:01.357216 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
--rw-r--r--   0 root         (0) root         (0)     4250 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:01.520695 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
--rw-r--r--   0 root         (0) root         (0)     5727 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
--rw-r--r--   0 root         (0) root         (0)    10328 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:02.247400 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/
--rw-r--r--   0 root         (0) root         (0)    15532 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
--rw-r--r--   0 root         (0) root         (0)    15490 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
--rw-r--r--   0 root         (0) root         (0)    17098 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
--rw-r--r--   0 root         (0) root         (0)    20252 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
--rw-r--r--   0 root         (0) root         (0)     7623 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
--rw-r--r--   0 root         (0) root         (0)     4327 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.266743 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:02.574189 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/device/
--rw-r--r--   0 root         (0) root         (0)    14684 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
--rw-r--r--   0 root         (0) root         (0)    15609 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:02.848722 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
--rw-r--r--   0 root         (0) root         (0)    11350 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
--rw-r--r--   0 root         (0) root         (0)     2228 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:03.014316 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/thread/
--rw-r--r--   0 root         (0) root         (0)     3110 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
--rw-r--r--   0 root         (0) root         (0)     6657 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
--rw-r--r--   0 root         (0) root         (0)     2047 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.278128 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:03.173376 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
--rw-r--r--   0 root         (0) root         (0)    25527 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
--rw-r--r--   0 root         (0) root         (0)     9501 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:03.339246 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/util/
--rw-r--r--   0 root         (0) root         (0)     2663 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
--rw-r--r--   0 root         (0) root         (0)     7474 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.425670 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.395235 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.309388 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.315027 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:04.073359 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
--rw-r--r--   0 root         (0) root         (0)     4118 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
--rw-r--r--   0 root         (0) root         (0)    16013 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
--rw-r--r--   0 root         (0) root         (0)    38763 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
--rw-r--r--   0 root         (0) root         (0)     4070 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
--rw-r--r--   0 root         (0) root         (0)    17934 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
--rw-r--r--   0 root         (0) root         (0)     2724 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
--rw-r--r--   0 root         (0) root         (0)     7904 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.328210 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.333794 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.339898 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:04.860227 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
--rw-r--r--   0 root         (0) root         (0)     2788 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
--rw-r--r--   0 root         (0) root         (0)     2460 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
--rw-r--r--   0 root         (0) root         (0)     6224 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:05.469823 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
--rw-r--r--   0 root         (0) root         (0)     2854 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:05.960494 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
--rw-r--r--   0 root         (0) root         (0)     5897 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
--rw-r--r--   0 root         (0) root         (0)     4763 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2650 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:06.137139 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
--rw-r--r--   0 root         (0) root         (0)     6845 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:07.143217 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
--rw-r--r--   0 root         (0) root         (0)     3003 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
--rw-r--r--   0 root         (0) root         (0)     6103 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
--rw-r--r--   0 root         (0) root         (0)     4698 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
--rw-r--r--   0 root         (0) root         (0)     8397 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
--rw-r--r--   0 root         (0) root         (0)     8830 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    12998 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
--rw-r--r--   0 root         (0) root         (0)     9454 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
--rw-r--r--   0 root         (0) root         (0)     9085 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
--rw-r--r--   0 root         (0) root         (0)    11975 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
--rw-r--r--   0 root         (0) root         (0)     6177 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
--rw-r--r--   0 root         (0) root         (0)     8017 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
--rw-r--r--   0 root         (0) root         (0)     7201 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
--rw-r--r--   0 root         (0) root         (0)    16970 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:07.426024 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
--rw-r--r--   0 root         (0) root         (0)     3674 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    21504 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
--rw-r--r--   0 root         (0) root         (0)     2328 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:08.124976 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
--rw-r--r--   0 root         (0) root         (0)     2115 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
--rw-r--r--   0 root         (0) root         (0)     4337 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
--rw-r--r--   0 root         (0) root         (0)     3694 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
--rw-r--r--   0 root         (0) root         (0)     8565 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
--rw-r--r--   0 root         (0) root         (0)     3902 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
--rw-r--r--   0 root         (0) root         (0)     5543 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
--rw-r--r--   0 root         (0) root         (0)     4855 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
--rw-r--r--   0 root         (0) root         (0)      811 2022-09-05 01:02:49.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.385392 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:08.392708 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
--rw-r--r--   0 root         (0) root         (0)     2651 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
--rw-r--r--   0 root         (0) root         (0)     2253 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
--rw-r--r--   0 root         (0) root         (0)     8826 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:08.672932 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
--rw-r--r--   0 root         (0) root         (0)     2139 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
--rw-r--r--   0 root         (0) root         (0)    18930 2023-01-27 22:40:09.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:10.046996 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/
--rw-r--r--   0 root         (0) root         (0)    22377 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
--rw-r--r--   0 root         (0) root         (0)    13850 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
--rw-r--r--   0 root         (0) root         (0)    42129 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
--rw-r--r--   0 root         (0) root         (0)    35777 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/handle.cu
--rw-r--r--   0 root         (0) root         (0)    12616 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
--rw-r--r--   0 root         (0) root         (0)     3782 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
--rw-r--r--   0 root         (0) root         (0)     5468 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
--rw-r--r--   0 root         (0) root         (0)    12873 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
--rw-r--r--   0 root         (0) root         (0)    11367 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:10.567988 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/
--rw-r--r--   0 root         (0) root         (0)     3190 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
--rw-r--r--   0 root         (0) root         (0)     6367 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
--rw-r--r--   0 root         (0) root         (0)    10269 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:11.120455 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/
--rw-r--r--   0 root         (0) root         (0)     6746 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
--rw-r--r--   0 root         (0) root         (0)     6286 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
--rw-r--r--   0 root         (0) root         (0)    17192 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     7199 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
--rw-r--r--   0 root         (0) root         (0)    14732 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
--rw-r--r--   0 root         (0) root         (0)     2857 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
--rw-r--r--   0 root         (0) root         (0)     2669 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
--rw-r--r--   0 root         (0) root         (0)    13134 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
--rw-r--r--   0 root         (0) root         (0)    11698 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
--rw-r--r--   0 root         (0) root         (0)    43707 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/util.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.418064 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:14.253365 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/
--rw-r--r--   0 root         (0) root         (0)    54134 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    18166 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    48660 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)    16040 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    36461 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
--rw-r--r--   0 root         (0) root         (0)    10623 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
--rw-r--r--   0 root         (0) root         (0)    17050 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
--rw-r--r--   0 root         (0) root         (0)    20436 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
--rw-r--r--   0 root         (0) root         (0)     7233 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     3233 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
--rw-r--r--   0 root         (0) root         (0)     2454 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
--rw-r--r--   0 root         (0) root         (0)    53641 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
--rw-r--r--   0 root         (0) root         (0)     7215 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
--rw-r--r--   0 root         (0) root         (0)     6841 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
--rw-r--r--   0 root         (0) root         (0)     4300 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
--rw-r--r--   0 root         (0) root         (0)     8296 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
--rw-r--r--   0 root         (0) root         (0)     6421 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
--rw-r--r--   0 root         (0) root         (0)    42366 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     8544 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     3875 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
--rw-r--r--   0 root         (0) root         (0)     2725 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
--rw-r--r--   0 root         (0) root         (0)     2340 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
--rw-r--r--   0 root         (0) root         (0)    22087 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     7876 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    27171 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
--rw-r--r--   0 root         (0) root         (0)     8773 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/options.h
--rw-r--r--   0 root         (0) root         (0)    14192 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
--rw-r--r--   0 root         (0) root         (0)     4337 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
--rw-r--r--   0 root         (0) root         (0)     2494 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
--rw-r--r--   0 root         (0) root         (0)     3941 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
--rw-r--r--   0 root         (0) root         (0)    37487 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
--rw-r--r--   0 root         (0) root         (0)    27749 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
--rw-r--r--   0 root         (0) root         (0)    25014 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6891 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24253 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6830 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)     5452 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    20688 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6471 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    26610 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6933 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
--rw-r--r--   0 root         (0) root         (0)    24431 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
--rw-r--r--   0 root         (0) root         (0)     6599 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.431154 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.436799 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.442311 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:16.328277 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
--rw-r--r--   0 root         (0) root         (0)     9774 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
--rw-r--r--   0 root         (0) root         (0)     5104 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
--rw-r--r--   0 root         (0) root         (0)     5953 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
--rw-r--r--   0 root         (0) root         (0)    17696 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
--rw-r--r--   0 root         (0) root         (0)    20881 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
--rw-r--r--   0 root         (0) root         (0)    10561 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
--rw-r--r--   0 root         (0) root         (0)     5219 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
--rw-r--r--   0 root         (0) root         (0)    11075 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
--rw-r--r--   0 root         (0) root         (0)    18653 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
--rw-r--r--   0 root         (0) root         (0)     5214 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
--rw-r--r--   0 root         (0) root         (0)     4007 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
--rw-r--r--   0 root         (0) root         (0)     4597 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
--rw-r--r--   0 root         (0) root         (0)     2674 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
--rw-r--r--   0 root         (0) root         (0)     4821 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
--rw-r--r--   0 root         (0) root         (0)    16745 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
--rw-r--r--   0 root         (0) root         (0)    20354 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     5890 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
--rw-r--r--   0 root         (0) root         (0)     1962 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:34:49.477545 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:16.507562 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
--rw-r--r--   0 root         (0) root         (0)     4606 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
--rw-r--r--   0 root         (0) root         (0)     3527 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:17.610676 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
--rw-r--r--   0 root         (0) root         (0)    48350 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
--rw-r--r--   0 root         (0) root         (0)    14296 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
--rw-r--r--   0 root         (0) root         (0)    10524 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     9652 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:18.087770 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
--rw-r--r--   0 root         (0) root         (0)     5381 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
--rw-r--r--   0 root         (0) root         (0)     6198 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)     5127 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    11615 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     7278 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)    46445 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     5294 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)    15964 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     4589 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:18.111170 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
--rw-r--r--   0 root         (0) root         (0)     5872 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:20.036593 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
--rw-r--r--   0 root         (0) root         (0)    28439 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
--rw-r--r--   0 root         (0) root         (0)     2766 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
--rw-r--r--   0 root         (0) root         (0)    17163 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
--rw-r--r--   0 root         (0) root         (0)     7097 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
--rw-r--r--   0 root         (0) root         (0)     7708 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
--rw-r--r--   0 root         (0) root         (0)     9441 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
--rw-r--r--   0 root         (0) root         (0)    11444 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
--rw-r--r--   0 root         (0) root         (0)     8148 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
--rw-r--r--   0 root         (0) root         (0)    10509 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
--rw-r--r--   0 root         (0) root         (0)    12296 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8440 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
--rw-r--r--   0 root         (0) root         (0)     8317 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
--rw-r--r--   0 root         (0) root         (0)     9027 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
--rw-r--r--   0 root         (0) root         (0)    43962 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
--rw-r--r--   0 root         (0) root         (0)     4757 2023-03-13 04:24:26.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
--rw-r--r--   0 root         (0) root         (0)     2133 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
--rw-r--r--   0 root         (0) root         (0)     6111 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
--rw-r--r--   0 root         (0) root         (0)     7670 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
--rw-r--r--   0 root         (0) root         (0)     9874 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
--rw-r--r--   0 root         (0) root         (0)     8285 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
--rw-r--r--   0 root         (0) root         (0)     8809 2023-01-27 22:40:10.000000 flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
--rw-r--r--   0 root         (0) root         (0)    30683 2023-04-11 20:32:31.000000 flash_attn-1.0.0/csrc/flash_attn/flash_api.cpp
--rw-r--r--   0 root         (0) root         (0)    32519 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/fmha_api.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:22.568727 flash_attn-1.0.0/csrc/flash_attn/src/
--rw-r--r--   0 root         (0) root         (0)     1664 2023-04-08 17:10:55.000000 flash_attn-1.0.0/csrc/flash_attn/src/block_info.h
--rw-r--r--   0 root         (0) root         (0)     3920 2023-04-09 19:53:14.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash.h
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      574 2023-04-12 03:22:08.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1124 2023-04-12 03:09:57.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:00.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      442 2023-04-08 07:26:34.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:20.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2458 2023-04-11 20:53:27.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      449 2023-04-08 07:29:33.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      598 2023-04-08 07:26:34.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    82323 2023-04-12 05:47:44.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    41051 2023-03-25 22:30:12.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_bak.h
--rw-r--r--   0 root         (0) root         (0)    39628 2023-03-25 21:25:30.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_new.h
--rw-r--r--   0 root         (0) root         (0)    39403 2023-04-08 17:10:55.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_reverse.h
--rw-r--r--   0 root         (0) root         (0)     8751 2023-04-12 03:19:57.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)      645 2023-04-07 21:42:50.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      592 2023-04-08 07:07:58.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      990 2023-04-07 18:39:34.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160.cu
--rw-r--r--   0 root         (0) root         (0)      495 2023-04-08 07:08:17.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1041 2023-04-07 18:40:27.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:08:35.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      945 2023-04-07 18:37:45.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:06:55.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)     1149 2023-04-07 22:02:41.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)      588 2023-04-08 07:07:20.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)      711 2023-04-07 22:17:50.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96.cu
--rw-r--r--   0 root         (0) root         (0)      492 2023-04-08 07:07:36.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
--rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
--rw-r--r--   0 root         (0) root         (0)    30219 2023-04-12 05:25:26.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_kernel.h
--rw-r--r--   0 root         (0) root         (0)    27253 2023-04-08 17:10:55.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_kernel_old.h
--rw-r--r--   0 root         (0) root         (0)     2346 2023-04-09 16:47:24.000000 flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_launch_template.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:22.707174 flash_attn-1.0.0/csrc/flash_attn/src/fmha/
--rw-r--r--   0 root         (0) root         (0)    17999 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/gemm.h
--rw-r--r--   0 root         (0) root         (0)    22872 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/gmem_tile.h
--rw-r--r--   0 root         (0) root         (0)     5997 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     4362 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/mask.h
--rw-r--r--   0 root         (0) root         (0)    74010 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/smem_tile.h
--rw-r--r--   0 root         (0) root         (0)    25514 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/softmax.h
--rw-r--r--   0 root         (0) root         (0)    41059 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha/utils.h
--rw-r--r--   0 root         (0) root         (0)     7152 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha.h
--rw-r--r--   0 root         (0) root         (0)     4118 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)     5292 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
--rw-r--r--   0 root         (0) root         (0)    23207 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)     2502 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_blockmask.h
--rw-r--r--   0 root         (0) root         (0)      465 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      727 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)     1713 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     6453 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)    37194 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
--rw-r--r--   0 root         (0) root         (0)    30832 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
--rw-r--r--   0 root         (0) root         (0)      445 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_hdim128.cu
--rw-r--r--   0 root         (0) root         (0)      724 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_hdim32.cu
--rw-r--r--   0 root         (0) root         (0)      725 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_hdim64.cu
--rw-r--r--   0 root         (0) root         (0)     4393 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_launch_template.h
--rw-r--r--   0 root         (0) root         (0)     3104 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_kernel.h
--rw-r--r--   0 root         (0) root         (0)     4892 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/fmha_utils.h
--rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 05:46:15.000000 flash_attn-1.0.0/csrc/flash_attn/src/kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     2927 2023-03-31 21:49:35.000000 flash_attn-1.0.0/csrc/flash_attn/src/mask.h
--rw-r--r--   0 root         (0) root         (0)     5462 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/philox.cuh
--rw-r--r--   0 root         (0) root         (0)    14205 2023-04-07 21:07:49.000000 flash_attn-1.0.0/csrc/flash_attn/src/softmax.h
--rw-r--r--   0 root         (0) root         (0)     1686 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/flash_attn/src/static_switch.h
--rw-r--r--   0 root         (0) root         (0)    13123 2023-04-12 03:28:52.000000 flash_attn-1.0.0/csrc/flash_attn/src/utils.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:22.731281 flash_attn-1.0.0/csrc/flash_gen/
--rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.0/csrc/flash_gen/decoder_masked_multihead_attention.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:23.294732 flash_attn-1.0.0/csrc/ft_attention/
--rw-r--r--   0 root         (0) root         (0)     8253 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/cuda_bf16_fallbacks.cuh
--rw-r--r--   0 root         (0) root         (0)      867 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/cuda_bf16_wrapper.h
--rw-r--r--   0 root         (0) root         (0)     7243 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention.cu
--rw-r--r--   0 root         (0) root         (0)     7463 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention.h
--rw-r--r--   0 root         (0) root         (0)    52690 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
--rw-r--r--   0 root         (0) root         (0)     7423 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/ft_attention/ft_attention.cpp
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:23.336155 flash_attn-1.0.0/csrc/fused_dense_lib/
--rw-r--r--   0 root         (0) root         (0)     8215 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_dense_lib/fused_dense.cpp
--rw-r--r--   0 root         (0) root         (0)    25273 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_dense_lib/fused_dense_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:23.446388 flash_attn-1.0.0/csrc/fused_softmax/
--rw-r--r--   0 root         (0) root         (0)     5037 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/fused_softmax.cpp
--rw-r--r--   0 root         (0) root         (0)    23616 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/scaled_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     4209 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)    24659 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
--rw-r--r--   0 root         (0) root         (0)     3154 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
--rw-r--r--   0 root         (0) root         (0)     1216 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/fused_softmax/type_shim.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:25.976331 flash_attn-1.0.0/csrc/layer_norm/
--rw-r--r--   0 root         (0) root         (0)     7248 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln.h
--rw-r--r--   0 root         (0) root         (0)    36418 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_api.cpp
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    25647 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.0/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_10240.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_12288.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_384.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_9216.cu
--rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
--rw-r--r--   0 root         (0) root         (0)    12721 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)     6655 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_kernel_traits.h
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1024.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-03-29 20:52:04.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_128.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1280.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1536.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_2048.cu
--rw-r--r--   0 root         (0) root         (0)     1032 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_256.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_2560.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_3072.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_4096.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_512.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_5120.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_6144.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_7168.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_768.cu
--rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_8192.cu
--rw-r--r--   0 root         (0) root         (0)    11720 2023-03-29 19:53:46.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)      977 2023-03-27 04:43:57.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_bwd_512.cu
--rw-r--r--   0 root         (0) root         (0)    24916 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    11515 2023-03-29 20:50:46.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
--rw-r--r--   0 root         (0) root         (0)    12530 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
--rw-r--r--   0 root         (0) root         (0)    29989 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/ln_utils.cuh
--rw-r--r--   0 root         (0) root         (0)     1278 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/layer_norm/static_switch.h
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.016899 flash_attn-1.0.0/csrc/rotary/
--rw-r--r--   0 root         (0) root         (0)     1806 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/rotary/rotary.cpp
--rw-r--r--   0 root         (0) root         (0)     1984 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/rotary/rotary_cuda.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.057279 flash_attn-1.0.0/csrc/xentropy/
--rw-r--r--   0 root         (0) root         (0)     2290 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/xentropy/interface.cpp
--rw-r--r--   0 root         (0) root         (0)    25783 2023-04-12 06:28:29.000000 flash_attn-1.0.0/csrc/xentropy/xentropy_kernel.cu
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.495990 flash_attn-1.0.0/flash_attn/
--rw-r--r--   0 root         (0) root         (0)        0 2022-07-04 00:53:37.000000 flash_attn-1.0.0/flash_attn/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.0/flash_attn/attention_kernl.py
--rw-r--r--   0 root         (0) root         (0)     5898 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/bert_padding.py
--rw-r--r--   0 root         (0) root         (0)     4722 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/flash_attention.py
--rw-r--r--   0 root         (0) root         (0)    21496 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/flash_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)    38148 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton.py
--rw-r--r--   0 root         (0) root         (0)    10593 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton_og.py
--rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton_single_query.py
--rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton_tmp.py
--rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton_tmp_og.py
--rw-rw-r--   0 root         (0) root         (0)    22919 2022-10-31 00:28:55.000000 flash_attn-1.0.0/flash_attn/flash_attn_triton_varlen.py
--rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.0/flash_attn/flash_blocksparse_attention.py
--rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.0/flash_attn/flash_blocksparse_attn_interface.py
--rw-r--r--   0 root         (0) root         (0)     7902 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/fused_softmax.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.631366 flash_attn-1.0.0/flash_attn/layers/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/layers/__init__.py
--rw-r--r--   0 root         (0) root         (0)     2039 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/layers/patch_embed.py
--rw-r--r--   0 root         (0) root         (0)    10656 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/layers/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.700488 flash_attn-1.0.0/flash_attn/losses/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/losses/__init__.py
--rw-r--r--   0 root         (0) root         (0)     6697 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/losses/cross_entropy.py
--rw-r--r--   0 root         (0) root         (0)     2122 2022-12-18 05:19:38.000000 flash_attn-1.0.0/flash_attn/losses/cross_entropy_apex.py
--rw-r--r--   0 root         (0) root         (0)     6649 2022-12-23 22:38:19.000000 flash_attn-1.0.0/flash_attn/losses/cross_entropy_parallel.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.840552 flash_attn-1.0.0/flash_attn/models/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/__init__.py
--rw-r--r--   0 root         (0) root         (0)    26630 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/bert.py
--rw-r--r--   0 root         (0) root         (0)    34989 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/gpt.py
--rw-r--r--   0 root         (0) root         (0)     4863 2023-03-22 21:08:53.000000 flash_attn-1.0.0/flash_attn/models/gpt_j.py
--rw-r--r--   0 root         (0) root         (0)     5025 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/gpt_neox.py
--rw-r--r--   0 root         (0) root         (0)     4387 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/gptj.py
--rw-r--r--   0 root         (0) root         (0)     5174 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/opt.py
--rw-r--r--   0 root         (0) root         (0)    13621 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/models/vit.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.932755 flash_attn-1.0.0/flash_attn/modules/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/modules/__init__.py
--rw-r--r--   0 root         (0) root         (0)    15244 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/modules/block.py
--rw-r--r--   0 root         (0) root         (0)     8620 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/modules/embedding.py
--rw-r--r--   0 root         (0) root         (0)    32394 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/modules/mha.py
--rw-r--r--   0 root         (0) root         (0)     1023 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/modules/mlp.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:27.023232 flash_attn-1.0.0/flash_attn/ops/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/ops/__init__.py
--rw-r--r--   0 root         (0) root         (0)    25573 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/ops/fused_dense.py
--rw-r--r--   0 root         (0) root         (0)     2685 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/ops/gelu_activation.py
--rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/ops/layer_norm.py
--rw-r--r--   0 root         (0) root         (0)     3159 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/ops/rms_norm.py
--rw-r--r--   0 root         (0) root         (0)     5855 2023-04-07 15:25:56.000000 flash_attn-1.0.0/flash_attn/rotary.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:27.058243 flash_attn-1.0.0/flash_attn/triton/
--rw-rw-r--   0 root         (0) root         (0)        0 2022-11-18 00:51:48.000000 flash_attn-1.0.0/flash_attn/triton/__init__.py
--rw-rw-r--   0 root         (0) root         (0)    14332 2022-10-23 23:52:09.000000 flash_attn-1.0.0/flash_attn/triton/fused_attention.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:27.155247 flash_attn-1.0.0/flash_attn/utils/
--rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/utils/__init__.py
--rw-r--r--   0 root         (0) root         (0)     5909 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/utils/benchmark.py
--rw-r--r--   0 root         (0) root         (0)     5545 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/utils/distributed.py
--rw-r--r--   0 root         (0) root         (0)    13140 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/utils/generation.py
--rw-r--r--   0 root         (0) root         (0)     1824 2023-04-12 06:28:29.000000 flash_attn-1.0.0/flash_attn/utils/pretrained.py
-drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 06:37:26.575750 flash_attn-1.0.0/flash_attn.egg-info/
--rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 06:34:34.000000 flash_attn-1.0.0/flash_attn.egg-info/PKG-INFO
--rw-rw-r--   0 root         (0) root         (0)   112777 2023-04-12 06:34:48.000000 flash_attn-1.0.0/flash_attn.egg-info/SOURCES.txt
--rw-rw-r--   0 root         (0) root         (0)        1 2023-04-12 06:34:34.000000 flash_attn-1.0.0/flash_attn.egg-info/dependency_links.txt
--rw-rw-r--   0 root         (0) root         (0)       13 2023-04-12 06:34:34.000000 flash_attn-1.0.0/flash_attn.egg-info/requires.txt
--rw-rw-r--   0 root         (0) root         (0)       27 2023-04-12 06:34:34.000000 flash_attn-1.0.0/flash_attn.egg-info/top_level.txt
--rw-rw-r--   0 root         (0) root         (0)       38 2023-04-12 06:37:27.167858 flash_attn-1.0.0/setup.cfg
--rw-r--r--   0 root         (0) root         (0)     7541 2023-04-12 06:30:02.000000 flash_attn-1.0.0/setup.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.953451 flash_attn-1.0.1/
+-rw-r--r--   0 root         (0) root         (0)       56 2022-11-17 23:40:55.000000 flash_attn-1.0.1/AUTHORS
+-rw-r--r--   0 root         (0) root         (0)     1558 2022-09-09 19:08:03.000000 flash_attn-1.0.1/LICENSE
+-rw-r--r--   0 root         (0) root         (0)      251 2023-04-12 06:28:28.000000 flash_attn-1.0.1/MANIFEST.in
+-rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 17:06:34.945114 flash_attn-1.0.1/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)     9529 2023-04-12 06:31:27.000000 flash_attn-1.0.1/README.md
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.501406 flash_attn-1.0.1/csrc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.664807 flash_attn-1.0.1/csrc/flash_attn/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.252994 flash_attn-1.0.1/csrc/flash_attn/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.691747 flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/
+-rw-r--r--   0 root         (0) root         (0)     2023 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/nop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.848846 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.718983 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.743425 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/
+-rw-r--r--   0 root         (0) root         (0)    13255 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.767196 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/
+-rw-r--r--   0 root         (0) root         (0)     7157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.862653 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/
+-rw-r--r--   0 root         (0) root         (0)     4478 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h
+-rw-r--r--   0 root         (0) root         (0)     6938 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h
+-rw-r--r--   0 root         (0) root         (0)     5819 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp
+-rw-r--r--   0 root         (0) root         (0)    11415 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.885714 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/
+-rw-r--r--   0 root         (0) root         (0)     8226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.909559 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15161 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.933226 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17570 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.961495 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18280 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.085657 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.112313 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    28124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.138242 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/
+-rw-r--r--   0 root         (0) root         (0)    21947 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.161586 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/
+-rw-r--r--   0 root         (0) root         (0)    23244 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.184950 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.882240 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26102 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    22877 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h
+-rw-r--r--   0 root         (0) root         (0)    28268 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h
+-rw-r--r--   0 root         (0) root         (0)    24493 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:55.924025 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/
+-rw-r--r--   0 root         (0) root         (0)    15552 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8756 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8759 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8712 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8787 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     8711 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7269 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7338 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7294 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7359 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7430 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu
+-rw-r--r--   0 root         (0) root         (0)     7627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu
+-rw-r--r--   0 root         (0) root         (0)     7634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.298799 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)     3973 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    26762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    26775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    28422 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    28073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    15658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.556494 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.332342 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    10368 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.781968 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    31443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21010 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    20493 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)     6047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    33824 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    33518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    21451 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    21065 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    27144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h
+-rw-r--r--   0 root         (0) root         (0)    27400 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.806412 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    18020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.830679 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    15042 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.855014 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    27755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.879154 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/
+-rw-r--r--   0 root         (0) root         (0)    12580 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.904258 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/
+-rw-r--r--   0 root         (0) root         (0)    14007 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.928016 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/
+-rw-r--r--   0 root         (0) root         (0)    13401 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.955358 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/
+-rw-r--r--   0 root         (0) root         (0)    12556 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:56.980547 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/
+-rw-r--r--   0 root         (0) root         (0)    17319 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.004357 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/
+-rw-r--r--   0 root         (0) root         (0)    21495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.028337 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/
+-rw-r--r--   0 root         (0) root         (0)    27520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.053094 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/
+-rw-r--r--   0 root         (0) root         (0)    50996 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.095680 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    26547 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu
+-rw-r--r--   0 root         (0) root         (0)    25628 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.123245 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/
+-rw-r--r--   0 root         (0) root         (0)    25538 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.148679 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/
+-rw-r--r--   0 root         (0) root         (0)    30446 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.271018 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/
+-rw-r--r--   0 root         (0) root         (0)    28159 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.294793 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/
+-rw-r--r--   0 root         (0) root         (0)    28403 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.320237 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/
+-rw-r--r--   0 root         (0) root         (0)    27335 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.345085 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/
+-rw-r--r--   0 root         (0) root         (0)    15206 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.370315 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/
+-rw-r--r--   0 root         (0) root         (0)    15907 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.393911 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/
+-rw-r--r--   0 root         (0) root         (0)    31803 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.421084 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/
+-rw-r--r--   0 root         (0) root         (0)    22378 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.486360 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/
+-rw-r--r--   0 root         (0) root         (0)    23114 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu
+-rw-r--r--   0 root         (0) root         (0)    16900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    18713 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.510391 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/
+-rw-r--r--   0 root         (0) root         (0)    20795 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.576023 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/
+-rw-r--r--   0 root         (0) root         (0)    31111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu
+-rw-r--r--   0 root         (0) root         (0)    14159 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    33916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.603054 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/
+-rw-r--r--   0 root         (0) root         (0)    47455 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:57.627710 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/
+-rw-r--r--   0 root         (0) root         (0)    37879 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.015760 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/
+-rw-r--r--   0 root         (0) root         (0)    18389 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h
+-rw-r--r--   0 root         (0) root         (0)    11865 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9885 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.088245 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/
+-rw-r--r--   0 root         (0) root         (0)    22349 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9162 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)    22349 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9162 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h
+-rw-r--r--   0 root         (0) root         (0)     6768 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h
+-rw-r--r--   0 root         (0) root         (0)    34379 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6666 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    38173 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu
+-rw-r--r--   0 root         (0) root         (0)    39997 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.325726 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3994 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h
+-rw-r--r--   0 root         (0) root         (0)     6241 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27198 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14091 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     6782 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h
+-rw-r--r--   0 root         (0) root         (0)    13959 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    72984 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    10878 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.453456 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/
+-rw-r--r--   0 root         (0) root         (0)    23855 2023-03-13 04:24:25.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     3142 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64480 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)    64500 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
+-rw-r--r--   0 root         (0) root         (0)     2435 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     9497 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
+-rw-r--r--   0 root         (0) root         (0)    48655 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h
+-rw-r--r--   0 root         (0) root         (0)    61195 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.583253 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/
+-rw-r--r--   0 root         (0) root         (0)     3747 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.751608 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/
+-rw-r--r--   0 root         (0) root         (0)    37396 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)    17839 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
+-rw-r--r--   0 root         (0) root         (0)    16152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.779737 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/
+-rw-r--r--   0 root         (0) root         (0)    23901 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.806032 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/
+-rw-r--r--   0 root         (0) root         (0)    23867 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.849107 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.773047 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.762185 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.936547 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6370 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4099 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    10439 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.962402 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6848 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.780119 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:58.990713 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    14747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h
+-rw-r--r--   0 root         (0) root         (0)    10231 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
+-rw-r--r--   0 root         (0) root         (0)     3745 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.070922 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.097454 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    16955 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    12669 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu
+-rw-r--r--   0 root         (0) root         (0)     2366 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h
+-rw-r--r--   0 root         (0) root         (0)    31360 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.122686 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    18422 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h
+-rw-r--r--   0 root         (0) root         (0)     3577 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.146645 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     5818 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.217104 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15613 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     7920 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    29976 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.242813 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/
+-rw-r--r--   0 root         (0) root         (0)    24464 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.268852 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/
+-rw-r--r--   0 root         (0) root         (0)    22676 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.293385 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/
+-rw-r--r--   0 root         (0) root         (0)    16736 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.317461 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/
+-rw-r--r--   0 root         (0) root         (0)    22625 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.344007 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/
+-rw-r--r--   0 root         (0) root         (0)    18635 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.369725 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/
+-rw-r--r--   0 root         (0) root         (0)     2849 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.393950 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/common/
+-rw-r--r--   0 root         (0) root         (0)     1434 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/common/helper.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.854874 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:59.420707 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/
+-rw-r--r--   0 root         (0) root         (0)    14342 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.869316 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:00.467959 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/
+-rw-r--r--   0 root         (0) root         (0)     3805 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.069393 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/
+-rw-r--r--   0 root         (0) root         (0)     3442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h
+-rw-r--r--   0 root         (0) root         (0)    12127 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h
+-rw-r--r--   0 root         (0) root         (0)     2691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h
+-rw-r--r--   0 root         (0) root         (0)    14313 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h
+-rw-r--r--   0 root         (0) root         (0)    10490 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    15125 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8001 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h
+-rw-r--r--   0 root         (0) root         (0)    11096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)     7040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     4193 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h
+-rw-r--r--   0 root         (0) root         (0)    16554 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    31766 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    55580 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     8254 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h
+-rw-r--r--   0 root         (0) root         (0)    43978 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     2622 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h
+-rw-r--r--   0 root         (0) root         (0)     3998 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h
+-rw-r--r--   0 root         (0) root         (0)     3656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     5102 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h
+-rw-r--r--   0 root         (0) root         (0)     8473 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h
+-rw-r--r--   0 root         (0) root         (0)     5286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     7746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h
+-rw-r--r--   0 root         (0) root         (0)     7616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    13397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array.h
+-rw-r--r--   0 root         (0) root         (0)     3662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    13154 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     6371 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/barrier.h
+-rw-r--r--   0 root         (0) root         (0)    13352 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h
+-rw-r--r--   0 root         (0) root         (0)     6309 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/blas3.h
+-rw-r--r--   0 root         (0) root         (0)     9372 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/block_striped.h
+-rw-r--r--   0 root         (0) root         (0)    15894 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/complex.h
+-rw-r--r--   0 root         (0) root         (0)    47943 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/constants.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.126118 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/
+-rw-r--r--   0 root         (0) root         (0)    21652 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)    16292 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     5915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.194122 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/
+-rw-r--r--   0 root         (0) root         (0)     9744 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    12443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    10044 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.776801 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/
+-rw-r--r--   0 root         (0) root         (0)     7671 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h
+-rw-r--r--   0 root         (0) root         (0)    53546 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    56838 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     4658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     4660 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     7907 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    28745 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)    10459 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     9324 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    14864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    11980 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    14883 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h
+-rw-r--r--   0 root         (0) root         (0)     7317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_depthwise_fprop.h
+-rw-r--r--   0 root         (0) root         (0)    18048 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15454 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h
+-rw-r--r--   0 root         (0) root         (0)    15709 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17222 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)    16749 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:01.801498 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/
+-rw-r--r--   0 root         (0) root         (0)     9689 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:02.803285 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    15306 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    19735 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18940 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    26137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11529 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)    11333 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    13664 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9314 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h
+-rw-r--r--   0 root         (0) root         (0)     9018 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h
+-rw-r--r--   0 root         (0) root         (0)    10286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    30197 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h
+-rw-r--r--   0 root         (0) root         (0)    11202 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    10350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    11520 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9043 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10832 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8450 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     9569 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    11020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    15132 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     7945 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)     8891 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    18249 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h
+-rw-r--r--   0 root         (0) root         (0)     9971 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    12024 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h
+-rw-r--r--   0 root         (0) root         (0)    10744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h
+-rw-r--r--   0 root         (0) root         (0)     8871 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h
+-rw-r--r--   0 root         (0) root         (0)    10747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h
+-rw-r--r--   0 root         (0) root         (0)     9899 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    20899 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     8921 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h
+-rw-r--r--   0 root         (0) root         (0)    12744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     8097 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    14329 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h
+-rw-r--r--   0 root         (0) root         (0)    30106 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20086 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    12174 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    26320 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    16915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12476 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     7071 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:02.876350 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/
+-rw-r--r--   0 root         (0) root         (0)     5577 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h
+-rw-r--r--   0 root         (0) root         (0)     9342 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h
+-rw-r--r--   0 root         (0) root         (0)    11581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/coord.h
+-rw-r--r--   0 root         (0) root         (0)    10900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/core_io.h
+-rw-r--r--   0 root         (0) root         (0)     7549 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/cutlass.h
+-rw-r--r--   0 root         (0) root         (0)     2665 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:53.939272 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:03.298191 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    17916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h
+-rw-r--r--   0 root         (0) root         (0)     4691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h
+-rw-r--r--   0 root         (0) root         (0)     9349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     8231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    13377 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h
+-rw-r--r--   0 root         (0) root         (0)    23649 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h
+-rw-r--r--   0 root         (0) root         (0)     9067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h
+-rw-r--r--   0 root         (0) root         (0)    15195 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h
+-rw-r--r--   0 root         (0) root         (0)     8065 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h
+-rw-r--r--   0 root         (0) root         (0)     3693 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h
+-rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h
+-rw-r--r--   0 root         (0) root         (0)     3058 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h
+-rw-r--r--   0 root         (0) root         (0)     9351 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h
+-rw-r--r--   0 root         (0) root         (0)    19348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h
+-rw-r--r--   0 root         (0) root         (0)     6760 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h
+-rw-r--r--   0 root         (0) root         (0)     3688 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h
+-rw-r--r--   0 root         (0) root         (0)     3669 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h
+-rw-r--r--   0 root         (0) root         (0)     8662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     3416 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h
+-rw-r--r--   0 root         (0) root         (0)     2656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.354829 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     9142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)     3234 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7209 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h
+-rw-r--r--   0 root         (0) root         (0)    23409 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7129 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    10846 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5817 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     5763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5947 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4409 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h
+-rw-r--r--   0 root         (0) root         (0)     7398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7303 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4098 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    18231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h
+-rw-r--r--   0 root         (0) root         (0)     8279 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h
+-rw-r--r--   0 root         (0) root         (0)     7455 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    13424 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h
+-rw-r--r--   0 root         (0) root         (0)    13983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h
+-rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    14610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)    16804 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h
+-rw-r--r--   0 root         (0) root         (0)    28708 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    29151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    13454 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7308 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h
+-rw-r--r--   0 root         (0) root         (0)    11733 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h
+-rw-rw-r--   0 root         (0) root         (0)     2912 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h
+-rw-r--r--   0 root         (0) root         (0)    19750 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h
+-rw-r--r--   0 root         (0) root         (0)    38356 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    18821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h
+-rw-r--r--   0 root         (0) root         (0)     5636 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h
+-rw-r--r--   0 root         (0) root         (0)    21249 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h
+-rw-r--r--   0 root         (0) root         (0)    13873 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    12047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)     9146 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h
+-rw-r--r--   0 root         (0) root         (0)    15536 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h
+-rw-r--r--   0 root         (0) root         (0)     7384 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    17404 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h
+-rw-r--r--   0 root         (0) root         (0)     7394 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.772609 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     7055 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5880 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)     9883 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     8924 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6045 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)     5979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    15547 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h
+-rw-r--r--   0 root         (0) root         (0)    19983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    22552 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h
+-rw-r--r--   0 root         (0) root         (0)    14052 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7600 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     7485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     3916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)    24535 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/fast_math.h
+-rw-r--r--   0 root         (0) root         (0)    35369 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/float8.h
+-rw-r--r--   0 root         (0) root         (0)     2645 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h
+-rw-r--r--   0 root         (0) root         (0)    61285 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/functional.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:04.793307 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:05.354252 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    17035 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    23124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h
+-rw-r--r--   0 root         (0) root         (0)    27616 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    25194 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)    22415 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    22725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     2591 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    13736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    17329 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    20450 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    14576 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7427 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h
+-rw-r--r--   0 root         (0) root         (0)    13153 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    13968 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    14853 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     5690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h
+-rw-r--r--   0 root         (0) root         (0)    18127 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)     2747 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16719 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h
+-rwxr-xr-x   0 root         (0) root         (0)    21050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h
+-rw-r--r--   0 root         (0) root         (0)    26464 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h
+-rw-r--r--   0 root         (0) root         (0)    11570 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:06.803061 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/
+-rw-r--r--   0 root         (0) root         (0)    29360 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    34810 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    13645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12385 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     6592 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5848 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    11104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     4932 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)    10934 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8086 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h
+-rwxr-xr-x   0 root         (0) root         (0)     5349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h
+-rw-r--r--   0 root         (0) root         (0)     8301 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    13392 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    10620 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h
+-rw-r--r--   0 root         (0) root         (0)    11406 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h
+-rwxr-xr-x   0 root         (0) root         (0)     9264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h
+-rwxr-xr-x   0 root         (0) root         (0)    13302 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h
+-rwxr-xr-x   0 root         (0) root         (0)    10315 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)     7999 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)     7948 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10850 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    28916 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h
+-rw-r--r--   0 root         (0) root         (0)    13381 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     8717 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h
+-rw-r--r--   0 root         (0) root         (0)     8785 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h
+-rw-r--r--   0 root         (0) root         (0)    14856 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h
+-rw-r--r--   0 root         (0) root         (0)     4314 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)    15768 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)    28517 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h
+-rwxr-xr-x   0 root         (0) root         (0)     6144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h
+-rw-r--r--   0 root         (0) root         (0)     5165 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)    23763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    19398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h
+-rw-r--r--   0 root         (0) root         (0)     4291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    23859 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    39288 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h
+-rw-r--r--   0 root         (0) root         (0)    25116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h
+-rw-r--r--   0 root         (0) root         (0)    24186 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     8090 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h
+-rwxr-xr-x   0 root         (0) root         (0)     8979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h
+-rw-r--r--   0 root         (0) root         (0)    16982 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     7148 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h
+-rw-r--r--   0 root         (0) root         (0)    23107 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    15792 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     4334 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h
+-rw-r--r--   0 root         (0) root         (0)    24162 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    17567 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    13610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h
+-rwxr-xr-x   0 root         (0) root         (0)    23900 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    19537 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:07.001332 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     3567 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h
+-rw-r--r--   0 root         (0) root         (0)    10767 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h
+-rw-r--r--   0 root         (0) root         (0)    29987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h
+-rw-r--r--   0 root         (0) root         (0)     8142 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:07.964748 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    31930 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h
+-rwxr-xr-x   0 root         (0) root         (0)     6979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h
+-rw-r--r--   0 root         (0) root         (0)    34241 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h
+-rw-r--r--   0 root         (0) root         (0)     5123 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h
+-rw-r--r--   0 root         (0) root         (0)    57426 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h
+-rw-r--r--   0 root         (0) root         (0)    19257 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    42310 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h
+-rw-r--r--   0 root         (0) root         (0)   102893 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    32106 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    12645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h
+-rw-r--r--   0 root         (0) root         (0)     7442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    20975 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7998 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     5110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     4627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     7113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h
+-rw-r--r--   0 root         (0) root         (0)     6323 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     7121 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h
+-rw-r--r--   0 root         (0) root         (0)     4959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h
+-rw-r--r--   0 root         (0) root         (0)    64521 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    25495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h
+-rw-r--r--   0 root         (0) root         (0)    19515 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h
+-rw-r--r--   0 root         (0) root         (0)    24047 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    13837 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h
+-rwxr-xr-x   0 root         (0) root         (0)     4726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h
+-rw-r--r--   0 root         (0) root         (0)     3652 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h
+-rw-r--r--   0 root         (0) root         (0)     7791 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h
+-rw-r--r--   0 root         (0) root         (0)    27413 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    32892 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    23286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    12351 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     6901 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h
+-rw-r--r--   0 root         (0) root         (0)    22805 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    14746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h
+-rw-r--r--   0 root         (0) root         (0)     9864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h
+-rw-r--r--   0 root         (0) root         (0)    27059 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h
+-rw-r--r--   0 root         (0) root         (0)     9210 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h
+-rw-r--r--   0 root         (0) root         (0)    25364 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    20471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h
+-rw-r--r--   0 root         (0) root         (0)    16220 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)    26485 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.617951 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    15979 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     6684 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5160 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     9026 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     3990 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     4685 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)     5725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     2619 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h
+-rw-r--r--   0 root         (0) root         (0)    27916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    23104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h
+-rw-r--r--   0 root         (0) root         (0)    78615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    12317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14589 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     6144 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8446 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h
+-rw-r--r--   0 root         (0) root         (0)     3079 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h
+-rw-r--r--   0 root         (0) root         (0)    59793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11758 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    14407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    15721 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h
+-rw-rw-r--   0 root         (0) root         (0)    18643 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2939 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h
+-rw-r--r--   0 root         (0) root         (0)     8966 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    11017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)   136033 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    99649 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    75179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h
+-rw-r--r--   0 root         (0) root         (0)    13151 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h
+-rw-r--r--   0 root         (0) root         (0)    27101 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h
+-rw-r--r--   0 root         (0) root         (0)     7241 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h
+-rw-r--r--   0 root         (0) root         (0)    17231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    19125 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     4610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h
+-rw-r--r--   0 root         (0) root         (0)     8728 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    23690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/half.h
+-rw-r--r--   0 root         (0) root         (0)     6908 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h
+-rw-r--r--   0 root         (0) root         (0)     2801 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.977399 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/
+-rw-r--r--   0 root         (0) root         (0)     3020 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)    34712 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     9286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h
+-rw-r--r--   0 root         (0) root         (0)     4696 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18295 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)    29599 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h
+-rw-r--r--   0 root         (0) root         (0)    33137 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h
+-rw-r--r--   0 root         (0) root         (0)    29336 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     3328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h
+-rw-r--r--   0 root         (0) root         (0)   364115 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     4991 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h
+-rw-r--r--   0 root         (0) root         (0)     2726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h
+-rw-r--r--   0 root         (0) root         (0)    46719 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h
+-rw-r--r--   0 root         (0) root         (0)     3477 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h
+-rw-r--r--   0 root         (0) root         (0)     5492 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:08.998202 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/
+-rw-r--r--   0 root         (0) root         (0)    25348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h
+-rw-r--r--   0 root         (0) root         (0)    15565 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h
+-rw-r--r--   0 root         (0) root         (0)    18987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/quaternion.h
+-rw-r--r--   0 root         (0) root         (0)     2369 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/real.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.019775 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.107279 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)     6823 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)     8152 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)    11579 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    11448 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.191671 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)     8762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h
+-rw-r--r--   0 root         (0) root         (0)     7897 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h
+-rw-r--r--   0 root         (0) root         (0)    20685 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h
+-rw-r--r--   0 root         (0) root         (0)    21662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.235108 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h
+-rw-r--r--   0 root         (0) root         (0)     6790 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h
+-rw-r--r--   0 root         (0) root         (0)     2936 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h
+-rw-r--r--   0 root         (0) root         (0)     5929 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h
+-rw-r--r--   0 root         (0) root         (0)     4222 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/semaphore.h
+-rw-r--r--   0 root         (0) root         (0)    16587 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h
+-rw-r--r--   0 root         (0) root         (0)     8964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)    12207 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h
+-rw-r--r--   0 root         (0) root         (0)    11201 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h
+-rw-r--r--   0 root         (0) root         (0)    10250 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    12853 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.259715 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/
+-rw-r--r--   0 root         (0) root         (0)     5931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     2581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/trace.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.282161 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/
+-rw-r--r--   0 root         (0) root         (0)    33157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:09.323356 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/
+-rw-r--r--   0 root         (0) root         (0)     3835 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h
+-rw-r--r--   0 root         (0) root         (0)     4309 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.021321 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)     6181 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44443 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    44309 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    12890 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    11097 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    70077 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    28232 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h
+-rwxr-xr-x   0 root         (0) root         (0)    10243 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h
+-rw-r--r--   0 root         (0) root         (0)    31412 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    62095 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    27175 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    28064 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h
+-rw-r--r--   0 root         (0) root         (0)    13088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8232 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     2638 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    13283 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    18623 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h
+-rw-r--r--   0 root         (0) root         (0)    27922 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    47789 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h
+-rw-r--r--   0 root         (0) root         (0)     2616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h
+-rw-r--r--   0 root         (0) root         (0)    16510 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h
+-rw-r--r--   0 root         (0) root         (0)    15486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h
+-rw-r--r--   0 root         (0) root         (0)    36050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h
+-rw-r--r--   0 root         (0) root         (0)    43663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h
+-rw-r--r--   0 root         (0) root         (0)     5226 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.045950 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/warp/
+-rw-r--r--   0 root         (0) root         (0)     8828 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h
+-rw-r--r--   0 root         (0) root         (0)     8113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/uint128.h
+-rw-r--r--   0 root         (0) root         (0)     3011 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.041580 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.071100 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:10.114281 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/
+-rw-r--r--   0 root         (0) root         (0)     4273 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h
+-rw-r--r--   0 root         (0) root         (0)     4288 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.059941 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.515294 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/
+-rw-r--r--   0 root         (0) root         (0)    21797 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h
+-rw-r--r--   0 root         (0) root         (0)     5344 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5443 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5243 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5378 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12054 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9603 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5267 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5357 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5089 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    13690 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5390 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5191 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11136 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rwxr-xr-x   0 root         (0) root         (0)     8278 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    20555 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    20647 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5155 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26114 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    26210 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     5194 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5738 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5439 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7363 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    38060 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    14471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4662 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26254 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21182 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     5179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5358 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7591 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10514 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23472 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    21524 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     5135 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5347 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3736 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5257 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12276 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h
+-rw-r--r--   0 root         (0) root         (0)    21659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     3622 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     6560 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5256 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    17700 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    18451 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    22194 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     9383 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     9387 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    10179 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.812096 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/
+-rw-r--r--   0 root         (0) root         (0)     7365 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/array.cu
+-rw-r--r--   0 root         (0) root         (0)     7353 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
+-rw-r--r--   0 root         (0) root         (0)     6981 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/complex.cu
+-rw-r--r--   0 root         (0) root         (0)     4009 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/float8.cu
+-rw-r--r--   0 root         (0) root         (0)    13001 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/functional.cu
+-rw-r--r--   0 root         (0) root         (0)     3553 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/half.cu
+-rw-r--r--   0 root         (0) root         (0)     5295 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     8592 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu
+-rw-r--r--   0 root         (0) root         (0)     7276 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/numeric_conversion.cu
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu
+-rw-r--r--   0 root         (0) root         (0)     5777 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu
+-rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu
+-rw-r--r--   0 root         (0) root         (0)     2050 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp
+-rw-r--r--   0 root         (0) root         (0)     7088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.086315 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:11.860527 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/
+-rw-r--r--   0 root         (0) root         (0)     3527 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu
+-rw-r--r--   0 root         (0) root         (0)    14320 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.113160 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/
+-rw-r--r--   0 root         (0) root         (0)     3332 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp
+-rw-r--r--   0 root         (0) root         (0)     4861 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp
+-rw-r--r--   0 root         (0) root         (0)     5620 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp
+-rw-r--r--   0 root         (0) root         (0)     7178 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp
+-rw-r--r--   0 root         (0) root         (0)    12569 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp
+-rw-r--r--   0 root         (0) root         (0)     4856 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp
+-rw-r--r--   0 root         (0) root         (0)     6702 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp
+-rw-r--r--   0 root         (0) root         (0)     6734 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp
+-rw-r--r--   0 root         (0) root         (0)     5914 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp
+-rw-r--r--   0 root         (0) root         (0)     3488 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp
+-rw-r--r--   0 root         (0) root         (0)     2342 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp
+-rw-r--r--   0 root         (0) root         (0)    13304 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.178557 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/
+-rw-r--r--   0 root         (0) root         (0)    14365 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu
+-rw-r--r--   0 root         (0) root         (0)    18990 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu
+-rw-r--r--   0 root         (0) root         (0)    13875 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.204699 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/
+-rw-r--r--   0 root         (0) root         (0)     4544 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.109314 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.275070 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/
+-rw-r--r--   0 root         (0) root         (0)    15783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu
+-rw-r--r--   0 root         (0) root         (0)     6534 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu
+-rw-r--r--   0 root         (0) root         (0)     9964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.739250 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    13824 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu
+-rw-r--r--   0 root         (0) root         (0)    27176 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu
+-rw-r--r--   0 root         (0) root         (0)    12061 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)    25275 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)    84612 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    70486 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    25293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    13012 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     7743 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    19178 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu
+-rw-r--r--   0 root         (0) root         (0)    28433 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)    11038 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    11734 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:12.800491 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/
+-rw-r--r--   0 root         (0) root         (0)     6783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     7275 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.148835 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.432677 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/
+-rw-r--r--   0 root         (0) root         (0)    10149 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17819 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8903 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8885 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16447 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16495 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8292 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6714 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6747 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7895 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7930 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6516 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6549 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9016 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9053 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4628 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6165 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9634 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16357 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13189 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13583 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13464 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9571 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6140 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     9544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16417 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13075 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3539 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7965 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13273 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3648 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8608 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3645 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16339 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13008 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8505 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11090 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6116 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11066 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    17114 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3528 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3540 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16457 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13266 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8933 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13540 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6130 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     8160 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7847 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8754 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11497 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6147 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     6107 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13518 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    13398 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7845 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16149 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6119 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7827 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16101 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9526 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7898 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    11470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     3584 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3473 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12967 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12930 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12895 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8349 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7300 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8348 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7291 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    10240 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    26146 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11339 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7346 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6859 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     7239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8121 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16882 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12637 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8388 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    10004 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9980 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    17544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9588 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    11288 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7977 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5693 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16691 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12408 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6864 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7744 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16531 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6675 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     7752 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16484 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     4663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4945 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     6616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    10581 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16950 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16902 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6854 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6686 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6687 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4726 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4718 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    16715 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    12841 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     4544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13157 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     6028 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6031 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6064 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4909 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6037 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5382 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5406 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5402 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    13055 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5388 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     6939 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7677 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7725 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3854 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     6396 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)     9189 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)    11186 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46795 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    54085 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8318 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46687 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8411 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    46578 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40533 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    47656 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    40354 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     3513 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89517 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89304 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    89091 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    69175 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    71438 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    67796 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70056 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     7156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     6067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu
+-rw-r--r--   0 root         (0) root         (0)     9063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu
+-rw-r--r--   0 root         (0) root         (0)    35759 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35678 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    35597 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    70872 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    73136 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8870 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    69488 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     8865 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    71755 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33231 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33156 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    33081 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5357 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5479 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     5238 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     5253 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     3875 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu
+-rw-r--r--   0 root         (0) root         (0)     3734 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)     5387 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7436 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7409 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)    17391 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    42504 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu
+-rw-r--r--   0 root         (0) root         (0)    22602 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    22874 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu
+-rw-r--r--   0 root         (0) root         (0)    42526 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)     3810 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     5976 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     9313 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu
+-rw-r--r--   0 root         (0) root         (0)     5986 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     7268 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu
+-rw-r--r--   0 root         (0) root         (0)     5923 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5926 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5959 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5962 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4839 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     5983 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5932 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5935 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15203 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8623 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4777 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     8103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8108 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8088 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8093 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8073 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8078 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8058 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8063 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15071 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8551 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14972 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5386 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5356 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5380 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5379 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12952 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5368 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7208 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5362 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7190 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4794 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4783 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4740 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    19145 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7991 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    11015 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7976 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12342 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     7961 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12321 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4786 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4775 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4993 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4987 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5011 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     5024 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4996 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     3793 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4990 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16083 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16041 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4530 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     7451 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9401 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    16027 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15985 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    18193 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h
+-rw-r--r--   0 root         (0) root         (0)     8140 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20594 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    19350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h
+-rw-r--r--   0 root         (0) root         (0)    16502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h
+-rw-r--r--   0 root         (0) root         (0)    16562 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    17002 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    14651 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h
+-rw-r--r--   0 root         (0) root         (0)    10134 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h
+-rw-r--r--   0 root         (0) root         (0)     9485 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)    20765 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h
+-rw-r--r--   0 root         (0) root         (0)    15566 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h
+-rw-r--r--   0 root         (0) root         (0)     8639 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h
+-rw-r--r--   0 root         (0) root         (0)    15773 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h
+-rw-r--r--   0 root         (0) root         (0)     6128 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h
+-rw-r--r--   0 root         (0) root         (0)    19865 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    20204 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h
+-rw-r--r--   0 root         (0) root         (0)    16492 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h
+-rw-r--r--   0 root         (0) root         (0)     2626 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h
+-rw-r--r--   0 root         (0) root         (0)     9916 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9988 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4989 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     4992 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9762 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15614 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8733 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14089 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     4608 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    12798 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12809 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12764 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12768 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12779 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    15504 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     8673 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13989 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    14344 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.477059 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/
+-rwxr-xr-x   0 root         (0) root         (0)    46470 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu
+-rwxr-xr-x   0 root         (0) root         (0)    14350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.553671 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/
+-rw-r--r--   0 root         (0) root         (0)     4847 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)    12503 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     3109 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:20.597100 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/
+-rw-r--r--   0 root         (0) root         (0)     5198 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu
+-rw-r--r--   0 root         (0) root         (0)     7161 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h
+-rw-r--r--   0 root         (0) root         (0)     7124 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.336925 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    24658 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu
+-rw-r--r--   0 root         (0) root         (0)     4345 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu
+-rw-r--r--   0 root         (0) root         (0)   135045 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu
+-rw-r--r--   0 root         (0) root         (0)     4644 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    94442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu
+-rw-r--r--   0 root         (0) root         (0)    17113 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    13131 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14539 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    49052 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu
+-rw-r--r--   0 root         (0) root         (0)     8407 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu
+-rw-r--r--   0 root         (0) root         (0)    18705 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    78122 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    21051 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    13413 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    14239 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h
+-rw-r--r--   0 root         (0) root         (0)    29772 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12395 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)     3502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    12138 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h
+-rw-r--r--   0 root         (0) root         (0)    16308 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    12502 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.628876 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/
+-rw-r--r--   0 root         (0) root         (0)    22029 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    10916 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)     9873 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    18220 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu
+-rw-r--r--   0 root         (0) root         (0)     4920 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu
+-rw-r--r--   0 root         (0) root         (0)     6291 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu
+-rw-r--r--   0 root         (0) root         (0)     9297 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)    37942 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu
+-rw-r--r--   0 root         (0) root         (0)    81659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     9089 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu
+-rw-r--r--   0 root         (0) root         (0)    48928 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    45327 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h
+-rw-r--r--   0 root         (0) root         (0)    25780 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu
+-rw-r--r--   0 root         (0) root         (0)     7544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu
+-rw-r--r--   0 root         (0) root         (0)     6487 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.789246 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/
+-rw-r--r--   0 root         (0) root         (0)     5788 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu
+-rw-r--r--   0 root         (0) root         (0)     5984 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu
+-rw-r--r--   0 root         (0) root         (0)     7081 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.195559 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.178056 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.820963 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/
+-rw-r--r--   0 root         (0) root         (0)     2096 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.190232 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.852556 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/
+-rw-r--r--   0 root         (0) root         (0)     2915 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.893736 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/
+-rw-rw-r--   0 root         (0) root         (0)        0 2022-06-02 16:47:41.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/assert.h
+-rw-r--r--   0 root         (0) root         (0)     4250 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:21.936548 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/
+-rw-r--r--   0 root         (0) root         (0)     5727 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu
+-rw-r--r--   0 root         (0) root         (0)    10328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.067403 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/
+-rw-r--r--   0 root         (0) root         (0)    15532 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu
+-rw-r--r--   0 root         (0) root         (0)    15490 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu
+-rw-r--r--   0 root         (0) root         (0)    17098 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu
+-rw-r--r--   0 root         (0) root         (0)    20252 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu
+-rw-r--r--   0 root         (0) root         (0)     7623 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu
+-rw-r--r--   0 root         (0) root         (0)     4327 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.227745 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.113067 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/
+-rw-r--r--   0 root         (0) root         (0)    14684 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu
+-rw-r--r--   0 root         (0) root         (0)    15609 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.156332 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/
+-rw-r--r--   0 root         (0) root         (0)    11350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu
+-rw-r--r--   0 root         (0) root         (0)     2228 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.201797 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/
+-rw-r--r--   0 root         (0) root         (0)     3110 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu
+-rw-r--r--   0 root         (0) root         (0)     6657 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h
+-rw-r--r--   0 root         (0) root         (0)     2047 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/test_unit.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.239268 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.354721 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/
+-rw-r--r--   0 root         (0) root         (0)    25527 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu
+-rw-r--r--   0 root         (0) root         (0)     9501 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.398216 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/
+-rw-r--r--   0 root         (0) root         (0)     2663 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu
+-rw-r--r--   0 root         (0) root         (0)     7474 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.390377 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.358558 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.270548 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.276418 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.644849 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/
+-rw-r--r--   0 root         (0) root         (0)     3974 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h
+-rw-r--r--   0 root         (0) root         (0)    16013 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h
+-rw-r--r--   0 root         (0) root         (0)    38274 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h
+-rw-r--r--   0 root         (0) root         (0)     4070 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h
+-rw-r--r--   0 root         (0) root         (0)    17934 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h
+-rw-r--r--   0 root         (0) root         (0)     7904 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.290002 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.295868 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.301682 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.830469 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/
+-rw-r--r--   0 root         (0) root         (0)     2788 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h
+-rw-r--r--   0 root         (0) root         (0)     2460 2023-01-27 22:40:09.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp
+-rw-r--r--   0 root         (0) root         (0)     6223 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:22.926451 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/
+-rw-r--r--   0 root         (0) root         (0)     2851 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.100886 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/
+-rw-r--r--   0 root         (0) root         (0)     5897 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h
+-rw-r--r--   0 root         (0) root         (0)     4763 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2650 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.141775 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/
+-rw-r--r--   0 root         (0) root         (0)     7072 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.374139 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/
+-rw-r--r--   0 root         (0) root         (0)     3003 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     6103 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h
+-rw-r--r--   0 root         (0) root         (0)     4698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h
+-rw-r--r--   0 root         (0) root         (0)     8442 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h
+-rw-r--r--   0 root         (0) root         (0)     8830 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    13040 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     9476 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h
+-rw-r--r--   0 root         (0) root         (0)     9085 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h
+-rw-r--r--   0 root         (0) root         (0)    12017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h
+-rw-r--r--   0 root         (0) root         (0)     6177 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h
+-rw-r--r--   0 root         (0) root         (0)     8017 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h
+-rw-r--r--   0 root         (0) root         (0)     7223 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h
+-rw-r--r--   0 root         (0) root         (0)    16985 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.440561 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/
+-rw-r--r--   0 root         (0) root         (0)     3673 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    23600 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h
+-rw-r--r--   0 root         (0) root         (0)     2328 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.501599 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/
+-rw-r--r--   0 root         (0) root         (0)     2115 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h
+-rw-r--r--   0 root         (0) root         (0)     3694 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h
+-rw-r--r--   0 root         (0) root         (0)     8445 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h
+-rw-r--r--   0 root         (0) root         (0)     3902 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h
+-rw-r--r--   0 root         (0) root         (0)     5563 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h
+-rw-r--r--   0 root         (0) root         (0)     4855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h
+-rw-r--r--   0 root         (0) root         (0)      811 2022-09-05 01:02:49.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.348665 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.566236 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/
+-rw-r--r--   0 root         (0) root         (0)     2651 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h
+-rw-r--r--   0 root         (0) root         (0)     2253 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     8826 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.608797 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/
+-rw-r--r--   0 root         (0) root         (0)     2139 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    18930 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:23.855928 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/
+-rw-r--r--   0 root         (0) root         (0)    13931 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    13857 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h
+-rw-r--r--   0 root         (0) root         (0)    42264 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    35709 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/handle.cu
+-rw-r--r--   0 root         (0) root         (0)    12616 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/library_internal.h
+-rw-r--r--   0 root         (0) root         (0)     3782 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp
+-rw-r--r--   0 root         (0) root         (0)     5468 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu
+-rw-r--r--   0 root         (0) root         (0)    12873 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:24.015101 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/
+-rw-r--r--   0 root         (0) root         (0)     3190 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     6367 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu
+-rw-r--r--   0 root         (0) root         (0)    10270 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:24.152668 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/
+-rw-r--r--   0 root         (0) root         (0)     6746 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu
+-rw-r--r--   0 root         (0) root         (0)     6286 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu
+-rw-r--r--   0 root         (0) root         (0)    17191 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     7199 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu
+-rw-r--r--   0 root         (0) root         (0)    14732 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h
+-rw-r--r--   0 root         (0) root         (0)     2857 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu
+-rw-r--r--   0 root         (0) root         (0)     2855 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/singleton.cu
+-rw-r--r--   0 root         (0) root         (0)    13134 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    11698 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h
+-rw-r--r--   0 root         (0) root         (0)    43704 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/util.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.382531 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.298429 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/
+-rw-r--r--   0 root         (0) root         (0)    53205 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    18014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    48659 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)    16043 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    36462 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu
+-rw-r--r--   0 root         (0) root         (0)    10627 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h
+-rw-r--r--   0 root         (0) root         (0)    16877 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp
+-rw-r--r--   0 root         (0) root         (0)    20397 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h
+-rw-r--r--   0 root         (0) root         (0)     7233 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     3233 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     2453 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/debug.h
+-rw-r--r--   0 root         (0) root         (0)    53643 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu
+-rw-r--r--   0 root         (0) root         (0)     7217 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h
+-rw-r--r--   0 root         (0) root         (0)     6841 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu
+-rw-r--r--   0 root         (0) root         (0)     4300 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h
+-rw-r--r--   0 root         (0) root         (0)     8296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp
+-rw-r--r--   0 root         (0) root         (0)     6421 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h
+-rw-r--r--   0 root         (0) root         (0)    41919 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     8544 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     3874 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp
+-rw-r--r--   0 root         (0) root         (0)     2724 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h
+-rw-r--r--   0 root         (0) root         (0)     2340 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp
+-rw-r--r--   0 root         (0) root         (0)    20944 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     7876 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    27172 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.cu
+-rw-r--r--   0 root         (0) root         (0)     8773 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.h
+-rw-r--r--   0 root         (0) root         (0)    14192 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp
+-rw-r--r--   0 root         (0) root         (0)     4337 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h
+-rw-r--r--   0 root         (0) root         (0)     2494 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu
+-rw-r--r--   0 root         (0) root         (0)     3941 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h
+-rw-r--r--   0 root         (0) root         (0)    37487 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp
+-rw-r--r--   0 root         (0) root         (0)    27747 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h
+-rw-r--r--   0 root         (0) root         (0)    25014 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6891 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24253 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6830 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)     5452 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    20688 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6471 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    26610 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6933 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h
+-rw-r--r--   0 root         (0) root         (0)    24431 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu
+-rw-r--r--   0 root         (0) root         (0)     6599 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.396404 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.403038 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.409158 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.695963 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/
+-rw-r--r--   0 root         (0) root         (0)     9774 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h
+-rw-r--r--   0 root         (0) root         (0)     5104 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h
+-rw-r--r--   0 root         (0) root         (0)     5953 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h
+-rw-r--r--   0 root         (0) root         (0)    17696 2023-03-13 04:24:26.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h
+-rw-r--r--   0 root         (0) root         (0)    20880 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h
+-rw-r--r--   0 root         (0) root         (0)    10561 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h
+-rw-r--r--   0 root         (0) root         (0)     5219 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h
+-rw-r--r--   0 root         (0) root         (0)    11067 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h
+-rw-r--r--   0 root         (0) root         (0)    18653 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h
+-rw-r--r--   0 root         (0) root         (0)     5214 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h
+-rw-r--r--   0 root         (0) root         (0)     4007 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h
+-rw-r--r--   0 root         (0) root         (0)     4597 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h
+-rw-r--r--   0 root         (0) root         (0)     2674 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h
+-rw-r--r--   0 root         (0) root         (0)     4821 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h
+-rw-r--r--   0 root         (0) root         (0)    16745 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h
+-rw-r--r--   0 root         (0) root         (0)    20354 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     4760 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h
+-rw-r--r--   0 root         (0) root         (0)     1962 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:05:54.452150 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.741069 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/
+-rw-r--r--   0 root         (0) root         (0)     4606 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h
+-rw-r--r--   0 root         (0) root         (0)     3527 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.929435 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/
+-rw-r--r--   0 root         (0) root         (0)    48350 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h
+-rw-r--r--   0 root         (0) root         (0)    14296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    10524 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9652 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:25.990855 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/
+-rw-r--r--   0 root         (0) root         (0)     5381 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     6198 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)     5126 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    11615 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7278 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)    46444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     5293 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)    15964 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     4589 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:26.017679 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/
+-rw-r--r--   0 root         (0) root         (0)     5872 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:26.499755 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/
+-rw-r--r--   0 root         (0) root         (0)    28439 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h
+-rw-r--r--   0 root         (0) root         (0)     2766 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h
+-rw-r--r--   0 root         (0) root         (0)    17163 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h
+-rw-r--r--   0 root         (0) root         (0)     7062 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     7708 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h
+-rw-r--r--   0 root         (0) root         (0)     9441 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h
+-rw-r--r--   0 root         (0) root         (0)    11444 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8148 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h
+-rw-r--r--   0 root         (0) root         (0)    10509 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h
+-rw-r--r--   0 root         (0) root         (0)    12296 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8440 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h
+-rw-r--r--   0 root         (0) root         (0)     8317 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h
+-rw-r--r--   0 root         (0) root         (0)     9023 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h
+-rw-r--r--   0 root         (0) root         (0)    41867 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h
+-rw-r--r--   0 root         (0) root         (0)     4756 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h
+-rw-r--r--   0 root         (0) root         (0)     2133 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h
+-rw-r--r--   0 root         (0) root         (0)     6111 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h
+-rw-r--r--   0 root         (0) root         (0)     7670 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h
+-rw-r--r--   0 root         (0) root         (0)     9874 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h
+-rw-r--r--   0 root         (0) root         (0)     8285 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h
+-rw-r--r--   0 root         (0) root         (0)     8809 2023-04-12 16:23:45.000000 flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h
+-rw-r--r--   0 root         (0) root         (0)    30683 2023-04-11 20:32:31.000000 flash_attn-1.0.1/csrc/flash_attn/flash_api.cpp
+-rw-r--r--   0 root         (0) root         (0)    32519 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/fmha_api.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.184711 flash_attn-1.0.1/csrc/flash_attn/src/
+-rw-r--r--   0 root         (0) root         (0)     1664 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/block_info.h
+-rw-r--r--   0 root         (0) root         (0)     3920 2023-04-09 19:53:14.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash.h
+-rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      574 2023-04-12 03:22:08.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1124 2023-04-12 03:09:57.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      983 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:00.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      442 2023-04-08 07:26:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      425 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim32_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)      450 2023-04-08 07:29:20.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     2458 2023-04-11 20:53:27.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     2074 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96.cu
+-rw-r--r--   0 root         (0) root         (0)      449 2023-04-08 07:29:33.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      598 2023-04-08 07:26:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      581 2023-04-07 15:25:56.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)    82323 2023-04-12 05:47:44.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    41051 2023-03-25 22:30:12.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_bak.h
+-rw-r--r--   0 root         (0) root         (0)    39628 2023-03-25 21:25:30.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_new.h
+-rw-r--r--   0 root         (0) root         (0)    39403 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_reverse.h
+-rw-r--r--   0 root         (0) root         (0)     8751 2023-04-12 03:19:57.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)      645 2023-04-07 21:42:50.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      592 2023-04-08 07:07:58.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      662 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      990 2023-04-07 18:39:34.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160.cu
+-rw-r--r--   0 root         (0) root         (0)      495 2023-04-08 07:08:17.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1007 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     1041 2023-04-07 18:40:27.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192.cu
+-rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:08:35.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1058 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      945 2023-04-07 18:37:45.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      494 2023-04-08 07:06:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      962 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)     1149 2023-04-07 22:02:41.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)      588 2023-04-08 07:07:20.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)     1166 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)      711 2023-04-07 22:17:50.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96.cu
+-rw-r--r--   0 root         (0) root         (0)      492 2023-04-08 07:07:36.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_bf16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu
+-rw-r--r--   0 root         (0) root         (0)      728 2023-04-08 06:50:03.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu
+-rw-r--r--   0 root         (0) root         (0)    30219 2023-04-12 05:25:26.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel.h
+-rw-r--r--   0 root         (0) root         (0)    27253 2023-04-08 17:10:55.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel_old.h
+-rw-r--r--   0 root         (0) root         (0)     2346 2023-04-09 16:47:24.000000 flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_launch_template.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.399535 flash_attn-1.0.1/csrc/flash_attn/src/fmha/
+-rw-r--r--   0 root         (0) root         (0)    17999 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/gemm.h
+-rw-r--r--   0 root         (0) root         (0)    22872 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/gmem_tile.h
+-rw-r--r--   0 root         (0) root         (0)     5997 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     4362 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/mask.h
+-rw-r--r--   0 root         (0) root         (0)    74010 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/smem_tile.h
+-rw-r--r--   0 root         (0) root         (0)    25514 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/softmax.h
+-rw-r--r--   0 root         (0) root         (0)    41059 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha/utils.h
+-rw-r--r--   0 root         (0) root         (0)     7152 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha.h
+-rw-r--r--   0 root         (0) root         (0)     4118 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    33506 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)     5292 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu
+-rw-r--r--   0 root         (0) root         (0)    23207 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)     2502 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_blockmask.h
+-rw-r--r--   0 root         (0) root         (0)      465 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      727 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)     1713 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     6453 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)    37194 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h
+-rw-r--r--   0 root         (0) root         (0)    30832 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h
+-rw-r--r--   0 root         (0) root         (0)      445 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim128.cu
+-rw-r--r--   0 root         (0) root         (0)      724 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim32.cu
+-rw-r--r--   0 root         (0) root         (0)      725 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim64.cu
+-rw-r--r--   0 root         (0) root         (0)     4393 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_launch_template.h
+-rw-r--r--   0 root         (0) root         (0)     3104 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_kernel.h
+-rw-r--r--   0 root         (0) root         (0)     4892 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/fmha_utils.h
+-rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 05:46:15.000000 flash_attn-1.0.1/csrc/flash_attn/src/kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     2927 2023-03-31 21:49:35.000000 flash_attn-1.0.1/csrc/flash_attn/src/mask.h
+-rw-r--r--   0 root         (0) root         (0)     5462 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/philox.cuh
+-rw-r--r--   0 root         (0) root         (0)    14205 2023-04-07 21:07:49.000000 flash_attn-1.0.1/csrc/flash_attn/src/softmax.h
+-rw-r--r--   0 root         (0) root         (0)     1686 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/flash_attn/src/static_switch.h
+-rw-r--r--   0 root         (0) root         (0)    13123 2023-04-12 03:28:52.000000 flash_attn-1.0.1/csrc/flash_attn/src/utils.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.425040 flash_attn-1.0.1/csrc/flash_gen/
+-rw-r--r--   0 root         (0) root         (0)     7018 2022-11-21 06:35:03.000000 flash_attn-1.0.1/csrc/flash_gen/decoder_masked_multihead_attention.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.789476 flash_attn-1.0.1/csrc/ft_attention/
+-rw-r--r--   0 root         (0) root         (0)     8253 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_fallbacks.cuh
+-rw-r--r--   0 root         (0) root         (0)      867 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_wrapper.h
+-rw-r--r--   0 root         (0) root         (0)     7243 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.cu
+-rw-r--r--   0 root         (0) root         (0)     7463 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.h
+-rw-r--r--   0 root         (0) root         (0)    52690 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention_utils.h
+-rw-r--r--   0 root         (0) root         (0)     7423 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/ft_attention/ft_attention.cpp
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:28.921366 flash_attn-1.0.1/csrc/fused_dense_lib/
+-rw-r--r--   0 root         (0) root         (0)     8215 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense.cpp
+-rw-r--r--   0 root         (0) root         (0)    25273 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:29.417411 flash_attn-1.0.1/csrc/fused_softmax/
+-rw-r--r--   0 root         (0) root         (0)     5037 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/fused_softmax.cpp
+-rw-r--r--   0 root         (0) root         (0)    23616 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     4209 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)    24659 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h
+-rw-r--r--   0 root         (0) root         (0)     3154 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu
+-rw-r--r--   0 root         (0) root         (0)     1216 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/fused_softmax/type_shim.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:33.876586 flash_attn-1.0.1/csrc/layer_norm/
+-rw-r--r--   0 root         (0) root         (0)     7248 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln.h
+-rw-r--r--   0 root         (0) root         (0)    36418 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_api.cpp
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      987 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      977 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      976 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    25647 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    19944 2023-01-19 07:34:02.000000 flash_attn-1.0.1/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:05:55.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_10240.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 09:07:15.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_12288.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:45:58.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2022-12-05 08:50:57.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_384.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)      925 2023-01-22 08:41:06.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_9216.cu
+-rw-r--r--   0 root         (0) root         (0)    18000 2022-12-06 21:18:58.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu
+-rw-r--r--   0 root         (0) root         (0)    12721 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)     6655 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_kernel_traits.h
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1095 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1145 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1085 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1084 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1024.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-03-29 20:52:04.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_128.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1280.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1536.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2048.cu
+-rw-r--r--   0 root         (0) root         (0)     1032 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_256.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2560.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_3072.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_4096.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_5120.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_6144.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_7168.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_768.cu
+-rw-r--r--   0 root         (0) root         (0)     1033 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_8192.cu
+-rw-r--r--   0 root         (0) root         (0)    11720 2023-03-29 19:53:46.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh
+-rw-r--r--   0 root         (0) root         (0)      977 2023-03-27 04:43:57.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_512.cu
+-rw-r--r--   0 root         (0) root         (0)    24916 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    11515 2023-03-29 20:50:46.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh
+-rw-r--r--   0 root         (0) root         (0)    12530 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh
+-rw-r--r--   0 root         (0) root         (0)    29989 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/ln_utils.cuh
+-rw-r--r--   0 root         (0) root         (0)     1278 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/layer_norm/static_switch.h
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:33.970751 flash_attn-1.0.1/csrc/rotary/
+-rw-r--r--   0 root         (0) root         (0)     1806 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/rotary/rotary.cpp
+-rw-r--r--   0 root         (0) root         (0)     1984 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/rotary/rotary_cuda.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.012112 flash_attn-1.0.1/csrc/xentropy/
+-rw-r--r--   0 root         (0) root         (0)     2290 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/xentropy/interface.cpp
+-rw-r--r--   0 root         (0) root         (0)    25783 2023-04-12 06:28:29.000000 flash_attn-1.0.1/csrc/xentropy/xentropy_kernel.cu
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.239596 flash_attn-1.0.1/flash_attn/
+-rw-r--r--   0 root         (0) root         (0)        0 2022-07-04 00:53:37.000000 flash_attn-1.0.1/flash_attn/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    20845 2022-10-31 02:25:05.000000 flash_attn-1.0.1/flash_attn/attention_kernl.py
+-rw-r--r--   0 root         (0) root         (0)     5898 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/bert_padding.py
+-rw-r--r--   0 root         (0) root         (0)     4722 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attention.py
+-rw-r--r--   0 root         (0) root         (0)    21496 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)    38148 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton.py
+-rw-r--r--   0 root         (0) root         (0)    10593 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_og.py
+-rw-r--r--   0 root         (0) root         (0)     8255 2022-11-18 03:30:00.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_single_query.py
+-rw-r--r--   0 root         (0) root         (0)    37797 2023-03-17 09:16:10.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp.py
+-rw-r--r--   0 root         (0) root         (0)    10640 2023-03-12 08:48:14.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp_og.py
+-rw-rw-r--   0 root         (0) root         (0)    22919 2022-10-31 00:28:55.000000 flash_attn-1.0.1/flash_attn/flash_attn_triton_varlen.py
+-rw-r--r--   0 root         (0) root         (0)     6819 2022-06-26 00:59:43.000000 flash_attn-1.0.1/flash_attn/flash_blocksparse_attention.py
+-rw-r--r--   0 root         (0) root         (0)     7036 2022-06-26 00:59:43.000000 flash_attn-1.0.1/flash_attn/flash_blocksparse_attn_interface.py
+-rw-r--r--   0 root         (0) root         (0)     7902 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/fused_softmax.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.375742 flash_attn-1.0.1/flash_attn/layers/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     2039 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/patch_embed.py
+-rw-r--r--   0 root         (0) root         (0)    10656 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/layers/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.439739 flash_attn-1.0.1/flash_attn/losses/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/losses/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     6697 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy.py
+-rw-r--r--   0 root         (0) root         (0)     2122 2022-12-18 05:19:38.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy_apex.py
+-rw-r--r--   0 root         (0) root         (0)     6649 2022-12-23 22:38:19.000000 flash_attn-1.0.1/flash_attn/losses/cross_entropy_parallel.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.664465 flash_attn-1.0.1/flash_attn/models/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    26630 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/bert.py
+-rw-r--r--   0 root         (0) root         (0)    34989 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gpt.py
+-rw-r--r--   0 root         (0) root         (0)     4863 2023-03-22 21:08:53.000000 flash_attn-1.0.1/flash_attn/models/gpt_j.py
+-rw-r--r--   0 root         (0) root         (0)     5025 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gpt_neox.py
+-rw-r--r--   0 root         (0) root         (0)     4387 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/gptj.py
+-rw-r--r--   0 root         (0) root         (0)     5174 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/opt.py
+-rw-r--r--   0 root         (0) root         (0)    13621 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/models/vit.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.742905 flash_attn-1.0.1/flash_attn/modules/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    15244 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/block.py
+-rw-r--r--   0 root         (0) root         (0)     8620 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/embedding.py
+-rw-r--r--   0 root         (0) root         (0)    32394 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/mha.py
+-rw-r--r--   0 root         (0) root         (0)     1023 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/modules/mlp.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.822671 flash_attn-1.0.1/flash_attn/ops/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/__init__.py
+-rw-r--r--   0 root         (0) root         (0)    25573 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/fused_dense.py
+-rw-r--r--   0 root         (0) root         (0)     2685 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/gelu_activation.py
+-rw-r--r--   0 root         (0) root         (0)    18374 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/layer_norm.py
+-rw-r--r--   0 root         (0) root         (0)     3159 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/ops/rms_norm.py
+-rw-r--r--   0 root         (0) root         (0)     5855 2023-04-07 15:25:56.000000 flash_attn-1.0.1/flash_attn/rotary.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.857568 flash_attn-1.0.1/flash_attn/triton/
+-rw-rw-r--   0 root         (0) root         (0)        0 2022-11-18 00:51:48.000000 flash_attn-1.0.1/flash_attn/triton/__init__.py
+-rw-rw-r--   0 root         (0) root         (0)    14332 2022-10-23 23:52:09.000000 flash_attn-1.0.1/flash_attn/triton/fused_attention.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.936363 flash_attn-1.0.1/flash_attn/utils/
+-rw-r--r--   0 root         (0) root         (0)        0 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/__init__.py
+-rw-r--r--   0 root         (0) root         (0)     5909 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/benchmark.py
+-rw-r--r--   0 root         (0) root         (0)     5545 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/distributed.py
+-rw-r--r--   0 root         (0) root         (0)    13140 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/generation.py
+-rw-r--r--   0 root         (0) root         (0)     1824 2023-04-12 06:28:29.000000 flash_attn-1.0.1/flash_attn/utils/pretrained.py
+drwxrwxr-x   0 root         (0) root         (0)        0 2023-04-12 17:06:34.324397 flash_attn-1.0.1/flash_attn.egg-info/
+-rw-rw-r--   0 root         (0) root         (0)    10037 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/PKG-INFO
+-rw-rw-r--   0 root         (0) root         (0)   113147 2023-04-12 17:05:53.000000 flash_attn-1.0.1/flash_attn.egg-info/SOURCES.txt
+-rw-rw-r--   0 root         (0) root         (0)        1 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/dependency_links.txt
+-rw-rw-r--   0 root         (0) root         (0)       13 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/requires.txt
+-rw-rw-r--   0 root         (0) root         (0)       27 2023-04-12 17:05:39.000000 flash_attn-1.0.1/flash_attn.egg-info/top_level.txt
+-rw-rw-r--   0 root         (0) root         (0)       38 2023-04-12 17:06:34.955113 flash_attn-1.0.1/setup.cfg
+-rw-r--r--   0 root         (0) root         (0)     7541 2023-04-12 17:04:32.000000 flash_attn-1.0.1/setup.py
```

### Comparing `flash_attn-1.0.0/LICENSE` & `flash_attn-1.0.1/LICENSE`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/PKG-INFO` & `flash_attn-1.0.1/PKG-INFO`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash_attn
-Version: 1.0.0
+Version: 1.0.1
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.0/README.md` & `flash_attn-1.0.1/README.md`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/cmake/nop.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/cmake/nop.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/00_basic_gemm/basic_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -43,15 +43,15 @@
 
   https://devblogs.nvidia.com/cutlass-linear-algebra-cuda/
 
   Aside from defining and launching the SGEMM kernel, this example does not use any other components
   or utilities within CUTLASS. Such utilities are demonstrated elsewhere in other examples and are
   prevalent in the CUTLASS unit tests.
 
-  This example has delibrately been kept similar to the basic_gemm example from cutlass-1.3 to
+  This example has delibrately been kept similar to the basic_gemm example from cutass-1.3 to 
   highlight the minimum amount of differences needed to transition to cutlass-2.0.
 
   Cutlass-1.3 sgemm: https://github.com/NVIDIA/cutlass/blob/master/examples/00_basic_gemm/basic_gemm.cu
 */
 
 // Standard Library includes
 #include <iostream>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/01_cutlass_utilities/cutlass_utilities.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/02_dump_reg_shmem/dump_reg_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/options.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.cu`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,23 +60,23 @@
       {"ColumnMajorInterleaved<4>",
        new VisualizeLayout<cutlass::layout::ColumnMajorInterleaved<4>>},
       {"RowMajorInterleaved<4>",
        new VisualizeLayout<cutlass::layout::RowMajorInterleaved<4>>},
       // All Ampere/Turing H/Integer matrix multiply tensor core kernels uses the same swizzling
       // layout implementation with different templates.
       //
-      // mma.sync.aligned.m8n8k128.s32.b1.b1.s32 Interleaved-256
-      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 Interleaved-256
+      // BMMA 88128  Interleaved-256
+      // BMMA 168256 Interleaved-256
       {"TensorOpMultiplicand<1,256>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 256>>},
-      // mma.sync.aligned.m8n8k128.s32.b1.b1.s32 TN kblock512
-      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 TN kblock512
+      // BMMA 88128  TN kblock512
+      // BMMA 168256 TN kblock512
       {"TensorOpMultiplicand<1,512>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 512>>},
-      // mma.sync.aligned.m16n8k256.s32.b1.b1.s32 TN kblock1024
+      // BMMA 168256 TN kblock1024
       {"TensorOpMultiplicand<1,1024>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<1, 1024>>},
       // Integer matrix multiply.int4 8832  Interleaved-64
       // Integer matrix multiply.int4 16864 Interleaved-64
       {"TensorOpMultiplicand<4,64>",
        new VisualizeLayout<cutlass::layout::TensorOpMultiplicand<4, 64>>},
       // Integer matrix multiply.int4 8832  TN kblock128
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/register_layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/03_visualize_layout/visualize_layout.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/04_tile_iterator/tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/05_batched_gemm/batched_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,28 +77,28 @@
 ---------------------------------------
 (0,4,0) | (0,4,1) | (1,4,0) | (1,4,1) |
 ---------------------------------------
 (0,5,0) | (0,5,1) | (1,5,0) | (1,5,1) |
 ---------------------------------------
      batch 0      |      batch 1
 , where batch size is 2, M is 6 and K is 2
-The stride (batch_stride_A) between the first element of two batches is lda * k
+The stride (batch_stride_B) between the first element of two batches is lda * k
 
 matrix B can be seen as
 -----------------------------
 (0,0,0) | (0,0,1) | (0,0,2) |
 ----------------------------- batch 0
 (0,1,0) | (0,1,1) | (0,1,2) |
 -------------------------------------
 (1,0,0) | (1,0,1) | (1,0,2) |
 ----------------------------- batch 1
 (1,1,0) | (1,1,1) | (1,1,2) |
 -----------------------------
 , where the batch size is 2, N is 3 and K is 2
-The stride (batch_stride_B) between the first element of two batches is k
+The stride (batch_stride_C) between the first element of two batches is k
 
 
 */
 
 cudaError_t cutlass_array_sgemm(
   int m,
   int n,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/06_splitK_gemm/splitk_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/07_volta_tensorop_gemm/volta_tensorop_gemm.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -71,15 +71,15 @@
 beta * C).
 
 Now that we setup the properties of data, we have to setup properties of computation.
 
 Second, we create template variables of tile sizes for thread-block, warp and mma-op to 128x128x32,
 64x64x32, 8x8x4 (MxNxK) respectively. When passed to instantiate CUTLASS GEMM kernel, it internally
 deduce the amount of threads needed per thread-block, amount of shared memory, storing data in
-bank-conflict free manner, and ton of other variables required to compose, initialize and launch a
+bank-conflict free manner, and ton of other variables required to compose, intialize and launch a
 high performance GEMM kernel. This is the beauty of CUTLASS, it relieves developer from
 understanding and coding complicated hardware optimizations which can easily go wrong.
 
 CUTLASS also supports multiple MMA pipelines in a CTA. What are MMA pipelines? MMA pipelines
 constitute the whole process of loading input data from global memory to shared memory, loading data
 from shared memory to registers, doing matrix multiplication, store to global memory. The below flow
 sequence shows a typical mma pipeline.
@@ -103,23 +103,23 @@
 
 There are few more template variables initialized such as, which threadblock tile of output matrix
 is done which threadblock launched on an SM, CUDA SM architecture of GPU you want to run on.
 
 These are all put together to create a template variable which describes CUTLASS GEMM kernel using
 cutlass::gemm::device::Gemm template.
 
-The next step is to initialize physical data, instantiate and initialize CUTLASS kernel and run it.
+The next step is to intialize physical data, instantiate and initialize CUTLASS kernel and run it.
 We use CUTLASS utilities to initialize, fill, compare matrices as they are simple and doesn't come
 in the way of learning CUTLASS.
 
 Once all the matrices are initialized and filled with data, create arguments tuple to launch CUTLASS
 kernel which takes problem size (M = 5120, N = 4096 and K = 4096), matrices, alpha, beta and the
 important one, split k-dimension factor. Along with that, we query CUTLASS if any scratch-space
 memory required by the kernel we instantiated. If yes, we create it and pass it along with other
-arguments created to initialize CUTLASS kernel then, the kernel is launched.
+arguments created to intialize CUTLASS kernel then, the kernel is launched.
 
 In this example, we later on launch a reference gemm kernel (from CUTLASS utilities) to compare if
 the output from CUTLASS kernel is same as reference GEMM kernel.
 */
 
 #include <iostream>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/08_turing_tensorop_gemm/turing_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -70,15 +70,15 @@
 and data type of computation of linear combination (alpha * X + beta * C).
 
 Now that we setup the properties of data, we have to setup properties of computation.
 
 Second, we create template variables of tile sizes for thread-block, warp and mma-op to 128x256x64,
 64x64x16, 8x8x16 (MxNxK) respectively. When passed to instantiate CUTLASS GEMM kernel, it internally
 deduce the amount of threads needed per thread-block, amount of shared memory, storing data in
-bank-conflict free manner, and ton of other variables required to compose, initialize and launch a
+bank-conflict free manner, and ton of other variables required to compose, intialize and launch a
 high performance GEMM kernel. This is the beauty of CUTLASS, it relieves developer from
 understanding and coding complicated hardware optimizations which can easily go wrong.
 
 CUTLASS also supports multiple MMA pipelines in a threadblock. What are MMA pipelines? MMA pipelines
 constitute the whole process of loading input data from global memory to shared memory, loading data
 from shared memory to registers, doing matrix multiplication, store to global memory. The below flow
 sequence shows a typical mma pipeline.
@@ -102,23 +102,23 @@
 
 There are few more template variables initialized such as, which threadblock tile of output matrix
 is done which threadblock launched on an SM, CUDA SM architecture of GPU you want to run on.
 
 These are all put together to create a template variable which describes CUTLASS GEMM kernel using
 cutlass::gemm::device::Gemm template.
 
-The next step is to initialize physical data, instantiate and initialize CUTLASS kernel and run it.
+The next step is to intialize physical data, instantiate and initialize CUTLASS kernel and run it.
 We use CUTLASS utilities to initialize, fill, compare matrices as they are simple and doesn't come
 in the way of learning CUTLASS.
 
 Once all the matrices are initialized and filled with data, create arguments tuple to launch CUTLASS
 kernel which takes problem size (M = 5120, N = 4096 and K = 4096), matrices, alpha, beta and the
 important one, split k-dimension factor. Along with that, we query CUTLASS if any scratch-space
 memory required by the kernel we instantiated. If yes, we create it and pass it along with other
-arguments created to initialize CUTLASS kernel then, the kernel is launched.
+arguments created to intialize CUTLASS kernel then, the kernel is launched.
 
 In this example, we later on launch a reference gemm kernel (from CUTLASS utilities) to compare if
 the output from CUTLASS kernel is same as reference GEMM kernel.
 */
 
 #include <iostream>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/09_turing_tensorop_conv2dfprop/turing_tensorop_conv2dfprop.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
 computation of linear combination (alpha * X + beta * C).
 
 Now that we setup the properties of data, we have to setup properties of computation.
 
 Second, we create template variables of tile sizes for thread-block, warp and mma-op to 128x128x128,
 64x64x128, 8x8x32 (MxNxK) respectively. When passed to instantiate CUTLASS Implicit GEMM kernel, it
 internally deduces the amount of threads needed per thread-block, amount of shared memory, storing
-data in bank-conflict free manner, and ton of other variables required to compose, initialize and
+data in bank-conflict free manner, and ton of other variables required to compose, intialize and
 launch a high performance Implicit GEMM kernel. This is the beauty of CUTLASS, it relieves developer
 from understanding and coding complicated hardware optimizations which can easily go wrong.
 
 CUTLASS also supports multiple MMA pipelines in a threadblock. What are MMA pipelines? MMA pipelines
 constitute the whole process of loading input data from global memory to shared memory, loading data
 from shared memory to registers, doing matrix multiplication, store to global memory. The below flow
 sequence shows a typical mma pipeline.
@@ -104,24 +104,24 @@
 
 There are few more template variables initialized such as, which threadblock tile of output matrix
 is done which threadblock launched on an SM, CUDA SM architecture of GPU you want to run on.
 
 These are all put together to create a template variable which describes CUTLASS Implicit GEMM
 kernel using cutlass::conv::device::ImplicitGemm template.
 
-The next step is to initialize physical data, instantiate and initialize CUTLASS kernel and run it.
+The next step is to intialize physical data, instantiate and initialize CUTLASS kernel and run it.
 We use CUTLASS utilities to initialize, fill, compare tensors as they are simple and doesn't come
 in the way of learning CUTLASS.
 
 Once all the tensors are initialized and filled with data, create arguments tuple to launch CUTLASS
 kernel which takes problem size (N = 1, H = 64, W = 64, C = 128), filter size (K = 64,
 R = 3, S = 3, C = 128 ), padding, strides, dilation, tensors, alpha, beta and the
 important one, split k-dimension factor. Along with that, we query CUTLASS if any scratch-space
 memory required by the kernel we instantiated. If yes, we create it and pass it along with other
-arguments created to initialize CUTLASS kernel then, the kernel is launched.
+arguments created to intialize CUTLASS kernel then, the kernel is launched.
 
 In this example, we later on launch a reference convolution kernel (from CUTLASS utilities) to
 compare if the output from CUTLASS kernel is same as the reference implicit GEMM kernel.
 */
 
 #include <iostream>
 #include <fstream>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/10_planar_complex/planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/11_planar_complex_array/planar_complex_array.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/12_gemm_bias_relu/gemm_bias_relu.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_conv2d_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_gemm_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_conv2d_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/b2b_interleaved_gemm_run.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/device/b2b_implicit_gemm_convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_f16_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_convs_s8_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm75_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_rf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_f16_sm80_shmem.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm75_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_rf.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/fused_two_gemms_s8_sm80_shmem.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/b2b_implicit_gemm_convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_conv2d_fprop_smem_accumulator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/kernel/default_b2b_gemm_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/reference/device/tensor_scale_bias.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_multistage_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -317,15 +317,15 @@
     ++this->warp_tile_iterator_B0_;
 
     Operator0 warp_mma0;
 
     int smem_write_stage_idx = 1;
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
@@ -457,15 +457,15 @@
     Operator1 warp_mma1;
 
     smem_write_stage_idx = 1;
     
     int gemm_k_iterations_1 = FragmentIteratorA1::Policy::kIterations / Base::kWarpGemmIterations1;
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_PRAGMA_UNROLL
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_implicit_gemm_pipelined_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -337,15 +337,15 @@
     ++this->warp_tile_iterator_B0_;
 
     Operator0 warp_mma0;
 
     int smem_write_stage_idx = 1;
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_base_smem_accumulator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -195,39 +195,39 @@
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
     static_assert(Base::kWarpGemmIterations1 > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
     /// Number of cp.async instructions to load one stage of operand A
-    static int const TBLoadIterationsA0 =
+    static int const TBLDGSTSIterationsA0 =
         IteratorA0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB0 =
+    static int const TBLDGSTSIterationsB0 =
         IteratorB0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB1 =
+    static int const TBLDGSTSIterationsB1 =
         IteratorB1::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA0 =
-        (TBLoadIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLDGSTSIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB0 =
-        (TBLoadIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLDGSTSIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB1 =
-        (TBLoadIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
+        (TBLDGSTSIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
   };
 
  private:
 
   using WarpLoadedFragmentA0 = typename Operator0::FragmentA;
   using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
   /// Warp Fragment of operand A1 loaded from accmulator tile
@@ -300,18 +300,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_0(IteratorA0 &iterator_A0, IteratorB0 &iterator_B0,
                               int group_start_A0 = 0, int group_start_B0 = 0) {
     iterator_A0.set_iteration_index(group_start_A0 *
                                    IteratorA0::kAccessesPerVector);
     this->smem_iterator_A0_.set_iteration_index(group_start_A0);
 
-    // Load for operand A
+    // LDGSTS for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA0; ++j) {
-      if (group_start_A0 + j < Detail::TBLoadIterationsA0) {
+      if (group_start_A0 + j < Detail::TBLDGSTSIterationsA0) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA0::Element>::value *
                               IteratorA0::ThreadMap::kElementsPerAccess /
                               IteratorA0::kAccessesPerVector / 8;
@@ -330,18 +330,18 @@
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B0 *
                                    IteratorB0::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B0);
 
-    // Load for operand B
+    // LDGSTS for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB0; ++j) {
-      if (group_start_B0 + j < Detail::TBLoadIterationsB0) {
+      if (group_start_B0 + j < Detail::TBLDGSTSIterationsB0) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
                               IteratorB0::ThreadMap::kElementsPerAccess /
                               IteratorB0::kAccessesPerVector / 8;
@@ -363,18 +363,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_1(IteratorB1 &iterator_B1,
                               int group_start_B1 = 0) {
     iterator_B1.set_iteration_index(group_start_B1 *
                                    IteratorB1::kAccessesPerVector);
     this->smem_iterator_B1_.set_iteration_index(group_start_B1);
 
-    // Load for operand B
+    // LDGSTS for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB1; ++j) {
-      if (group_start_B1 + j < Detail::TBLoadIterationsB1) {
+      if (group_start_B1 + j < Detail::TBLDGSTSIterationsB1) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
                               IteratorB1::ThreadMap::kElementsPerAccess /
                               IteratorB1::kAccessesPerVector / 8;
@@ -426,17 +426,17 @@
 
       iterator_A0.clear_mask(gemm_k_iterations_0 == 0);
       iterator_B0.clear_mask(gemm_k_iterations_0 == 0);
 
       iterator_A0.set_iteration_index(0);
       this->smem_iterator_A0_.set_iteration_index(0);
 
-      // Load for operand A
+      // LDGSTS for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsA0; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsA0; ++j) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -454,17 +454,17 @@
 
         ++this->smem_iterator_A0_;
       }
 
       iterator_B0.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
 
-      // Load for operand B
+      // LDGSTS for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB0; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB0; ++j) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -670,17 +670,17 @@
          ++stage, --gemm_k_iterations_1) {
 
       iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
 
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
-      // Load for operand B
+      // LDGSTS for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB1; ++j) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           int const kSrcBytes =
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_multistage_smem_accumulator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -201,39 +201,39 @@
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
     static_assert(Base::kWarpGemmIterations1 > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
     /// Number of cp.async instructions to load one stage of operand A
-    static int const TBLoadIterationsA0 =
+    static int const TBLDGSTSIterationsA0 =
         IteratorA0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB0 =
+    static int const TBLDGSTSIterationsB0 =
         IteratorB0::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB1 =
+    static int const TBLDGSTSIterationsB1 =
         IteratorB1::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA0 =
-        (TBLoadIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLDGSTSIterationsA0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB0 =
-        (TBLoadIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
+        (TBLDGSTSIterationsB0 + Base::kWarpGemmIterations0 - 1) / Base::kWarpGemmIterations0;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB1 =
-        (TBLoadIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
+        (TBLDGSTSIterationsB1 + Base::kWarpGemmIterations1 - 1) / Base::kWarpGemmIterations1;
   };
 
  private:
 
   using WarpLoadedFragmentA0 = typename Operator0::FragmentA;
   using WarpLoadedFragmentB0 = typename Operator0::FragmentB;
   using WarpLoadedFragmentA1 = typename Operator1::FragmentA;
@@ -323,18 +323,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_0(IteratorA0 &iterator_A0, IteratorB0 &iterator_B0,
                               int group_start_A0 = 0, int group_start_B0 = 0) {
     iterator_A0.set_iteration_index(group_start_A0 *
                                    IteratorA0::kAccessesPerVector);
     this->smem_iterator_A0_.set_iteration_index(group_start_A0);
 
-    // cp.async for operand A
+    // LDGSTS for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA0; ++j) {
-      if (group_start_A0 + j < Detail::TBLoadIterationsA0) {
+      if (group_start_A0 + j < Detail::TBLDGSTSIterationsA0) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA0::Element>::value *
                               IteratorA0::ThreadMap::kElementsPerAccess /
                               IteratorA0::kAccessesPerVector / 8;
@@ -353,18 +353,18 @@
       }
     }
 
     iterator_B0.set_iteration_index(group_start_B0 *
                                    IteratorB0::kAccessesPerVector);
     this->smem_iterator_B0_.set_iteration_index(group_start_B0);
 
-    // cp.async for operand B
+    // LDGSTS for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB0; ++j) {
-      if (group_start_B0 + j < Detail::TBLoadIterationsB0) {
+      if (group_start_B0 + j < Detail::TBLDGSTSIterationsB0) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB0::Element>::value *
                               IteratorB0::ThreadMap::kElementsPerAccess /
                               IteratorB0::kAccessesPerVector / 8;
@@ -386,18 +386,18 @@
   CUTLASS_DEVICE
   void copy_tiles_and_advance_1(IteratorB1 &iterator_B1,
                               int group_start_B1 = 0) {
     iterator_B1.set_iteration_index(group_start_B1 *
                                    IteratorB1::kAccessesPerVector);
     this->smem_iterator_B1_.set_iteration_index(group_start_B1);
 
-    // cp.async for operand B
+    // LDGSTS for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB1; ++j) {
-      if (group_start_B1 + j < Detail::TBLoadIterationsB1) {
+      if (group_start_B1 + j < Detail::TBLDGSTSIterationsB1) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB1::Element>::value *
                               IteratorB1::ThreadMap::kElementsPerAccess /
                               IteratorB1::kAccessesPerVector / 8;
@@ -449,17 +449,17 @@
 
       iterator_A0.clear_mask(gemm_k_iterations_0 == 0);
       iterator_B0.clear_mask(gemm_k_iterations_0 == 0);
 
       iterator_A0.set_iteration_index(0);
       this->smem_iterator_A0_.set_iteration_index(0);
 
-      // cp.async for operand A
+      // LDGSTS for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsA0; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsA0; ++j) {
         typename IteratorA0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA0::AccessType *>(
                 this->smem_iterator_A0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -477,17 +477,17 @@
 
         ++this->smem_iterator_A0_;
       }
 
       iterator_B0.set_iteration_index(0);
       this->smem_iterator_B0_.set_iteration_index(0);
 
-      // cp.async for operand B
+      // LDGSTS for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB0; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB0; ++j) {
         typename IteratorB0::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB0::AccessType *>(
                 this->smem_iterator_B0_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB0::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -685,17 +685,17 @@
          ++stage, --gemm_k_iterations_1) {
 
       iterator_B1.clear_mask(gemm_k_iterations_1 == 0);
 
       iterator_B1.set_iteration_index(0);
       this->smem_iterator_B1_.set_iteration_index(0);
 
-      // cp.async for operand B
+      // LDGSTS for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB1; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB1; ++j) {
         typename IteratorB1::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB1::AccessType *>(
                 this->smem_iterator_B1_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB1::kAccessesPerVector; ++v) {
           int const kSrcBytes =
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -321,15 +321,15 @@
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
     iterator_A.clear_mask(gemm_k_iterations_0 <= 1);
     iterator_B0.clear_mask(gemm_k_iterations_0 <= 1);
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/b2b_mma_pipelined_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -342,15 +342,15 @@
     int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
     iterator_A.clear_mask(gemm_k_iterations_0 <= 1);
     iterator_B0.clear_mask(gemm_k_iterations_0 <= 1);
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/threadblock/default_b2b_mma_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -69,15 +69,15 @@
 computation of linear combination (alpha * X + beta * C).
 
 Now that we setup the properties of data, we have to setup properties of computation.
 
 Second, we create template variables of tile sizes for thread-block, warp and mma-op to 128x128x64,
 64x64x64, 16x8x16 (MxNxK) respectively. When passed to instantiate CUTLASS Implicit GEMM kernel, it
 internally deduces the amount of threads needed per thread-block, amount of shared memory, storing
-data in bank-conflict free manner, and ton of other variables required to compose, initialize and
+data in bank-conflict free manner, and ton of other variables required to compose, intialize and
 launch a high performance Implicit GEMM kernel. This is the beauty of CUTLASS, it relieves developer
 from understanding and coding complicated hardware optimizations which can easily go wrong.
 
 CUTLASS also supports multiple MMA pipelines in a threadblock. What are MMA pipelines? MMA pipelines
 constitute the whole process of loading input data from global memory to shared memory, loading data
 from shared memory to registers, doing matrix multiplication, store to global memory. The below flow
 sequence shows a typical mma multistage pipeline.
@@ -91,24 +91,24 @@
 
 There are few more template variables initialized such as, which threadblock tile of output matrix
 is done which threadblock launched on an SM, CUDA SM architecture of GPU you want to run on.
 
 These are all put together to create a template variable which describes CUTLASS Implicit GEMM
 kernel using cutlass::conv::device::ImplicitGemm template.
 
-The next step is to initialize physical data, instantiate and initialize CUTLASS kernel and run it.
+The next step is to intialize physical data, instantiate and initialize CUTLASS kernel and run it.
 We use CUTLASS utilities to initialize, fill, compare tensors as they are simple and doesn't come
 in the way of learning CUTLASS.
 
 Once all the tensors are initialized and filled with data, create arguments tuple to launch CUTLASS
 kernel which takes problem size (N = 1, H = 64, W = 64, C = 128), filter size (K = 64,
 R = 3, S = 3, C = 128 ), padding, strides, dilation, tensors, alpha, beta and the
 important one, split k-dimension factor. Along with that, we query CUTLASS if any scratch-space
 memory required by the kernel we instantiated. If yes, we create it and pass it along with other
-arguments created to initialize CUTLASS kernel then, the kernel is launched.
+arguments created to intialize CUTLASS kernel then, the kernel is launched.
 
 In this example, we later on launch a reference convolution kernel (from CUTLASS utilities) to
 compare if the output from CUTLASS kernel is same as the reference implicit GEMM kernel.
 */
 
 #include <iostream>
 #include <fstream>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/17_fprop_per_channel_bias/fprop_per_channel_bias.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/19_tensorop_canonical/tensorop_canonical.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/20_simt_canonical/simt_canonical.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/21_quaternion_gemm/quaternion_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/22_quaternion_conv/quaternion_conv.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,24 +32,24 @@
 /**
 The example demenstrates how to reduce one of the operands of the GEMM along the k-dimension when
 computing GEMM.  So the output also contains either a Mx1 or 1XN vector.  It only works with Ampere
 16x8x16 FP16/BF16 tensor cores, though it is not difficult to apply to other Turing/Ampere tensor
 core instructions.
 
 Most of the reduction is done in gemm/warp level, see gemm/warp/mma_with_reduction_tensor_op.h
-A few bit of reduction is done in the epilogue before storing the vector, see
+A few bit of reduction is done in the epilouge before storing the vector, see
 epilogue/threadblock/epilogue_gemm_k_reduction.h 
 */
 
 #include <iostream>
 #include <fstream>
 #include <sstream>
 
 #include "cutlass/cutlass.h"
-#include "cutlass/gemm/device/gemm_with_k_reduction.h"
+#include "cutlass/gemm/device/gemm_universal_adapter.h"
 #include "cutlass/gemm/kernel/default_gemm_with_k_reduction.h"
 #include "cutlass/reduction/device/reduce_split_k.h"
 #include "cutlass/reduction/kernel/reduce_split_k.h"
 #include "cutlass/reduction/thread/reduction_operators.h"
 #include "cutlass/matrix_coord.h"
 
 #include "cutlass/util/command_line.h"
@@ -97,49 +97,41 @@
 
 // Number of pipelines you want to use
 constexpr int NumStages = 4;
 
 // Reduce A or B operand along the K dimension
 constexpr bool ReduceKForA = true;
 
-// Alignment of A operand
-constexpr int AlignmentA = 8;
-
-// Alignment of B operand
-constexpr int AlignmentB = 8;
-
 // This code section describes the epilogue part of the kernel, we use default value
 using EpilogueOp = cutlass::epilogue::thread::LinearCombination<
     ElementOutput,                                        // Data type of output matrix.
     128 / cutlass::sizeof_bits<ElementOutput>::value,     // The number of elements per vectorized.
                                                           // memory access. This becomes the vector width of
                                                           // math instructions in the epilogue too.
     ElementAccumulator,                                   // Data type of accumulator
     ElementComputeEpilogue>;
 
-using Gemm = typename cutlass::gemm::device::GemmWithKReduction<
-  ElementInputA, LayoutInputA,
-  ElementInputB, LayoutInputB,
+using GemmKernel = typename cutlass::gemm::kernel::DefaultGemmWithKReduction<
+  ElementInputA, LayoutInputA, cutlass::ComplexTransform::kNone, 8,
+  ElementInputB, LayoutInputB, cutlass::ComplexTransform::kNone, 8,
   ElementOutput, LayoutOutput,
   ElementAccumulator,
   MMAOp,
   ReduceKForA,
   SmArch,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOp,
   SwizzleThreadBlock,
   NumStages,
-  AlignmentA,
-  AlignmentB,
-  cutlass::arch::OpMultiplyAdd,
-  cutlass::ComplexTransform::kNone,
-  cutlass::ComplexTransform::kNone
->;
+  cutlass::arch::OpMultiplyAdd
+>::GemmKernel;
+
+using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
 
 // Below is the reduction kernel used in the case of parallel split-k
 using ReduceGemmSplitKShape = cutlass::MatrixShape<4, 64>;;
 
 using ReduceOp = cutlass::reduction::thread::ReduceAdd<
     ElementAccumulator,
     ElementOutput,
@@ -372,29 +364,29 @@
 
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_reduction({reduce_vector_length, 1});
   cutlass::HostTensor<ElementOutput, LayoutGemmKReduction> tensor_ref_reduction({reduce_vector_length, 1});
 
   // Fill input and output matrices on host using CUTLASS helper functions
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_a.host_view(),
-      1997,
+      1,
       ElementInputA(2),
       ElementInputA(-2),
       0);  // <- Fill tensor A on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_b.host_view(),
-      2003,
+      1,
       ElementInputB(2),
       ElementInputB(-2),
       0);  // <- Fill tensor B on host with uniform-distribution random data
 
   cutlass::reference::host::TensorFillRandomUniform(
       tensor_c.host_view(),
-      2017,
+      1,
       ElementOutput(2),
       ElementOutput(-2),
       0);  // <- Fill matrix C on host with uniform-distribution random data
   cutlass::reference::host::TensorFill(
       tensor_d.host_view());  // <- fill matrix D on host with zeros
   cutlass::reference::host::TensorFill(
       tensor_ref_d.host_view());  // <- fill matrix D for reference on host with zeros
@@ -422,15 +414,15 @@
                      cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel :
                      cutlass::gemm::GemmUniversalMode::kGemm;
 
   int batch_count = options.split_k_slices;
 
   // Create a tuple of gemm kernel arguments. This is later passed as arguments to launch
   // instantiated CUTLASS kernel
-  typename Gemm::Arguments arguments(
+  typename Gemm::Arguments arguments{
     mode,
     options.problem_size,
     batch_count,
     {alpha, beta},
     tensor_a.device_ref().data(),              // <- reference to tensor A on device
     tensor_b.device_ref().data(),              // <- reference to tensor B on device
     tensor_c.device_ref().data(),              // <- reference to matrix C on device
@@ -441,15 +433,16 @@
     options.problem_size.m() * options.problem_size.n(),
     options.problem_size.m() * options.problem_size.n(),
     reduce_vector_length,
     tensor_a.layout().stride(0),
     tensor_b.layout().stride(0),
     tensor_c.layout().stride(0),
     tensor_d.layout().stride(0),
-    tensor_reduction.layout().stride(0));
+    tensor_reduction.layout().stride(0)
+  };                    
 
   // Instantiate CUTLASS kernel depending on templates
   Gemm gemm_op;
 
   // Using the arguments, query for extra workspace required for matrix multiplication computation
   size_t workspace_size = Gemm::get_workspace_size(arguments);
 
@@ -510,22 +503,23 @@
     ElementOutput *workspace_vector_ptr = static_cast<ElementOutput *>(workspace_gemm_ptr) + batch_count * options.problem_size.m() * options.problem_size.n();
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> workspace_vector_tensorref(workspace_vector_ptr, splitk_vector_layout);
 
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> tensor_reduction_tensorref(tensor_reduction.device_ref().data(), splitk_vector_layout);
 
     cutlass::TensorRef<ElementOutput, cutlass::layout::RowMajor> tensor_nullptr_tensorref(nullptr, splitk_vector_layout);
 
-    typename ReduceVectorSplitK::Arguments reduce_vector_splitk_arguments(
+    typename ReduceVectorSplitK::Arguments reduce_vector_splitk_arguments{
       cutlass::MatrixCoord(1, reduce_vector_length),
       batch_count,
       size_t(reduce_vector_length),
       workspace_vector_tensorref,
       tensor_reduction_tensorref,
       tensor_nullptr_tensorref,
-      {1.0f, 0.0f});
+      {1.0f, 0.0f} 
+    };
 
     ReduceVectorSplitK reduce_vector_splitk_op;
    
     result.status = reduce_vector_splitk_op.initialize(reduce_vector_splitk_arguments); 
     CUTLASS_CHECK(result.status);
 
     result.status = reduce_vector_splitk_op();
@@ -563,15 +557,15 @@
   
     // Copy output data from CUTLASS and reference kernel to host for comparison
     tensor_d.sync_host();
     tensor_ref_d.sync_host();
   
     tensor_reduction.sync_host();
   
-    // Reduce K in host code
+    // Compute bias + relu in host code
     if (ReduceKForA) {
       for (int m = 0; m < options.problem_size.m(); ++m) {
         for (int k = 0; k < options.problem_size.k(); ++k) {
           tensor_ref_reduction.at({m, 0}) += 
             tensor_a.at(cutlass::MatrixCoord(m, k));
         }
       }
@@ -583,15 +577,15 @@
         }
       }
     }
   
     // Check if output from CUTLASS kernel and reference kernel are equal or not
     bool pass = cutlass::reference::host::TensorEquals(tensor_d.host_view(),
                                                        tensor_ref_d.host_view());
-
+  
     pass &= cutlass::reference::host::TensorEquals(tensor_ref_reduction.host_view(),
                                                    tensor_reduction.host_view());
 
     if (!pass) {
       result.reference_check = cutlass::Status::kErrorInternal;
       std::cout << "ERROR - results miscompared.\n";
     } else {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/24_gemm_grouped/gemm_grouped.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1483,16 +1483,16 @@
 
   using LayoutA = cutlass::layout::ColumnMajor;
   using LayoutB = cutlass::layout::ColumnMajor;
   using LayoutC = cutlass::layout::ColumnMajor;
 
   // Gemm operator cutlass_tensorop_f16_s16816gemm_f16_128x128_32x4_nt_align8
   using GemmBatched = cutlass::gemm::device::GemmUniversal<
-    ElementA, LayoutA,
-    ElementB, LayoutB,
+    cutlass::half_t, LayoutA,
+    cutlass::half_t, LayoutB,
     ElementOutput,   LayoutC,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
     cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<128, 128, 32>,
     cutlass::gemm::GemmShape<64, 64, 32>,
     cutlass::gemm::GemmShape<16, 8, 16>,
@@ -1506,19 +1506,19 @@
     4
   >;
 
   // Define a grouped GEMM kernel with all template parameters set except
   // for scheduling mode. This will be used as the template for all scheduling
   // modes executed.
   using GemmKernel = typename cutlass::gemm::kernel::DefaultGemmGrouped<
-    ElementA,
+    cutlass::half_t, 
     LayoutA,
     cutlass::ComplexTransform::kNone,
     8,
-    ElementB,
+    cutlass::half_t,
     LayoutB,
     cutlass::ComplexTransform::kNone,
     8,
     ElementOutput, LayoutC,
     ElementAccumulator, 
     cutlass::arch::OpClassTensorOp, 
     cutlass::arch::Sm80,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_3d_fprop_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/30_wgrad_split_k/30_wgrad_split_k.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -527,25 +527,25 @@
     typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         // Reduction input
         {
             reinterpret_cast<ElementAccumulator*> (workspace.get()),
-            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         // Destination
         {
             tensor_d.device_data(),
-            ReductionStrideIndex(tensor_d.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_d.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         // Source
         {
             tensor_c.device_data(),
-            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::UnderlyingKernel::kTensorCStrideIdx])
+            ReductionStrideIndex(tensor_c.stride()[ImplicitGemm::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {options.alpha, options.beta}
     );
 
     status = reduction_op.initialize(reduction_args, nullptr);
     status = reduction_op();
   }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/31_basic_syrk/basic_syrk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/32_basic_trmm/basic_trmm.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/33_ampere_3xtf32_tensorop_symm/ampere_3xtf32_tensorop_symm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/34_transposed_conv2d/34_transposed_conv2d.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_softmax.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_epilogue_visitor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -363,14 +363,20 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+
   #define SPLIT_K_ENABLED 1
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/35_gemm_softmax/gemm_with_softmax.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/36_gather_scatter_fusion/gather_scatter_fusion.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_layernorm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_epilogue_visitor.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -305,14 +305,20 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/37_gemm_layernorm_gemm_fusion/gemm_with_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,15 +31,15 @@
 
 /*! \file
     \brief A file contains all functioning classes needed by GemmLayernorm.
 
     GemmLayernorm example =  GEMM0 with partial reduction fused in epilogue (EpilogueVisitorLayerNorm)
                           +  lightweight full reduction kernel (ApplyFinalReduction)
                           +  GEMM1 with elemenwise operations fused in mainloop (GemmLayernormMainloopFusion)
-
+                          
 */
 
 #pragma once
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include <cmath>
@@ -73,29 +73,29 @@
 
 template <
   typename ElementVariance_,
   typename ElementMean_,
   typename ElementLayernormCompute_,
   typename ElementOutput,
   typename ThreadblockShape_,
-  bool IsShiftedVariance_ = false
+  bool IsShiftedVariance_ = false 
 >
 class ApplyFinalReduction {
 public:
 
   using ElementVariance = ElementVariance_;
   using ElementMean = ElementMean_;
   using ElementLayernormCompute = ElementLayernormCompute_;
   using ThreadblockShape = ThreadblockShape_;
 
   // Pre-processing has ensured the layout equivelent to RowMajor
   using Layout = cutlass::layout::RowMajor;
 
   using TensorVariance = TensorRef<ElementVariance, Layout>;
-  using TensorMean = TensorRef<ElementMean, Layout>;
+  using TensorMean = TensorRef<ElementMean, Layout>;  
 
   static bool const kIsShiftedVariance = IsShiftedVariance_;
 
   //
   // Arguments
   //
 
@@ -459,15 +459,15 @@
       CUTLASS_PRAGMA_UNROLL
       for (int iter_idx = 0; iter_idx < kIterations; ++iter_idx) {
         int step_offset = iter_idx * OutputTileIterator::Shape::kRow;
         CUTLASS_PRAGMA_UNROLL
         for (int rid = 0; rid < kRowIterations; ++rid) {
           int row_step_offset = rid * kDeltaRow;
           int row_offset = thread_offset_row_base + step_offset + row_step_offset;
-          bool is_load = (row_offset < extent_.row());
+          bool is_load = (row_offset < extent_.row());  
           shift_k_frag_[iter_idx * kRowIterations + rid] = load_shift_k_(row_offset, is_load);
         }
 
       }
 
     }
 
@@ -500,17 +500,17 @@
     int frag_idx,
     AccumulatorFragment const &accum) {
 
     using Mul = cutlass::multiplies<ElementLayernormCompute>;
     using Minus = cutlass::minus<ElementLayernormCompute>;
     using Exp   = cutlass::fast_exp_op<ElementLayernormCompute>;
 
-    [[maybe_unused]] Minus minus;
-    [[maybe_unused]] Mul   mul;
-    [[maybe_unused]] Exp   exponential;
+    Minus     minus;
+    Mul       mul;
+    Exp       exponential;
 
     LayernormFragment result;
 
     thread_offset_ =
       iterator_D_.thread_start() +
       OutputTileIterator::ThreadMap::iteration_offset(frag_idx);
 
@@ -601,24 +601,24 @@
   }
 
 private:
 
   CUTLASS_DEVICE
   ElementLayernormCompute load_shift_k_(int row_offset, bool is_load) {
     using ConvertShiftK = cutlass::NumericConverter<ElementLayernormCompute, ElementOutput>;
-    ConvertShiftK convert_shift_k;
+    ConvertShiftK convert_shift_k;    
     ElementOutput shift_k_val;
 
     // Computes the address to load shift_k element
     ElementOutput *curr_ptr_shift_k = params_.ptr_Shifted_K + row_offset;
     // Conditionally loads from global memory
     arch::global_load<ElementOutput, sizeof(ElementOutput)>(shift_k_val, (void *)curr_ptr_shift_k, is_load);
     // Converts data type to return
     ElementLayernormCompute converted_shift_k_val = convert_shift_k(shift_k_val);
-
+    
     return converted_shift_k_val;
   }
 
   CUTLASS_DEVICE
   ElementLayernormCompute square_sum_accumulator_(LayernormFragment const &accum) {
     ElementLayernormCompute sum_ = ElementLayernormCompute(0);
 
@@ -685,46 +685,46 @@
 public:
 
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   //
   // Type definitions
   //
-
+  
   static bool const kInternalTranspose = cutlass::platform::is_same<LayoutOutput_, cutlass::layout::ColumnMajor>::value;
   static bool const kIsShiftedVariance = IsShiftedVariance_;
 
   // These is mandatory layout.
   using LayoutInputScaleBias = cutlass::layout::RowMajor;
 
   // These are mandatory data types.
   using ElementLayernormCompute = float;
   using ElementInputScaleBias = cutlass::half_t;
 
   // These are mandatory params required by mainloop fusion
   using OperatorClass       = cutlass::arch::OpClassTensorOp;
   using ArchTag             = cutlass::arch::Sm80;
 
-  // These are mandatory layouts and data types
+  // These are mandatory layouts and data types 
   // that are inheritated from pre-defined params
-
+  
   using LayoutSumSqr = LayoutInputScaleBias;
   using LayoutSum = LayoutInputScaleBias;
 
   using ElementMean = ElementInputScaleBias;
-  using ElementVariance = ElementInputScaleBias;
+  using ElementVariance = ElementInputScaleBias;  
 
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   using LayoutInputA0 = LayoutInputA0_;
   using LayoutInputB0 = LayoutInputB0_;
   using LayoutInputA1 = LayoutOutput_;
   using LayoutInputB1 = LayoutOutput_;
   using LayoutOutputC0 = LayoutOutput_;
-  using LayoutOutputC1 = LayoutOutput_;
+  using LayoutOutputC1 = LayoutOutput_;  
 
   using ElementInputA0 = ElementInputA0_;
   using ElementInputB0 = ElementInputB0_;
   using ElementOutputC0 = ElementOutput_;
   using ElementCompute = ElementCompute_;
   using ElementInputB1 = ElementInputB0_;
 
@@ -743,15 +743,15 @@
   using WarpShape        = WarpShape_;
   using InstructionShape = InstructionShape_;
 
   static int const kStages0 = Stages0;
   static int const kStages1 = Stages1;
 
   using SwizzleThreadBlock = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;
-
+  
   ///////////////////////////////////////////////////////////////////////////////////////////////
 
   using MapArguments = cutlass::gemm::kernel::detail::MapArguments<
     ElementInputA0,
     LayoutInputA0,
     cutlass::ComplexTransform::kNone,
     128 / cutlass::sizeof_bits<ElementInputA0>::value,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/38_syr2k_grouped/syr2k_grouped.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/39_gemm_permute/gemm_permute.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -220,15 +220,15 @@
     out << "39_gemm_permute\n\n"
       << " 1) This example firstly profiles the performance of a batched GEMM kernel with BMM whole output"
       << " (including output matrices for each batch) as permuted 4D Tensor."
       << " The BMM tensor output in shape of [B, M, N] is reshaped as [B/D1, D1, M, N] and then permuted with"
       << " permute([0, 2, 1, 3]) to be in shape of [B/D1, M, D1, N].\n\n"
       << " 2) This example also profiles the performance of a normal GEMM kernel with output as permuted 5D Tensor."
       << " The GEMM matrix output in shape of [M, N]  is reshaped as [M/T1, T1, T2, T3, N/T2/T3] and then permuted"
-      << " with permute([2, 0, 3, 1, 4]) to be in shape of [T2, M/T1, T3, T1, N/T2/T3].\n\n"
+      << " with permute([2, 0, 3, 1, 4]) to be in shape of [T2, M/T1, T3, T1, N//T2/T3].\n\n"
       << " Note: D1, T1, T2, T3 are compile-time constants defined in gemm_permute.cu\n\n"
       << "Options:\n\n"
       << "  --help                      If specified, displays this usage statement.\n\n"
       << "  --batch-count=<int>         Sets the number of batches in batched GEMM (batch number for BMM). (default: --batch-count=768)\n"
       << "  --m=<int>                   Sets the M dimension for both batched GEMM and normal GEMM problems. (default: --m=128)\n"
       << "  --n=<int>                   Sets the N dimension for both batched GEMM and normal GEMM problems. (default: --n=192)\n"
       << "  --k=<int>                   Sets the K dimension for both batched GEMM and normal GEMM problems. (default: --k=128)\n"
@@ -686,15 +686,15 @@
       problem.n(),
       problem.n()
     };
 
     // Initialize the GEMM object
     GemmBatched gemm;
 
-    result.status = gemm.initialize(arguments, nullptr);
+    result.status = gemm.initialize(arguments);
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS Batched GEMM kernel." << std::endl;
       return result;
     }
 
     // Run the batched GEMM object
@@ -850,15 +850,15 @@
       problem.n(),
       problem.n()
     };
 
     // Initialize the GEMM object
     GemmPermute gemm_normal;
 
-    result.status = gemm_normal.initialize(arguments, nullptr);
+    result.status = gemm_normal.initialize(arguments);
 
     if (result.status != cutlass::Status::kSuccess) {
       std::cerr << "Failed to initialize CUTLASS Batched GEMM kernel." << std::endl;
       return result;
     }
 
     // Run the normal GEMM object
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/attention_scaling_coefs_updater.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/debug_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/default_fmha_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_rescale_output.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/epilogue_thread_apply_logsumexp.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/find_default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fmha_grouped_problem_visitor.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_fixed_seqlen.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/fused_multihead_attention_variable_seqlen.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/custom_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/find_default_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_accum_lambda_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm/mma_from_smem.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/gemm_kernel_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/kernel_forward.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/mma_from_smem.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/output_tile_thread_map_for_fused_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/warp/fused_bias_act_fragment_iterator_tensor_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/gemm/warp/mma_tensor_op_fragment_iterator_without_output_op.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/device/dual_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_common.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/dual_gemm_run.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/kernel/dual_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/test_run.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/13_two_tensor_op_fusion/test_run.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/thread/left_silu_and_mul.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_epilogue.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/45_dual_gemm/threadblock/dual_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/49_hopper_gemm_schedules_with_collective_builder/49_hopper_gemm_schedules_with_collective_builder.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/50_hopper_gemm_with_epilogue_swizzle/50_hopper_gemm_with_epilogue_swizzle.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/60_cutlass_import/main.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/common/helper.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu`

 * *Files 26% similar despite different names*

```diff
@@ -24,84 +24,75 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-#pragma once
+/*! \file
+    \brief Tests for device-wide GEMM interface
+*/
 
-#include "cuda_runtime.h"
+#include <iostream>
 
-/**
- * Panic wrapper for unwinding CUTLASS errors
- */
-#define CUTLASS_CHECK(status)                                                                    \
-  {                                                                                              \
-    cutlass::Status error = status;                                                              \
-    if (error != cutlass::Status::kSuccess) {                                                    \
-      std::cerr << "Got cutlass error: " << cutlassGetStatusString(error) << " at: " << __LINE__ \
-                << std::endl;                                                                    \
-      exit(EXIT_FAILURE);                                                                        \
-    }                                                                                            \
-  }
-
-
-/**
- * Panic wrapper for unwinding CUDA runtime errors
- */
-#define CUDA_CHECK(status)                                              \
-  {                                                                     \
-    cudaError_t error = status;                                         \
-    if (error != cudaSuccess) {                                         \
-      std::cerr << "Got bad cuda status: " << cudaGetErrorString(error) \
-                << " at line: " << __LINE__ << std::endl;               \
-      exit(EXIT_FAILURE);                                               \
-    }                                                                   \
-  }
-
-
-/**
- * GPU timer for recording the elapsed time across kernel(s) launched in GPU stream
- */
-struct GpuTimer
-{
-    cudaStream_t _stream_id;
-    cudaEvent_t _start;
-    cudaEvent_t _stop;
-
-    /// Constructor
-    GpuTimer() : _stream_id(0)
-    {
-        CUDA_CHECK(cudaEventCreate(&_start));
-        CUDA_CHECK(cudaEventCreate(&_stop));
-    }
-
-    /// Destructor
-    ~GpuTimer()
-    {
-        CUDA_CHECK(cudaEventDestroy(_start));
-        CUDA_CHECK(cudaEventDestroy(_stop));
-    }
-
-    /// Start the timer for a given stream (defaults to the default stream)
-    void start(cudaStream_t stream_id = 0)
-    {
-        _stream_id = stream_id;
-        CUDA_CHECK(cudaEventRecord(_start, _stream_id));
-    }
-
-    /// Stop the timer
-    void stop()
-    {
-        CUDA_CHECK(cudaEventRecord(_stop, _stream_id));
-    }
-
-    /// Return the elapsed time (in milliseconds)
-    float elapsed_millis()
-    {
-        float elapsed = 0.0;
-        CUDA_CHECK(cudaEventSynchronize(_stop));
-        CUDA_CHECK(cudaEventElapsedTime(&elapsed, _start, _stop));
-        return elapsed;
-    }
-};
+#include "cutlass/cutlass.h"
+#include "cute/tensor.hpp"
+#include "cute/atom/mma_atom.hpp"
+
+#include "cutlass/numeric_types.h"
+
+#include "cutlass/gemm/device/gemm_universal_adapter.h"
+#include "default_gemm_configuration.hpp"
+
+#include "../../common/cutlass_unit_test.h"
+
+#include "gemm_testbed_3x.hpp"
+
+using namespace cute;
+
+//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Gemm_f64n_f64t_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+
+  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
+    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double>;
+
+  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
+      Shape<int,int,int,int>,
+      Config::CollectiveMainloop,
+      Config::CollectiveEpilogue
+  >;
+
+  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM80_Device_Gemm_f64t_f64n_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+
+  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
+    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
+    double, cutlass::layout::RowMajor,
+    double, cutlass::layout::ColumnMajor,
+    double, cutlass::layout::ColumnMajor,
+    double>;
+
+  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
+      Shape<int,int,int,int>,
+      Config::CollectiveMainloop,
+      Config::CollectiveEpilogue
+  >;
+
+  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
+  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+// #endif
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/cute/tutorial/sgemm_nt_1.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/aligned_buffer.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -76,17 +76,17 @@
 
   typedef T value_type;
   typedef size_t size_type;
   typedef ptrdiff_t difference_type;
   typedef value_type *pointer;
   typedef value_type const * const_pointer;
 
-  using Array = Array<T, N>;
-  using reference = typename Array::reference;
-  using const_reference = typename Array::const_reference;
+  using ArrayType = Array<T, N>;
+  using reference = typename ArrayType::reference;
+  using const_reference = typename ArrayType::const_reference;
 
 public:
 
   CUTLASS_HOST_DEVICE
   pointer data() {
     return reinterpret_cast<pointer>(storage); 
   }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/arch.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,16 +30,14 @@
  **************************************************************************************************/
 /*! \file
     \brief Defines tags for architecture-specific configurations.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace arch {
 
 #if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
 
@@ -83,18 +81,14 @@
 struct Sm80 {
   static int const kMinComputeCapability = 80; 
 };
 struct Sm86 {
   static int const kMinComputeCapability = 86;
 };
 
-struct Sm90 {
-  static int const kMinComputeCapability = 90; 
-};
-
 /// Triggers a breakpoint on the device
 CUTLASS_DEVICE
 void device_breakpoint() {
 #if defined(__CUDA_ARCH__)
   asm volatile ("  brkpt;\n");
 #endif
 }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/cache_operation.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -447,15 +447,15 @@
 }
 
 /// st.shared - 128b
 template <>
 CUTLASS_DEVICE
 void shared_store<16>(uint32_t ptr, void const *src) {
   uint4 const *dst_u128 = reinterpret_cast<uint4 const *>(src);
-  asm volatile("st.shared.v4.u32 [%0], {%1, %2, %3, %4};\n"
+  asm volatile("ld.shared.v4.u32 [%0], {%1, %2, %3, %4};\n"
     : :
       "r"(ptr),
       "r"(dst_u128->x),
       "r"(dst_u128->y),
       "r"(dst_u128->z),
       "r"(dst_u128->w)
     );
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,255 +25,231 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Architecture-specific operators on memory added for SM75
+    \brief Describes the lane policy used by warp-level matrix multiply operators targeting SIMT
+      instructions
 */
 
 #pragma once
 
+#include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-#include "cutlass/layout/matrix.h"
-#include "cute/arch/util.hpp"
-
-namespace cutlass {
-namespace arch {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <
-  /// Layout of destination matrix (column-major implies transpose)
-  typename Layout,
-  /// .x1, .x2, or .x4
-  int MatrixCount
->
-inline __device__ void ldsm(Array<unsigned, MatrixCount> & D, void const* ptr);
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Determine the appropriate way to target PTX's "ldmatrix" instruction.
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-#if (__CUDACC_VER_MAJOR__ == 10 && __CUDACC_VER_MINOR__ >= 2) || (__CUDACC_VER_MAJOR__ >= 11)
-
-#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 750)
-#define CUDA_LDMATRIX_ACTIVATED 1
-#endif
-
-#define CUDA_LDMATRIX_SUPPORTED 1
-#endif
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// CUTLASS helper to get SMEM pointer
-inline __device__ unsigned cutlass_get_smem_pointer(void *ptr) {
-  return cute::cast_smem_ptr_to_uint(ptr);
-}
-
-/// CUTLASS helper to get SMEM pointer
-inline __device__ unsigned cutlass_get_smem_pointer(void const *ptr) {
-  return cutlass_get_smem_pointer(const_cast<void *>(ptr));
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <>
-inline __device__ void ldsm<layout::RowMajor, 1>(
-    Array<unsigned, 1> & D,
-    void const* ptr) {
-
-  #if defined(CUDA_LDMATRIX_ACTIVATED)
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x;
-    asm volatile ("ldmatrix.sync.aligned.x1.m8n8.shared.b16 {%0}, [%1];" : "=r"(x) : "r"(addr));
-    reinterpret_cast<int &>(D) = x;
-
-  #else
-
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
-
-  #endif
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <>
-inline __device__ void ldsm<layout::RowMajor, 2>(
-    Array<unsigned, 2> & D,
-    void const* ptr) {
-
-  #if defined(CUDA_LDMATRIX_ACTIVATED)
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x, y;
-    asm volatile ("ldmatrix.sync.aligned.x2.m8n8.shared.b16 {%0, %1}, [%2];" : "=r"(x), "=r"(y) : "r"(addr));
-    reinterpret_cast<int2 &>(D) = make_int2(x, y);
-
-  #else
-
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
-
-  #endif
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <>
-inline __device__ void ldsm<layout::RowMajor, 4>(
-    Array<unsigned, 4> & D,
-    void const* ptr) {
-
-  #if defined(CUDA_LDMATRIX_ACTIVATED)
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x, y, z, w;
-    asm volatile ("ldmatrix.sync.aligned.x4.m8n8.shared.b16 {%0, %1, %2, %3}, [%4];" : "=r"(x), "=r"(y), "=r"(z), "=r"(w) : "r"(addr));
-    reinterpret_cast<int4 &>(D) = make_int4(x, y, z, w);
-
-  #else
+#include "cutlass/tensor_ref.h"
+#include "cutlass/matrix_shape.h"
 
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
+#include "cutlass/arch/memory_sm75.h"
 
-  #endif
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Transpose on 16b granularity
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <>
-inline __device__ void ldsm<layout::ColumnMajor, 1>(
-    Array<unsigned, 1> & D,
-    void const* ptr) {
-
-  #if CUDA_LDMATRIX_ACTIVATED
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x;
-    asm volatile ("ldmatrix.sync.aligned.x1.trans.m8n8.shared.b16 {%0}, [%1];" : "=r"(x) : "r"(addr));
-    reinterpret_cast<int &>(D) = x;
-
-  #else
-
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
+#include "cutlass/layout/matrix.h"
 
-  #endif
-}
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
+#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <>
-inline __device__ void ldsm<layout::ColumnMajor, 2>(
-    Array<unsigned, 2> & D,
-    void const* ptr) {
-
-  #if defined(CUDA_LDMATRIX_ACTIVATED)
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x, y;
-    asm volatile ("ldmatrix.sync.aligned.x2.trans.m8n8.shared.b16 {%0, %1}, [%2];" : "=r"(x), "=r"(y) : "r"(addr));
-    reinterpret_cast<int2 &>(D) = make_int2(x, y);
-
-  #else
-
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
-
-  #endif
-}
+namespace cutlass {
+namespace conv {
+namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <>
-inline __device__ void ldsm<layout::ColumnMajor, 4>(
-    Array<unsigned, 4> & D,
-    void const* ptr) {
-
-  #if defined(CUDA_LDMATRIX_ACTIVATED)
-
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    int x, y, z, w;
-    asm volatile ("ldmatrix.sync.aligned.x4.trans.m8n8.shared.b16 {%0, %1, %2, %3}, [%4];" : "=r"(x), "=r"(y), "=r"(z), "=r"(w) : "r"(addr));
-    reinterpret_cast<int4 &>(D) = make_int4(x, y, z, w);
-
-  #else
-
-    CUTLASS_UNUSED(D);
-    CUTLASS_UNUSED(ptr);
-    CUTLASS_NOT_IMPLEMENTED();
-
-  #endif
-}
+/// Iterates over operands to warp-level matrix multiply operations targeting SIMT instructions
+///
+/// concept: MutableRandomAccessContiguousTileIteratorConcept
+///
+template <
+  /// Size of the matrix to load (concept: MatrixShape)
+  typename Shape_,
+  /// Operand identity
+  cutlass::gemm::Operand Operand,
+  /// Data type of A elements
+  typename Element_,
+  /// Layout of operand
+  typename Layout_,
+  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+  typename Policy_,
+  /// Number of partitions along K dimension - used in sliced-K
+  int PartitionsK = 1,
+  /// Group Size along kPartition - used in sliced-K
+  int PartitionGroupSize = 1
+>
+class DepthwiseMmaSimtTileIterator;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename AccessType, int Bytes>
-struct shared_load_op {
-  CUTLASS_DEVICE
-  shared_load_op(AccessType &D, void const *ptr) {
-    D = *reinterpret_cast<AccessType const *>(ptr);  
+/// Specialization for B operands of row-major layouts
+///
+/// Concept: MutableRandomAccessContiguousTileIteratorConcept
+///
+template <
+    /// Size of the matrix to load (concept: MatrixShape)
+    typename Shape_,
+    /// Data type of A elements
+    typename Element_,
+    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
+    typename Policy_,
+    /// Number of partitions along K dimension
+    int PartitionsK,
+    /// Group Size along kPartition - used in sliced-K
+    int PartitionGroupSize>
+class DepthwiseMmaSimtTileIterator<Shape_,
+                                   cutlass::gemm::Operand::kB,
+                                   Element_,
+                                   layout::RowMajor,
+                                   Policy_,
+                                   PartitionsK,
+                                   PartitionGroupSize>
+    : public cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
+                                               cutlass::gemm::Operand::kB,
+                                               Element_,
+                                               layout::RowMajor,
+                                               Policy_,
+                                               PartitionsK,
+                                               PartitionGroupSize> {
+
+  using Base = cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
+                                               cutlass::gemm::Operand::kB,
+                                               Element_,
+                                               layout::RowMajor,
+                                               Policy_,
+                                               PartitionsK,
+                                               PartitionGroupSize>;
+ public:
+  /// Shape of tile to load (concept: MatrixShape)
+  using Shape = Shape_;
+
+  /// Operand tag
+  static cutlass::gemm::Operand const kOperand = cutlass::gemm::Operand::kB;
+
+  /// Element type
+  using Element = Element_;
+
+  /// Layout of policy
+  using Layout = layout::RowMajor;
+
+  /// Decomposition of elements among threads
+  using Policy = Policy_;
+
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = typename Base::TensorRef;
+
+  /// Index type
+  using Index = typename TensorRef::Index;
+
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
+
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
+
+  /// Thread-level shape of a fragment
+  using ThreadShape = typename Base::ThreadShape;
+
+  /// Number of individual loads
+  using Iterations =  typename Base::Iterations;
+
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = typename Base::Fragment;
+
+  static_assert(Policy::LaneMmaShape::kN == 1, "Each thread should be 1 element per LDS along the k-dim");
+  
+private:
+
+  MatrixCoord lane_offset_;
+  int channel_idx_;
+  int base_channel_idx_;
+  int warps_n_;
+
+ public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  DepthwiseMmaSimtTileIterator():Base() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  DepthwiseMmaSimtTileIterator(
+    TensorRef ref, 
+    int lane_id
+  ) : Base(ref, lane_id) {
+
+    // compute offset based on thread ID and lane layout
+    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+
+    warps_n_ = -1;
+    channel_idx_ = 0;
+    base_channel_idx_ = 0;
+    lane_offset_ = lane_layout.inverse(lane_id) * MatrixCoord(0, Policy::LaneMmaShape::kN);
   }
-};
-
-template <typename AccessType>
-CUTLASS_DEVICE void shared_load(AccessType &D, void const *ptr) {
-  shared_load_op<AccessType, int(sizeof(AccessType))>(D, ptr);
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename AccessType>
-struct shared_load_op<AccessType, 16> {
-  CUTLASS_DEVICE
-  shared_load_op(AccessType &D, void const *ptr) {
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    uint4 v;
-    asm volatile ("ld.shared.v4.b32 {%0, %1, %2, %3}, [%4];" : 
-      "=r"(v.x), "=r"(v.y), "=r"(v.z), "=r"(v.w) : "r"(addr));
-
-    D = reinterpret_cast<AccessType const &>(v);
+  
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  DepthwiseMmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+
+    if(warps_n_ == -1){
+        warps_n_ = coord.column();
+    }
+    
+    Base::add_tile_offset(coord);
+    return *this;
   }
-};
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr =
+        reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int k = 0; k < Iterations::kRow; ++k) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Iterations::kColumn; ++n) {
+
+        void const *ptr = this->ref_.data() +
+                          this->ref_.offset({-(channel_idx_ - base_channel_idx_),
+                                             n * Policy::WarpShape::kColumn}) +
+                          pointer_offset / Policy::LaneMmaShape::kN;
+
+        // Base_k of a warp +  Base_k of current threads.
+        int thread_k_base_idx =
+            warps_n_ * Shape::kColumn / Policy::LaneMmaShape::kN + lane_offset_.column();
+
+        if (channel_idx_ + k == thread_k_base_idx + n * Policy::WarpShape::kColumn) {
+          // Depthwise kernel would only do computation when channel == k.
+          // Loads an element when the current computation channel == the k corresponding to this thread.
+          arch::shared_load(dst_ptr[n + k * Iterations::kColumn], ptr);
+        } else {
+          // Reduce SMEM load
+          dst_ptr[n + k * Iterations::kColumn].fill(Element(0));
+        }
+      }
+    }
+  }
 
-template <typename AccessType>
-struct shared_load_op<AccessType, 8> {
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
+  
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index
   CUTLASS_DEVICE
-  shared_load_op(AccessType &D, void const *ptr) {
-    unsigned addr = cutlass_get_smem_pointer(ptr);
-
-    uint2 v;
-    asm volatile ("ld.shared.v2.b32 {%0, %1}, [%2];" : 
-      "=r"(v.x), "=r"(v.y) : "r"(addr));
-
-    D = reinterpret_cast<AccessType const &>(v);
+  void set_kgroup_index(int k_group) {
+    if(k_group % PartitionGroupSize == 0 && k_group != 0){
+      base_channel_idx_ = k_group;
+    }
+    channel_idx_ = k_group;
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace arch
+} // namespace warp
+} // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/memory_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,15 +32,14 @@
 /*! \file
     \brief Architecture-specific operators on memory added for SM80
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/complex.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/arch/cache_operation.h"
 
 #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
   #define CUDA_CP_ASYNC_ACTIVATED 1
 #else
@@ -50,51 +49,51 @@
 namespace cutlass {
 namespace arch {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Initiates an asynchronous copy from global memory to shared memory.
 ///
-/// cp.async
+/// LDGSTS
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async;
 
 /// Initiates an asynchronous copy from global memory to shared memory. Rather than predicate
 /// the entire transfer, zeros are written to SMEM if the guard predicate is false.
 ///
-/// cp.async
+/// LDGSTS
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async_zfill;
 
 /// Initiates an asynchronous copy from global memory to shared memory. Rather than predicate
 /// the entire transfer, nans (0x7eff) are written to SMEM if the guard predicate is false.
 ///
-/// cp.async
+/// LDGSTS
 ///
 template <
     /// Size of the access in bytes
     int SizeInBytes,
     /// Cache operation
     CacheOperation::Kind cache_op = CacheOperation::Always>
 struct cp_async_nan;
 
 /// Either 0 or 1 are written to SMEM based on input element type
 /// Used for diagonal elements of triangular matrix of BLAS3 functions
 ///
-/// st.shared
+/// STS
 ///
 template <
    /// Type of Element
    typename Element,
    /// If the data is for a Hermitian matrix diagonal
    bool IsHermitianData = false>
 struct cp_async_diag;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -45,69 +45,69 @@
 
 namespace cutlass {
 namespace arch {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the operation implied by MMA.
-struct OpMultiplyAdd {};
+struct OpMultiplyAdd;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the result is saturated to MAX_FLOAT|MIN_FLOAT or MAX_INT|MIN_INT
-struct OpMultiplyAddSaturate {};
+struct OpMultiplyAddSaturate;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to a narrower type (BF16)
-struct OpMultiplyAddFastBF16 {};
+struct OpMultiplyAddFastBF16;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to a narrower type (F16)
-struct OpMultiplyAddFastF16 {};
+struct OpMultiplyAddFastF16;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the input is converted to 2 (big and small) TF32 components
 //  Perform 3xTF32 or 4xTF32 for every F32 output element
-struct OpMultiplyAddFastF32 {};
+struct OpMultiplyAddFastF32;
 
 /// Tag indicating the input is converted to 2 (big and small) TF32 components
 //  Perform 3xTF32 or 4xTF32 for every complex<F32> output element
-struct OpMultiplyAddComplexFastF32 {};
+struct OpMultiplyAddComplexFastF32;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the complex multiply-add operation
-struct OpMultiplyAddComplex {};
+struct OpMultiplyAddComplex;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the gaussian complex multiply-add operation
-struct OpMultiplyAddGaussianComplex {};
+struct OpMultiplyAddGaussianComplex;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag indicating the inner product is defined by (XOR, POPC)
-struct OpXorPopc {};
+struct OpXorPopc;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tag classifying math operators as thread-level operations.
-struct OpClassSimt {};
+struct OpClassSimt;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tag classifying operators as Tensor Core operations.
-struct OpClassTensorOp {};
+/// Tag classifing operators as Tensor Core operations.
+struct OpClassTensorOp;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/// Tag classifying operators as WMMA Tensor Core operations
-struct OpClassWmmaTensorOp {};
+/// Tag classifing operators as WMMA Tensor Core operations
+struct OpClassWmmaTensorOp;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Matrix multiply-add operation
 template <
   /// Size of the matrix product (concept: GemmShape)
   typename Shape_,
@@ -219,9 +219,8 @@
 #include "cutlass/arch/mma_sm50.h"
 #include "cutlass/arch/mma_sm60.h"
 #include "cutlass/arch/mma_sm61.h"
 #include "cutlass/arch/mma_sm70.h"
 #include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 #include "cutlass/arch/mma_sparse_sm80.h"
-#include "cutlass/arch/mma_sm90.h"
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm50.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm61.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1243,16 +1243,15 @@
     FragmentC &d,
     FragmentA const &a,
     FragmentB const &b,
     FragmentC const &c
   ) const {
 
 #if defined(CUTLASS_ARCH_MMA_SM75_ENABLED)
-
-#if (__CUDA_ARCH__ >= 900) || (defined(CUTLASS_ARCH_WMMA_ENABLED))
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
   using WmmaFragmentA = nvcuda::wmma::fragment<
           nvcuda::wmma::matrix_a,
           Shape::kM,
           Shape::kN,
           Shape::kK,
           nvcuda::wmma::experimental::precision::b1,
           nvcuda::wmma::row_major>;
@@ -1276,26 +1275,32 @@
   WmmaFragmentB const & B = reinterpret_cast<WmmaFragmentB const &>(b);
 
   WmmaFragmentC const & C = reinterpret_cast<WmmaFragmentC const &>(c);
   WmmaFragmentC & D = reinterpret_cast<WmmaFragmentC &>(d);
 
   nvcuda::wmma::bmma_sync(D, A, B, C, nvcuda::wmma::experimental::bmmaBitOpXOR, 
                                           nvcuda::wmma::experimental::bmmaAccumulateOpPOPC);
-
 #else
 
   CUTLASS_UNUSED(a);
   CUTLASS_UNUSED(b);
   CUTLASS_UNUSED(c);
   CUTLASS_UNUSED(d);
   assert(0); // WMMA must be supported to issue binary matrix multiply-accumulate instructions.
 
 #endif // defined(CUTLASS_ARCH_WMMA_ENABLED)
 
+#else
+    CUTLASS_UNUSED(a);
+    CUTLASS_UNUSED(b);
+    CUTLASS_UNUSED(c);
+    CUTLASS_UNUSED(d);
+    assert(0);
 #endif
+
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace arch
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -2152,15 +2152,14 @@
 #if defined(CUTLASS_ARCH_MMA_SM80_ENABLED)
 
     uint32_t const *A = reinterpret_cast<uint32_t const *>(&a);
     uint32_t const *B = reinterpret_cast<uint32_t const *>(&b);
 
     int const *C = reinterpret_cast<int const *>(&c);
     int *D = reinterpret_cast<int *>(&d);
-
     asm volatile(
         "mma.sync.aligned.m16n8k256.row.col.s32.b1.b1.s32.xor.popc {%0,%1,%2,%3}, "
         "{%4,%5,%6,%7}, "
         "{%8,%9}, {%10,%11,%12,%13};\n"
         : "=r"(D[0]), "=r"(D[1]), "=r"(D[2]), "=r"(D[3])
         : "r"(A[0]), "r"(A[1]), "r"(A[2]), "r"(A[3]), "r"(B[0]), "r"(B[1]),
           "r"(C[0]), "r"(C[1]), "r"(C[2]), "r"(C[3]));
@@ -2168,15 +2167,15 @@
 #else
     
     CUTLASS_UNUSED(a);
     CUTLASS_UNUSED(b);
     CUTLASS_UNUSED(c);
     CUTLASS_UNUSED(d);
     assert(0);
-
+    
 #endif // defined(CUTLASS_ARCH_MMA_SM80_ENABLED)
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace arch
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sm90.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/mma_sparse_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/reg_reconfig.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/simd_sm61.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm72.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/arch/wmma_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/functional.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-/***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+  /***************************************************************************************************
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,546 +25,556 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
-           and is safe to use in a union.
+    \brief Define basic numeric operators with specializations for Array<T, N>. SIMD-ize where possible.
+
+    This is inspired by the Standard Library's <functional> header.
 */
 
 #pragma once
+
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
 #include "cutlass/numeric_types.h"
+#include "cutlass/complex.h"
+#include "cutlass/quaternion.h"
+#include "cutlass/array.h"
 #include "cutlass/half.h"
 
 namespace cutlass {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Statically sized array for any data type
-template <
-  typename T,
-  int N,
-  bool RegisterSized = sizeof_bits<T>::value >= 32
->
-class Array;
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Defines the size of an Array<> in bits
-template <typename T, int N, bool RegisterSized>
-struct sizeof_bits<Array<T, N, RegisterSized> > {
-  static int const value =
-    int(sizeof(typename Array<T, N, RegisterSized>::Storage)) * 8 * int(Array<T, N, RegisterSized>::kStorageElements);
+template <typename T>
+struct absolute_value_op {
+  CUTLASS_HOST_DEVICE
+  T operator()(T lhs) const {
+    return abs(lhs);
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Returns true if the argument is a power of 2
-CUTLASS_HOST_DEVICE
-constexpr bool ispow2(unsigned x) {
-  return x && (!(x & (x - 1)));
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Returns the largest power of two not greater than the argument.
-CUTLASS_HOST_DEVICE
-constexpr unsigned floor_pow_2(unsigned x) {
-  return (x == 0 || ispow2(x)) ? x : ((floor_pow_2(x >> 1)) << 1);
-}
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Statically sized array for any data type
-template <
-  typename T,
-  int N
->
-class Array<T, N, true> {
-public:
-
-  /// Storage type
-  using Storage = T;
-
-  /// Element type
-  using Element = T;
-
-  /// Number of storage elements
-  //static std::size_t const kStorageElements = N;
-  static size_t const kStorageElements = N;
-
-  /// Number of logical elements
-  static size_t const kElements = N;
-
-  //
-  // C++ standard members
-  //
-
-  typedef T value_type;
-  typedef size_t size_type;
-  typedef ptrdiff_t difference_type;
-  typedef value_type &reference;
-  typedef value_type const & const_reference;
-  typedef value_type *pointer;
-  typedef value_type const * const_pointer;
-
-  //
-  // Iterators
-  //
-
-  /// Bidirectional iterator over elements
-  class iterator {
-
-    /// Pointer to object
-    T *ptr_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    iterator(): ptr_(nullptr) { }
-
-    CUTLASS_HOST_DEVICE
-    iterator(T *_ptr): ptr_(_ptr) { }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator++() {
-      ++ptr_;
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator &operator--() {
-      --ptr_;
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator++(int) {
-      iterator ret(*this);
-      ++ptr_;
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    iterator operator--(int) {
-      iterator ret(*this);
-      --ptr_;
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    T &operator*() const {
-      return *ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator==(iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(iterator const &other) const {
-      return ptr_ != other.ptr_;
-    }
-  };
-
-  /// Bidirectional constant iterator over elements
-  class const_iterator {
-
-    /// Pointer to object
-    const T *ptr_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    const_iterator(): ptr_(nullptr) { }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator(T const *_ptr): ptr_(_ptr) { }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator &operator++() {
-      ++ptr_;
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator &operator--() {
-      --ptr_;
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator operator++(int) {
-      const_iterator ret(*this);
-      ++ptr_;
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    const_iterator operator--(int) {
-      const_iterator ret(*this);
-      --ptr_;
-      return ret;
-    }
-
-    CUTLASS_HOST_DEVICE
-    T const &operator*() const {
-      return *ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator==(const_iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
-
-    CUTLASS_HOST_DEVICE
-    bool operator!=(const_iterator const &other) const {
-      return ptr_ != other.ptr_;
-    }
-  };
-
-  /// Bidirectional iterator over elements
-  class reverse_iterator {
-
-    /// Pointer to object
-    T *ptr_;
-
-  public:
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator(): ptr_(nullptr) { }
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator(T *_ptr): ptr_(_ptr) { }
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator &operator++() {
-      --ptr_;
-      return *this;
-    }
-
-    CUTLASS_HOST_DEVICE
-    reverse_iterator &operator--() {
-      ++ptr_;
-      return *this;
-    }
+template <typename T>
+struct plus {
+  CUTLASS_HOST_DEVICE
+  T operator()(T lhs, T const &rhs) const {
+    lhs += rhs;
+    return lhs;
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    reverse_iterator operator++(int) {
-      iterator ret(*this);
-      --ptr_;
-      return ret;
-    }
+template <typename T>
+struct minus {
+  CUTLASS_HOST_DEVICE
+  T operator()(T lhs, T const &rhs) const {
+    lhs -= rhs;
+    return lhs;
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    reverse_iterator operator--(int) {
-      iterator ret(*this);
-      ++ptr_;
-      return ret;
-    }
+template <typename T>
+struct multiplies {
+  CUTLASS_HOST_DEVICE
+  T operator()(T lhs, T const &rhs) const {
+    lhs *= rhs;
+    return lhs;
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    T &operator*() const {
-      return *(ptr_ - 1);
-    }
+template <typename T>
+struct multiplies<Quaternion<T>> {
+  CUTLASS_HOST_DEVICE
+  Quaternion<T> operator()(Quaternion<T> lhs, Quaternion<T> const &rhs) const {
+    lhs = lhs * rhs;
+    return lhs;
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    bool operator==(reverse_iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
+/// Squares with optional conversion
+template <typename T, typename Output = T>
+struct square {
+  CUTLASS_HOST_DEVICE
+  Output operator()(T lhs) const {
+    multiplies<Output> mul_op;
 
-    CUTLASS_HOST_DEVICE
-    bool operator!=(reverse_iterator const &other) const {
-      return ptr_ != other.ptr_;
-    }
-  };
+    Output y = Output(lhs);
+    return mul_op(y, y);
+  }
+};
 
-  /// Bidirectional constant iterator over elements
-  class const_reverse_iterator {
+/// Returns the magnitude squared of an element.
+template <typename T, typename Output = T>
+struct magnitude_squared {
+  CUTLASS_HOST_DEVICE
+  Output operator()(T lhs) const {
+    multiplies<Output> mul_op;
 
-    /// Pointer to object
-    T const *ptr_;
+    Output y = Output(lhs);
+    return mul_op(y, y);
+  }
+};
 
-  public:
+/// Squares with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared<complex<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(complex<T> lhs) const {
+    multiplies<Output> mul_op;
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator(): ptr_(nullptr) { }
+    Output y_r = Output(lhs.real());
+    Output y_i = Output(lhs.imag());
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator(T const *_ptr): ptr_(_ptr) { }
+    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator &operator++() {
-      --ptr_;
-      return *this;
-    }
+/// Squares with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared<Quaternion<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(Quaternion<T> lhs) const {
+    multiplies<Output> mul_op;
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator &operator--() {
-      ++ptr_;
-      return *this;
-    }
+    Output y_w = Output(lhs.w());
+    Output y_x = Output(lhs.x());
+    Output y_y = Output(lhs.y());
+    Output y_z = Output(lhs.z());
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator operator++(int) {
-      const_reverse_iterator ret(*this);
-      --ptr_;
-      return ret;
-    }
+    return mul_op(y_w, y_w) + mul_op(y_x, y_x) + mul_op(y_y, y_y) + \
+           mul_op(y_z, y_z);
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    const_reverse_iterator operator--(int) {
-      const_reverse_iterator ret(*this);
-      ++ptr_;
-      return ret;
-    }
+/// Computes the square of a difference with optional conversion
+template <typename T, typename Output = T>
+struct square_difference {
+  CUTLASS_HOST_DEVICE
+  Output operator()(T lhs, T rhs) const {
+    multiplies<Output> mul_op;
 
-    CUTLASS_HOST_DEVICE
-    T const &operator*() const {
-      return *(ptr_ - 1);
-    }
+    Output y = Output(lhs) - Output(rhs);
+    return mul_op(y, y);
+  }
+};
 
-    CUTLASS_HOST_DEVICE
-    bool operator==(const_iterator const &other) const {
-      return ptr_ == other.ptr_;
-    }
+/// Computes the square of a difference with optional conversion
+template <typename T, typename Output = T>
+struct magnitude_squared_difference {
+  CUTLASS_HOST_DEVICE
+  Output operator()(T lhs, T rhs) const {
+    multiplies<Output> mul_op;
 
-    CUTLASS_HOST_DEVICE
-    bool operator!=(const_iterator const &other) const {
-      return ptr_ != other.ptr_;
-    }
-  };
+    Output y = Output(lhs) - Output(rhs);
+    return mul_op(y, y);
+  }
+};
 
-private:
+/// Computes the square of a difference with optional conversion
+template <typename T, typename Output>
+struct magnitude_squared_difference<complex<T>, Output> {
+  CUTLASS_HOST_DEVICE
+  Output operator()(complex<T> lhs, complex<T> rhs) const {
+    multiplies<Output> mul_op;
 
-  /// Internal storage
-  Storage storage[kElements];
+    Output y_r = Output(lhs.real()) - Output(rhs.real());
+    Output y_i = Output(lhs.imag()) - Output(rhs.imag());
 
-public:
+    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
+  }
+};
 
-  #if 0
+template <typename T>
+struct divides {
   CUTLASS_HOST_DEVICE
-  Array() { }
+  T operator()(T lhs, T const &rhs) const {
+    lhs /= rhs;
+    return lhs;
+  }
+};
+
 
+template <typename T>
+struct negate {
   CUTLASS_HOST_DEVICE
-  Array(Array const &x) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
-      storage[i] = x.storage[i];
-    }
+  T operator()(T lhs) const {
+    return -lhs;
   }
-  #endif
+};
 
-  /// Efficient clear method
+/// Greater equal 
+template <typename T>
+struct greater_equal {
   CUTLASS_HOST_DEVICE
-  void clear() {
-    fill(T(0));
+  bool operator()(T const &lhs, T const &rhs) const {
+    return (lhs >= rhs);
   }
+};
 
+/// Greater  
+template <typename T>
+struct greater {
   CUTLASS_HOST_DEVICE
-  reference at(size_type pos) {
-    return reinterpret_cast<reference>(storage[pos]);
+  bool operator()(T const &lhs, T const &rhs) const {
+    return (lhs > rhs);
   }
+};
 
+/// Less equal 
+template <typename T>
+struct less_equal {
   CUTLASS_HOST_DEVICE
-  const_reference at(size_type pos) const {
-    return reinterpret_cast<const_reference>(storage[pos]);
+  bool operator()(T const &lhs, T const &rhs) const {
+    return (lhs <= rhs);
   }
+};
 
+/// Less  
+template <typename T>
+struct less {
   CUTLASS_HOST_DEVICE
-  reference operator[](size_type pos) {
-    return reinterpret_cast<reference>(storage[pos]);
+  bool operator()(T const &lhs, T const &rhs) const {
+    return (lhs < rhs);
   }
+};
+
+template <typename T>
+struct maximum {
 
   CUTLASS_HOST_DEVICE
-  const_reference operator[](size_type pos) const {
-    return reinterpret_cast<const_reference>(storage[pos]);
+  T operator()(T const &lhs, T const &rhs) const {
+    return (lhs < rhs ? rhs : lhs);
   }
+};
 
+template <>
+struct maximum<float> {
   CUTLASS_HOST_DEVICE
-  reference front() {
-    return reinterpret_cast<reference>(storage[0]);
+  float operator()(float const &lhs, float const &rhs) const {
+    return fmaxf(lhs, rhs);
   }
+};
+
+template <typename T>
+struct minimum {
 
   CUTLASS_HOST_DEVICE
-  const_reference front() const {
-    return reinterpret_cast<const_reference>(storage[0]);
+  T operator()(T const &lhs, T const &rhs) const {
+    return (rhs < lhs ? rhs : lhs);
   }
+};
 
+template <>
+struct minimum<float> {
   CUTLASS_HOST_DEVICE
-  reference back() {
-    return reinterpret_cast<reference>(storage[kStorageElements - 1]);
+  float operator()(float const &lhs, float const &rhs) const {
+    return fminf(lhs, rhs);
   }
+};
 
+/// Fused multiply-add
+template <typename A, typename B = A, typename C = A>
+struct multiply_add {
   CUTLASS_HOST_DEVICE
-  const_reference back() const {
-    return reinterpret_cast<const_reference>(storage[kStorageElements - 1]);
+  C operator()(A const &a, B const &b, C const &c) const {
+    return C(a) * C(b) + c;
   }
+};
 
+/// Fused multiply-add
+template <typename A, typename B = A, typename C = A>
+struct multiply_add_relu0 {
   CUTLASS_HOST_DEVICE
-  pointer data() {
-    return reinterpret_cast<pointer>(storage);
+  C operator()(A const &a, B const &b, C const &c) const {
+    maximum<C> mx;
+    return mx(C(a) * C(b) + c, C(0));
   }
+};
 
+/// Fused multiply-add
+template <typename T>
+struct and_add {
   CUTLASS_HOST_DEVICE
-  const_pointer data() const {
-    return reinterpret_cast<const_pointer>(storage);
+  T operator()(T const &a, T const &b, T const &c) const {
+    return ((a & b) + c);
   }
-  
+};
+
+
+/// Fused multiply-add
+template <typename T>
+struct xor_add {
   CUTLASS_HOST_DEVICE
-  pointer raw_data() {
-    return reinterpret_cast<pointer>(storage);
+  T operator()(T const &a, T const &b, T const &c) const {
+    return ((a ^ b) + c);
   }
+};
 
+template <typename T>
+struct conjugate {
   CUTLASS_HOST_DEVICE
-  const_pointer raw_data() const {
-    return reinterpret_cast<const_pointer>(storage);
+  T operator()(T const &a) const {
+    return a;
   }
+};
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+template <typename T>
+struct logical_and {
   CUTLASS_HOST_DEVICE
-  constexpr bool empty() const {
-    return !kElements;
+  T operator()(T const &a, T const &b) const {
+    return ((a && b) ? T(1) : T());
   }
+};
 
+template <typename T>
+struct logical_or {
   CUTLASS_HOST_DEVICE
-  constexpr size_type size() const {
-    return kElements;
+  T operator()(T const &a, T const &b) const {
+    return ((a || b) ? T(1) : T());
   }
+};
 
+template <typename T>
+struct logical_not {
   CUTLASS_HOST_DEVICE
-  constexpr size_type max_size() const {
-    return kElements;
+  T operator()(T const &a) const {
+    return T(!(a));
   }
+};
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename T>
+struct bit_and {
   CUTLASS_HOST_DEVICE
-  void fill(T const &value) {
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kElements; ++i) {
-      storage[i] = static_cast<Storage>(value);
-    }
+  T operator()(T const &a, T const &b) const {
+    return a & b;
   }
+};
 
+template <typename T>
+struct bit_or {
   CUTLASS_HOST_DEVICE
-  iterator begin() {
-    return iterator(storage);
+  T operator()(T const &a, T const &b) const {
+    return a | b;
   }
+};
 
+template <typename T>
+struct bit_not {
   CUTLASS_HOST_DEVICE
-  const_iterator begin() const {
-    return cbegin();
+  T operator()(T const &a) const {
+    return ~a;
   }
+};
 
+template <typename T>
+struct bit_xor {
   CUTLASS_HOST_DEVICE
-  const_iterator cbegin() const {
-    return const_iterator(storage);
+  T operator()(T const &a, T const &b) const {
+    return a ^ b;
   }
+};
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+// Partial specializations for Arrays
+template <int N>
+struct bit_and<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  iterator end() {
-    return iterator(reinterpret_cast<pointer>(storage + kStorageElements));
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] & b_data[i]);
+    }
+
+    return result;
   }
+};
 
+// Partial specializations for Arrays
+template <int N>
+struct bit_or<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  const_iterator end() const {
-    return cend();
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] | b_data[i]);
+    }
+
+    return result;
   }
+};
 
+// Partial specializations for Arrays
+template <int N>
+struct bit_not<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  const_iterator cend() const {
-    return const_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (~a_data[i]);
+    }
+
+    return result;
   }
+};
 
+// Partial specializations for Arrays
+template <int N>
+struct bit_xor<Array<uint1b_t, N>> {
   CUTLASS_HOST_DEVICE
-  reverse_iterator rbegin() {
-    return reverse_iterator(reinterpret_cast<pointer>(storage + kStorageElements));
+  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
+    using ArrayType = Array<uint1b_t, N>;
+    using Storage = typename ArrayType::Storage;
+    ArrayType result;
+
+    Storage *result_data = result.raw_data();
+    Storage const *a_data = a.raw_data();
+    Storage const *b_data = b.raw_data();
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
+      result_data[i] = (a_data[i] ^ b_data[i]);
+    }
+
+    return result;
   }
+};
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename T>
+struct conjugate<complex<T>>  {
   CUTLASS_HOST_DEVICE
-  const_reverse_iterator rbegin() const {
-    return crbegin();
+  complex<T> operator()(complex<T> const &a) const {
+    return conj(a);
   }
+};
 
+template <typename T, int N>
+struct conjugate<Array<T, N> >  {
   CUTLASS_HOST_DEVICE
-  const_reverse_iterator crbegin() const {
-    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage + kStorageElements));
+  Array<T, N> operator()(Array<T, N> const &a) const {
+
+    conjugate<T> conj_op;
+
+    Array<T, N> ca;
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < N; ++i) {
+      ca[i] = conj_op(a[i]);
+    }
+    return ca;
   }
+};
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specialization for complex<T> to target four scalar fused multiply-adds.
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<complex<T>, complex<T>, complex<T>> {
   CUTLASS_HOST_DEVICE
-  reverse_iterator rend() {
-    return reverse_iterator(reinterpret_cast<pointer>(storage));
+  complex<T> operator()(
+    complex<T> const &a, 
+    complex<T> const &b, 
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a.real() * b.real();
+    real += -a.imag() * b.imag();
+    imag += a.real() * b.imag();
+    imag += a.imag () * b.real();
+
+    return complex<T>{
+      real,
+      imag
+    };
   }
+};
 
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<complex<T>, T, complex<T>> {
   CUTLASS_HOST_DEVICE
-  const_reverse_iterator rend() const {
-    return crend();
+  complex<T> operator()(
+    complex<T> const &a, 
+    T const &b, 
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a.real() * b;
+    imag += a.imag () * b;
+
+    return complex<T>{
+      real,
+      imag
+    };
   }
+};
 
+/// Fused multiply-add
+template <typename T>
+struct multiply_add<T, complex<T>, complex<T>> {
   CUTLASS_HOST_DEVICE
-  const_reverse_iterator crend() const {
-    return const_reverse_iterator(reinterpret_cast<const_pointer>(storage));
+  complex<T> operator()(
+    T const &a, 
+    complex<T> const &b, 
+    complex<T> const &c) const {
+
+    T real = c.real();
+    T imag = c.imag();
+
+    real += a * b.real();
+    imag += a * b.imag();
+
+    return complex<T>{
+      real,
+      imag
+    };
   }
-
-  //
-  // Comparison operators
-  //
-
 };
 
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-// Factories
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 1> make_Array(Element x) {
-  Array<Element, 1> m;
-  m[0] = x;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 2> make_Array(Element x, Element y) {
-  Array<Element, 2> m;
-  m[0] = x;
-  m[1] = y;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 3> make_Array(Element x, Element y, Element z) {
-  Array<Element, 3> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  return m;
-}
-
-template <typename Element>
-CUTLASS_HOST_DEVICE
-Array<Element, 4> make_Array(Element x, Element y, Element z, Element w) {
-  Array<Element, 4> m;
-  m[0] = x;
-  m[1] = y;
-  m[2] = z;
-  m[3] = w;
-  return m;
-}
-
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
-// functional.h numeric specializations
+//
+// Partial specializations for Array<T, N>
+//
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T, int N>
 struct absolute_value_op< Array<T, N> > {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs) const {
@@ -581,43 +591,43 @@
   }
 };
 
 template <typename T, int N>
 struct plus<Array<T, N>> {
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     plus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -626,43 +636,43 @@
   }
 };
 template <typename T, int N>
 struct minus<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     minus<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -672,43 +682,43 @@
 };
 
 template <typename T, int N>
 struct multiplies<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     multiplies<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -718,43 +728,43 @@
 };
 
 template <typename T, int N>
 struct divides<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     divides<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -764,43 +774,43 @@
 };
 
 template <typename T, int N>
 struct maximum<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     maximum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -815,43 +825,43 @@
   CUTLASS_HOST_DEVICE
   static T scalar_op(T const &lhs, T const &rhs) {
     return (rhs < lhs ? rhs : lhs);
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], rhs[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs, T const &scalar) const {
-
+    
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i], scalar);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()( T const &scalar, Array<T, N> const &rhs) const {
-
+    
     Array<T, N> result;
     minimum<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, rhs[i]);
     }
@@ -861,15 +871,15 @@
 };
 
 template <typename T, int N>
 struct negate<Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &lhs) const {
-
+    
     Array<T, N> result;
     negate<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(lhs[i]);
     }
@@ -880,43 +890,43 @@
 
 /// Fused multiply-add
 template <typename T, int N>
 struct multiply_add<Array<T, N>, Array<T, N>, Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, Array<T, N> const &b, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(a[i], b[i], c[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, T const &scalar, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(a[i], scalar, c[i]);
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(T const &scalar, Array<T, N> const &b, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = scalar_op(scalar, b[i], c[i]);
     }
@@ -927,78 +937,62 @@
 
 /// Fused multiply-add-relu0
 template <typename T, int N>
 struct multiply_add_relu0<Array<T, N>, Array<T, N>, Array<T, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, Array<T, N> const &b, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(a[i], b[i], c[i]), T(0));
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &a, T const &scalar, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(a[i], scalar, c[i]), T(0));
     }
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(T const &scalar, Array<T, N> const &b, Array<T, N> const &c) const {
-
+    
     Array<T, N> result;
     multiply_add<T> scalar_op;
     maximum<T> mx;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
       result[i] = mx(scalar_op(scalar, b[i], c[i]), T(0));
     }
 
     return result;
   }
 };
 
-
-template <typename T, int N>
-struct conjugate<Array<T, N> >  {
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &a) const {
-
-    conjugate<T> conj_op;
-
-    Array<T, N> ca;
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N; ++i) {
-      ca[i] = conj_op(a[i]);
-    }
-    return ca;
-  }
-};
-
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
-// functional.h numeric specializations targeting SIMD instructions in device code.
+//
+// Partial specializations for Array<half_t, N> targeting SIMD instructions in device code.
+//
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <int N>
 struct plus<Array<half_t, N>> {
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(Array<half_t, N> const & lhs, Array<half_t, N> const &rhs) const {
     Array<half_t, N> result;
@@ -1246,15 +1240,15 @@
       result_ptr[i] = __hmul2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmul(
-        reinterpret_cast<__half const &>(lhs),
+        reinterpret_cast<__half const &>(lhs), 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1281,15 +1275,15 @@
       result_ptr[i] = __hmul2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmul(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1320,15 +1314,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hdiv(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1355,15 +1349,15 @@
       result_ptr[i] = __h2div(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hdiv(
-        reinterpret_cast<__half const &>(lhs),
+        reinterpret_cast<__half const &>(lhs), 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1390,15 +1384,15 @@
       result_ptr[i] = __h2div(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hdiv(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1447,18 +1441,18 @@
 
 /// Fused multiply-add
 template <int N>
 struct multiply_add<Array<half_t, N>, Array<half_t, N>, Array<half_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    Array<half_t, N> const &b,
+    Array<half_t, N> const &a, 
+    Array<half_t, N> const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1471,16 +1465,16 @@
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1],
-        b_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
+        b_residual_ptr[N - 1], 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1493,18 +1487,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    half_t const &a,
-    Array<half_t, N> const &b,
+    half_t const &a, 
+    Array<half_t, N> const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 a_pair = __half2half2(reinterpret_cast<__half const &>(a));
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1515,16 +1509,16 @@
     }
 
     if (N % 2) {
 
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
       __half d_residual = __hfma(
-        reinterpret_cast<__half const &>(a),
-        b_residual_ptr[N - 1],
+        reinterpret_cast<__half const &>(a), 
+        b_residual_ptr[N - 1], 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1537,18 +1531,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    half_t const &b,
+    Array<half_t, N> const &a, 
+    half_t const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 b_pair = __half2half2(reinterpret_cast<__half const &>(b));
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1560,16 +1554,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1],
-        reinterpret_cast<__half const &>(b),
+        a_residual_ptr[N - 1], 
+        reinterpret_cast<__half const &>(b), 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1582,18 +1576,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    Array<half_t, N> const &b,
+    Array<half_t, N> const &a, 
+    Array<half_t, N> const &b, 
     half_t const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 530)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 c_pair = __half2half2(reinterpret_cast<__half const &>(c));
@@ -1605,16 +1599,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
 
       __half d_residual = __hfma(
-        a_residual_ptr[N - 1],
-        b_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
+        b_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(c));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1632,18 +1626,18 @@
 
 /// Fused multiply-add-relu0
 template <int N>
 struct multiply_add_relu0<Array<half_t, N>, Array<half_t, N>, Array<half_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    Array<half_t, N> const &b,
+    Array<half_t, N> const &a, 
+    Array<half_t, N> const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1656,16 +1650,16 @@
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1],
-        b_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
+        b_residual_ptr[N - 1], 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1679,18 +1673,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    half_t const &a,
-    Array<half_t, N> const &b,
+    half_t const &a, 
+    Array<half_t, N> const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 a_pair = __half2half2(reinterpret_cast<__half const &>(a));
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1701,16 +1695,16 @@
     }
 
     if (N % 2) {
 
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
       __half d_residual = __hfma_relu(
-        reinterpret_cast<__half const &>(a),
-        b_residual_ptr[N - 1],
+        reinterpret_cast<__half const &>(a), 
+        b_residual_ptr[N - 1], 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1724,18 +1718,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    half_t const &b,
+    Array<half_t, N> const &a, 
+    half_t const &b, 
     Array<half_t, N> const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 b_pair = __half2half2(reinterpret_cast<__half const &>(b));
     __half2 const *c_ptr = reinterpret_cast<__half2 const *>(&c);
@@ -1747,16 +1741,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *c_residual_ptr = reinterpret_cast<__half const *>(&c);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1],
-        reinterpret_cast<__half const &>(b),
+        a_residual_ptr[N - 1], 
+        reinterpret_cast<__half const &>(b), 
         c_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1770,18 +1764,18 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(
-    Array<half_t, N> const &a,
-    Array<half_t, N> const &b,
+    Array<half_t, N> const &a, 
+    Array<half_t, N> const &b, 
     half_t const &c) const {
-
+    
     Array<half_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     __half2 *result_ptr = reinterpret_cast<__half2 *>(&result);
     __half2 const *a_ptr = reinterpret_cast<__half2 const *>(&a);
     __half2 const *b_ptr = reinterpret_cast<__half2 const *>(&b);
     __half2 c_pair = __half2half2(reinterpret_cast<__half const &>(c));
@@ -1793,16 +1787,16 @@
 
     if (N % 2) {
 
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&a);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&b);
 
       __half d_residual = __hfma_relu(
-        a_residual_ptr[N - 1],
-        b_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
+        b_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(c));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1836,15 +1830,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmin(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1871,15 +1865,15 @@
       result_ptr[i] = __hmin2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmin(
-        reinterpret_cast<__half const &>(lhs),
+        reinterpret_cast<__half const &>(lhs), 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1906,15 +1900,15 @@
       result_ptr[i] = __hmin2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmin(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1945,15 +1939,15 @@
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmax(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -1980,15 +1974,15 @@
       result_ptr[i] = __hmax2(lhs_pair, rhs_ptr[i]);
     }
 
     if (N % 2) {
       __half const *b_residual_ptr = reinterpret_cast<__half const *>(&rhs);
 
       __half d_residual = __hmax(
-        reinterpret_cast<__half const &>(lhs),
+        reinterpret_cast<__half const &>(lhs), 
         b_residual_ptr[N - 1]);
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -2015,15 +2009,15 @@
       result_ptr[i] = __hmax2(lhs_ptr[i], rhs_pair);
     }
 
     if (N % 2) {
       __half const *a_residual_ptr = reinterpret_cast<__half const *>(&lhs);
 
       __half d_residual = __hmax(
-        a_residual_ptr[N - 1],
+        a_residual_ptr[N - 1], 
         reinterpret_cast<__half const &>(rhs));
 
       result[N - 1] = reinterpret_cast<half_t const &>(d_residual);
     }
 
     #else
 
@@ -2033,49 +2027,51 @@
     }
     #endif
 
     return result;
   }
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /// Fused multiply-add
 template <int N>
 struct multiply_add<Array<bfloat16_t, N>, Array<bfloat16_t, N>, Array<bfloat16_t, N>> {
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a,
-    Array<bfloat16_t, N> const &b,
+    Array<bfloat16_t, N> const &a, 
+    Array<bfloat16_t, N> const &b, 
     Array<bfloat16_t, N> const &c) const {
-
+    
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
-        : "=r"(result_ptr[i])
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
+        : "=r"(result_ptr[i]) 
         : "r"(a_ptr[i]), "r"(b_ptr[i]), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
-        : "=h"(result_ptr[N - 1])
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
+        : "=h"(result_ptr[N - 1]) 
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2087,46 +2083,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    bfloat16_t const &a,
-    Array<bfloat16_t, N> const &b,
+    bfloat16_t const &a, 
+    Array<bfloat16_t, N> const &b, 
     Array<bfloat16_t, N> const &c) const {
-
+    
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
 
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     unsigned a_packed = static_cast<unsigned>(a.raw());
     a_packed = (a_packed | (a_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
-        : "=r"(result_ptr[i])
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
+        : "=r"(result_ptr[i]) 
         : "r"(a_packed), "r"(b_ptr[i]), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
-        : "=h"(result_ptr[N - 1])
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
+        : "=h"(result_ptr[N - 1]) 
         : "h"(a_residual_ptr[0]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2138,46 +2134,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a,
-    bfloat16_t const &b,
+    Array<bfloat16_t, N> const &a, 
+    bfloat16_t const &b, 
     Array<bfloat16_t, N> const &c) const {
-
+    
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
-
+    
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *c_ptr = reinterpret_cast<unsigned const *>(&c);
 
     unsigned b_packed = static_cast<unsigned>(b.raw());
     b_packed = (b_packed | (b_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
-        : "=r"(result_ptr[i])
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
+        : "=r"(result_ptr[i]) 
         : "r"(a_ptr[i]), "r"(b_packed), "r"(c_ptr[i])
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
-        : "=h"(result_ptr[N - 1])
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
+        : "=h"(result_ptr[N - 1]) 
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[0]), "h"(c_residual_ptr[N - 1])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2189,46 +2185,46 @@
     #endif
 
     return result;
   }
 
   CUTLASS_HOST_DEVICE
   Array<bfloat16_t, N> operator()(
-    Array<bfloat16_t, N> const &a,
-    Array<bfloat16_t, N> const &b,
+    Array<bfloat16_t, N> const &a, 
+    Array<bfloat16_t, N> const &b, 
     bfloat16_t const &c) const {
-
+    
     Array<bfloat16_t, N> result;
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
     unsigned *result_ptr = reinterpret_cast<unsigned *>(&result);
-
+    
     unsigned const *a_ptr = reinterpret_cast<unsigned const *>(&a);
     unsigned const *b_ptr = reinterpret_cast<unsigned const *>(&b);
 
     unsigned c_packed = static_cast<unsigned>(c.raw());
     c_packed = (c_packed | (c_packed << 16));
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N / 2; ++i) {
-      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n"
-        : "=r"(result_ptr[i])
+      asm ("fma.rn.bf16x2 %0, %1, %2, %3;\n" 
+        : "=r"(result_ptr[i]) 
         : "r"(a_ptr[i]), "r"(b_ptr[i]), "r"(c_packed)
       );
     }
 
     if (N % 2) {
 
       uint16_t *result_ptr = reinterpret_cast<uint16_t *>(&result);
       uint16_t const *a_residual_ptr = reinterpret_cast<uint16_t const *>(&a);
       uint16_t const *b_residual_ptr = reinterpret_cast<uint16_t const *>(&b);
       uint16_t const *c_residual_ptr = reinterpret_cast<uint16_t const *>(&c);
 
-      asm ("fma.rn.bf16 %0, %1, %2, %3;\n"
-        : "=h"(result_ptr[N - 1])
+      asm ("fma.rn.bf16 %0, %1, %2, %3;\n" 
+        : "=h"(result_ptr[N - 1]) 
         : "h"(a_residual_ptr[N - 1]), "h"(b_residual_ptr[N - 1]), "h"(c_residual_ptr[0])
       );
     }
 
     #else
 
     multiply_add<bfloat16_t> op;
@@ -2239,110 +2235,17 @@
     }
     #endif
 
     return result;
   }
 };
 
-
-/// bit_and
-template <int N>
-struct bit_and<Array<uint1b_t, N>> {
-  CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] & b_data[i]);
-    }
-
-    return result;
-  }
-};
-
-
-/// bit_or
-template <int N>
-struct bit_or<Array<uint1b_t, N>> {
-  CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] | b_data[i]);
-    }
-
-    return result;
-  }
-};
-
-
-/// bit_not
-template <int N>
-struct bit_not<Array<uint1b_t, N>> {
-  CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (~a_data[i]);
-    }
-
-    return result;
-  }
-};
-
-
-/// bit_xor
-template <int N>
-struct bit_xor<Array<uint1b_t, N>> {
-  CUTLASS_HOST_DEVICE
-  Array<uint1b_t, N> operator()(Array<uint1b_t, N> const &a, Array<uint1b_t, N> const &b) const {
-    using ArrayType = Array<uint1b_t, N>;
-    using Storage = typename ArrayType::Storage;
-    ArrayType result;
-
-    Storage *result_data = result.raw_data();
-    Storage const *a_data = a.raw_data();
-    Storage const *b_data = b.raw_data();
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < ArrayType::kStorageElements; ++i) {
-      result_data[i] = (a_data[i] ^ b_data[i]);
-    }
-
-    return result;
-  }
-};
-
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-// Operator overloads
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+
 template <typename T, int N>
 CUTLASS_HOST_DEVICE
 Array<T, N> operator+(Array<T, N> const &lhs, Array<T, N> const &rhs) {
   plus<Array<T, N>> op;
   return op(lhs, rhs);
 }
 
@@ -2413,45 +2316,57 @@
 CUTLASS_HOST_DEVICE
 Array<T, N> fma(Array<T, N> const &a, Array<T, N> const &b, T c) {
   multiply_add<Array<T, N>> op;
   return op(a, b, c);
 }
 
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-
-} // namespace cutlass
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-#include "cutlass/array_subbyte.h"
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-namespace cutlass {
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Partial specializations for Quaternion<T> fused multiply-add
+//
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-// AlignedArray
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Aligned array type
-template <
-  /// Element type
-  typename T,
-  /// Number of elements in the array
-  int N,
-  /// Alignment requirement in bytes
-  int Alignment = sizeof_bits<T>::value * N / 8
->
-class alignas(Alignment) AlignedArray: public Array<T, N> {
-public:
+template <typename T>
+struct multiply_add<Quaternion<T>, Quaternion<T>, Quaternion<T>> {
+  CUTLASS_HOST_DEVICE
+  Quaternion<T> operator()(
+    Quaternion<T> const &a,
+    Quaternion<T> const &b,
+    Quaternion<T> const &c) const {
+
+    T x = c.x();
+    T y = c.y();
+    T z = c.z();
+    T w = c.w();
+
+    x += a.w() * b.x();
+    x += b.w() * a.x();
+    x += a.y() * b.z();
+    x += -a.z() * b.y(),
+
+    y += a.w() * b.y();
+    y += b.w() * a.y();
+    y += a.z() * b.x();
+    y += -a.x() * b.z();
+
+    z += a.w() * b.z();
+    z += b.w() * a.z();
+    z += a.x() * b.y();
+    z += -a.y() * b.x();
+
+    w += a.w() * b.w();
+    w += -a.x() * b.x();
+    w += -a.y() * b.y();
+    w += -a.z() * b.z();
+    
+    return cutlass::make_Quaternion(x, y, z, w);
 
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace cutlass
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+} // namespace cutlass
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_planar_complex.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/array_subbyte.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -366,14 +366,16 @@
   public:
 
     CUTLASS_HOST_DEVICE
     reverse_iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
     reverse_iterator(Storage *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
+
+    // TODO
   };
 
   /// Bidirectional constant iterator over elements
   class const_reverse_iterator {
 
     /// Pointer to storage element
     Storage const *ptr_;
@@ -384,14 +386,16 @@
   public:
 
     CUTLASS_HOST_DEVICE
     const_reverse_iterator(): ptr_(nullptr), idx_(0) { }
 
     CUTLASS_HOST_DEVICE
     const_reverse_iterator(Storage const *ptr, int idx = 0): ptr_(ptr), idx_(idx) { }
+
+    // TODO
   };
 
 private:
 
   /// Internal storage
   Storage storage[kStorageElements];
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/barrier.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/barrier.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/bfloat16.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,17 +31,15 @@
 /*!
     \file
     \brief Defines a proxy class for storing non-standard 16-bit floating point values with
           8 bits of exponent and 7 bit of mantissa.
 */
 #pragma once
 
-#if defined(__CUDACC_RTC__)
-#include "cutlass/floating_point_nvrtc.h"
-#else
+#if !defined(__CUDACC_RTC__)
 #include <cmath>
 #include <limits>
 #include <cstdint>
 #include <cstring>
 #endif
 
 #include "cutlass/cutlass.h"
@@ -69,15 +67,16 @@
   static bfloat16_t bitcast(uint16_t x) {
     bfloat16_t h;
     h.storage = x;
     return h;
   }
 
   /// Default constructor
-  bfloat16_t() = default;
+  CUTLASS_HOST_DEVICE
+  bfloat16_t() : storage(0) { }
 
   /// Floating-point conversion - round toward nearest
   CUTLASS_HOST_DEVICE
   explicit bfloat16_t(float x) {
 
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800) && (__CUDACC_VER_MAJOR__ >= 11)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/blas3.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/blas3.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -36,15 +36,14 @@
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/coord.h"
-#include "cutlass/complex.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/block_striped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/block_striped.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/complex.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,42 +28,37 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 #pragma once
 
 #include <cuComplex.h>
 
-#include <cuda_fp16.h>
-
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cstdint>
 #else
 #include <cstdint>
 #endif
 
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
 #include "cutlass/half.h"
 #include "cutlass/real.h"
 
 #include "cutlass/bfloat16.h"
 #include "cutlass/tfloat32.h"
 
 #include "cutlass/fast_math.h"
 
 #if !defined(__CUDACC_RTC__)
 #include <iosfwd>
 #endif
 
 namespace cutlass {
 
+//////////////////////////////////////////////////////////////////////////////////////////////////
 
-
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 /// Enumeraed type describing a transformation on a complex value.
 enum class ComplexTransform {
   kNone,
   kConjugate
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -148,26 +143,23 @@
 
  public:
 
 //
 // Methods
 //
 
-  /// Default constructor
-  complex() = default;
-
-  /// Constructor
+/// Constructor
   CUTLASS_HOST_DEVICE
-  complex(T r) : _real(r), _imag(T(0)) {}
+  complex(T r = T(0)) : _real(r), _imag(T(0)) {}
 
-  /// Constructor
+/// Constructor
   CUTLASS_HOST_DEVICE
   complex(T r, T i) : _real(r), _imag(i) {}
-
-  /// Constructor
+  //
+/// Constructor
   template<typename A>
   CUTLASS_HOST_DEVICE
   complex(complex<A> const &z) : _real(static_cast<T>(z.real())), _imag(static_cast<T>(z.imag())) {}
 
 
   #if !defined(__CUDACC_RTC__)
   /// Conversion from cuFloatComplex
@@ -201,32 +193,14 @@
 
   /// Addition
     template <typename A>
   CUTLASS_HOST_DEVICE complex<T> operator+(complex<A> const &rhs) const {
     return complex<T>(this->real() + rhs.real(), this->imag() + rhs.imag());
   }
 
-  /// Reduction into memory address.  Components may update out of order.
-  template <typename OtherT>
-  CUTLASS_DEVICE void red(complex<OtherT> *ptr) const {
-    static_assert(platform::is_same<T, OtherT>::value, "Component type must match");
-    cutlass::red<T> reduce;
-    reduce(&ptr->_real, _real);
-    reduce(&ptr->_imag, _imag);
-  }
-
-  /// Reduction into memory address.  Components may update out of order.  (Half specialization)
-  CUTLASS_DEVICE void red(complex<half_t> *ptr) const {
-    static_assert(platform::is_same<T, half_t>::value, "Component type must match");
-    half2 *h2_ptr = reinterpret_cast<half2*>(ptr);
-    half2 h2_data = reinterpret_cast<half2&>(*this);
-    cutlass::red<half2> reduce;
-    reduce(h2_ptr, h2_data);
-  }
-
   /// Subtraction
     template <typename A>
   CUTLASS_HOST_DEVICE complex<T> operator-(complex<A> const &rhs) const {
     return complex<T>(this->real() - rhs.real(), this->imag() - rhs.imag());
   }
 
   /// Multiplication
@@ -528,22 +502,21 @@
   return true; 
 }
 
 //////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex-valued type.
 template <typename T>
-struct RealType< complex<T> >
-{
+struct RealType< complex<T> > {
   using Type = T;
 
   /// Number of elements
   static int const kExtent = 2;
 
-  CUTLASS_HOST_DEVICE
+CUTLASS_HOST_DEVICE
   static complex<T> from_real(double x) {
     return complex<T>(static_cast<T>(x));
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -573,133 +546,12 @@
 };
 
 template <typename T>
 struct is_complex<complex<T>> {
   static bool const value = true;
 };
 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-// functional.h numeric specializations
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Squares with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared<complex<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(complex<T> lhs) const {
-    multiplies<Output> mul_op;
-
-    Output y_r = Output(lhs.real());
-    Output y_i = Output(lhs.imag());
-
-    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
-  }
-};
-
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<complex<T>, complex<T>, complex<T>> {
-  CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    complex<T> const &a,
-    complex<T> const &b,
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a.real() * b.real();
-    real += -a.imag() * b.imag();
-    imag += a.real() * b.imag();
-    imag += a.imag () * b.real();
-
-    return complex<T>{
-      real,
-      imag
-    };
-  }
-};
-
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<complex<T>, T, complex<T>> {
-  CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    complex<T> const &a,
-    T const &b,
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a.real() * b;
-    imag += a.imag () * b;
-
-    return complex<T>{
-      real,
-      imag
-    };
-  }
-};
-
-/// Fused multiply-add
-template <typename T>
-struct multiply_add<T, complex<T>, complex<T>> {
-  CUTLASS_HOST_DEVICE
-  complex<T> operator()(
-    T const &a,
-    complex<T> const &b,
-    complex<T> const &c) const {
-
-    T real = c.real();
-    T imag = c.imag();
-
-    real += a * b.real();
-    imag += a * b.imag();
-
-    return complex<T>{
-      real,
-      imag
-    };
-  }
-};
-
-/// Conjugate
-template <typename T>
-struct conjugate<complex<T>>  {
-  CUTLASS_HOST_DEVICE
-  complex<T> operator()(complex<T> const &a) const {
-    return conj(a);
-  }
-};
-
-/// Computes the square of a difference with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared_difference<complex<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(complex<T> lhs, complex<T> rhs) const {
-    multiplies<Output> mul_op;
-
-    Output y_r = Output(lhs.real()) - Output(rhs.real());
-    Output y_i = Output(lhs.imag()) - Output(rhs.imag());
-
-    return mul_op(y_r, y_r) + mul_op(y_i, y_i);
-  }
-};
-
-/// Reduces value into the data pointed to by ptr (complex<T> specialization)
-template <typename T>
-struct red<complex<T>> {
-  CUTLASS_DEVICE
-  void operator()(complex<T> *ptr, const complex<T> &data)
-  {
-    data.red(ptr);
-  }
-};
-
-
 //////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace cutlass
 
 //////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/constants.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/constants.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv2d_problem_size.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -217,20 +217,20 @@
     return tmp; 
   }
 
   /// Equality operator (ignores mode and split_k_slice)
   CUTLASS_HOST_DEVICE
   bool operator==(Conv2dProblemSize const &conv) const {
     return (
-      (N == conv.N) && (H == conv.H) && (W == conv.W) && (C == conv.C) &&
+      (N == conv.N) && (W == conv.H) && (W == conv.W) && (C == conv.C) &&
       (K == conv.K) && (R == conv.R) && (S == conv.S) &&
       (P == conv.P) && (Q == conv.Q) &&
       (pad_h == conv.pad_h) && (pad_w == conv.pad_w) &&
       (stride_h == conv.stride_h) && (stride_w == conv.stride_w) &&
-      (dilation_h == conv.dilation_h) && (dilation_w == conv.dilation_w)
+      (dilation_h == conv.dilation_h) && (dilation_h == conv.dilation_h)
     );  
   }
 
   /// Inequality operator
   CUTLASS_HOST_DEVICE
   bool operator!=(Conv2dProblemSize const &rhs) const {
     return !(*this == rhs);
@@ -243,15 +243,15 @@
     return cutlass::Tensor4DCoord ({N, H, W, C});
   }
 
   /// Returns filter extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord filter_extent() const {
 
-    return cutlass::Tensor4DCoord ({K, R, S, C / groups});
+    return cutlass::Tensor4DCoord ({K, R, S, C});
   }
 
   /// Returns output extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord output_extent() const {
 
     return cutlass::Tensor4DCoord ({N, P, Q, K});
@@ -274,15 +274,15 @@
   /// Returns output size in number of elements
   CUTLASS_HOST_DEVICE
   int64_t output_size() const {
 
     return (N * P * Q * K);
   }
   
-  /// Returns padding as Tensor4DCoord
+  /// Returns output extent as Tensor4DCoord
   CUTLASS_HOST_DEVICE
   cutlass::Tensor4DCoord padding() const {
 
     return cutlass::Tensor4DCoord ({pad_h, pad_h, pad_w, pad_w});
   }
 
   /// Returns stride as MatrixCoord
@@ -332,15 +332,15 @@
   Conv2dProblemSize const &problem_size) {
   // Compute problem size
   switch (conv_operator) {
   case Operator::kFprop:
     return gemm::GemmCoord(
       problem_size.N * problem_size.P * problem_size.Q,
       problem_size.K,
-      problem_size.R * problem_size.S * problem_size.C / problem_size.groups
+      problem_size.R * problem_size.S * problem_size.C
     );
   case Operator::kDgrad:
     return gemm::GemmCoord(
       problem_size.N * problem_size.H * problem_size.W,
       problem_size.C,
       problem_size.R * problem_size.S * problem_size.K
     );
@@ -447,53 +447,22 @@
             }
           }
           break;
 
         default:
           break;
       }
-    } else if (algorithm == IteratorAlgorithm::kOptimized) {
-      // Current optimized iterator only support GroupMode::kSingleGroup
-      if (group_mode == GroupMode::kSingleGroup) {
-        switch (conv_operator) {
-          case Operator::kFprop:
-            iterations = problem_size.R * problem_size.S * ((channels_per_group + threadblock_K - 1) / threadblock_K);
-            break;
-
-          default:
-            break;
-        }
-      }
     }
 
   }
 
   return iterations;
 }
 
 
-template <int N = 1, int Output_P = 1, int Output_Q = 1>
-CUTLASS_HOST_DEVICE
-int depthwise_gemm_k_iterations(
-  Operator conv_operator, 
-  int threadblock_K, 
-  Conv2dProblemSize const &problem_size,
-  IteratorAlgorithm algorithm = IteratorAlgorithm::kAnalytic,
-  GroupMode group_mode = GroupMode::kNone,
-  int threadblock_N = 0) {
-
-    int n =  problem_size.N;
-    int p = (problem_size.P + Output_P - 1) /  Output_P;
-    int q = (problem_size.Q + Output_Q - 1) /  Output_Q;
-
-    int iterations = (n * p * q + problem_size.split_k_slices - 1) / problem_size.split_k_slices;
-    return iterations;
-}
-
-
 CUTLASS_HOST_DEVICE
 int implicit_gemm_k_iterations_per_channel(
     Operator conv_operator,
     int threadblock_K,
     Conv2dProblemSize const &problem_size,
     IteratorAlgorithm algorithm = IteratorAlgorithm::kAnalytic) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/conv3d_problem_size.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -201,16 +201,16 @@
   CUTLASS_HOST_DEVICE
   bool operator==(Conv3dProblemSize const &conv) const {
     return (
       (N == conv.N) && (D == conv.D) && (H == conv.H) && (W == conv.W) && (C == conv.C) &&
       (K == conv.K) && (T == conv.T) && (R == conv.R) && (S == conv.S) &&
       (Z == conv.Z) &&(P == conv.P) && (Q == conv.Q) &&
       (pad_d == conv.pad_d) && (pad_h == conv.pad_h) && (pad_w == conv.pad_w) &&
-      (stride_d == conv.stride_d) && (stride_h == conv.stride_h) && (stride_w == conv.stride_w) &&
-      (dilation_d == conv.dilation_d) && (dilation_h == conv.dilation_h) && (dilation_w == conv.dilation_w)
+      (stride_d == conv.stride_d) && (stride_h == conv.stride_h) && (stride_w == conv.stride_h) &&
+      (dilation_d == conv.dilation_d) && (dilation_h == conv.dilation_h) && (dilation_h == conv.dilation_h)
     );  
   }
 
   /// Inequality operator
   CUTLASS_HOST_DEVICE
   bool operator!=(Conv3dProblemSize const &rhs) const {
     return !(*this == rhs);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/convolution.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -96,24 +96,22 @@
 };
 
 /// Selects among several implementation variants trading off performance with simplicity
 enum class IteratorAlgorithm { 
   kAnalytic,      ///< functionally correct in all cases but lower performance
   kOptimized,     ///< optimized for R <= 32, S <= 32 and unity-stride dgrad
   kFixedChannels, ///< Analytic algorithm optimized for fixed channel count (C == AccessSize)
-  kFewChannels,   ///< Analytic algorithm optimized for few channels (C divisible by AccessSize)
-  kFixedStrideDilation ///< Optimized for fixed stride and dilation
+  kFewChannels    ///< Analytic algorithm optimized for few channels (C divisible by AccessSize)
 };
 
 /// Distinguishes among partial specializations that accelerate certain problems where convolution
 /// stride is unit.
 enum class StrideSupport {
   kStrided,       ///< arbitrary convolution stride
-  kUnity,         ///< unit convolution stride
-  kFixed          ///< fixed convolution stride
+  kUnity          ///< unit convolution stride
 };
 
 /// Identifies split-K mode
 enum class SplitKMode { 
   kNone, 
   kSerial, 
   kParallel
@@ -123,45 +121,13 @@
 enum class GroupMode {
   kNone,
   kSingleGroup,   ///< One CTA calculates one group or less
   kMultipleGroup, ///< One CTA calculates multiple groups
   kDepthwise      ///< One CTA calculates cta_n groups (problem_size.C == problem_size.K == problem_size.groups)
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Shape of a tensor
-template <
-  int N = 1,
-  int H = 1,
-  int W = 1,
-  int C = 1
->
-struct TensorNHWCShape {
-  static int const kN = N;
-  static int const kH = H;
-  static int const kW = W;
-  static int const kC = C;
-
-  static int const kHW = H * W;
-  static int const kNHW = N * kHW;
-  static int const kNHWC = N * H * W * C;
-
-  static int const kCount = kNHWC;
-
-  //
-  // Static member functions
-  //
-
-  /// Returns a Coord object
-  CUTLASS_HOST_DEVICE
-  static Coord<4> toCoord() {
-    return make_Coord(kN, kH, kW, kC);
-  }
-};
-
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace conv
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,70 +48,70 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template<typename ImplicitGemmKernel_>
 class ImplicitGemmConvolution {
 public:
 
-  using UnderlyingKernel = ImplicitGemmKernel_;
+  using ImplicitGemmKernel = ImplicitGemmKernel_;
 
-  using ElementA = typename UnderlyingKernel::ElementA;
-  using LayoutA = typename UnderlyingKernel::LayoutA;
-  using ElementB = typename UnderlyingKernel::ElementB;
-  using LayoutB = typename UnderlyingKernel::LayoutB;
-  using ElementC = typename UnderlyingKernel::ElementC;
-  using LayoutC = typename UnderlyingKernel::LayoutC;
-  using ElementAccumulator = typename UnderlyingKernel::ElementAccumulator;
-  using ElementCompute = typename UnderlyingKernel::ElementCompute;
-  using OperatorClass = typename UnderlyingKernel::OperatorClass;
-  using ArchTag = typename UnderlyingKernel::ArchTag;
-  using ThreadblockShape = typename UnderlyingKernel::ThreadblockShape;
-  using WarpShape = typename UnderlyingKernel::WarpShape;
-  using InstructionShape = typename UnderlyingKernel::InstructionShape;
-  using ThreadblockSwizzle = typename UnderlyingKernel::ThreadblockSwizzle;
-  using EpilogueOutputOp = typename UnderlyingKernel::EpilogueOutputOp;
-  static int const kStages = UnderlyingKernel::kStages;
-  static int const kConvDim = UnderlyingKernel::kConvDim;
-  using WarpMmaOperator = typename UnderlyingKernel::WarpMmaOperator;
-  using ArchMmaOperator = typename UnderlyingKernel::ArchMmaOperator;
-  using MathOperator = typename UnderlyingKernel::MathOperator; 
-
-  static cutlass::conv::Operator const kConvolutionalOperator = UnderlyingKernel::kConvolutionalOperator;
-  static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = UnderlyingKernel::kIteratorAlgorithm;
-  static cutlass::conv::StrideSupport const kStrideSupport = UnderlyingKernel::kStrideSupport;
-  static cutlass::conv::GroupMode const kGroupMode = UnderlyingKernel::kGroupMode;
+  using ElementA = typename ImplicitGemmKernel::ElementA;
+  using LayoutA = typename ImplicitGemmKernel::LayoutA;
+  using ElementB = typename ImplicitGemmKernel::ElementB;
+  using LayoutB = typename ImplicitGemmKernel::LayoutB;
+  using ElementC = typename ImplicitGemmKernel::ElementC;
+  using LayoutC = typename ImplicitGemmKernel::LayoutC;
+  using ElementAccumulator = typename ImplicitGemmKernel::ElementAccumulator;
+  using ElementCompute = typename ImplicitGemmKernel::ElementCompute;
+  using OperatorClass = typename ImplicitGemmKernel::OperatorClass;
+  using ArchTag = typename ImplicitGemmKernel::ArchTag;
+  using ThreadblockShape = typename ImplicitGemmKernel::ThreadblockShape;
+  using WarpShape = typename ImplicitGemmKernel::WarpShape;
+  using InstructionShape = typename ImplicitGemmKernel::InstructionShape;
+  using ThreadblockSwizzle = typename ImplicitGemmKernel::ThreadblockSwizzle;
+  using EpilogueOutputOp = typename ImplicitGemmKernel::EpilogueOutputOp;
+  static int const kStages = ImplicitGemmKernel::kStages;
+  static int const kConvDim = ImplicitGemmKernel::kConvDim;
+  using WarpMmaOperator = typename ImplicitGemmKernel::WarpMmaOperator;
+  using ArchMmaOperator = typename ImplicitGemmKernel::ArchMmaOperator;
+  using MathOperator = typename ImplicitGemmKernel::MathOperator; 
+
+  static cutlass::conv::Operator const kConvolutionalOperator = ImplicitGemmKernel::kConvolutionalOperator;
+  static cutlass::conv::IteratorAlgorithm const kIteratorAlgorithm = ImplicitGemmKernel::kIteratorAlgorithm;
+  static cutlass::conv::StrideSupport const kStrideSupport = ImplicitGemmKernel::kStrideSupport;
+  static cutlass::conv::GroupMode const kGroupMode = ImplicitGemmKernel::kGroupMode;
 
   static int const kWarpCount = 
     (ThreadblockShape::kM / WarpShape::kM) * 
     (ThreadblockShape::kN / WarpShape::kN) *
     (ThreadblockShape::kK / WarpShape::kK);
 
   /// Argument structure
-  using Arguments = typename UnderlyingKernel::Arguments;
+  using Arguments = typename ImplicitGemmKernel::Arguments;
 
 private:
 
   /// Kernel parameters object
-  typename UnderlyingKernel::Params params_;
+  typename ImplicitGemmKernel::Params params_;
 
 public:
 
   /// Constructs Implicit GEMM
   ImplicitGemmConvolution() { }
 
   /// Determines whether the Implicit GEMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     // dispatch to iterators
-    Status status = UnderlyingKernel::Mma::IteratorA::can_implement(args.problem_size);
+    Status status = ImplicitGemmKernel::Mma::IteratorA::can_implement(args.problem_size);
     if (Status::kSuccess != status) {
       return status;
     }
 
-    status = UnderlyingKernel::Mma::IteratorB::can_implement(args.problem_size);
+    status = ImplicitGemmKernel::Mma::IteratorB::can_implement(args.problem_size);
     if (Status::kSuccess != status) {
       return status;
     }
 
     // check group conv constraint
     if (args.problem_size.groups != 1) {
       if (kGroupMode == conv::GroupMode::kNone) {
@@ -134,23 +134,17 @@
       if (kGroupMode == conv::GroupMode::kSingleGroup && k_per_group % ThreadblockShape::kN) {
         return Status::kErrorInvalidProblem;
       }
       // ThreadblockShape::kN should be divisible by k_per_group, one CTA calculate multiple groups
       if (kGroupMode == conv::GroupMode::kMultipleGroup && ThreadblockShape::kN % k_per_group) {
         return Status::kErrorInvalidProblem;
       }
-
-      // current optimized iterator algo only supports SingleGroup mode
-      if (kIteratorAlgorithm == IteratorAlgorithm::kOptimized &&
-        kGroupMode != conv::GroupMode::kSingleGroup) {
-        return Status::kErrorInvalidProblem;
-      }
     }
 
-    static int const kAlignmentC = UnderlyingKernel::Epilogue::OutputTileIterator::kElementsPerAccess;
+    static int const kAlignmentC = ImplicitGemmKernel::Epilogue::OutputTileIterator::kElementsPerAccess;
     if (kConvolutionalOperator == conv::Operator::kFprop) {
       if (args.problem_size.K % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     } else if (kConvolutionalOperator == conv::Operator::kDgrad) {
        if (args.problem_size.C % kAlignmentC)
         return Status::kErrorMisalignedOperand;
     } else if (kConvolutionalOperator == conv::Operator::kWgrad) {
@@ -158,14 +152,23 @@
         return Status::kErrorMisalignedOperand;
     }
 
     // check for unsupported problem sizes for strided dgrad implementation
     if (kConvolutionalOperator == conv::Operator::kDgrad && 
       kStrideSupport == conv::StrideSupport::kStrided) {
 
+      // Unity stride (1x1) is supported by strided dgrad but disabled for performance 
+      // reasons. For unity stride, use strided dgrad optimized unity stride specialization.
+      // Note that unit tests strided dgrad for unity stride to make sure that strided 
+      // dgrad implemetnation is functionaly sound. 
+      // Strided dgrad implementation also support mixed strides, i.e., (1x2) and (2x1)
+      if(args.problem_size.stride_h == 1 && args.problem_size.stride_w == 1) {
+        return Status::kErrorNotSupported;
+      }
+
       // split-k (serial or parallel) is not supported for strided dgrad
       if(args.problem_size.split_k_slices > 1) {
         return Status::kErrorNotSupported;
       }
       
       // dilation > {1x1} is not supported for strided dgrad
       if(args.problem_size.dilation_h > 1 || args.problem_size.dilation_w > 1) {
@@ -242,23 +245,23 @@
 
       if (status != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
 
     // initialize the params structure from the arguments
-    params_ = typename UnderlyingKernel::Params(
+    params_ = typename ImplicitGemmKernel::Params(
     	args,
     	static_cast<int *>(workspace)
     );
     
-    int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename ImplicitGemmKernel::SharedStorage));
 
     if (smem_size >= (48 << 10)) {
-      cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<UnderlyingKernel>,
+      cudaError_t result = cudaFuncSetAttribute(cutlass::Kernel<ImplicitGemmKernel>,
                                     cudaFuncAttributeMaxDynamicSharedMemorySize,
                                     smem_size);
 
       if (result != cudaSuccess) {
         return Status::kErrorInternal;
       }
     }
@@ -285,17 +288,17 @@
 
 
     ThreadblockSwizzle threadblock_swizzle;
 
     dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
     dim3 block(32 * kWarpCount, 1, 1);
 
-    int smem_size = int(sizeof(typename UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename ImplicitGemmKernel::SharedStorage));
 
-    cutlass::Kernel<UnderlyingKernel><<<grid, block, smem_size, stream>>>(params_);
+    cutlass::Kernel<ImplicitGemmKernel><<<grid, block, smem_size, stream>>>(params_);
 
     cudaError_t result = cudaGetLastError();
 
     return result == cudaSuccess ? Status::kSuccess : Status::kErrorInternal;
   }
 
   /// Runs the kernel using initialized state.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/device/implicit_gemm_convolution_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -103,15 +103,15 @@
   using Epilogue = typename cutlass::conv::kernel::detail::DefaultConvEpilogueWithBroadcastTensorOp<
     ArchTag,
     typename ImplicitGemmBase::Epilogue::Shape,
     typename ImplicitGemmBase::Epilogue::WarpMmaOperator,
     ImplicitGemmBase::Epilogue::kPartitionsK,
     ElementC,
     typename EpilogueOutputOp::ElementT,
-    typename EpilogueOutputOp::ElementVector,
+    ElementC,
     EpilogueOutputOp,
     ImplicitGemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Define the kernel
   using Kernel = cutlass::conv::kernel::ImplicitGemmConvolutionWithFusedEpilogue<
     typename ImplicitGemmBase::Mma,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_fprop_with_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_group_fprop.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h`

 * *Files 22% similar despite different names*

```diff
@@ -26,465 +26,361 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
-    Default kernel-level implicit GEMM convolution definitions combine threadblock-scoped 
-      matrix multiply-add with the appropriate threadblock-scoped epilogue.  
+    \brief Template for a GEMM kernel that can broadcast bias vector in the
+           epigloue.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/conv/kernel/default_conv2d.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/arch/arch.h"
+#include "cutlass/epilogue/thread/linear_combination_bias_elementwise.h"
+#include "cutlass/device_kernel.h"
+
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
+#include "cutlass/gemm/kernel/gemm_universal.h"
+
+#include "cutlass/gemm/kernel/default_gemm_universal.h"
+#include "cutlass/gemm/kernel/default_gemm_with_broadcast.h"
+#include "cutlass/gemm/device/default_gemm_configuration.h"
+#include "cutlass/gemm/device/gemm_universal_base.h"
 
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h"
-
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h"
-#include "cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h"
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
-namespace kernel {
+namespace gemm {
+namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/// Defines a kernel for Conv2dGroupFpro
-template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename OperatorClass,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::GroupMode GroupMode,
-  conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kOptimized,
-  conv::StrideSupport StrideSupport = StrideSupport::kStrided,
-  /// Access granularity of A matrix in units of elements
-  int AlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value,
-  /// Access granularity of B matrix in units of elements
-  int AlignmentB = 128 / cutlass::sizeof_bits<ElementB>::value
-> struct DefaultConv2dGroupFprop;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//                         OpClassTensorOp convolutions 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Defines a kernel for Conv2dGroupFprop specialization for Analytic IteratorAlgorithm and multistage 
-/// pipeline that supports all GroupMode.
+/*!
+  The universal GEMM with a broadcast epilogue.
+  Supports
+*/
 template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::GroupMode GroupMode,
-  conv::StrideSupport StrideSupport, 
-  int AlignmentA,
-  int AlignmentB
+    /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for C and D matrix operands
+    typename ElementC_,
+    /// Layout type for C and D matrix operands
+    typename LayoutC_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_ = ElementC_,
+    /// Operator class tag
+    typename OperatorClass_ = arch::OpClassSimt,
+    /// Tag indicating architecture to tune for.  This is the minimum SM that
+    /// supports the intended feature. The device kernel can be built
+    /// targeting any SM larger than this number.
+    typename ArchTag_ = arch::Sm70,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::InstructionShape,
+    /// Epilogue output operator      - must satisfy concept of 'EpilogueWithBroadcastOp'
+    typename EpilogueOutputOp_ = cutlass::epilogue::thread::LinearCombinationBiasElementwise<
+        ElementC_, ElementAccumulator_, ElementAccumulator_,
+        ElementC_, ElementC_, 128 / cutlass::sizeof_bits<ElementC_>::value>,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle_ = threadblock::GemmIdentityThreadblockSwizzle<>,
+    /// Number of stages used in the pipelined mainloop
+    int Stages =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kStages,
+    /// Access granularity of A matrix in units of elements
+    int AlignmentA =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kAlignmentA,
+    /// Access granularity of B matrix in units of elements
+    int AlignmentB =
+        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
+                                 ElementC_, ElementAccumulator_>::kAlignmentB,
+    /// Operation performed by GEMM
+    typename Operator_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::Operator,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB = ComplexTransform::kNone
 >
-struct DefaultConv2dGroupFprop <
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  arch::OpClassTensorOp,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  Stages,
-  MathOperatorTag,
-  GroupMode,
-  IteratorAlgorithm::kAnalytic,
-  StrideSupport,
-  AlignmentA,
-  AlignmentB
-> {
-
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-
-  // Define the core components from GEMM
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
-      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, MathOperatorTag>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::AlignedArray<ElementA, AlignmentA>;
-  using IteratorA =
-    cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorAnalytic<
-      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-      ElementA, LayoutA,
-      ThreadMapA,
-      AccessTypeA,
-      GroupMode
-    >;
-
-  using SmemIteratorA = typename MmaCore::SmemIteratorA;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::AlignedArray<ElementB, AlignmentB>;
-  using IteratorB =
-    cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorAnalytic<
-      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-      ElementB, LayoutB,
-      ThreadMapB,
-      AccessTypeB,
-      GroupMode
-    >;
-  
-  using SmemIteratorB = typename MmaCore::SmemIteratorB;
-
-  // Warp-level GEMM components
-  using WarpMmaTensorOp = typename MmaCore::MmaTensorOp;
-  using MmaPolicy = typename MmaCore::MmaPolicy;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * AlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the Mma
-  using Mma = threadblock::ImplicitGemmMultistage<
-    ThreadblockShape,
-    IteratorA,
-    SmemIteratorA,
-    arch::CacheOperation::Always,
-    IteratorB,
-    SmemIteratorB,
-    CacheOpB,
-    MmaPolicy,
-    Stages 
+class GemmUniversalWithBroadcast :
+  public GemmUniversalBase<
+    typename kernel::DefaultGemmWithBroadcast<
+      ElementA_,
+      LayoutA_,
+      TransformA,
+      AlignmentA,
+      ElementB_,
+      LayoutB_,
+      TransformB,
+      AlignmentB,
+      ElementC_,
+      LayoutC_,
+      ElementAccumulator_,
+      OperatorClass_,
+      ArchTag_,
+      ThreadblockShape_,
+      WarpShape_,
+      InstructionShape_,
+      EpilogueOutputOp_,
+      ThreadblockSwizzle_,
+      Stages,
+      Operator_
+    >::GemmKernel
+  > {
+
+ public:
+
+  using ElementAccumulator = ElementAccumulator_;
+  using OperatorClass = OperatorClass_;
+  using ArchTag = ArchTag_;
+  using ThreadblockShape = ThreadblockShape_;
+  using WarpShape = WarpShape_;
+  using InstructionShape = InstructionShape_;
+  using EpilogueOutputOp = EpilogueOutputOp_;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  using Operator = Operator_;
+  static int const kStages = Stages;
+  static int const kAlignmentA = AlignmentA;
+  static int const kAlignmentB = AlignmentB;
+  static int const kAlignmentC = EpilogueOutputOp::kCount;
+  static ComplexTransform const kTransformA = TransformA;
+  static ComplexTransform const kTransformB = TransformB;
+
+  using Base = GemmUniversalBase<
+    typename kernel::DefaultGemmWithBroadcast<
+      ElementA_,
+      LayoutA_,
+      TransformA,
+      AlignmentA,
+      ElementB_,
+      LayoutB_,
+      TransformB,
+      AlignmentB,
+      ElementC_,
+      LayoutC_,
+      ElementAccumulator_,
+      OperatorClass_,
+      ArchTag_,
+      ThreadblockShape_,
+      WarpShape_,
+      InstructionShape_,
+      EpilogueOutputOp_,
+      ThreadblockSwizzle_,
+      Stages,
+      Operator_
+    >::GemmKernel
   >;
 
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+  using Arguments = typename Base::Arguments;
+  using GemmKernel = typename Base::GemmKernel;
+};
+
+////////////////////////////////////////////////////////////////////////////////
 
-  // Define the epilogue
-  using Epilogue = typename epilogue::threadblock::DefaultEpilogueTensorOp<
+/// Partial specialization for column-major output exchanges problem size and operand.
+template <
+    /// Element type for A matrix operand
+    typename ElementA_,
+    /// Layout type for A matrix operand
+    typename LayoutA_,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Element type for C and D matrix operands
+    typename ElementC_,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for.  This is the minimum SM that
+    /// supports the intended feature. The device kernel can be built
+    /// targeting any SM larger than this number.
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Epilogue output operator
+    typename EpilogueOutputOp_,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Access granularity of A matrix in units of elements
+    int AlignmentA,
+    /// Access granularity of B matrix in units of elements
+    int AlignmentB,
+    /// Operation performed by GEMM
+    typename Operator_,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB>
+class GemmUniversalWithBroadcast<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
+           layout::ColumnMajor,  // partially specialized on LayoutC
+           ElementAccumulator_, OperatorClass_, ArchTag_, ThreadblockShape_,
+           WarpShape_, InstructionShape_, EpilogueOutputOp_,
+           ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB,
+           Operator_, TransformA, TransformB> {
+ public:
+
+  using ElementA = ElementA_;
+  using LayoutA = LayoutA_;
+  using TensorRefA = TensorRef<ElementA const, LayoutA>;
+  using ElementB = ElementB_;
+  using LayoutB = LayoutB_;
+  using TensorRefB = TensorRef<ElementB const, LayoutB>;
+  using ElementC = ElementC_;
+  using LayoutC = layout::ColumnMajor;
+  using TensorRefC = TensorRef<ElementC const, LayoutC>;
+  using TensorRefD = TensorRef<ElementC, LayoutC>;
+  using ElementAccumulator = ElementAccumulator_;
+  using OperatorClass = OperatorClass_;
+  using ArchTag = ArchTag_;
+  using ThreadblockShape = ThreadblockShape_;
+  using WarpShape = WarpShape_;
+  using InstructionShape = InstructionShape_;
+  using EpilogueOutputOp = EpilogueOutputOp_;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+  using Operator = Operator_;
+  static int const kStages = Stages;
+  static int const kAlignmentA = AlignmentA;
+  static int const kAlignmentB = AlignmentB;
+  static ComplexTransform const kTransformA = TransformA;
+  static ComplexTransform const kTransformB = TransformB;
+
+  using UnderlyingOperator = typename GemmUniversalWithBroadcast<
+    ElementB,
+    typename layout::LayoutTranspose<LayoutB>::type,
+    ElementA,
+    typename layout::LayoutTranspose<LayoutA>::type,
+    ElementC,
+    layout::RowMajor,
+    ElementAccumulator,
+    OperatorClass,
+    ArchTag,
     ThreadblockShape,
-    WarpMmaTensorOp,
-    kPartitionsK,
+    WarpShape,
+    InstructionShape,
     EpilogueOutputOp,
-    EpilogueOutputOp::kCount
-  >::Epilogue;
-
-  // Define the kernel
-  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
-    Mma,
-    Epilogue,
     ThreadblockSwizzle,
-    conv::Operator::kFprop,
-    Conv2dProblemSize,
-    GroupMode
-  >;
-};
+    Stages,
+    kAlignmentB,
+    kAlignmentA,
+    Operator,
+    kTransformB,
+    kTransformA
+  >::Base;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  using GemmKernel = typename UnderlyingOperator::GemmKernel;
+  static int const kAlignmentC = EpilogueOutputOp::kCount;
 
-/// Defines a kernel for Conv2dGroupFprop specialization for Optimized IteratorAlgorithm and multistage
-/// pipeline that supports GroupMode::kSingleGroup.
-template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  int Stages,
-  typename MathOperatorTag,
-  conv::StrideSupport StrideSupport,
-  int AlignmentA,
-  int AlignmentB
->
-struct DefaultConv2dGroupFprop <
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  arch::OpClassTensorOp,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  Stages,
-  MathOperatorTag,
-  GroupMode::kSingleGroup,
-  IteratorAlgorithm::kOptimized,
-  StrideSupport,
-  AlignmentA,
-  AlignmentB
-> {
-
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-
-  // Define the core components from GEMM
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
-      ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, MathOperatorTag>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::AlignedArray<ElementA, AlignmentA>;
-  using IteratorA =
-    cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<
-      cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-      ElementA, LayoutA,
-      ThreadMapA,
-      AccessTypeA
-    >;
-
-  using SmemIteratorA = typename MmaCore::SmemIteratorA;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::AlignedArray<ElementB, AlignmentB>;
-  using IteratorB =
-    cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<
-      cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-      ElementB, LayoutB,
-      ThreadMapB,
-      AccessTypeB
-    >;
-
-  using SmemIteratorB = typename MmaCore::SmemIteratorB;
-
-  // Warp-level GEMM components
-  using WarpMmaTensorOp = typename MmaCore::MmaTensorOp;
-  using MmaPolicy = typename MmaCore::MmaPolicy;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * AlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
+  /// Argument structure
+  using Arguments = typename UnderlyingOperator::Arguments;
 
-  // Define the Mma
-  using Mma = threadblock::ImplicitGemmMultistage<
-    ThreadblockShape,
-    IteratorA,
-    SmemIteratorA,
-    arch::CacheOperation::Always,
-    IteratorB,
-    SmemIteratorB,
-    CacheOpB,
-    MmaPolicy,
-    Stages
-  >;
+private:
 
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+  UnderlyingOperator underlying_operator_;
 
-  // Define the epilogue
-  using Epilogue = typename epilogue::threadblock::DefaultEpilogueTensorOp<
-    ThreadblockShape,
-    WarpMmaTensorOp,
-    kPartitionsK,
-    EpilogueOutputOp,
-    EpilogueOutputOp::kCount
-  >::Epilogue;
+public:
 
-  // Define the kernel
-  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
-    Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    conv::Operator::kFprop,
-    Conv2dProblemSize,
-    GroupMode::kSingleGroup
-  >;
-};
+  /// Constructs the GEMM.
+  GemmUniversalWithBroadcast() { }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// Helper to construct a transposed equivalent for the underying GEMM operator
+  static Arguments to_underlying_arguments(Arguments const &args) {
+    return args.transposed_problem();
+  }
 
-/// Defines a kernel for Conv2dGroupFprop specialization for Optimized IteratorAlgorithm and
-/// 2 stage pipeline that supports GroupMode::kSingleGroup.
-template <
-  typename ElementA,
-  typename LayoutA,
-  typename ElementB,
-  typename LayoutB,
-  typename ElementC,
-  typename LayoutC,
-  typename ElementAccumulator,
-  typename ArchTag,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename EpilogueOutputOp,
-  typename ThreadblockSwizzle,
-  typename MathOperatorTag,
-  conv::StrideSupport StrideSupport,
-  int AlignmentA,
-  int AlignmentB
->
-struct DefaultConv2dGroupFprop <
-  ElementA,
-  LayoutA,
-  ElementB,
-  LayoutB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  arch::OpClassTensorOp,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  2,
-  MathOperatorTag,
-  GroupMode::kSingleGroup,
-  IteratorAlgorithm::kOptimized,
-  StrideSupport,
-  AlignmentA,
-  AlignmentB
-> {
-
-  static_assert(std::is_same<LayoutA, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutB, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-  static_assert(std::is_same<LayoutC, cutlass::layout::TensorNHWC>::value,
-    "Current group conv only support NHWC layout");
-
-  // Define the core components from GEMM
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-    ThreadblockShape, WarpShape, InstructionShape, ElementA, layout::RowMajor,
-    ElementB, layout::ColumnMajor, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-    2, MathOperatorTag>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::AlignedArray<ElementA, AlignmentA>;
-  using IteratorA =
-    cutlass::conv::threadblock::TileIterator<
-      cutlass::conv::threadblock::Conv2dFpropActivationTileAccessIteratorOptimized<
-        cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-        ElementA,
-        LayoutA,
-        ThreadMapA,
-        AccessTypeA
-      >
-    >;
-
-  using SmemIteratorA = typename MmaCore::SmemIteratorA;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::AlignedArray<ElementB, AlignmentB>;
-  using IteratorB =
-    cutlass::conv::threadblock::TileIterator<
-      cutlass::conv::threadblock::Conv2dFpropFilterTileAccessIteratorOptimized<
-        cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-        ElementB,
-        LayoutB,
-        ThreadMapB,
-        AccessTypeB
-      >
-    >;
-
-  using SmemIteratorB = typename MmaCore::SmemIteratorB;
-
-  // Warp-level GEMM components
-  using WarpMmaTensorOp = typename MmaCore::MmaTensorOp;
-  using MmaPolicy = typename MmaCore::MmaPolicy;
+  /// Determines whether the GEMM can execute the given problem.
+  static Status can_implement(Arguments const &args) {
 
-  // Define the Mma
-  using Mma = threadblock::ImplicitGemmPipelined<
-    ThreadblockShape,
-    IteratorA,
-    SmemIteratorA,
-    IteratorB,
-    SmemIteratorB,
-    ElementC,
-    LayoutC,
-    MmaPolicy
-  >;
+    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
+  }
 
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
+  /// Gets the workspace size
+  static size_t get_workspace_size(Arguments const &args) {
 
-  // Define the epilogue
-  using Epilogue = typename detail::DefaultConvEpilogue<
-    ArchTag,
-    ThreadblockShape,
-    WarpMmaTensorOp,
-    kPartitionsK,
-    EpilogueOutputOp
-  >::Epilogue;
-
-  // Define the kernel
-  using Kernel = cutlass::conv::kernel::ImplicitGemmConvolution<
-    Mma,
-    Epilogue,
-    ThreadblockSwizzle,
-    conv::Operator::kFprop,
-    Conv2dProblemSize,
-    GroupMode::kSingleGroup
-  >;
+    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
+  }
+
+  /// Computes the grid shape
+  static dim3 get_grid_shape(Arguments const &args) {
+    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
+  }
+
+  /// Computes the maximum number of active blocks per multiprocessor
+  static int maximum_active_blocks(int smem_capacity = -1) {
+    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
+  }
+
+  /// Initializes GEMM state from arguments.
+  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
+
+    return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
+  }
+
+  /// Lightweight update given a subset of arguments
+  Status update(Arguments const &args, void *workspace = nullptr) {
+
+    return underlying_operator_.update(to_underlying_arguments(args), workspace);
+  }
+
+  /// Runs the kernel using initialized state.
+  Status run(cudaStream_t stream = nullptr) {
+
+    return underlying_operator_.run(stream);
+  }
+
+  /// Runs the kernel using initialized state.
+  Status operator()(cudaStream_t stream = nullptr) {
+    return run(stream);
+  }
+
+  /// Runs the kernel using initialized state.
+  Status operator()(
+    Arguments const &args,
+    void *workspace = nullptr,
+    cudaStream_t stream = nullptr) {
+
+    Status status = initialize(args, workspace, stream);
+
+    if (status == Status::kSuccess) {
+      status = run(stream);
+    }
+
+    return status;
+  }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace kernel
-} // namespace conv
+} // namespace device
+} // namespace gemm
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv2d_wgrad_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_fprop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/default_conv3d_wgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/direct_convolution.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -328,15 +328,15 @@
         threadblock_tile_idx.k() * Mma::Shape::kK,
         threadblock_tile_idx.n() * Mma::Shape::kN
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -335,15 +335,15 @@
                   // Wgrad
                   (threadblock_tile_idx.n() * Mma::Shape::kN)
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_strided_dgrad.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -331,15 +331,15 @@
 
     typename Mma::FragmentC accumulators;
 
     accumulators.clear();
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     // Check if CTA contributes valid MMA (Dy * w) and accumulator will be non-zero after MMA
     if (start_r < params.problem_size.R && start_s < params.problem_size.S) {
       // Scale gemm_k_iterations for strided dgrad
       int gemm_k_iterations = (params.gemm_k_iterations / (params.problem_size.R * params.problem_size.S)
                               ) * params.problem_size.num_gemm_k_filter_positions(start_r, start_s);
@@ -385,23 +385,24 @@
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
 
     // Construct the semaphore.
     int block_idx = threadblock_tile_idx.m() + threadblock_tile_idx.n() * params.grid_tiled_shape.m();
-    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
+    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+    
     // Compute logical position within grid
     threadblock_tile_idx =
         threadblock_swizzle.get_tile_offset(params.grid_tiled_shape);
 
     // If performing a reduction via split-K, fetch the initial synchronization
     if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-
+        
       // Fetch the synchronization lock initially but do not block.
       semaphore.fetch();
 
       // Indicate which position in a serial reduction the output operator is currently updating
       output_op.set_k_partition(threadblock_tile_idx.k(), params.grid_tiled_shape.k());
     }
 
@@ -416,75 +417,74 @@
       params.ptr_D,
       ConvOutputIteratorParameter::extent(params.problem_size),
       thread_idx,
       params.stride_h_divmod, params.stride_w_divmod,
       start_r, start_s,
       threadblock_offset
     );
+    
+    // Tile iterator reading from source accumulator tensor
+    typename Epilogue::OutputTileIterator iterator_C(
+      params.iterator_C,
+      params.ptr_C,
+      ConvOutputIteratorParameter::extent(params.problem_size),
+      thread_idx,
+      params.stride_h_divmod, params.stride_w_divmod,
+      start_r, start_s,
+      threadblock_offset
+    );
+
 
     // Construct the epilogue
     Epilogue epilogue(
-      shared_storage.epilogue,
-      thread_idx,
-      warp_idx,
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
       lane_idx);
 
-    if (output_op.is_source_needed())
-    {
-      // Tile iterator reading from source accumulator tensor
-      typename Epilogue::OutputTileIterator iterator_C(
-        params.iterator_C,
-        params.ptr_C,
-        ConvOutputIteratorParameter::extent(params.problem_size),
-        thread_idx,
-        params.stride_h_divmod, params.stride_w_divmod,
-        start_r, start_s,
-        threadblock_offset);
-
-      // Wait on the semaphore - this latency may have been covered by iterator construction
-      if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
-
-        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-        if (threadblock_tile_idx.k()) {
-          iterator_C = iterator_D;
-        }
-
-        semaphore.wait(threadblock_tile_idx.k());
+    // Wait on the semaphore - this latency may have been covered by iterator construction
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+        
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+      if (threadblock_tile_idx.k()) {
+        iterator_C = iterator_D;
       }
 
-      // Run epilogue with addend source iterator
-      epilogue(output_op, iterator_D, accumulators, iterator_C);
+      semaphore.wait(threadblock_tile_idx.k());
+
     }
-    else
-    {
-      // Run epilogue without addend source iterator
-      epilogue(output_op, iterator_D, accumulators);
+    // Each split-k-slice writes to a unique tensor location
+    else if (params.split_k_mode == SplitKMode::kParallel) {
+      iterator_D.add_pointer_offset(threadblock_tile_idx.k() * 
+        cutlass::conv::implicit_gemm_tensor_c_size(ConvOperator, params.problem_size));
     }
 
+    // Run efficient epilogue
+    epilogue(output_op, iterator_D, accumulators, iterator_C);
+  
     //
     // Release the semaphore
     //
 
-    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) {
+    if (params.split_k_mode == SplitKMode::kSerial && params.grid_tiled_shape.k() > 1) { 
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_idx.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
       else {
         // Otherwise, the semaphore is incremented
         lock = threadblock_tile_idx.k() + 1;
       }
-
+      
       semaphore.release(lock);
     }
-
-  }
+  } 
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
 } // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/kernel/implicit_gemm_convolution_with_fused_epilogue.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -337,15 +337,15 @@
         threadblock_tile_idx.k() * Mma::Shape::kK,
         threadblock_tile_idx.n() * Mma::Shape::kN
       )
     );
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/thread/depthwise_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -226,15 +226,15 @@
       
       // Access (p, q) coordinates for Dy tensor for filter position in gemm_k=0
       // note that (h + pad_h - filter_r) and (w + pad_w - filter_s) are ensured to be 
       // divisible by stride_h and stride_w
       offset_p[s] = (mapped_h + problem_size_.pad_h - filter_r) / problem_size_.stride_h;
       offset_q[s] = (mapped_w + problem_size_.pad_w - filter_s) / problem_size_.stride_w;
 
-      // Initialize pointers for gemm_k=0
+      // Intialize pointers for gemm_k=0
       TensorCoord coord{offset_n[s], offset_p[s], offset_q[s], filter_k_};
 
       pointer_[s] += params_.layout(coord) * sizeof_bits<Element>::value / 8;
     }
 
     //
     // Precompute mask predicates
@@ -337,25 +337,25 @@
       // Move filter_r by stride_h
       filter_r_ += problem_size_.stride_h;
 #if 0
       if (filter_r_ < problem_size_.R) {
 
         next_idx = 1;
 
-        // Restore bytes in q coordinate (Mma in filter s dimension)
+        // Restore bytes in q coordinate (Mma in filter s dimenstion)
         reset_bytes = reset_bytes_s_;
 
       } else {
 
         // Restore filter_r
         filter_r_ = start_r_;
 
         next_idx = 2;
 
-        // Restore bytes in p and q coordinate (Mma in filter s and r dimension)
+        // Restore bytes in p and q coordinate (Mma in filter s and r dimenstion)
         reset_bytes = reset_bytes_r_;
       }
 #else
       asm volatile(
           "{\n\t"
           " .reg .pred %%p;\n\t"
           " setp.lt.s32 %%p, %3, %4;\n\t"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_few_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_fixed_channels.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -141,15 +141,14 @@
   LongIndex iteration_strided_;
   LongIndex iteration_vector_;
   char const *pointer_;
 
   uint32_t predicates_[kAccessesPerVector];
   int filter_rs_;
   int filter_c_;
-  int channels_per_group_;
 
   //
   // Assertions
   //
 
   // We map predicates into bits packed in this uint32_t container
   static_assert(ThreadMap::Iterations::kStrided < sizeof(predicates_) * 8,
@@ -172,29 +171,28 @@
     filter_rs_(0),
     filter_c_(0) {
 
     layout::PitchLinearCoord thread_coord = ThreadMap::initial_offset(thread_idx);
 
     filter_c_ = threadblock_offset.row() + thread_coord.contiguous();
     Index column = threadblock_offset.column() + thread_coord.strided();
-    channels_per_group_ = problem_size_.C / problem_size_.groups;
 
     CUTLASS_PRAGMA_UNROLL
     for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
       uint32_t pred = ((column + s * ThreadMap::Delta::kStrided < problem_size_.K) ? 1u : 0);
 
       CUTLASS_PRAGMA_UNROLL
       for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
         predicates_[v_idx] |= (pred << s);
       }
     }
 
     CUTLASS_PRAGMA_UNROLL
     for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
-      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= channels_per_group_);
+      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= problem_size_.C);
     }
 
     pointer_ += (
       params_.layout({filter_c_, column}) 
     ) * sizeof_bits<Element>::value / 8;
 
     set_iteration_index(0);
@@ -227,15 +225,15 @@
       filter_rs_ = 0;
       next = params_.inc_next_c;
       filter_c_ += params_.filter_c_delta;
     }
  
     CUTLASS_PRAGMA_UNROLL
     for (int v_idx = 0; v_idx < kAccessesPerVector; ++v_idx) {
-      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= channels_per_group_);
+      clear_mask(v_idx, filter_c_ + v_idx * AccessType::kElements >= problem_size_.C);
     }
       
     pointer_ += next;
   }
 
   /// Clears the predicates
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -191,15 +191,15 @@
     if (kAccessesPerVector == 1) {
       /// One 128b aligned access fetching more than one element
       c = filter_c_[iteration_contiguous_];
       r = filter_r_[iteration_contiguous_];
       s = filter_s_[iteration_contiguous_];
     }  
     else {
-      /// Multiple access to support non-128b alignment in contiguous dimension
+      /// Multiple access to support non-128b alignment in contiguous dimenstion
       c = (filter_c_[iteration_contiguous_] + iteration_vector_ * AccessType::kElements) % problem_size_.C;
       int wrap_c = (filter_c_[iteration_contiguous_] + iteration_vector_ * AccessType::kElements) / problem_size_.C;
       s = (filter_s_[iteration_contiguous_] + wrap_c) % problem_size_.S;
       int wrap_s = (filter_s_[iteration_contiguous_] + wrap_c) / problem_size_.S;
       r = filter_r_[iteration_contiguous_] + wrap_s;
     }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -208,15 +208,15 @@
   TensorCoord at() const {
     int r = precomputed_filter_r_[iteration_contiguous_];
     int s = precomputed_filter_s_[iteration_contiguous_];
     int c = filter_c_[iteration_contiguous_];
 
     if (kAccessesPerVector > 1) {
       // This code section is only to support non-128b alignment
-      // Multiple access to support non-128b alignment in contiguous dimension
+      // Multiple access to support non-128b alignment in contiguous dimenstion
       int wrap_c;
       params_.c_divmod(wrap_c, c, c + iteration_vector_ * AccessType::kElements);
 
       if (problem_size_.mode == Mode::kConvolution) {
         s -= (problem_size_.dilation_w * wrap_c);
         
         int wrap_s;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv2d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_analytic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_dgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_analytic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_fprop_filter_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_activation_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_analytic.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/conv3d_wgrad_output_gradient_tile_access_iterator_optimized.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_direct_conv_params.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_fixed_stride_dilation.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_activation_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_direct_conv_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_filter_tile_access_iterator_direct_conv_optimized.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -237,15 +237,15 @@
 
     int smem_write_stage_idx = 1;
     // Depthwise specific
     int channel_start_index = 0;
     int rs_plane_idx = 0;
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sparse_sm80.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,139 +24,57 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Defines basic properties needed by CTA-level GEMMs assuming expectations about data
-      layout of the global memory fragments, data types, and internal tile sizes.
+    \brief Defines basic properties needed by CTA-level GEMMs assuming
+   expectations about data layout of the global memory fragments, data types,
+   and internal tile sizes.
 
-      Partial specializations for threadblock::Mma operations targeting depthwise related simt instructions.
+      Partial specializations for threadblock::Mma operations targeting sparse
+   TensorOp instructions.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
 #include "cutlass/array.h"
+#include "cutlass/cutlass.h"
 
-#include "cutlass/numeric_types.h"
-#include "cutlass/matrix_shape.h"
-
-#include "cutlass/gemm/warp/mma.h"
-
-#include "cutlass/conv/convolution.h"
-#include "cutlass/conv/warp/mma_depthwise_simt.h"
-
-#include "cutlass/gemm/threadblock/mma_pipelined.h"
-#include "cutlass/gemm/threadblock/mma_singlestage.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm80.h"
 
-#include "cutlass/gemm/threadblock/mma_base.h"
-#include "cutlass/conv/threadblock/depthwise_mma_base.h"
+#include "cutlass/gemm/warp/mma_simt_policy.h"
+#include "cutlass/gemm/warp/mma_simt.h"
+#include "cutlass/gemm/warp/default_mma_sparse_tensor_op.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
 
-#include "cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h"
+#include "cutlass/gemm/threadblock/default_mma_core.h"
 
-#include "cutlass/arch/cache_operation.h" 
+#include "cutlass/matrix_shape.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/transform/pitch_linear_thread_map.h"
+#include "cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h"
+#include "cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h"
+#include "cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h"
+#include "cutlass/gemm/threadblock/mma_sparse_multistage.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
+namespace gemm {
 namespace threadblock {
 
-namespace detail {
-//
-// Convert a WarpShapeM which is the whole tile of elements into the number of elements (2D) held by
-// each partitions within warp. 
-// The goal is for each thread's tile of elements to be as square as
-// possible for performance (4x4 will be faster than 2x8).
-template<int WarpShapeM,  // The number of elements (1D) contained in the entire warp
-         int WarpNumThreadsM> // The number of partitions within the warp
-struct SimtWarpShape {
-  // kP * kQ * WarpNumThreadsM = WarpShapeM
-  // If needed, enable more specializations.
-};
-template <>
-struct SimtWarpShape<4, 4> {
-  static constexpr int kP = 1;
-  static constexpr int kQ = 1;
-};
-
-template <>
-struct SimtWarpShape<4, 2> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 1;
-};
-
-template <>
-struct SimtWarpShape<4, 1> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 2;
-};
-
-template <>
-struct SimtWarpShape<8, 1> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 4;
-};
-template <>
-struct SimtWarpShape<8, 2> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 2;
-};
-template <>
-struct SimtWarpShape<8, 4> {
-  static constexpr int kP = 1;
-  static constexpr int kQ = 2;
-};
-
-template <>
-struct SimtWarpShape<16, 1> {
-  static constexpr int kP = 4;
-  static constexpr int kQ = 4;
-};
-template <>
-struct SimtWarpShape<16, 2> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 4;
-};
-template <>
-struct SimtWarpShape<16, 4> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 2;
-};
-
-template <int WarpNumThreadsM>
-struct SimtWarpShape<25, WarpNumThreadsM> {
-  static_assert(WarpNumThreadsM == 1, "WarpShapeM could not be evenly splited by threads");
-  static constexpr int kP = 5;
-  static constexpr int kQ = 5;
-};
-
-template <>
-struct SimtWarpShape<32, 1> {
-  static constexpr int kP = 4;
-  static constexpr int kQ = 8;
-};
-
-template <>
-struct SimtWarpShape<32, 2> {
-  static constexpr int kP = 4;
-  static constexpr int kQ = 4;
-};
-
-template <>
-struct SimtWarpShape<32, 4> {
-  static constexpr int kP = 2;
-  static constexpr int kQ = 4;
-};
-
-}  // namespace detail
+////////////////////////////////////////////////////////////////////////////////
 
+/// Template defininng default matrix multiply operators inferred from threadblock tile size,
+/// global memory data layout, and target math instruction.
 template <
     /// Shape of threadblock-scoped matrix multiply operator
     typename Shape,
     /// Shape of warp-level matrix multiply operator
     typename WarpShape,
     /// Shape of one matrix production operation (concept: GemmShape)
     typename InstructionShape,
@@ -170,783 +88,747 @@
     typename LayoutB,
     /// Data type of accumulator
     typename ElementC,
     /// Layout of accumulator
     typename LayoutC,
     /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
     typename OperatorClass,
-    /// Size of a warp-scoped per thread access
-    int kLaneAccessSizeA_ = 0,
-    /// Size of a warp-scoped per thread access 
-    int kLaneAccessSizeB_ = 0,
     /// Number of stages
-    int Stages = 2,
-    /// Operation performed by MMA
-    typename Operator = typename platform::conditional<
-        (platform::is_same<OperatorClass,
-                           cutlass::arch::OpClassTensorOp>::value) &&
-            (platform::is_same<ElementA, int8_t>::value ||
-             platform::is_same<ElementA, int4b_t>::value ||
-             platform::is_same<ElementA, uint8_t>::value ||
-             platform::is_same<ElementA, uint4b_t>::value),
-        cutlass::arch::OpMultiplyAddSaturate,
-        cutlass::arch::OpMultiplyAdd>::type,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
-    /// Cache operation of operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA =
-        cutlass::arch::CacheOperation::Global,
-    /// Cache operation of operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB =
-        cutlass::arch::CacheOperation::Global,
-    /// per-element transformation for elements of A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// per-element transformation for elements of B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
->
-struct DepthwiseMmaCoreWithLaneAccessSize;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <
-    /// Shape of threadblock-scoped matrix multiply operator
-    typename Shape,
-    /// Shape of threadblock-scoped output tile 
-    typename ThreadBlockOutputShape,
-    /// Shape of filter shape per threadblock
-    typename FilterShape,
-    /// Shape of warp-level matrix multiply operator
-    typename WarpShape,
-    /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape,
-    /// Element data type of A operand
-    typename ElementA,
-    /// Layout of operand A
-    typename LayoutA,
-    /// Element data type of B operand
-    typename ElementB,
-    /// Layout of operand B
-    typename LayoutB,
-    /// Data type of accumulator
-    typename ElementC,
-    /// Layout of accumulator
-    typename LayoutC,
-    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
-    typename OperatorClass,
-    /// Size of a warp-scoped per thread access
-    int kLaneAccessSizeA_ = 0,
-    /// Size of a warp-scoped per thread access 
-    int kLaneAccessSizeB_ = 0,
-    /// Number of stages
-    int Stages = 2,
+    int Stages,
     /// Operation performed by MMA
     typename Operator = typename platform::conditional<
         (platform::is_same<OperatorClass,
                            cutlass::arch::OpClassTensorOp>::value) &&
             (platform::is_same<ElementA, int8_t>::value ||
              platform::is_same<ElementA, int4b_t>::value ||
              platform::is_same<ElementA, uint8_t>::value ||
              platform::is_same<ElementA, uint4b_t>::value),
         cutlass::arch::OpMultiplyAddSaturate,
         cutlass::arch::OpMultiplyAdd>::type,
-    /// Iterator algo type
-    conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kAnalytic,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape = cutlass::MatrixShape<-1, -1>,   
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape =  cutlass::MatrixShape<-1, -1>,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape = cutlass::conv::TensorNHWCShape<-1,-1,-1,-1>,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false,
+    bool AccumulatorsInRowMajor = false
     /// Cache operation of operand A
-    cutlass::arch::CacheOperation::Kind CacheOpA =
+    , cutlass::arch::CacheOperation::Kind CacheOpA =
         cutlass::arch::CacheOperation::Global,
     /// Cache operation of operand B
     cutlass::arch::CacheOperation::Kind CacheOpB =
-        cutlass::arch::CacheOperation::Global,
-    /// per-element transformation for elements of A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// per-element transformation for elements of B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
+        cutlass::arch::CacheOperation::Global
 >
-struct DepthwiseDirectConvMmaCoreWithLaneAccessSize;
+struct DefaultSparseMmaCore;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
+/// Partial specialization:
+///
+///   A: column-major
+///   B: row-major
+///   Operator: tensor op class
+///
+/// This uses the default warp-level operator given tile sizes
 template <
-    /// Shape of threadblock-scoped matrix multiply operator
-    typename Shape,
-    /// Shape of warp-level matrix multiply operator
-    typename WarpShape,
+    /// Shape of threadblock-scoped matrix multiply operator (concept:
+    /// GemmShape)
+    typename Shape_,
+    /// Shape of warp-level matrix multiply operator (concept: GemmShape)
+    typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape,
-    /// Element data type of A operand
-    typename ElementA,
-    /// Layout of operand A
-    typename LayoutA,
-    /// Element data type of B operand
-    typename ElementB,
-    /// Layout of operand B
-    typename LayoutB,
+    typename InstructionShape_,
+    /// Data type of A operand
+    typename ElementA_,
+    /// Data type of B operand
+    typename ElementB_,
     /// Data type of accumulator
-    typename ElementC,
+    typename ElementC_,
     /// Layout of accumulator
-    typename LayoutC,
-    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
-    typename OperatorClass,
+    typename LayoutC_,
     /// Number of stages
     int Stages,
     /// Operation performed by MMA
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor,
+    typename Operator_,
     /// Cache operation of operand A
     cutlass::arch::CacheOperation::Kind CacheOpA,
     /// Cache operation of operand B
-    cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// per-element transformation for elements of A
-    ComplexTransform TransformA,
-    /// per-element transformation for elements of B
-    ComplexTransform TransformB,
-    bool IsComplex
->
-struct DepthwiseMmaCoreWithLaneAccessSize<
-    Shape, WarpShape, InstructionShape,
-    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, -1, -1, Stages, Operator, AccumulatorsInRowMajor,
-    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> : cutlass::gemm::threadblock::DefaultMmaCore<
-    Shape, WarpShape, InstructionShape,
-    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, Stages, Operator, AccumulatorsInRowMajor,
-    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> {};
+    cutlass::arch::CacheOperation::Kind CacheOpB>
+struct DefaultSparseMmaCore<Shape_, WarpShape_, InstructionShape_, ElementA_,
+                      layout::ColumnMajor, ElementB_, layout::RowMajor,
+                      ElementC_, LayoutC_, arch::OpClassTensorOp, Stages,
+                      Operator_, false, CacheOpA, CacheOpB> {
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using InstructionShape = InstructionShape_;
+  using ElementA = ElementA_;
+  using LayoutA = layout::ColumnMajor;
+  using ElementB = ElementB_;
+  using LayoutB = layout::RowMajor;
+  using ElementC = ElementC_;
+  using LayoutC = LayoutC_;
+  static int const kStages = Stages;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
+
+  static int const kSparse = 2;
+
+  /// Number of warps present
+  using WarpCount = GemmShape<Shape::kM / WarpShape::kM,
+                              Shape::kN / WarpShape::kN, 
+                              Shape::kK / WarpShape::kK>;
+
+  // Divisility requirements
+  static_assert(
+      !(Shape::kM % WarpShape::kM) && !(Shape::kN % WarpShape::kN),
+      "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size.");
+
+  /// Number of threads per warp
+  static int const kWarpSize = warp::WarpSize<arch::OpClassTensorOp>::value;
+ 
+  /// Number of threads total
+  static int const kThreads = WarpCount::kCount * kWarpSize;
+
+  /// Size of a threadblock-scoped access
+  static int const kAccessSizeInBits = 128;
+
+  /// Default Operator
+  using Operator = Operator_;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  //
+  // Shared memory layouts
+  //
+
+  using SmemLayoutA = layout::ColumnMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
+
+  // Shared memory layout
+  using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
+
+  //
+  // Iterators to write to shared memory
+  //
+
+  /// ThreadMap of iterator A
+  using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kM, Shape::kK / kSparse>, kThreads,
+      layout::PitchLinearShape<8, 4>,
+      kAccessSizeInBits / sizeof_bits<ElementA>::value>;
+
+  /// Shared memory iterator to A operand
+  using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM, Shape::kK / kSparse>, ElementA, SmemLayoutA, 1,
+      IteratorThreadMapA>;
+
+  /// ThreadMap of iterator B
+  using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kN, Shape::kK>, kThreads,
+      layout::PitchLinearShape<8, 4>,
+      kAccessSizeInBits / sizeof_bits<ElementB>::value>;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 0,
+      IteratorThreadMapB>;
+
+  //
+  // Warp-level matrix multiply operator
+  //
+
+  // Define the warp-level tensor op
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultSparseMmaTensorOp<
+      WarpShape, InstructionShape, ElementA, SmemLayoutA, ElementB, SmemLayoutB,
+      ElementC, LayoutC, Operator, WarpCount::kK>::Type;
+
+  /// Cache operation of operand E
+  static cutlass::arch::CacheOperation::Kind const kCacheOpE =
+      cutlass::arch::CacheOperation::Global;
+
+  static int const kInterleavedE = MmaTensorOp::kInterleaved;
+  static int const kMetaSizeInBits = MmaTensorOp::kMetaSizeInBits;
+  static int const kMaxID2 = MmaTensorOp::kMaxID2;
+  static int const kElementsPerElementE = MmaTensorOp::kElementsPerElementE;
+
+  using ElementE = typename MmaTensorOp::ElementE;
+  using GmemLayoutE = cutlass::layout::ColumnMajorInterleaved<kInterleavedE>;
+
+  // Shared memory layout.  Interleaved layout is mapped to PitchLinear layout.
+  using SmemLayoutE = typename MmaTensorOp::LayoutE;
+
+  /// ThreadMap of iterator E
+  static int const kElementsPerAccessE =
+      kAccessSizeInBits / sizeof_bits<ElementE>::value;
+
+  /// E is tiny.  Not all warps are needed.
+  static int const kThreadsE =
+      (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+           (kAccessSizeInBits / sizeof_bits<ElementE>::value) >
+       kThreads)
+          ? kThreads
+          : (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+             (kAccessSizeInBits / sizeof_bits<ElementE>::value));
+
+  using IteratorThreadMapE = transform::PitchLinearStripminedThreadMap<
+      layout::PitchLinearShape<Shape::kM * kInterleavedE,
+                               Shape::kK / kSparse / kElementsPerElementE /
+                                   kInterleavedE>,
+      kThreadsE, kElementsPerAccessE>;
+
+  /// Shared memory iterator to E operand
+  using SmemIteratorE = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM * kInterleavedE,
+                  Shape::kK / kSparse / kElementsPerElementE / kInterleavedE>,
+      ElementE, SmemLayoutE, 0, IteratorThreadMapE>;
+
+  /// Policy used to define MmaPipelined
+  using MmaPolicy =
+      SparseMmaPolicy<MmaTensorOp, MatrixShape<0, 0>, MatrixShape<0, 0>,
+                      MatrixShape<0, 0>, WarpCount::kK>;
+};
+
+////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization:
 ///
 ///   A: row-major
 ///   B: column-major
-///   Operator: simt class
+///   Operator: tensor op class
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
     typename WarpShape_,
+    /// Shape of one matrix production operation (concept: GemmShape)
+    typename InstructionShape_,
     /// Data type of A operand
     typename ElementA_,
     /// Data type of B operand
     typename ElementB_,
     /// Data type of accumulator
     typename ElementC_,
     /// Layout of accumulator
     typename LayoutC_,
-    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
-    int kLaneAccessSizeA_,
-    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
-    int kLaneAccessSizeB_,
-    /// Operation performed by GEMM
-    typename Operator_>
-struct DepthwiseMmaCoreWithLaneAccessSize<Shape_,
-                                        WarpShape_,
-                                        cutlass::gemm::GemmShape<1, 1, 1>,
-                                        ElementA_,
-                                        layout::RowMajor,
-                                        ElementB_,
-                                        layout::ColumnMajor,
-                                        ElementC_,
-                                        LayoutC_,
-                                        arch::OpClassSimt,
-                                        kLaneAccessSizeA_,
-                                        kLaneAccessSizeB_,
-                                        2,
-                                        Operator_> : public cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
-                                                                           WarpShape_,
-                                                                           cutlass::gemm::GemmShape<1, 1, 1>,
-                                                                           ElementA_,
-                                                                           layout::RowMajor,
-                                                                           ElementB_,
-                                                                           layout::ColumnMajor,
-                                                                           ElementC_,
-                                                                           LayoutC_,
-                                                                           arch::OpClassSimt,
-                                                                           2,
-                                                                           Operator_> {
-  using Base = cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
-                              WarpShape_,
-                              cutlass::gemm::GemmShape<1, 1, 1>,
-                              ElementA_,
-                              layout::RowMajor,
-                              ElementB_,
-                              layout::ColumnMajor,
-                              ElementC_,
-                              LayoutC_,
-                              arch::OpClassSimt,
-                              2,
-                              Operator_>;
-
+    /// Number of stages
+    int Stages,
+    /// Operation performed by MMA
+    typename Operator_,
+    /// Cache operation of operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA,
+    /// Cache operation of operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB>
+struct DefaultSparseMmaCore<Shape_, WarpShape_, InstructionShape_, ElementA_,
+                      layout::RowMajor, ElementB_, layout::ColumnMajor,
+                      ElementC_, LayoutC_, arch::OpClassTensorOp, Stages,
+                      Operator_, false, CacheOpA, CacheOpB> {
   using Shape = Shape_;
   using WarpShape = WarpShape_;
-  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  using InstructionShape = InstructionShape_;
   using ElementA = ElementA_;
   using LayoutA = layout::RowMajor;
   using ElementB = ElementB_;
   using LayoutB = layout::ColumnMajor;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
-  using OperatorClass = arch::OpClassSimt;
-
-  static int const kLaneAccessSizeA = kLaneAccessSizeA_;
-  static int const kLaneAccessSizeB = kLaneAccessSizeB_;
+  static int const kStages = Stages;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
-  // Divisility requirements
-  static_assert( kLaneAccessSizeA > 0 && kLaneAccessSizeB > 0,
-    "Size of a warp-scoped per thread access should be larger then ZERO" );
-
-  /// Default Operator
-  using Operator = Operator_;
+  static int const kSparse = 2;
 
   /// Number of warps present
-  using WarpCount = typename Base::WarpCount;
+  using WarpCount = GemmShape<Shape::kM / WarpShape::kM,
+                              Shape::kN / WarpShape::kN, 
+                              Shape::kK / WarpShape::kK>;
 
   // Divisility requirements
   static_assert(
-    !(Shape::kM % WarpShape::kM) &&
-    !(Shape::kN % WarpShape::kN),
-    "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
-  );
+      !(Shape::kM % WarpShape::kM) && !(Shape::kN % WarpShape::kN),
+      "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size.");
 
   /// Number of threads per warp
-  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
+  static int const kWarpSize = warp::WarpSize<arch::OpClassTensorOp>::value;
+
+  /// Number of threads total
+  static int const kThreads = WarpCount::kCount * kWarpSize;
+
+  /// Size of a threadblock-scoped access
+  static int const kAccessSizeInBits = 128;
+
+  /// Default Operator
+  using Operator = Operator_;
+
+  // Warp thread arrangement
+  static int const kWarpThreadArrangementContiguousA =
+      Shape::kK / kSparse / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
+
+  // crosswise cannot be larger than 1024 bit.
+  static int const kCrosswiseB =
+      (Shape::kK > (1024 / sizeof_bits<ElementB>::value))
+          ? (1024 / sizeof_bits<ElementB>::value)
+          : Shape::kK;
 
-  static int const kElementsPerAccess = 1;
+  static int const kWarpThreadArrangementContiguousB =
+      kCrosswiseB / (kAccessSizeInBits / sizeof_bits<ElementB>::value);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
 
   //
   // Shared memory layouts
   //
 
-  using SmemLayoutA = layout::ColumnMajor;
-  using SmemLayoutB = layout::RowMajor;
+  using SmemLayoutA = layout::RowMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<ElementA>::value, Shape::kK / kSparse>;
+
+  // Shared memory layout
+  using SmemLayoutB = layout::ColumnMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<ElementB>::value, kCrosswiseB>;
 
   //
-  // Iterators to write to shared memory are same as base class
+  // Iterators to write to shared memory
   //
 
+  /// ThreadMap of iterator A
+  using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kK / kSparse, Shape::kM>, kThreads,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                               kWarpThreadArrangementStridedA>,
+      kAccessSizeInBits / sizeof_bits<ElementA>::value>;
+
+  /// Shared memory iterator to A operand
+  using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM, Shape::kK / kSparse>, ElementA, SmemLayoutA, 0,
+      IteratorThreadMapA>;
+
+  /// ThreadMap of iterator B
+  using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kK, Shape::kN>, kThreads,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                               kWarpThreadArrangementStridedB>,
+      kAccessSizeInBits / sizeof_bits<ElementB>::value>;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 1,
+      IteratorThreadMapB>;
+
   //
   // Warp-level matrix multiply operator
   //
 
-  // Define the warp-level op
-  static const int WarpNumThreadsM = cutlass::gemm::threadblock::detail::simt_get_warp_threads_m<WarpShape>(); 
-  static const int WarpNumThreadsN = kWarpSize / WarpNumThreadsM;
-  static const int ThreadTileM = WarpShape::kM / WarpNumThreadsM;
-  static const int ThreadTileN = WarpShape::kN / WarpNumThreadsN;
-  static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
-      "WarpShape must be divisible by ThreadTile shape.");
-  static const int LaneLayout = ThreadTileM > 4 && ThreadTileN > 4 ? 2 : 1;
-  static const int numElementsA = kLaneAccessSizeA / sizeof_bits<ElementA>::value;
-  static const int numElementsB = kLaneAccessSizeB / sizeof_bits<ElementB>::value;
-  static const int LaneM = cutlass::const_min(numElementsA, ThreadTileM);
-  static const int LaneN = cutlass::const_min(numElementsB, ThreadTileN);
-
-  static int const kPaddingM = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementA>::value);
-  static int const kPaddingN = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementB>::value);
-
-  static_assert(!(kPaddingM % LaneM) && !(kPaddingN % LaneN),
-                "Padding must be divisible by Lane");
-
-  // these should have max of thread tile also
-  using LaneMmaShape = cutlass::gemm::GemmShape<
-      LaneM,
-      LaneN,
-      1>;
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-      cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
-      cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
-      LaneMmaShape
-  >;
-
-  using MmaWarpSimt = cutlass::conv::warp::MmaDepthwiseSimt<
-      WarpShape,      /// Size of the Gemm problem - concept: gemm::GemmShape<>
-      ElementA,       /// Data type of A elements
-      SmemLayoutA,    /// Layout of A matrix (concept: MatrixLayout)
-      ElementB,       /// Data type of B elements
-      SmemLayoutB,    /// Layout of B matrix (concept: MatrixLayout)
-      ElementC,       /// Element type of C matrix
-      LayoutC,        /// Layout of C matrix (concept: MatrixLayout)
-      Policy          /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-  >;
-
-  /// Policy used to define MmaPipelined 
-  using MmaPolicy = cutlass::gemm::threadblock::MmaPolicy<
-    MmaWarpSimt,
-    MatrixShape<kPaddingM, 0>,    // skew for A matrix to avoid SMEM bank conflicts
-    MatrixShape<0, kPaddingN>,    // skew for B matrix to avoid SMEM bank conflicts
-    WarpCount::kK
-  >;
+  // Define the warp-level tensor op
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultSparseMmaTensorOp<
+      WarpShape, InstructionShape, ElementA, SmemLayoutA, ElementB, SmemLayoutB,
+      ElementC, LayoutC, Operator, WarpCount::kK>::Type;
+
+  /// Cache operation of operand E
+  static cutlass::arch::CacheOperation::Kind const kCacheOpE =
+      cutlass::arch::CacheOperation::Global;
+
+  static int const kInterleavedE = MmaTensorOp::kInterleaved;
+  static int const kMetaSizeInBits = MmaTensorOp::kMetaSizeInBits;
+  static int const kMaxID2 = MmaTensorOp::kMaxID2;
+  static int const kElementsPerElementE = MmaTensorOp::kElementsPerElementE;
+
+  using ElementE = typename MmaTensorOp::ElementE;
+  using GmemLayoutE = cutlass::layout::ColumnMajorInterleaved<kInterleavedE>;
+
+  // Shared memory layout.  Interleaved layout is mapped to PitchLinear layout.
+  using SmemLayoutE = typename MmaTensorOp::LayoutE;
+
+  /// ThreadMap of iterator E
+  static int const kElementsPerAccessE =
+      kAccessSizeInBits / sizeof_bits<ElementE>::value;
+
+  /// E is tiny.  Not all warps are needed.
+  static int const kThreadsE =
+      (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+           (kAccessSizeInBits / sizeof_bits<ElementE>::value) >
+       kThreads)
+          ? kThreads
+          : (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+             (kAccessSizeInBits / sizeof_bits<ElementE>::value));
+
+  using IteratorThreadMapE = transform::PitchLinearStripminedThreadMap<
+      layout::PitchLinearShape<Shape::kM * kInterleavedE,
+                               Shape::kK / kSparse / kElementsPerElementE /
+                                   kInterleavedE>,
+      kThreadsE, kElementsPerAccessE>;
+
+
+  /// Shared memory iterator to E operand
+  using SmemIteratorE = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM * kInterleavedE,
+                  Shape::kK / kSparse / kElementsPerElementE / kInterleavedE>,
+      ElementE, SmemLayoutE, 0, IteratorThreadMapE>;
+
+  /// Policy used to define MmaPipelined
+  using MmaPolicy =
+      SparseMmaPolicy<MmaTensorOp, MatrixShape<0, 0>, MatrixShape<0, 0>,
+                      MatrixShape<0, 0>, WarpCount::kK>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization:
 ///
-///   A: row-major
-///   B: row-major
-///   Operator: simt class
+///   A: column-major
+///   B: column-major
+///   Operator: tensor op class
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
-    /// Shape of threadblock-scoped output tile (concept: TensorNHWCShape)
-    typename ThreadBlockOutputShape_,
-    /// Shape of filter shape per threadblock
-    typename FilterShape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
     typename WarpShape_,
+    /// Shape of one matrix production operation (concept: GemmShape)
+    typename InstructionShape_,
     /// Data type of A operand
     typename ElementA_,
     /// Data type of B operand
     typename ElementB_,
     /// Data type of accumulator
     typename ElementC_,
     /// Layout of accumulator
     typename LayoutC_,
-    /// Size of a warp-scoped per thread access
-    int kLaneAccessSizeA_,
     /// Number of stages
-    int Stages_,
-    /// Operation performed by GEMM
-    typename Operator_>
-struct DepthwiseDirectConvMmaCoreWithLaneAccessSize<Shape_,
-                                                    ThreadBlockOutputShape_,
-                                                    FilterShape_,
-                                                    WarpShape_,
-                                                    cutlass::gemm::GemmShape<1, 1, 1>,
-                                                    ElementA_,
-                                                    layout::RowMajor,
-                                                    ElementB_,
-                                                    layout::ColumnMajor,
-                                                    ElementC_,
-                                                    LayoutC_,
-                                                    arch::OpClassSimt,
-                                                    kLaneAccessSizeA_,
-                                                    128,
-                                                    Stages_,
-                                                    Operator_> {
+    int Stages,
+    /// Operation performed by MMA
+    typename Operator_,
+    /// Cache operation of operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA,
+    /// Cache operation of operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB>
+struct DefaultSparseMmaCore<Shape_, WarpShape_, InstructionShape_, ElementA_,
+                      layout::ColumnMajor, ElementB_, layout::ColumnMajor,
+                      ElementC_, LayoutC_, arch::OpClassTensorOp, Stages,
+                      Operator_, false, CacheOpA, CacheOpB> {
   using Shape = Shape_;
-  using FilterShape = FilterShape_;
   using WarpShape = WarpShape_;
-  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  using InstructionShape = InstructionShape_;
   using ElementA = ElementA_;
-  using LayoutA = layout::RowMajor;
+
+  using LayoutA = layout::ColumnMajor;
   using ElementB = ElementB_;
   using LayoutB = layout::ColumnMajor;
+
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
-  using OperatorClass = arch::OpClassSimt;
+  static int const kStages = Stages;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
-  static int const kLaneAccessSizeB = 128;
-
-  // Divisility requirements
-  static_assert( kLaneAccessSizeB > 0,
-    "Size of a warp-scoped per thread access should be larger then ZERO" );
-
-  /// Default Operator
-  using Operator = Operator_;
+  static int const kSparse = 2;
 
   /// Number of warps present
-  using WarpCount = cutlass::gemm::GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    1
-  >;
+  using WarpCount = GemmShape<Shape::kM / WarpShape::kM,
+                              Shape::kN / WarpShape::kN, 
+                              Shape::kK / WarpShape::kK>;
 
   // Divisility requirements
   static_assert(
-    !(Shape::kM % WarpShape::kM) &&
-    !(Shape::kN % WarpShape::kN),
-    "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
-  );
+      !(Shape::kM % WarpShape::kM) && !(Shape::kN % WarpShape::kN),
+      "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size.");
 
   /// Number of threads per warp
-  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
+  static int const kWarpSize = warp::WarpSize<arch::OpClassTensorOp>::value;
 
   /// Number of threads total
   static int const kThreads = WarpCount::kCount * kWarpSize;
-  
-  // For Gmem load
-  static int const kElementsPerAccessA = 128 / sizeof_bits<ElementA>::value;
-  static int const kElementsPerAccessB = 128 / sizeof_bits<ElementB>::value;
+
+  /// Size of a threadblock-scoped access
+  static int const kAccessSizeInBits = 128;
+
+  /// Default Operator
+  using Operator = Operator_;
+
+  // Warp thread arrangement
+  // crosswise cannot be larger than 1024 bit.
+  static int const kCrosswiseB =
+      (Shape::kK > (1024 / sizeof_bits<ElementB>::value))
+          ? (1024 / sizeof_bits<ElementB>::value)
+          : Shape::kK;
+
+  static int const kWarpThreadArrangementContiguousB =
+      kCrosswiseB / (kAccessSizeInBits / sizeof_bits<ElementB>::value);
+
+  static int const kWarpThreadArrangementStridedB =
+      kWarpSize / kWarpThreadArrangementContiguousB;
 
   //
   // Shared memory layouts
   //
 
-  using SmemLayoutA = layout::RowMajor;
-  using SmemLayoutB = layout::RowMajor;
+  using SmemLayoutA = layout::ColumnMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<ElementA>::value, int(128 / sizeof(ElementA))>;
 
+  // Shared memory layout
+  using SmemLayoutB = layout::ColumnMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<ElementB>::value, kCrosswiseB>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
-  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kN, 1>, // Set kStrided = 1 because activation shape is runtime value.
-    kThreads,
-    kElementsPerAccessA
-  >;
-
-  /// ThreadMap of iterator A
-  using SmemThreadMapA = IteratorThreadMapA;
+  using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kM, Shape::kK / kSparse>, kThreads,
+      layout::PitchLinearShape<8, 4>,
+      kAccessSizeInBits / sizeof_bits<ElementA>::value>;
 
   /// Shared memory iterator to A operand
-  using SmemIteratorA = transform::threadblock::RegularTileAccessIteratorDirectConv<
-    MatrixShape<1, Shape::kN>, // set kRow is 1 because it is a runtime value
-    ElementA, 
-    SmemLayoutA,
-    0,
-    SmemThreadMapA, // was IteratorThreadMapA
-    true  // Dynamic iterations.
-  >;
+  using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM, Shape::kK / kSparse>, ElementA, SmemLayoutA, 1,
+      IteratorThreadMapA>;
 
   /// ThreadMap of iterator B
-  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kN, FilterShape::kCount>,
-    kThreads,
-    kElementsPerAccessB
-  >;
-
-  /// Transpose the ThreadMap of iterator B
-  using SmemThreadMapB = IteratorThreadMapB;
-
-  /// Shared memory iterator to B operand                                                  
-  using SmemIteratorB = transform::threadblock::RegularTileAccessIteratorDirectConv<
-    MatrixShape<FilterShape::kCount, Shape::kN>,
-    ElementB, 
-    SmemLayoutB,
-    0,
-    SmemThreadMapB, // was IteratorThreadMapB
-    false   // static iterations.
-  >;
+  using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kK, Shape::kN>, kThreads,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousB,
+                               kWarpThreadArrangementStridedB>,
+      kAccessSizeInBits / sizeof_bits<ElementB>::value>;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 1,
+      IteratorThreadMapB>;
 
   //
   // Warp-level matrix multiply operator
   //
-  // Groups per threads
-  // Fp32: 2 groups
-  // Fp16: 2 groups
-  static const int GroupsPerThread = sizeof(ElementB) > 1 ? 2 : 4;
-  // Define the warp-level op  
-  static const int WarpNumThreadsN = cutlass::const_min(WarpShape::kN / GroupsPerThread, kWarpSize);
-  static const int WarpNumThreadsM = kWarpSize / WarpNumThreadsN; 
-
-  static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
-      "WarpShape must be divisible by ThreadTile shape.");
-
-  // Get output P, Q per thread
-  static const int TileP = cutlass::conv::threadblock::detail::SimtWarpShape<WarpShape::kM, WarpNumThreadsM>::kP;
-  static const int TileQ = cutlass::conv::threadblock::detail::SimtWarpShape<WarpShape::kM, WarpNumThreadsM>::kQ;
-
-  static const int LaneLayout = 1;
-  static const int numElementsB = kLaneAccessSizeB / sizeof_bits<ElementB>::value;
-  static const int LaneN = cutlass::const_min(numElementsB, WarpShape::kN / WarpNumThreadsN);
-  
-  // Define the output tile computed by each thread
-  using ThreadOutputShape = cutlass::conv::TensorNHWCShape<1, TileP, TileQ, LaneN>;
-
-  // Fetch the channel with same access size
-  static const int LaneM = LaneN;
-
-  // No paddings
-  static int const kPaddingM = 0;
-  static int const kPaddingN = 0;
-
-  static_assert(!(kPaddingM % LaneM) && !(kPaddingN % LaneN),
-                "Padding must be divisible by Lane");
-
-  // these should have max of thread tile also
-  using LaneMmaShape = cutlass::gemm::GemmShape<
-      LaneM,
-      LaneN,
-      1>;
-  
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-      cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
-      cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
-      LaneMmaShape
-  >;
-
-  using MmaWarpSimt = cutlass::conv::warp::MmaDepthwiseDirectConvSimt<
-      WarpShape,      /// Size of the Gemm problem - concept: gemm::GemmShape<>
-      FilterShape,    /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
-      ThreadOutputShape, /// Size of the output tile computed by thread - concept: conv::TensorNHWCShape<>
-      ThreadBlockOutputShape_, /// Size of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
-      ElementA,       /// Data type of A elements
-      SmemLayoutA,    /// Layout of A matrix (concept: MatrixLayout)
-      ElementB,       /// Data type of B elements
-      SmemLayoutB,    /// Layout of B matrix (concept: MatrixLayout)
-      ElementC,       /// Element type of C matrix
-      LayoutC,        /// Layout of C matrix (concept: MatrixLayout)
-      Policy          /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-  >;
-
-  /// Policy used to define MmaPipelined 
-  using MmaPolicy = cutlass::conv::threadblock::DepthwiseDirectConvMmaPolicy<
-    MmaWarpSimt,
-    MatrixShape<kPaddingM, 0>,    // skew for A matrix to avoid SMEM bank conflicts
-    MatrixShape<0, kPaddingN>,    // skew for B matrix to avoid SMEM bank conflicts
-    IteratorThreadMapA,
-    IteratorThreadMapB,
-    WarpCount::kK
-  >;
+
+  // Define the warp-level tensor op
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultSparseMmaTensorOp<
+      WarpShape, InstructionShape, ElementA, SmemLayoutA, ElementB, SmemLayoutB,
+      ElementC, LayoutC, Operator, WarpCount::kK>::Type;
+
+  /// Cache operation of operand E
+  static cutlass::arch::CacheOperation::Kind const kCacheOpE =
+      cutlass::arch::CacheOperation::Global;
+
+  static int const kInterleavedE = MmaTensorOp::kInterleaved;
+  static int const kMetaSizeInBits = MmaTensorOp::kMetaSizeInBits;
+  static int const kMaxID2 = MmaTensorOp::kMaxID2;
+  static int const kElementsPerElementE = MmaTensorOp::kElementsPerElementE;
+
+  using ElementE = typename MmaTensorOp::ElementE;
+  using GmemLayoutE = cutlass::layout::ColumnMajorInterleaved<kInterleavedE>;
+
+  // Shared memory layout.  Interleaved layout is mapped to PitchLinear layout.
+  using SmemLayoutE = typename MmaTensorOp::LayoutE;
+
+  /// ThreadMap of iterator E
+  static int const kElementsPerAccessE =
+      kAccessSizeInBits / sizeof_bits<ElementE>::value;
+
+  /// E is tiny.  Not all warps are needed.
+  static int const kThreadsE =
+      (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+           (kAccessSizeInBits / sizeof_bits<ElementE>::value) >
+       kThreads)
+          ? kThreads
+          : (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+             (kAccessSizeInBits / sizeof_bits<ElementE>::value));
+
+  using IteratorThreadMapE = transform::PitchLinearStripminedThreadMap<
+      layout::PitchLinearShape<Shape::kM * kInterleavedE,
+                               Shape::kK / kSparse / kElementsPerElementE /
+                                   kInterleavedE>,
+      kThreadsE, kElementsPerAccessE>;
+
+  /// Shared memory iterator to E operand
+  using SmemIteratorE = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM * kInterleavedE,
+                  Shape::kK / kSparse / kElementsPerElementE / kInterleavedE>,
+      ElementE, SmemLayoutE, 0, IteratorThreadMapE>;
+
+  /// Policy used to define MmaPipelined
+  using MmaPolicy =
+      SparseMmaPolicy<MmaTensorOp, MatrixShape<0, 0>, MatrixShape<0, 0>,
+                      MatrixShape<0, 0>, WarpCount::kK>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization:
 ///
 ///   A: row-major
 ///   B: row-major
-///   Operator: simt class
+///   Operator: tensor op class
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
-    /// Shape of threadblock-scoped output tile (concept: TensorNHWCShape)
-    typename ThreadBlockOutputShape_,
-    /// Shape of filter shape per threadblock
-    typename FilterShape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
     typename WarpShape_,
+    /// Shape of one matrix production operation (concept: GemmShape)
+    typename InstructionShape_,
     /// Data type of A operand
     typename ElementA_,
     /// Data type of B operand
     typename ElementB_,
     /// Data type of accumulator
     typename ElementC_,
     /// Layout of accumulator
     typename LayoutC_,
-    /// Size of a warp-scoped per thread access
-    int kLaneAccessSizeA_,
     /// Number of stages
-    int Stages_,
-    /// Operation performed by GEMM
+    int Stages,
+    /// Operation performed by MMA
     typename Operator_,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape_,   
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape_,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape_>
-struct DepthwiseDirectConvMmaCoreWithLaneAccessSize<Shape_,
-                                                    ThreadBlockOutputShape_,
-                                                    FilterShape_,
-                                                    WarpShape_,
-                                                    cutlass::gemm::GemmShape<1, 1, 1>,
-                                                    ElementA_,
-                                                    layout::RowMajor,
-                                                    ElementB_,
-                                                    layout::ColumnMajor,
-                                                    ElementC_,
-                                                    LayoutC_,
-                                                    arch::OpClassSimt,
-                                                    kLaneAccessSizeA_,
-                                                    128,
-                                                    Stages_,
-                                                    Operator_,
-                                                    IteratorAlgorithm::kFixedStrideDilation,
-                                                    StrideShape_,
-                                                    DilationShape_,
-                                                    ActivationShape_> {
+    /// Cache operation of operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA,
+    /// Cache operation of operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB>
+struct DefaultSparseMmaCore<Shape_, WarpShape_, InstructionShape_, ElementA_,
+                      layout::RowMajor, ElementB_, layout::RowMajor, ElementC_,
+                      LayoutC_, arch::OpClassTensorOp, Stages, Operator_,
+                      false, CacheOpA, CacheOpB> {
   using Shape = Shape_;
-  using FilterShape = FilterShape_;
   using WarpShape = WarpShape_;
-  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
+  using InstructionShape = InstructionShape_;
   using ElementA = ElementA_;
   using LayoutA = layout::RowMajor;
   using ElementB = ElementB_;
-  using LayoutB = layout::ColumnMajor;
+  using LayoutB = layout::RowMajor;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
-  using OperatorClass = arch::OpClassSimt;
-  using StrideShape = StrideShape_;
-  using DilationShape = DilationShape_; 
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
-  using ActivationShape = ActivationShape_;
+  static int const kStages = Stages;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
-  static int const kLaneAccessSizeB = 128;
-
-  // Divisility requirements
-  static_assert( kLaneAccessSizeB > 0,
-    "Size of a warp-scoped per thread access should be larger then ZERO" );
-
-  /// Default Operator
-  using Operator = Operator_;
+  static int const kSparse = 2;
 
   /// Number of warps present
-  using WarpCount = cutlass::gemm::GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    1
-  >;
+  using WarpCount = GemmShape<Shape::kM / WarpShape::kM,
+                              Shape::kN / WarpShape::kN, 
+                              Shape::kK / WarpShape::kK>;
 
   // Divisility requirements
   static_assert(
-    !(Shape::kM % WarpShape::kM) &&
-    !(Shape::kN % WarpShape::kN),
-    "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
-  );
+      !(Shape::kM % WarpShape::kM) && !(Shape::kN % WarpShape::kN),
+      "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size.");
 
   /// Number of threads per warp
-  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
+  static int const kWarpSize = warp::WarpSize<arch::OpClassTensorOp>::value;
 
   /// Number of threads total
   static int const kThreads = WarpCount::kCount * kWarpSize;
-  
-  // For Gmem load
-  static int const kElementsPerAccessA = 128 / sizeof_bits<ElementA>::value;
-  static int const kElementsPerAccessB = 128 / sizeof_bits<ElementB>::value;
+
+  /// Size of a threadblock-scoped access
+  static int const kAccessSizeInBits = 128;
+
+  /// Default Operator
+  using Operator = Operator_;
+
+  // Warp thread arrangement
+  static int const kWarpThreadArrangementContiguousA =
+      Shape::kK / kSparse / (kAccessSizeInBits / sizeof_bits<ElementA>::value);
+
+  static int const kWarpThreadArrangementStridedA =
+      kWarpSize / kWarpThreadArrangementContiguousA;
 
   //
   // Shared memory layouts
   //
 
-  using SmemLayoutA = layout::RowMajor;
-  using SmemLayoutB = layout::RowMajor;
+  using SmemLayoutA = layout::RowMajorTensorOpMultiplicandCrosswise<
+      sizeof_bits<ElementA>::value, Shape::kK / kSparse>;
 
+  // Shared memory layout
+  using SmemLayoutB = layout::RowMajorTensorOpMultiplicandCongruous<
+      sizeof_bits<ElementB>::value, int(128 / sizeof(ElementB))>;
 
   //
   // Iterators to write to shared memory
   //
 
   /// ThreadMap of iterator A
-  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<ActivationShape::kC, ActivationShape::kNHW>,
-    kThreads,
-    kElementsPerAccessA
-  >;
-
-  /// ThreadMap of iterator A
-  using SmemThreadMapA = IteratorThreadMapA;
+  using IteratorThreadMapA = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kK / kSparse, Shape::kM>, kThreads,
+      layout::PitchLinearShape<kWarpThreadArrangementContiguousA,
+                               kWarpThreadArrangementStridedA>,
+      kAccessSizeInBits / sizeof_bits<ElementA>::value>;
 
   /// Shared memory iterator to A operand
-  using SmemIteratorA = transform::threadblock::RegularTileAccessIteratorDirectConv<
-    MatrixShape<ActivationShape::kNHW, ActivationShape::kC>,
-    ElementA,
-    SmemLayoutA,
-    0,
-    SmemThreadMapA, // was IteratorThreadMapA
-    false  // static iterations.
-  >;
+  using SmemIteratorA = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM, Shape::kK / kSparse>, ElementA, SmemLayoutA, 0,
+      IteratorThreadMapA>;
 
   /// ThreadMap of iterator B
-  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kN, FilterShape::kCount>,
-    kThreads,
-    kElementsPerAccessB
-  >;
-
-  /// Transpose the ThreadMap of iterator B
-  using SmemThreadMapB = IteratorThreadMapB;
-
-  /// Shared memory iterator to B operand                                                  
-  using SmemIteratorB = transform::threadblock::RegularTileAccessIteratorDirectConv<
-    MatrixShape<FilterShape::kCount, Shape::kN>,
-    ElementB, 
-    SmemLayoutB,
-    0,
-    SmemThreadMapB, // was IteratorThreadMapB
-    false   // static iterations.
-  >;
+  using IteratorThreadMapB = transform::PitchLinearWarpRakedThreadMap<
+      layout::PitchLinearShape<Shape::kN, Shape::kK>, kThreads,
+      layout::PitchLinearShape<8, 4>,
+      kAccessSizeInBits / sizeof_bits<ElementB>::value>;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kK, Shape::kN>, ElementB, SmemLayoutB, 0,
+      IteratorThreadMapB>;
 
   //
   // Warp-level matrix multiply operator
   //
-  // Groups per threads
-  // Fp32: 2 groups
-  // Fp16: 2 groups
-  static const int GroupsPerThread = sizeof(ElementB) > 1 ? 2 : 4;
-  // Define the warp-level op  
-  static const int WarpNumThreadsN = cutlass::const_min(WarpShape::kN / GroupsPerThread, kWarpSize);
-  static const int WarpNumThreadsM = kWarpSize / WarpNumThreadsN; 
-
-  static const int TileP = cutlass::conv::threadblock::detail::SimtWarpShape<WarpShape::kM, WarpNumThreadsM>::kP;
-  static const int TileQ = cutlass::conv::threadblock::detail::SimtWarpShape<WarpShape::kM, WarpNumThreadsM>::kQ;
-
-  static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
-      "WarpShape must be divisible by ThreadTile shape.");
-
-  static const int LaneLayout = 1;
-  static const int numElementsB = kLaneAccessSizeB / sizeof_bits<ElementB>::value;
-  static const int LaneN = cutlass::const_min(numElementsB, WarpShape::kN / WarpNumThreadsN);
-  
-  // Define the output tile computed by each thread
-  using ThreadOutputShape = cutlass::conv::TensorNHWCShape<1, TileP, TileQ, LaneN>;
-
-  // Fetch the channel with same access size
-  static const int LaneM = LaneN;
-
-  // No paddings
-  static int const kPaddingM = 0;
-  static int const kPaddingN = 0;
-
-  static_assert(!(kPaddingM % LaneM) && !(kPaddingN % LaneN),
-                "Padding must be divisible by Lane");
-
-  // these should have max of thread tile also
-  using LaneMmaShape = cutlass::gemm::GemmShape<
-      LaneM,
-      LaneN,
-      1>;
-  
-  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
-      cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
-      cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
-      LaneMmaShape
-  >;
-
-  using MmaWarpSimt = cutlass::conv::warp::MmaDepthwiseDirectConvSimt<
-      WarpShape,      /// Size of the Gemm problem - concept: gemm::GemmShape<>
-      FilterShape,    /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
-      ThreadOutputShape, /// Size of the output tile computed by thread - concept: conv::TensorNHWCShape<>
-      ThreadBlockOutputShape, /// Size of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
-      ElementA,       /// Data type of A elements
-      SmemLayoutA,    /// Layout of A matrix (concept: MatrixLayout)
-      ElementB,       /// Data type of B elements
-      SmemLayoutB,    /// Layout of B matrix (concept: MatrixLayout)
-      ElementC,       /// Element type of C matrix
-      LayoutC,        /// Layout of C matrix (concept: MatrixLayout)
-      Policy,          /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-      IteratorAlgorithm::kFixedStrideDilation, /// Iterator algo type
-      StrideShape,   /// Stride ( MatrixShape<Height, Width> )
-      DilationShape,  /// Dilation ( MatrixShape<Height, Width> )
-      ActivationShape /// Activation Shape loaded by threadblock
-  >;
-
-  /// Policy used to define MmaPipelined 
-  using MmaPolicy = cutlass::conv::threadblock::DepthwiseDirectConvMmaPolicy<
-    MmaWarpSimt,
-    MatrixShape<kPaddingM, 0>,    // skew for A matrix to avoid SMEM bank conflicts
-    MatrixShape<0, kPaddingN>,    // skew for B matrix to avoid SMEM bank conflicts
-    IteratorThreadMapA,
-    IteratorThreadMapB,
-    WarpCount::kK
-  >;
-};
-} // namespace threadblock
-} // namespace conv
-} // namespace cutlass
+
+  // Define the warp-level tensor op
+  using MmaTensorOp = typename cutlass::gemm::warp::DefaultSparseMmaTensorOp<
+      WarpShape, InstructionShape, ElementA, SmemLayoutA, ElementB, SmemLayoutB,
+      ElementC, LayoutC, Operator, WarpCount::kK>::Type;
+
+  /// Cache operation of operand E
+  static cutlass::arch::CacheOperation::Kind const kCacheOpE =
+      cutlass::arch::CacheOperation::Global;
+
+  static int const kInterleavedE = MmaTensorOp::kInterleaved;
+  static int const kMetaSizeInBits = MmaTensorOp::kMetaSizeInBits;
+  static int const kMaxID2 = MmaTensorOp::kMaxID2;
+  static int const kElementsPerElementE = MmaTensorOp::kElementsPerElementE;
+
+  using ElementE = typename MmaTensorOp::ElementE;
+  using GmemLayoutE = cutlass::layout::ColumnMajorInterleaved<kInterleavedE>;
+
+  // Shared memory layout.  Interleaved layout is mapped to PitchLinear layout.
+  using SmemLayoutE = typename MmaTensorOp::LayoutE;
+
+  /// ThreadMap of iterator E
+  static int const kElementsPerAccessE =
+      kAccessSizeInBits / sizeof_bits<ElementE>::value;
+
+  /// E is tiny.  Not all warps are needed.
+  static int const kThreadsE =
+      (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+           (kAccessSizeInBits / sizeof_bits<ElementE>::value) >
+       kThreads)
+          ? kThreads
+          : (Shape::kM * Shape::kK / kSparse / kElementsPerElementE /
+             (kAccessSizeInBits / sizeof_bits<ElementE>::value));
+
+  using IteratorThreadMapE = transform::PitchLinearStripminedThreadMap<
+      layout::PitchLinearShape<Shape::kM * kInterleavedE,
+                               Shape::kK / kSparse / kElementsPerElementE /
+                                   kInterleavedE>,
+      kThreadsE, kElementsPerAccessE>;
+
+  /// Shared memory iterator to E operand
+  using SmemIteratorE = transform::threadblock::RegularTileAccessIterator<
+      MatrixShape<Shape::kM * kInterleavedE,
+                  Shape::kK / kSparse / kElementsPerElementE / kInterleavedE>,
+      ElementE, SmemLayoutE, 0, IteratorThreadMapE>;
+
+  /// Policy used to define MmaPipelined
+  using MmaPolicy =
+      SparseMmaPolicy<MmaTensorOp, MatrixShape<0, 0>, MatrixShape<0, 0>,
+                      MatrixShape<0, 0>, WarpCount::kK>;
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -234,15 +234,15 @@
     ++this->warp_tile_iterator_B_;
 
     Operator warp_mma;
 
     int smem_write_stage_idx = 1;
 
     // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/threadblock_swizzle.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -63,15 +63,15 @@
   // and point-wise fusion
   int tile_m = tile_m_per_filter * int(problem_size.stride().product());
 
   // There is a possible performance optimization here that leads up to 2x speeds than the current 
   // CUTLASS strided dgrad performance for stride > filter, i.e., stride={2x2} and filter={1x1})
   //
   // * Optimization * 
-  // Only launch CTAs in M dimension which contribute to a row in Dx output
+  // Only launch CTAs in M dimenstion which contribute to a row in Dx output
   // 
   // 
   // * Constraints *
   // (A) stride <= filter, for example, stride={2x2} and filter={3x3}: 
   //       - (A.1): There are no constraints for this case and the optimization does 
   //                affect this case functionality or performance. 
   // (B) stride > filter, for example, stride={2x2} and filter={1x1}: 
@@ -103,15 +103,15 @@
 
     gemm::GemmCoord implicit_gemm_problem_size = 
     cutlass::conv::implicit_gemm_problem_size(conv_operator, problem_size);
 
     // compute number of tiles in m dimension
     int tile_m = get_strided_dgrad_tile_m(problem_size, tile_size.m());
 
-    // compute number of tiles in n dimension
+    // compute number of tiles in n dimenstion 
     int tile_n = (implicit_gemm_problem_size.n() + tile_size.n() - 1) / tile_size.n();
 
     return gemm::GemmCoord(
       tile_m,
       tile_n,
       split_k_slices);
   }
@@ -144,15 +144,15 @@
 
     gemm::GemmCoord implicit_gemm_problem_size = 
     cutlass::conv::implicit_gemm_problem_size(conv_operator, problem_size);
 
     // compute number of tiles in m dimension
     int tile_m = get_strided_dgrad_tile_m(problem_size, tile_size.m());
 
-    // compute number of tiles in n dimension
+    // compute number of tiles in n dimenstion 
     int tile_n = (implicit_gemm_problem_size.n() + tile_size.n() - 1) / tile_size.n();
 
     return gemm::GemmCoord(
       tile_m,
       tile_n,
       split_k_slices);
   }
@@ -161,33 +161,11 @@
   /// For GEMM problem size (MxNxK) (Do not use base class get_tiled_shape())
   private:
     using Base::get_tiled_shape;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Threadblock swizzling function for GEMMs
-template <int N = 1, int Output_N = 1, int Output_P = 1, int Output_Q = 1>
-struct DepthwiseDirect2dConvIdentityThreadblockSwizzle
-    : public gemm::threadblock::GemmIdentityThreadblockSwizzle<N> {
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvIdentityThreadblockSwizzle() {}
-
-  /// Returns the shape of the problem in units of logical tiles
-  CUTLASS_HOST_DEVICE
-  gemm::GemmCoord get_tiled_shape(cutlass::conv::Operator conv_operator,
-                            cutlass::conv::Conv2dProblemSize const &problem_size,
-                            gemm::GemmCoord tile_size,
-                            int split_k_slices) const {
-        
-    gemm::GemmCoord implicit_gemm_problem_size =
-        cutlass::conv::implicit_gemm_problem_size(conv_operator, problem_size);
-
-    return gemm::GemmCoord(1,
-                     (implicit_gemm_problem_size.n() + tile_size.n() - 1) / tile_size.n(),
-                     split_k_slices);
-  }
-};
 
 } // namespace threadblock
-} // namespace conv
+} // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,358 +24,361 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Templates implementing warp-level matrix multiply-accumulate operations.
+    \brief 
+      Default kernel-level GEMM definitions combine threadblock-scoped matrix multiply-add with
+      the appropriate threadblock-scoped epilogue.
+  
+      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
+      accommodated by exchanging A and B operands and assuming transposed layouts. Partial
+      specializations here choose 'device::GemmTransposed' to implement this functionality.
+
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma.h"
-
-#include "cutlass/gemm/thread/mma.h"
-#include "cutlass/conv/convolution.h"
-#include "cutlass/conv/thread/depthwise_mma.h"
 
+#include "cutlass/complex.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
+#include "cutlass/gemm/kernel/gemm_grouped.h"
+#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
+#include "cutlass/gemm/kernel/default_gemm.h"
+#include "cutlass/gemm/kernel/default_gemm_complex.h"
+#include "cutlass/gemm/device/default_gemm_configuration.h"
 
-#include "cutlass/gemm/warp/mma_simt.h"
-#include "cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h"
+#include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
-namespace warp {
+namespace gemm {
+namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    /// Data type of A elements
+    /// Element type for A matrix operand
     typename ElementA_,
-    /// Layout of A matrix (concept: MatrixLayout)
+    /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Data type of B elements
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
     typename ElementB_,
-    /// Layout of B matrix (concept: MatrixLayout)
+    /// Layout type for B matrix operand
     typename LayoutB_,
-    /// Element type of C matrix
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
     typename ElementC_,
-    /// Layout of C matrix (concept: MatrixLayout)
+    /// Layout type for C and D matrix operands
     typename LayoutC_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK = 1,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaDepthwiseSimt
-    : public cutlass::gemm::warp::
-          MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_> {
-  using Base = cutlass::gemm::warp::
-      MmaSimt<Shape_, ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_, LayoutC_, Policy_>;
-      
-public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
-  using Shape = Shape_;
-
-  /// Data type of multiplicand A
-  using ElementA = ElementA_;
-
-  /// Layout of multiplicand A
-  using LayoutA = LayoutA_;
-
-  /// Data type of multiplicand B
-  using ElementB = ElementB_;
-
-  /// Layout of multiplicand B
-  using LayoutB = LayoutB_;
-
-  /// Data type of accumulator matrix C
-  using ElementC = ElementC_;
-
-  /// Layout of accumulator matrix C
-  using LayoutC = LayoutC_;
-
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
-  using Policy = Policy_;
-
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassSimt;
-
-  /// Hard-coded for now
-  using ArchTag = arch::Sm50;
-
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
-public:
-
-  /// Iterates over the B operand in memory
-  using IteratorB = cutlass::conv::warp::DepthwiseMmaSimtTileIterator<
-    MatrixShape<Policy::LaneMmaShape::kK, Shape::kN>,
-    cutlass::gemm::Operand::kB,
-    ElementB,
-    LayoutB,
-    Policy,
-    PartitionsK,
-    Shape::kK
-  >;
-
-  /// Storage for B tile
-  using FragmentB = typename IteratorB::Fragment;
-
-  /// Storage for transformed A tile
-  using TransformedFragmentB = FragmentB;
-
-public:
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Operator class tag
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
+    /// Operation performed by GEMM
+    typename Operator = typename device::DefaultGemmConfiguration<
+        OperatorClass, ArchTag, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator>::Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
+    /// Permute result D
+    typename PermuteDLayout = layout::NoPermute,
+    ///
+    typename Enable = void
+    >
+struct DefaultGemmGrouped;
 
-  //
-  // Methods
-  //
-
-  /// Ctor
-  CUTLASS_DEVICE
-  MmaDepthwiseSimt():Base() {}
-};
+/////////////////////////////////////////////////////////////////////////////////////////////////
+//
+// Real-valued GEMM kernels
+//
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
-    /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
-    typename FilterShape_,
-    /// Shape of the output tile computed by thread- concept: conv::TensorNHWCShape<>
-    typename ThreadOutputShape_,
-    /// Shape of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
-    typename ThreadBlockOutputShape_,
-    /// Data type of A elements
-    typename ElementA_,
-    /// Layout of A matrix (concept: MatrixLayout)
-    typename LayoutA_,
-    /// Data type of B elements
-    typename ElementB_,
-    /// Layout of B matrix (concept: MatrixLayout)
-    typename LayoutB_,
-    /// Element type of C matrix
-    typename ElementC_,
-    /// Layout of C matrix (concept: MatrixLayout)
-    typename LayoutC_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Iterator algo type
-    conv::IteratorAlgorithm IteratorAlgorithm_ = IteratorAlgorithm::kAnalytic,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape_ = cutlass::MatrixShape<-1, -1>,   
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape_ =  cutlass::MatrixShape<-1, -1>,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape_ = cutlass::conv::TensorNHWCShape<-1,-1,-1,-1>,
-    /// Number of partitions along K dimension
-    int PartitionsK = 1,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaDepthwiseDirectConvSimt {
- public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
-  using Shape = Shape_;
-
-  /// Shape of filter shape per threadblock - concept: gemm::GemmShape<Depth, Height, Width>
-  using FilterShape = FilterShape_;
-
-  /// Shape of the output tile computed by thread- concept: conv::TensorNHWCShape<>
-  using ThreadOutputShape = ThreadOutputShape_;
-
-  /// Shape of the output tile computed by threadblock - concept: conv::TensorNHWCShape<>
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
-
-  /// Data type of multiplicand A
-  using ElementA = ElementA_;
-
-  /// Layout of multiplicand A
-  using LayoutA = LayoutA_;
-
-  /// Data type of multiplicand B
-  using ElementB = ElementB_;
-
-  /// Layout of multiplicand B
-  using LayoutB = LayoutB_;
-
-  /// Data type of accumulator matrix C
-  using ElementC = ElementC_;
-
-  /// Layout of accumulator matrix C
-  using LayoutC = LayoutC_;
-
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
-  using Policy = Policy_;
-
-  /// Iterator algo type
-  static conv::IteratorAlgorithm const IteratorAlgorithm = IteratorAlgorithm_;
-
-  /// Stride ( MatrixShape<Height, Width> )
-  using StrideShape = StrideShape_; 
-
-  /// Dilation ( MatrixShape<Height, Width> )
-  using DilationShape = DilationShape_;
-  
-  /// Activation Shape loaded by threadblock
-  using ActivationShape = ActivationShape_;
-
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassSimt;
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Operator class tag
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear,
+    /// Permute result D
+    typename PermuteDLayout
+>
+struct DefaultGemmGrouped<
+  ElementA,
+  LayoutA,
+  ComplexTransform::kNone,   // transform A
+  kAlignmentA,
+  ElementB,
+  LayoutB,
+  ComplexTransform::kNone,   // transform B
+  kAlignmentB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  OperatorClass,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  GroupScheduleMode_,
+  Operator,
+  SharedMemoryClear,
+  PermuteDLayout,
+  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+> {
 
-  /// Hard-coded for now
-  using ArchTag = arch::Sm50;
+  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
+  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
-  static constexpr bool use_dp4a = (platform::is_same< layout::ColumnMajorInterleaved<4>, LayoutA>::value || 
-                                    platform::is_same< layout::RowMajorInterleaved<4>, LayoutA >::value) && 
-                                    platform::is_same< ElementA, int8_t >::value && 
-                                    platform::is_same< ElementB, int8_t >::value;
-
-  using dp4a_type = typename platform::conditional< use_dp4a , int8_t, bool >::type;
-
-  /// Thread-level matrix multiply accumulate operator
-  using ThreadMma = cutlass::conv::thread::DepthwiseDirectConvElementwiseInnerProduct<
-    cutlass::gemm::GemmShape<
-      Shape::kM / Policy::WarpShape::kRow,    // number of output pixels proccessed per thread
-      Shape::kN / Policy::WarpShape::kColumn, // number of channels proccessed per thread
-      1>,
+  using MapArguments = kernel::detail::MapArguments<
     ElementA,
+    LayoutA,
+    ComplexTransform::kNone,
+    kAlignmentA,
     ElementB,
-    ElementC,
-    arch::OpMultiplyAdd,
-    dp4a_type
+    LayoutB,
+    ComplexTransform::kNone,
+    kAlignmentB,
+    LayoutC,
+    kInternalTranspose
   >;
 
-  /// Underlying matrix multiply operator (concept: arch::Mma)
-  using ArchMmaOperator = typename ThreadMma::ArchMmaOperator;
-
-  /// Indicates math operator 
-  using MathOperator = typename ArchMmaOperator::Operator;
-  
-  /// Shape of the underlying instruction
-  using InstructionShape = cutlass::gemm::GemmShape<1,1,use_dp4a ? 4 : 1>;
+  // Define the default GEMM kernel
+  using DefaultGemmKernel = typename kernel::DefaultGemm<
+    typename MapArguments::ElementA,
+    typename MapArguments::LayoutA,
+    MapArguments::kAlignmentA,
+    typename MapArguments::ElementB,
+    typename MapArguments::LayoutB,
+    MapArguments::kAlignmentB,
+    ElementC,
+    typename MapArguments::LayoutC,
+    ElementAccumulator,
+    OperatorClass,
+    ArchTag,
+    ThreadblockShape,
+    WarpShape,
+    InstructionShape,
+    EpilogueOutputOp,
+    ThreadblockSwizzle,
+    Stages,
+    true,
+    Operator,
+    SharedMemoryClear,
+    false, /*GatherA*/
+    false, /*GatherB*/
+    false, /*ScatterD*/
+    PermuteDLayout
+  >::GemmKernel;
+
+    /// Define the kernel in terms of the default kernel
+  using GemmKernel = kernel::GemmGrouped<
+    typename DefaultGemmKernel::Mma,
+    typename DefaultGemmKernel::Epilogue,
+    ThreadblockSwizzle,
+    GroupScheduleMode_,
+    kInternalTranspose
+  >;
+};
 
-public:
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  /// Iterates over the A operand in memory
-  using IteratorA = cutlass::conv::warp::DepthwiseDirect2dConvSimtTileIterator<
-    MatrixShape<Shape::kM, Shape::kN>, // <output tile=(P*Q), output channels> per warp
-    FilterShape,
-    ThreadOutputShape,
-    ThreadBlockOutputShape,
-    cutlass::gemm::Operand::kA,
-    ElementA,
-    Policy,
-    IteratorAlgorithm,
-    StrideShape,
-    DilationShape,
-    ActivationShape,
-    PartitionsK,
-    Shape::kK
-  >;
+//
+// Complex-valued GEMM kernels
+//
 
-  /// Storage for A tile
-  using FragmentA = typename IteratorA::Fragment;
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Layout type for C and D matrix operands
+    typename LayoutC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Operator class tag
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear
+  >
+struct DefaultGemmGrouped<
+  ElementA,
+  LayoutA,
+  TransformA,
+  kAlignmentA,
+  ElementB,
+  LayoutB,
+  TransformB,
+  kAlignmentB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  OperatorClass,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  GroupScheduleMode_,
+  Operator,
+  SharedMemoryClear,
+  layout::NoPermute, /*PermuteDLayout*/
+  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+> {
 
-  /// Storage for transformed A tile
-  using TransformedFragmentA = FragmentA;
+  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
+  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  /// Iterates over the B operand in memory
-  using IteratorB = cutlass::gemm::warp::MmaSimtTileIterator<
-    MatrixShape<1, Shape::kN>,
-    cutlass::gemm::Operand::kB,
+  using MapArguments = kernel::detail::MapArguments<
+    ElementA,
+    LayoutA,
+    TransformA,
+    kAlignmentA,
     ElementB,
     LayoutB,
-    Policy,
-    PartitionsK,
-    Shape::kK
+    TransformB,
+    kAlignmentB,
+    LayoutC,
+    kInternalTranspose
   >;
 
-  /// Storage for B tile
-  using FragmentB = typename IteratorB::Fragment;
-
-  /// Storage for transformed A tile
-  using TransformedFragmentB = FragmentB;
-
-  /// Iterates over the C operand in memory
-  using IteratorC = cutlass::gemm::warp::MmaSimtTileIterator<
-    MatrixShape<Shape::kM, Shape::kN>,
-    cutlass::gemm::Operand::kC,
+  using DefaultGemmKernel = typename kernel::DefaultGemmComplex<
+    typename MapArguments::ElementA,
+    typename MapArguments::LayoutA,
+    typename MapArguments::ElementB,
+    typename MapArguments::LayoutB,
     ElementC,
-    LayoutC,
-    Policy
+    typename MapArguments::LayoutC,
+    ElementAccumulator,
+    OperatorClass,
+    ArchTag,
+    ThreadblockShape,
+    WarpShape,
+    InstructionShape,
+    EpilogueOutputOp,
+    ThreadblockSwizzle,
+    Stages,
+    MapArguments::kTransformA,
+    MapArguments::kTransformB,
+    Operator,
+    false
+  >::GemmKernel;
+
+  /// Define the kernel in terms of the default kernel
+  using GemmKernel = kernel::GemmGrouped<
+    typename DefaultGemmKernel::Mma,
+    typename DefaultGemmKernel::Epilogue, 
+    ThreadblockSwizzle,
+    GroupScheduleMode_,
+    kInternalTranspose
   >;
-
-  /// Storage for C tile
-  using FragmentC = typename ThreadMma::FragmentC;
-
-public:
-
-  //
-  // Methods
-  //
-
-  /// Ctor
-  CUTLASS_DEVICE
-  MmaDepthwiseDirectConvSimt() {}
-
-  /// Performs a warp-level matrix multiply-accumulate operation
-  CUTLASS_DEVICE
-  void operator()(
-    FragmentC &d, 
-    FragmentA a, 
-    FragmentB b, 
-    FragmentC const &c, int group_idx = 0) const {
-
-    ThreadMma mma;
-
-    mma(d, a, b, c);
-  }
-
-  /// Transform the mma operands to the required types
-  CUTLASS_DEVICE
-  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
-                 FragmentA const &A, FragmentB const &B) const {
-    //TODO: Implement this
-    dst_A = A;
-    dst_B = B;
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace warp
-} // namespace conv
-} // namespace cutlass
+}  // namespace kernel
+}  // namespace gemm
+}  // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/mma_depthwise_simt_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,838 +25,835 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Describes the lane policy used by warp-level matrix multiply operators targeting SIMT
-      instructions
+    \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
+      Tensor Cores.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
+
 #include "cutlass/array.h"
-#include "cutlass/tensor_ref.h"
+#include "cutlass/complex.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
-
-#include "cutlass/conv/convolution.h"
+#include "cutlass/functional.h"
 
 #include "cutlass/arch/memory_sm75.h"
+#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm80.h"
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-#include "cutlass/layout/matrix.h"
+#include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma_simt_policy.h"
-#include "cutlass/gemm/warp/mma_simt_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace conv {
+namespace gemm {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Iterates over operands to warp-level matrix multiply operations targeting SIMT instructions
-///
-/// concept: MutableRandomAccessContiguousTileIteratorConcept
-///
-template <
-  /// Size of the matrix to load (concept: MatrixShape)
-  typename Shape_,
-  /// Operand identity
-  cutlass::gemm::Operand Operand,
-  /// Data type of A elements
-  typename Element_,
-  /// Layout of operand
-  typename Layout_,
-  /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-  typename Policy_,
-  /// Number of partitions along K dimension - used in sliced-K
-  int PartitionsK = 1,
-  /// Group Size along kPartition - used in sliced-K
-  int PartitionGroupSize = 1
->
-class DepthwiseMmaSimtTileIterator;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+namespace detail {
 
-/// Specialization for B operands of row-major layouts
-///
-/// Concept: MutableRandomAccessContiguousTileIteratorConcept
-///
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK,
-    /// Group Size along kPartition - used in sliced-K
-    int PartitionGroupSize>
-class DepthwiseMmaSimtTileIterator<Shape_,
-                                   cutlass::gemm::Operand::kB,
-                                   Element_,
-                                   layout::RowMajor,
-                                   Policy_,
-                                   PartitionsK,
-                                   PartitionGroupSize>
-    : public cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
-                                               cutlass::gemm::Operand::kB,
-                                               Element_,
-                                               layout::RowMajor,
-                                               Policy_,
-                                               PartitionsK,
-                                               PartitionGroupSize> {
-
-  using Base = cutlass::gemm::warp::MmaSimtTileIterator<Shape_,
-                                               cutlass::gemm::Operand::kB,
-                                               Element_,
-                                               layout::RowMajor,
-                                               Policy_,
-                                               PartitionsK,
-                                               PartitionGroupSize>;
- public:
-  /// Shape of tile to load (concept: MatrixShape)
-  using Shape = Shape_;
-
-  /// Operand tag
-  static cutlass::gemm::Operand const kOperand = cutlass::gemm::Operand::kB;
+  /// Data type of real & imag members of complex numbers in the SourceFragment
+  typename RealElement,
+  /// Destination fragment required by the mma operation 
+  typename DestinationFragment,
+  /// Source fragment holding complex<RealElement> elements
+  typename SourceFragment,
+  /// Number of mma operations performed
+  typename MmaIterations,
+  /// Shape of operand elements
+  typename MmaOperandShape,
+  /// Complex transform on A operand
+  ComplexTransform Transform_,
+  /// Operand A or Operand B
+  Operand Operand_,
+  /// Floating-point rounding style
+  FloatRoundStyle Round_>
+struct UnpackComplexConvertAndPackForMma;
 
-  /// Element type
-  using Element = Element_;
-
-  /// Layout of policy
-  using Layout = layout::RowMajor;
-
-  /// Decomposition of elements among threads
-  using Policy = Policy_;
+// Partial specialization for OperandA and Congruous smem layout
+template <
+  typename RealElement,
+  typename DestinationFragment, 
+  typename SourceFragment,
+  typename MmaIterations,
+  typename MmaOperandShape,
+  ComplexTransform Transform_,
+  FloatRoundStyle Round_>
+struct UnpackComplexConvertAndPackForMma <
+  RealElement,
+  DestinationFragment,
+  SourceFragment,
+  MmaIterations,
+  MmaOperandShape,
+  Transform_,
+  Operand::kA,
+  Round_> {
+  
+  //
+  // Type definitions
+  //
+  static Operand const kOperand = Operand::kA;
+  static ComplexTransform const kTransform = Transform_;
+  static FloatRoundStyle const kRound = Round_;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = typename Base::TensorRef;
+  // Data type of elements in the destination fragment
+  using MmaElement = typename DestinationFragment::Element;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
+  // Numeric convertor MmaElement <= RealElement
+  using Converter = NumericConverter<MmaElement, RealElement, kRound>;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  // Operand layout parameters
+  using SourceFragmentLayout = layout::ColumnMajor;
+  static int const kLdm = MmaIterations::kRow * MmaOperandShape::kRow;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+  /// Ctor
+  CUTLASS_DEVICE
+  UnpackComplexConvertAndPackForMma() {}
 
-  /// Thread-level shape of a fragment
-  using ThreadShape = typename Base::ThreadShape;
+  CUTLASS_DEVICE
+  void operator()(DestinationFragment *dest, SourceFragment const &source) {
+    
+    Converter convert_op;
+    SourceFragmentLayout layout(kLdm);
 
-  /// Number of individual loads
-  using Iterations =  typename Base::Iterations;
+    CUTLASS_PRAGMA_UNROLL
+    for(int i=0; i<MmaIterations::kRow; i++) {
+      int pos = 0;
+      CUTLASS_PRAGMA_UNROLL
+      for(int c=0; c<MmaOperandShape::kColumn; c++) {
+        CUTLASS_PRAGMA_UNROLL
+        for(int r=0; r<MmaOperandShape::kRow; r++) {
+          // Logical position of element in source fragment
+          int row = r + i * MmaOperandShape::kRow;
+          int col = c;
+
+          // Access complex<RealElement> and apply rounding on real and imag parts
+          MmaElement a = convert_op(source[layout(MatrixCoord{row,col})].real());
+          MmaElement b = convert_op(source[layout(MatrixCoord{row,col})].imag());
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest[i][pos] = a;
+          dest[i+MmaIterations::kRow][pos++] = (kTransform == ComplexTransform::kConjugate ? -b : b);
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = typename Base::Fragment;
+        }
+      }
+    }
+  }
+};
 
-  static_assert(Policy::LaneMmaShape::kN == 1, "Each thread should be 1 element per LDS along the k-dim");
+// Partial specialization for OperandB and Congruous smem layout
+template <
+  typename RealElement,
+  typename DestinationFragment, 
+  typename SourceFragment,
+  typename MmaIterations,
+  typename MmaOperandShape,
+  ComplexTransform Transform_,
+  FloatRoundStyle Round_>
+struct UnpackComplexConvertAndPackForMma <
+  RealElement,
+  DestinationFragment,
+  SourceFragment,
+  MmaIterations,
+  MmaOperandShape,
+  Transform_,
+  Operand::kB,
+  Round_> {
   
-private:
+  //
+  // Type definitions
+  //
+  static Operand const kOperand = Operand::kB;
+  static ComplexTransform const kTransform = Transform_;
+  static FloatRoundStyle const kRound = Round_;
 
-  MatrixCoord lane_offset_;
-  int channel_idx_;
-  int base_channel_idx_;
-  int warps_n_;
+  // Data type of elements in the destination fragment
+  using MmaElement = typename DestinationFragment::Element;
 
- public:
-  
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator():Base() { }
+  // Numeric convertor MmaElement <= RealElement
+  using Converter = NumericConverter<MmaElement, RealElement, kRound>;
 
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator(
-    TensorRef ref, 
-    int lane_id
-  ) : Base(ref, lane_id) {
-
-    // compute offset based on thread ID and lane layout
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
-
-    warps_n_ = -1;
-    channel_idx_ = 0;
-    base_channel_idx_ = 0;
-    lane_offset_ = lane_layout.inverse(lane_id) * MatrixCoord(0, Policy::LaneMmaShape::kN);
-  }
-  
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  DepthwiseMmaSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
+  // Operand layout parameters
+  using SourceFragmentLayout = layout::RowMajor;
+  static int const kLdm = MmaIterations::kColumn * MmaOperandShape::kColumn;
 
-    if(warps_n_ == -1){
-        warps_n_ = coord.column();
-    }
-    
-    Base::add_tile_offset(coord);
-    return *this;
-  }
+  /// Ctor
+  CUTLASS_DEVICE
+  UnpackComplexConvertAndPackForMma() {}
 
-  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
   CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
-    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr =
-        reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+  void operator()(DestinationFragment *dest, SourceFragment const &source) {
+    
+    Converter convert_op;
+    SourceFragmentLayout layout(kLdm);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int k = 0; k < Iterations::kRow; ++k) {
+    for(int i=0; i<MmaIterations::kColumn; i++) {
+      int pos = 0;
       CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Iterations::kColumn; ++n) {
-
-        void const *ptr = this->ref_.data() +
-                          this->ref_.offset({-(channel_idx_ - base_channel_idx_),
-                                             n * Policy::WarpShape::kColumn}) +
-                          pointer_offset / Policy::LaneMmaShape::kN;
-
-        // Base_k of a warp +  Base_k of current threads.
-        int thread_k_base_idx =
-            warps_n_ * Shape::kColumn / Policy::LaneMmaShape::kN + lane_offset_.column();
-
-        if (channel_idx_ + k == thread_k_base_idx + n * Policy::WarpShape::kColumn) {
-          // Depthwise kernel would only do computation when channel == k.
-          // Loads an element when the current computation channel == the k corresponding to this thread.
-          arch::shared_load(dst_ptr[n + k * Iterations::kColumn], ptr);
-        } else {
-          // Reduce SMEM load
-          dst_ptr[n + k * Iterations::kColumn].fill(Element(0));
+      for(int c=0; c<MmaOperandShape::kColumn; c++) {
+        CUTLASS_PRAGMA_UNROLL
+        for(int r=0; r<MmaOperandShape::kRow; r++) {
+          // Logical position of element in source fragment
+          int row = r;
+          int col = c + i * MmaOperandShape::kColumn;
+
+          // Access complex<RealElement> apply rounding on real and imag parts
+          MmaElement a = convert_op(source[layout(MatrixCoord{row,col})].real());
+          MmaElement b = convert_op(source[layout(MatrixCoord{row,col})].imag());
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest[i][pos] = a;
+          dest[i+MmaIterations::kColumn][pos++] = (kTransform == ComplexTransform::kConjugate ? -b : b);
         }
       }
     }
   }
-
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
-  
-  /// Notify the iterator which k-group it is currently pointing to.
-  ///
-  /// This does not advance the iterator. Rather, it overrides its internal
-  /// tracking with constant-valued k-group index
-  CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    if(k_group % PartitionGroupSize == 0 && k_group != 0){
-      base_channel_idx_ = k_group;
-    }
-    channel_idx_ = k_group;
-  }
 };
+} // namespace detail 
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Size of filter (concept: gemm::GemmShape<Depth, Height, Width>)
-    typename FilterShape_,
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename ThreadOutputShape_,
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename ThreadBlockOutputShape_,
-    /// Operand identity
-    cutlass::gemm::Operand Operand,
-    /// Data type of A elements
-    typename Element_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Iterator algo type
-    conv::IteratorAlgorithm IteratorAlgorithm = IteratorAlgorithm::kAnalytic,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape = cutlass::MatrixShape<-1, -1>,   
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape =  cutlass::MatrixShape<-1, -1>,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape = cutlass::conv::TensorNHWCShape<-1,-1,-1,-1>,
-    /// Number of partitions along K dimension - used in sliced-K
-    int PartitionsK = 1,
-    /// Group Size along kPartition - used in sliced-K
-    int PartitionGroupSize = 1>
-class DepthwiseDirect2dConvSimtTileIterator;
-
-
-/// Specialization for A operands of row-major layouts
-///
-/// Concept: MutableRandomAccessContiguousTileIteratorConcept
-///
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename RealElementA,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename RealElementB,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename RealElementC,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Complex transform on A operand
+  ComplexTransform TransformA = ComplexTransform::kNone,
+  /// Complex transform on B operand
+  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Do source operands need more than one elements
+  bool GeneralizedOperatorElements = false,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaComplexTensorOp;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for complex*complex+complex => complex using real-valued TensorOps
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Size of filter (concept: gemm::GemmShape<Depth, Height, Width>)
-    typename FilterShape_,
-    /// Size of the matrix to load (concept: TensorNHWC)
-    typename ThreadOutputShape_,
-    /// Size of the matrix to load (concept: TensorNHWC)
-    typename ThreadBlockOutputShape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Iterator algo type
-    conv::IteratorAlgorithm IteratorAlgorithm,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape,   
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape,
-    /// Number of partitions along K dimension - used in sliced-K
-    int PartitionsK,
-    /// Group Size along kPartition - used in sliced-K
-    int PartitionGroupSize>
-class DepthwiseDirect2dConvSimtTileIterator<Shape_,
-                                            FilterShape_,
-                                            ThreadOutputShape_,
-                                            ThreadBlockOutputShape_,
-                                            cutlass::gemm::Operand::kA,
-                                            Element_,
-                                            Policy_,
-                                            IteratorAlgorithm,
-                                            StrideShape,   
-                                            DilationShape,
-                                            ActivationShape,
-                                            PartitionsK,
-                                            PartitionGroupSize> {
- public:
-  /// Shape of tile to load (concept: MatrixShape)
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename RealElementA,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename RealElementB,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename RealElementC,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Complex transform on A operand
+  ComplexTransform TransformA,
+  /// Complex transform on B operand
+  ComplexTransform TransformB
+>
+class MmaComplexTensorOp<
+  Shape_, 
+  complex<RealElementA>, 
+  LayoutA_, 
+  complex<RealElementB>,
+  LayoutB_,
+  complex<RealElementC>,
+  LayoutC_,
+  Policy_,
+  TransformA,
+  TransformB>  {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Shape of filter (concept: gemm::GemmShape<Depth, Height, Width>)
-  using FilterShape = FilterShape_;
+  /// Data type of multiplicand A
+  using ElementA = complex<RealElementA>;
 
-  /// Shape of tile to load (concept: TensorNHWC)
-  using ThreadOutputShape = ThreadOutputShape_;
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
 
-  /// Shape of tile to load (concept: TensorNHWC)
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
+  /// Data type of multiplicand B
+  using ElementB = complex<RealElementB>;
 
-  /// Operand tag
-  static cutlass::gemm::Operand const kOperand = cutlass::gemm::Operand::kA;
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
 
-  /// Element type
-  using Element = Element_;
+  /// Data type of accumulator matrix C
+  using ElementC = complex<RealElementC>;
 
-  /// Layout of policy
-  using Layout = layout::RowMajor;
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
 
-  /// Decomposition of elements among threads
+  /// Shape of the warp in units of thread (concept: MmaLanePolicyTensorOp)
   using Policy = Policy_;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = TensorRef<Element, Layout>;
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename Policy::Operator;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
+  /// Architecture tag from underlying instruction
+  using ArchTag = typename ArchMmaOperator::ArchTag;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassTensorOp;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
 
-  //
-  // Derived quantities
-  //
+  /// Indicates math operator 
+  using MathOperator = arch::OpMultiplyAddComplex;
+
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
 
-  static_assert(!(Shape::kRow % Policy::WarpShape::kRow), 
-    "The warp-level GEMM M size must be divisible by the number of threads arranged along the M dimension.");
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
 
-  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
-  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
-  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
-  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0, "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
 
-// Thread-level shape of a fragment
-  using ThreadShape = MatrixShape<
-    ThreadOutputShape::kNHW, // Output tile shape Computed by current threads
-    ThreadOutputShape::kC
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+    Policy::OpDelta::kRow,
+    32,
+    1
   >;
 
-  static_assert(!(ThreadShape::kColumn % Policy::LaneMmaShape::kN), 
-    "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  using TransformedFragmentA = FragmentA;
 
-  /// Number of individual loads
-  using Iterations = MatrixShape<
-    ThreadShape::kRow,
-    ThreadShape::kColumn / Policy::LaneMmaShape::kN
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
+    Policy::OpDelta::kColumn,
+    32,
+    1
   >;
 
-  using ThreadTileCount = MatrixShape<
-    ThreadBlockOutputShape::kH / ThreadOutputShape::kH,
-    ThreadBlockOutputShape::kW / ThreadOutputShape::kW
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed B tile
+  using TransformedFragmentB = FragmentB;
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of mma operations performed
+  using MmaIterations = MatrixShape<
+    Shape::kM / ArchMmaOperator::Shape::kM,
+    Shape::kN / ArchMmaOperator::Shape::kN
   >;
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = Array<Element, ThreadShape::kCount>;
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, 
+     ElementC, 
+     LayoutC,
+     typename ArchMmaOperator::Shape, 
+     typename Policy::OpDelta>;
+
+  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
+  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
+  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
+  /// of Tensor Cores which are always real-valued matrix multiplies.
+  using FragmentC = typename IteratorC::Fragment;
+
+  static_assert(
+    FragmentC::kElements == 2 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
+    "Unexpected planar complex fragment length.");
 
-protected:
+private:
 
-  /// Internal reference
-  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kN>, layout::RowMajor> ref_;
+  //
+  // Data members
+  //
 
-  int activation_offset[ThreadOutputShape::kH][ThreadOutputShape::kW][Iterations::kColumn];
-  int iterator_r_;
-  int iterator_s_;
-  int iterator_offset_;
+  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
 
-  int inc_next_s_ ;
-  int inc_next_r_ ;
-  
-  MatrixCoord lane_offset_;
 public:
-  
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator() { }
 
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator(
-    TensorRef ref, 
-    int lane_id
-  ) {
+  //
+  // Methods
+  //
 
-    // compute offset based on thread ID and lane layout
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaComplexTensorOp() {}
 
-    // Set channel offset
-    lane_offset_ = lane_layout.inverse(lane_id) * MatrixCoord(0, Policy::LaneMmaShape::kN);
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &D, 
+    FragmentA const &A, 
+    FragmentB const &B, 
+    FragmentC const &C
+  ) const {
+
+    // Alias types for underlying real-valued matrix multiply operator
+    using MmaOperandA = typename ArchMmaOperator::FragmentA;
+    using MmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+    static_assert(MmaOperandA::kElements == 1, 
+      "This implementation only supports math instructions in which exactly one element is needed for the A operand."
+      "We can geneneralize later.");
+
+    static_assert(MmaOperandB::kElements == 1, 
+      "This implementation only supports math instructions in which exactly one element is needed for the B operand."
+      "We can geneneralize later.");
 
-    ref.add_coord_offset(lane_offset_);
+    D = C;
 
-    ref_.reset(reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(ref.data()),
-               ref.stride(0) / Policy::LaneMmaShape::kN);
+    CUTLASS_PRAGMA_UNROLL
+    for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-    iterator_r_ = 0;
-    iterator_s_ = 0;
-    iterator_offset_ = 0;
-  }
+      // mma(accum.real(), a.real(), b.real(), accum.real());
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-  /// Adds a pointer offset to internal pointer(s) to advance through memory
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &add_pointer_offset(LongIndex offset) {
-    ref_.add_pointer_offset(offset);
-    return *this;
-  }
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_A;
+        MmaOperandB operand_B;
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  template<typename Params>
-  CUTLASS_HOST_DEVICE
-  void setup_initial_status(Params const& params)  {
-  
-    inc_next_s_ = params.inc_next[0];
-    inc_next_r_ = params.inc_next[1];
+        operand_A[0] = A[m].real();
+        operand_B[0] = B[n].real();
 
-    // Get base HW offset of current threads
-    int threadgroup = threadIdx.x / (ThreadBlockOutputShape::kC / ThreadOutputShape::kC);
-    int base_p_ =
-        (threadgroup / (ThreadTileCount::kColumn)) * ThreadOutputShape::kH;
-    int base_q_ =
-        (threadgroup % (ThreadTileCount::kColumn)) * ThreadOutputShape::kW;
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int p = 0; p < ThreadOutputShape::kH; ++p) {
+          mma(*accum, operand_A, operand_B, *accum);
+      }
+
+      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
       CUTLASS_PRAGMA_UNROLL
-      for (int q = 0; q < ThreadOutputShape::kW; ++q) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int col = 0; col < Iterations::kColumn; ++col) {
-          int base_w = (base_q_ + q) * params.stride[0];
-          int base_h = (base_p_ + p) * params.stride[1];
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
-          int offset = base_h * params.activation_tile_w + base_w;
-          activation_offset[p][q][col] = offset;
-        }
-      }
-    }
-  }
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_A;
+        MmaOperandB operand_B;
 
+        operand_A[0] = A[m].real();
+        operand_B[0] = (kTransformB == ComplexTransform::kConjugate ? -B[n].imag() : B[n].imag());
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
-    // Set warp row and col start
-    lane_offset_ = MatrixCoord({lane_offset_.row() + coord.row() * Shape::kRow, lane_offset_.column()});
-    return *this;
-  }
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  void advance(int32_t pointer_offset) {
-    ref_.reset(ref_.data() + pointer_offset / sizeof(Element) / Policy::LaneMmaShape::kN);
-    iterator_s_ = 0;
-    iterator_r_ = 0;
-    iterator_offset_ = 0;
-  }
+        mma(*accum, operand_A, operand_B, *accum);
+      }
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &operator++() {
-    ++iterator_s_;
-    if (iterator_s_ < FilterShape::kColumn) {
-      iterator_offset_ += inc_next_s_;
+      // mma(accum.real(), -a.imag(), b.imag(), accum.real())
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-      return *this;
-    }
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_A;
+        MmaOperandB operand_B;
 
-    iterator_s_ = 0;
+        // A imaginary part is intentionally negated
+        operand_A[0] = (kTransformA == ComplexTransform::kConjugate ? A[m].imag() : -A[m].imag());
+        operand_B[0] = (kTransformB == ComplexTransform::kConjugate ? -B[n].imag() : B[n].imag());
 
-    ++iterator_r_;
-    if (iterator_r_ < FilterShape::kRow) {
-      iterator_offset_ += inc_next_r_;
-      return *this;
-    }
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
 
-    iterator_r_ = 0;
-    iterator_offset_ = 0;
-    return *this;
-  }
+        mma(*accum, operand_A, operand_B, *accum);
+      }
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator & operator--() {
-    // Do nothing
-    return *this;
-  }
+      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
-  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+        // Pack operands together. This may result in actual MOVs 
+        MmaOperandA operand_A;
+        MmaOperandB operand_B;
 
-    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr = 
-      reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+        operand_A[0] = (kTransformA == ComplexTransform::kConjugate ? -A[m].imag() : A[m].imag());
+        operand_B[0] = B[n].real();
 
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int p = 0; p < ThreadOutputShape::kH; ++p) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int q = 0; q < ThreadOutputShape::kW; ++q) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int n = 0; n < Iterations::kColumn; ++n) {
-          void const *ptr = ref_.data() +
-                            ref_.offset({activation_offset[p][q][n] + (iterator_offset_),
-                                         n * Policy::WarpShape::kColumn}) +
-                            pointer_offset / Policy::LaneMmaShape::kN;
-          arch::shared_load(dst_ptr[n + q + p * ThreadOutputShape::kW], ptr);
-        }
+        mma(*accum, operand_A, operand_B, *accum);
       }
     }
   }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
-  
-  /// Stores a fragment to memory at the location pointed to by the iterator
-  CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
-    // Do nothing at present.
-  }
-
-  /// Stores a fragment to memory at the location pointed to by the iterator
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag, Index pointer_offset) const {
-    store_with_pointer_offset(frag, 0);
-  }
-
+  /// Transform the mma operands to the required types
   CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    // no operation here
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    //TODO: Implement this
+    dst_A = A;
+    dst_B = B;
   }
 };
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
-/// Specialization for A operands of row-major layouts
-///
-/// Concept: MutableRandomAccessContiguousTileIteratorConcept
-///
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for complex*complex+complex => complex:
+//  Operands data type: complex<float>
+//  Rounding: float -> tfloat32_t (round half_ulp_truncate nearest)
+//  Math instruction: MMA.1688.F32.TF32
+//  Output data type: complex<float>
+// 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Size of filter (concept: gemm::GemmShape<Depth, Height, Width>)
-    typename FilterShape_,
-    /// Size of the matrix to load (concept: TensorNHWC)
-    typename ThreadOutputShape_,
-    /// Size of the matrix to load (concept: TensorNHWC)
-    typename ThreadBlockOutputShape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Shape of the warp in units of thread (concept: MmaSimtPolicy)
-    typename Policy_,
-    /// Stride ( MatrixShape<Height, Width> )
-    typename StrideShape_,
-    /// Dilation ( MatrixShape<Height, Width> )
-    typename DilationShape_,
-    /// Activation Shape loaded by threadblock
-    typename ActivationShape_,
-    /// Number of partitions along K dimension - used in sliced-K
-    int PartitionsK,
-    /// Group Size along kPartition - used in sliced-K
-    int PartitionGroupSize>
-class DepthwiseDirect2dConvSimtTileIterator<Shape_,
-                                            FilterShape_,
-                                            ThreadOutputShape_,
-                                            ThreadBlockOutputShape_,
-                                            cutlass::gemm::Operand::kA,
-                                            Element_,
-                                            Policy_,
-                                            IteratorAlgorithm::kFixedStrideDilation,
-                                            StrideShape_,
-                                            DilationShape_,
-                                            ActivationShape_,
-                                            PartitionsK,
-                                            PartitionGroupSize> {
- public:
-  /// Shape of tile to load (concept: MatrixShape)
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Complex transform on A operand
+  ComplexTransform TransformA,
+  /// Complex transform on B operand
+  ComplexTransform TransformB
+>
+class MmaComplexTensorOp<
+  Shape_, 
+  complex<float>, 
+  LayoutA_, 
+  complex<float>,
+  LayoutB_,
+  complex<float>,
+  LayoutC_,
+  Policy_,
+  TransformA,
+  TransformB>  {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Shape of filter (concept: gemm::GemmShape<Depth, Height, Width>)
-  using FilterShape = FilterShape_;
+  /// Data type of members of complex multiplicand A
+  using RealElementA = float;
 
-  /// Shape of tile to load (concept: TensorNHWC)
-  using ThreadOutputShape = ThreadOutputShape_;
+  /// Data type of multiplicand A
+  using ElementA = complex<RealElementA>;
 
-  /// Shape of tile to load (concept: TensorNHWC)
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
 
-  /// Stride ( MatrixShape<Height, Width> )
-  using StrideShape = StrideShape_;
+  /// Data type of members of complex multiplicand B
+  using RealElementB = float;
 
-  /// Dilation ( MatrixShape<Height, Width> )
-  using DilationShape = DilationShape_;
+  /// Data type of multiplicand B
+  using ElementB = complex<RealElementB>;
 
-  /// Activation Shape loaded by threadblock
-  using ActivationShape = ActivationShape_;
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
 
-  /// Operand tag
-  static cutlass::gemm::Operand const kOperand = cutlass::gemm::Operand::kA;
+  /// Data type of members of complex accumulator matrix C
+  using RealElementC = float;
 
-  /// Element type
-  using Element = Element_;
+  /// Data type of accumulator matrix C
+  using ElementC = complex<RealElementC>;
 
-  /// Layout of policy
-  using Layout = layout::RowMajor;
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
 
-  /// Decomposition of elements among threads
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = TensorRef<Element, Layout>;
-
-  /// Index type
-  using Index = typename TensorRef::Index;
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename Policy::Operator;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
-
-  //
-  // Derived quantities
-  //
+  /// Underlying arch tag
+  using ArchTag = typename ArchMmaOperator::ArchTag;
 
-  static_assert(!(Shape::kRow % Policy::WarpShape::kRow),
-                "The warp-level GEMM M size must be divisible by the number of threads arranged "
-                "along the M dimension.");
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassTensorOp;
 
-  static_assert(Shape::kRow > 0, "Shape::kRow must be greater than zero.");
-  static_assert(Shape::kColumn > 0, "Shape::kColumn must be greater than zero.");
-  static_assert(Policy::WarpShape::kRow > 0, "Policy::WarpShape::kRow must be greater than zero.");
-  static_assert(Shape::kRow / Policy::WarpShape::kRow > 0,
-                "Shape::kRow / Policy::WarpShape::kRow must be greater than zero.");
-
-  // Activations loaded by threadblock
-  static int const ThreadActivationShapeH = (ThreadOutputShape::kH - 1) * StrideShape::kRow +
-                                            (FilterShape::kRow - 1) * DilationShape::kRow + 1;
-
-  static int const ThreadActivationShapeW = (ThreadOutputShape::kW - 1) * StrideShape::kColumn +
-                                            (FilterShape::kColumn - 1) * DilationShape::kColumn + 1;
-
-  using ThreadActivationShape = cutlass::conv::
-      TensorNHWCShape<1, ThreadActivationShapeH, ThreadActivationShapeW, ThreadOutputShape::kC>;
-
-  // Thread-level shape of a fragment
-  using ThreadShape =
-      MatrixShape<ThreadOutputShape::kNHW,
-                  ThreadOutputShape::kC>;
+  /// Indicates math operator 
+  using MathOperator = typename arch::OpMultiplyAddComplex;
+  
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
 
-  static_assert(!(ThreadShape::kColumn % Policy::LaneMmaShape::kN),
-                "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
 
-  /// Number of individual loads
-  using Iterations =
-      MatrixShape<ThreadShape::kRow, ThreadShape::kColumn / Policy::LaneMmaShape::kN>;
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
 
-  using ThreadTileCount = MatrixShape<ThreadBlockOutputShape::kH / ThreadOutputShape::kH,
-                                      ThreadBlockOutputShape::kW / ThreadOutputShape::kW>;
+public:
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = Array<Element, ThreadShape::kCount>;
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+    Policy::OpDelta::kRow,
+    32,
+    1
+  >;
 
- protected:
-  /// Internal reference
-  cutlass::TensorRef<Array<Element, Policy::LaneMmaShape::kN>, layout::RowMajor> ref_;
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
 
-  Array<Element, Policy::LaneMmaShape::kN>
-      activation[ThreadActivationShape::kH][ThreadActivationShape::kW][Iterations::kColumn];
-  int iterator_r_;
-  int iterator_s_;
+  /// Storage for transformed A tile
+  using TransformedFragmentA =
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements * 2>;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
+    Policy::OpDelta::kColumn,
+    32,
+    1
+  >;
 
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
 
-  MatrixCoord lane_offset_;
+  /// Storage for transformed B tile
+  using TransformedFragmentB =
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements * 2>;
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of complex products operations performed (one complex product needs four mma instructions)
+  using MmaIterations = MatrixShape<
+    Shape::kM / ArchMmaOperator::Shape::kM,
+    Shape::kN / ArchMmaOperator::Shape::kN
+  >;
 
- public:
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator() {}
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, 
+     ElementC, 
+     LayoutC,
+     typename ArchMmaOperator::Shape, 
+     typename Policy::OpDelta>;
+
+  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
+  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
+  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
+  /// of Tensor Cores which are always real-valued matrix multiplies.
+  using FragmentC = typename IteratorC::Fragment;
 
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator(TensorRef ref, int lane_id) {
-    // compute offset based on thread ID and lane layout
-    typename Policy::LaneLayout lane_layout = Policy::get_lane_layout();
+private:
 
-    // Set channel offset
-    lane_offset_ = lane_layout.inverse(lane_id) * MatrixCoord(0, Policy::LaneMmaShape::kN);
+  //
+  // Data members
+  //
 
-    ref.add_coord_offset(lane_offset_);
+  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
 
-    ref_.reset(reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(ref.data()),
-               ref.stride(0) / Policy::LaneMmaShape::kN);
+public:
 
-    iterator_r_ = 0;
-    iterator_s_ = 0;
-  }
+  //
+  // Methods
+  //
 
-  /// Adds a pointer offset to internal pointer(s) to advance through memory
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &add_pointer_offset(LongIndex offset) {
-    ref_.add_pointer_offset(offset);
-    return *this;
-  }
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaComplexTensorOp() {}
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  template <typename Params>
-  CUTLASS_HOST_DEVICE void setup_initial_status(
-      Params const &params) {
-
-    // Get base HW offset of current threads
-    int threadgroup = threadIdx.x / (ThreadBlockOutputShape::kC / ThreadOutputShape::kC);
-    int base_h =
-        (threadgroup / (ThreadTileCount::kColumn)) * ThreadOutputShape::kH * StrideShape::kRow;
-    int base_w =
-        (threadgroup % (ThreadTileCount::kColumn)) * ThreadOutputShape::kW * StrideShape::kColumn;
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &D, 
+    TransformedFragmentA const &A, 
+    TransformedFragmentB const &B, 
+    FragmentC const &C
+  ) const {
+
+    // Alias types for underlying real-valued matrix multiply operator
+    using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
+    using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
+    using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+    static_assert(platform::is_same<cutlass::gemm::GemmShape<16, 8, 8>, typename ArchMmaOperator::Shape>::value, 
+      "This implementation only supports MMA.1688 math instructions.");
+
+    static_assert(InstMmaOperandA::kElements == 4, 
+      "This implementation only supports math instructions in which exactly four element is needed for the A operand."
+      "We can geneneralize later.");
+
+    static_assert(InstMmaOperandB::kElements == 2, 
+      "This implementation only supports math instructions in which exactly two element is needed for the B operand."
+      "We can geneneralize later.");
+
+    // Instruction Operands A & B holding real part followed by imaginary part for mma operations
+    InstMmaOperandA const *operand_A = reinterpret_cast<InstMmaOperandA const *>(&A);
+    InstMmaOperandB const *operand_B = reinterpret_cast<InstMmaOperandB const *>(&B);
+
+    //
+    // Accumulate in place
+    //
+    D = C;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int h = 0; h < ThreadActivationShape::kH; ++h) {
+    for (int m = 0; m < MmaIterations::kRow; ++m) {
+
+      // mma(accum.real(), a.real(), b.real(), accum.real());
       CUTLASS_PRAGMA_UNROLL
-      for (int w = 0; w < ThreadActivationShape::kW; ++w) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int col = 0; col < Iterations::kColumn; ++col) {
-          int offset = (base_h + h) * ActivationShape::kW + (base_w + w);
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-          void const *ptr = ref_.data() + ref_.offset({offset, col * Policy::WarpShape::kColumn});
-          arch::shared_load(activation[h][w][col], ptr);
-        }
-      }
-    }
-  }
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &add_tile_offset(TensorCoord const &coord) {
-    // Set warp row and col start
-    lane_offset_ =
-        MatrixCoord({lane_offset_.row() + coord.row() * Shape::kRow, lane_offset_.column()});
-    return *this;
-  }
+          mma(*accum, operand_A[m], operand_B[n], *accum);
+      }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  void advance(int32_t pointer_offset) {
-    ref_.reset(ref_.data() + pointer_offset / sizeof(Element) / Policy::LaneMmaShape::kN);
-    iterator_s_ = 0;
-    iterator_r_ = 0;
-  }
+      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &operator++() {
-    ++iterator_s_;
-    if (iterator_s_ < FilterShape::kColumn) {
-      return *this;
-    }
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
 
-    iterator_s_ = 0;
+        mma(*accum, operand_A[m], operand_B[n+MmaIterations::kColumn], *accum);
+      }
 
-    ++iterator_r_;
-    if (iterator_r_ < FilterShape::kRow) {
-      return *this;
-    }
+      // mma(accum.real(), a.imag(), -b.imag(), accum.real())
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-    iterator_r_ = 0;
-    return *this;
-  }
+        // negate OperandB to accumulate  -(a.imag()*b.imag())
+        // negating OperandB emits less instrucitons than negating OperandA as OperandB has less elements
+        negate<InstMmaOperandB> negate_op;
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_HOST_DEVICE
-  DepthwiseDirect2dConvSimtTileIterator &operator--() {
-    // Do nothing
-    return *this;
-  }
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
 
-  /// Loads a fragment from memory at the location pointed to by the iterator. (vector loads)
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
-    Array<Element, Policy::LaneMmaShape::kN> *dst_ptr =
-        reinterpret_cast<Array<Element, Policy::LaneMmaShape::kN> *>(&frag);
+        mma(*accum, operand_A[m+MmaIterations::kRow], negate_op(operand_B[n+MmaIterations::kColumn]), *accum);
+      }
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int p = 0; p < ThreadOutputShape::kH; ++p) {
+      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
       CUTLASS_PRAGMA_UNROLL
-      for (int q = 0; q < ThreadOutputShape::kW; ++q) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int n = 0; n < Iterations::kColumn; ++n) {
-          const int h = p * StrideShape::kRow + iterator_r_ * DilationShape::kRow;
-          const int w = q * StrideShape::kColumn + iterator_s_ * DilationShape::kColumn;
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
 
-          dst_ptr[n + q + p * ThreadOutputShape::kW] = activation[h][w][n];
-        }
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+
+        mma(*accum, operand_A[m+MmaIterations::kRow], operand_B[n], *accum);
       }
     }
   }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const { load_with_pointer_offset(frag, 0); }
-
-  /// Stores a fragment to memory at the location pointed to by the iterator
-  CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) const {
-    // Do nothing at present.
-  }
-
-  /// Stores a fragment to memory at the location pointed to by the iterator
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag, Index pointer_offset) const {
-    store_with_pointer_offset(frag, 0);
-  }
-
+  /// Transform the mma operands to the required types
   CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    // no operation here
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+    // Alias types for underlying real-valued matrix multiply operator
+    using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
+    using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
+
+    //
+    // Define conversions from source type to instruction operands' type
+    //
+
+    FloatRoundStyle const kRoundA = FloatRoundStyle::round_half_ulp_trunc_dntz; 
+    FloatRoundStyle const kRoundB = FloatRoundStyle::round_half_ulp_trunc_dntz;
+
+    detail::UnpackComplexConvertAndPackForMma <
+      RealElementA,
+      InstMmaOperandA,
+      FragmentA,
+      MmaIterations,
+      MatrixShape<2, 2>,
+      kTransformA,
+      Operand::kA,
+      kRoundA> convert_A;
+
+    detail::UnpackComplexConvertAndPackForMma <
+      RealElementB,
+      InstMmaOperandB,
+      FragmentB,
+      MmaIterations,
+      MatrixShape<2, 1>,
+      kTransformB,
+      Operand::kB,
+      kRoundB> convert_B;
+
+    // Convert Fragment[A|B] holding complex<RealElement[A|B]> to InstMmaOperand[A|B] holding InstMmaOperand[A|B]::Element
+    convert_A(reinterpret_cast<InstMmaOperandA *>(&dst_A), A); 
+    convert_B(reinterpret_cast<InstMmaOperandB *>(&dst_B), B); 
   }
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+// TODO - partial specializations of real*complex and complex*real
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 } // namespace warp
-} // namespace conv
+} // namespace gemm
 } // namespace cutlass
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/warp/scale_bias_relu_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/coord.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -96,29 +96,20 @@
   CUTLASS_HOST_DEVICE
   Coord(Index const (&_idx)[kRank]) {
     for (int i = 0; i < kRank; ++i) {
       idx[i] = _idx[i];
     }
   }
 
-  /// Constructs from some other Coord
-  template <int R, typename I, typename L>
-  CUTLASS_HOST_DEVICE
-  Coord(Coord<R, I, L> other) {
-    for (int i = 0; i < kRank; ++i) {
-      idx[i] = other[i];
-    }
-  }
-
   /// Returns a slice of the Coord which may be larger or smaller in rank
   /// than this.
   template <int Slice>
   CUTLASS_HOST_DEVICE
-  Coord<Slice, Index, LongIndex> slice(int start = 0, Index identity = 0) const {
-    Coord<Slice, Index, LongIndex> result;
+  Coord<Slice> slice(int start = 0, Index identity = 0) const {
+    Coord<Slice> result;
     for (int i = 0; i < Slice; ++i) {
       if (i + start < kRank) {
         result[i] = idx[i + start];
       }
       else {
         result[i] = identity;
       }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/core_io.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/core_io.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -55,17 +55,15 @@
 /// Output operator for CUDA built-in dim3 type
 inline std::ostream &operator<<(std::ostream &out, dim3 d) {
   return out << d.x << ", " << d.y << ", " << d.z;
 }
 
 /// Output operator for CUDA built-in error type
 inline std::ostream &operator<<(std::ostream &out, cudaError_t error) {
-#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) || defined(__CUDACC_RTC__)
   return out << cudaGetErrorString(error);
-#endif
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
@@ -250,17 +248,16 @@
 //                         stream operators for cutlass::conv namespace                          //
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 namespace conv {
 /// Default printing to ostream for Conv2dProblemSize
 inline
 std::ostream& operator<<(std::ostream& out, Conv2dProblemSize const& problem) {
   out << "NHWC: (" << problem.N << ", " << problem.H << ", " << problem.W << ", " << problem.C << ")" << std::endl
-      << "KRSC: (" << problem.K << ", " << problem.R << ", " << problem.S << ", " << problem.C / problem.groups << ")" << std::endl
+      << "KRSC: (" << problem.K << ", " << problem.R << ", " << problem.S << ", " << problem.C << ")" << std::endl
       << "NPQK: (" << problem.N << ", " << problem.P << ", " << problem.Q << ", " << problem.K << ")" << std::endl
-      << "groups: (" << problem.groups << ")" << std::endl
       << "Pad_h, Pad_w: (" << problem.pad_h << ", " << problem.pad_w << ")" << std::endl
       << "Stride_h, Stride_w: (" << problem.stride_h << ", " << problem.stride_w << ")" << std::endl
       << "Dilation_h, Dilation_w: (" << problem.dilation_h << ", " << problem.dilation_w << ")" << std::endl
       << "split_k_slices: (" << problem.split_k_slices << ")" << std::endl
       << "mode: (" << ((problem.mode==conv::Mode::kConvolution) ? "conv" : "xcross") << ")";
 
   return out;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/cutlass.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/cutlass.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -34,68 +34,58 @@
 */
 
 #pragma once
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 #ifdef CUTLASS_NAMESPACE
-#define concat_tok(a, b) a ## b
-#define mkcutlassnamespace(pre, ns) concat_tok(pre, ns)
-#define cutlass mkcutlassnamespace(cutlass_, CUTLASS_NAMESPACE)
+#define cutlass CUTLASS_NAMESPACE
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
-#define CUTLASS_HOST_DEVICE __forceinline__ __device__ __host__
-#define CUTLASS_DEVICE __forceinline__ __device__
-#elif defined(__CUDACC_RTC__)
-#define CUTLASS_HOST_DEVICE __forceinline__ __device__
-#define CUTLASS_DEVICE __forceinline__ __device__
-#else
-#define CUTLASS_HOST_DEVICE inline
-#define CUTLASS_DEVICE inline
-#endif
+#define CUTLASS_UNUSED(expr) do { ; } while (&expr != &expr)
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
+#if !defined(__CUDACC_RTC__)
 
-template<typename T>
-CUTLASS_HOST_DEVICE void __CUTLASS_UNUSED(T const &) 
-{ }
+#include <assert.h>
 
-#if defined(__GNUC__)
-  #define CUTLASS_UNUSED(expr) __CUTLASS_UNUSED(expr)
+#if defined(_MSC_VER)
+  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
 #else
-  #define CUTLASS_UNUSED(expr) do { ; } while (&expr != &expr)
+  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
 #endif
 
-#if !defined(__CUDACC_RTC__)
-
-#include <assert.h>
+#else
 
-  #if defined(__CUDA_ARCH__)
-    #if defined(_MSC_VER)
-      #define CUTLASS_NOT_IMPLEMENTED() { printf("%s not implemented\n", __FUNCSIG__); asm volatile ("brkpt;\n"); }
-    #else
-      #define CUTLASS_NOT_IMPLEMENTED() { printf("%s not implemented\n", __PRETTY_FUNCTION__); asm volatile ("brkpt;\n"); }
-    #endif
+#if defined(_MSC_VER)
+  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
+#else
+  #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
+#endif
 
-  #else
-    #if defined(_MSC_VER)
-      #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __FUNCSIG__)
-    #else
-      #define CUTLASS_NOT_IMPLEMENTED() assert(0 && __PRETTY_FUNCTION__)
-    #endif
-  #endif
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
+
+#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__))
+#define CUTLASS_HOST_DEVICE __forceinline__ __device__ __host__
+#define CUTLASS_DEVICE __forceinline__ __device__
+#elif defined(__CUDACC_RTC__)
+#define CUTLASS_HOST_DEVICE __forceinline__ __device__
+#define CUTLASS_DEVICE __forceinline__ __device__
+#else
+#define CUTLASS_HOST_DEVICE inline
+#define CUTLASS_DEVICE inline
+#endif
+
 /// Status code returned by CUTLASS operations
 enum class Status {
   kSuccess,                    ///< Operation was successful.
   kErrorMisalignedOperand,     ///< operands fail alignment requirements.
   kErrorInvalidDataType,       ///< DataType fails requirement.
   kErrorInvalidLayout,         ///< Layout fails alignment requirement.
   kErrorInvalidProblem,        ///< Specified problem size is not supported by operator.
@@ -177,51 +167,28 @@
     #define CUTLASS_PRAGMA_NO_UNROLL
     #define CUTLASS_GEMM_LOOP
 
 #endif
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-static const int NumThreadsPerWarp = 32;
-static const int NumThreadsPerWarpGroup = 128;
-static const int NumThreadsPerHalfWarp = NumThreadsPerWarp / 2;
-static const int NumThreadsPerQuad = 4;
-static const int NumThreadsPerQuadPair = NumThreadsPerQuad * 2;
+static const int NUM_THREADS_PER_WARP = 32;
+static const int NUM_THREADS_PER_HALF_WARP = NUM_THREADS_PER_WARP / 2;
+static const int NUM_THREADS_PER_QUAD = 4;
+static const int NUM_THREADS_PER_QUAD_PAIR = NUM_THREADS_PER_QUAD * 2;
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Helper function to return true when called by thread 0 of threadblock 0.
 CUTLASS_HOST_DEVICE bool thread0() {
   #if defined(__CUDA_ARCH__)
     return (!threadIdx.x && !threadIdx.y && !threadIdx.z) && (!blockIdx.x && !blockIdx.y && !blockIdx.z);
   #else
     return false;
   #endif
 }
 
-/// Returns a warp-uniform value indicating the canonical warp index of the calling threads.
-/// Threads within the warp must be converged.
-CUTLASS_DEVICE
-int canonical_warp_idx() { 
-  #if defined(__CUDA_ARCH__)
-    return __shfl_sync(0xffffffff, threadIdx.x / NumThreadsPerWarp, 0);
-  #else
-    return 0;
-  #endif
-}
-
-/// Returns a warp-uniform value indicating the canonical warp group index of the calling threads.
-/// Threads within the warp must be converged.
-CUTLASS_DEVICE
-int canonical_warp_group_idx() {
-  #if defined(__CUDA_ARCH__)
-    return __shfl_sync(0xffffffff, threadIdx.x / NumThreadsPerWarpGroup, 0);
-  #else
-    return 0;
-  #endif
-}
-
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/device_kernel.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,89 +25,77 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for generic CUTLASS kernel.
+    \brief Defines basic structures needed for implementing the warp-scoped phase of the epilogue.
+          These quantities assume a 'column-major' arrangement of TensorOp instructions, of which
+          a row-oriented slice is visible per iteration.
 */
 
 #pragma once
 
-// __grid_constant__ was introduced in CUDA 11.7.
-#if ((__CUDACC_VER_MAJOR__ >= 12) || ((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 7)))
-#  define CUTLASS_GRID_CONSTANT_SUPPORTED
-#endif
-
-// __grid_constant__ can be enabled only on SM70+
-#if defined(CUTLASS_GRID_CONSTANT_SUPPORTED) && defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 700)
-#  define CUTLASS_GRID_CONSTANT_ENABLED
-#endif
+#include "cutlass/arch/wmma.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/layout/matrix.h"
 
-#if ! defined(CUTLASS_GRID_CONSTANT)
-#  if defined(CUTLASS_GRID_CONSTANT_ENABLED)
-#    define CUTLASS_GRID_CONSTANT __grid_constant__
-#  else
-#    define CUTLASS_GRID_CONSTANT
-#  endif
-#endif
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
+namespace epilogue {
+namespace warp {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__
-void Kernel(typename Operator::Params params) {
-  // Dynamic shared memory base pointer
-  extern __shared__ int SharedStorageBase[];
-
-  // Declare pointer to dynamic shared memory.
-  typename Operator::SharedStorage *shared_storage =
-      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);
-
-  Operator op;
-
-  op(params, *shared_storage);
-}
+/// Policy details related to the epilogue
+template <
+  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
+  typename OperatorShape, ///< matrix multiply operation shape (concept: gemm:GemmShape)
+  typename Layout         ///< target shared memory layout
+>
+struct WmmaTensorOpPolicy; 
 
+////////////////////////////////////////////////////////////////////////////////
 
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__
-void Kernel2(typename Operator::Params params) {
-  // Dynamic shared memory base pointer
-  extern __shared__ int SharedStorageBase[];
+/// Partial specialization for row-major
+template <
+  typename WarpShape,           ///< shape of warp-level GEMM (concept: MatrixShape)
+  typename OperatorShape        ///< matrix multiply operation shape (concept: gemm::GemmShape)
+>
+struct WmmaTensorOpPolicy<WarpShape, OperatorShape, layout::RowMajor> {
+
+  /// Number of operations
+  using OperatorCount = MatrixShape<
+    WarpShape::kM / OperatorShape::kM,
+    WarpShape::kN / OperatorShape::kN
+  >;
+
+  //
+  // Hard-coded constants regarding Tensor Operations
+  //
+  static int const kElementsPerAccess = 2;
+  static int const kRowsPerIteration = OperatorShape::kM;
+  static int const kWmmaFragmentsPerAccess = 1;
+
+  //
+  // Derived quantities
+  //
 
-  // Declare pointer to dynamic shared memory.
-  typename Operator::SharedStorage *shared_storage =
-      reinterpret_cast<typename Operator::SharedStorage *>(SharedStorageBase);
+  // Number of externally visible iterations
+  static int const kIterations = OperatorCount::kRow;
 
-  Operator::invoke(params, *shared_storage);
+};
 
-}
+////////////////////////////////////////////////////////////////////////////////
 
+} // namespace warp
+} // namespace epilogue
+} // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
-//
-// 3.0 specific launch
-//
-////////////////////////////////////////////////////////////////////////////////
 
-/// Generic CUTLASS kernel template.
-template <typename Operator>
-__global__ __launch_bounds__(Operator::MaxThreadsPerBlock, Operator::MinBlocksPerMultiprocessor)
-void device_kernel(CUTLASS_GRID_CONSTANT typename Operator::Params const params)
-{
-  // Dynamic shared memory base pointer
-  extern __shared__ char smem[];
-
-  Operator op;
-  op(params, smem);
-}
+#endif
 
-////////////////////////////////////////////////////////////////////////////////
-} /// namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/activation.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,14 +46,24 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <typename T>
+struct Identity {
+  CUTLASS_HOST_DEVICE
+  T operator()(T value) const {
+    return value;
+  }
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename T>
 struct LinearCombinationGenericParams {
   T alpha;                  ///< scales accumulators
   T beta;                   ///< scales source tensor
   T const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
   T const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
 
@@ -79,47 +89,14 @@
     T const *alpha_ptr,
     T const *beta_ptr = nullptr
   ): alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) { }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-// Identity operator
-template <typename T>
-struct Identity {
-  static const bool kIsHeavy=false;
-
-  CUTLASS_HOST_DEVICE
-  T operator()(T value) const {
-    return value;
-  }
-
-  using Params = LinearCombinationGenericParams<T>;
-
-  CUTLASS_HOST_DEVICE
-  T operator()(T const &value, Params const &params_) const {
-    return this->operator()(value);
-  }
-};
-
-template <typename T, int N>
-struct Identity<Array<T, N> > {
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
-    return value;
-  }
-
-  using Params = LinearCombinationGenericParams<T>;
-
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
-  }
-};
-
 /// ReLu operator - propagates NaNs
 /// Always put threshold in the right hand side of max to propagate NaN.
 template <typename T>
 struct ReLu {
   static const bool kIsHeavy=false;
   CUTLASS_HOST_DEVICE
   T operator()(T const & threshold, T value) const {
@@ -179,15 +156,15 @@
     // Methods
     using LinearCombinationGenericParams<T>::LinearCombinationGenericParams;
 
     CUTLASS_HOST_DEVICE
     Params():
       LinearCombinationGenericParams<T>(),
       leaky_alpha(T(1)) {}
- 
+    
     CUTLASS_HOST_DEVICE
     Params(
       T alpha,
       T beta,
       T leaky_alpha = T(1)
     ): LinearCombinationGenericParams<T>(alpha, beta), leaky_alpha(leaky_alpha) {}
   };
@@ -213,40 +190,40 @@
 
     // Methods
 
     CUTLASS_HOST_DEVICE
     Params():
       LinearCombinationGenericParams<T>(),
       leaky_alpha(T(1)) {}
-
+    
     CUTLASS_HOST_DEVICE
     Params(
       T alpha,
       T beta,
       T leaky_alpha = T(1)
     ): LinearCombinationGenericParams<T>(alpha, beta), leaky_alpha(leaky_alpha) {}
   };
 
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, T const & alpha_recip) const {
+  Array<T, N> operator()(Array<T, N> const &rhs, T const & alpha_recip) const {
     Array<T, N> y;
     LeakyReLU<T> leaky_op;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < int(value.size()); ++i) {
-      y[i] = leaky_op(value[i], alpha_recip);
+    for (int i = 0; i < int(rhs.size()); ++i) {
+      y[i] = leaky_op(rhs[i], alpha_recip);
     }
 
     return y;
   }
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value, params_.leaky_alpha);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs, params_.leaky_alpha);
   }
 };
 
 // Tanh operator
 template <typename T>
 struct Tanh {
   CUTLASS_HOST_DEVICE
@@ -261,31 +238,31 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct Tanh<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Array<T, N> y;
     Tanh<T> tanh_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = tanh_op(value[i]);
+      y[i] = tanh_op(rhs[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs);
   }
 };
 
 template <int N>
 struct Tanh<Array<half_t, N>> {
   using T = half_t;
 
@@ -295,16 +272,16 @@
     return tanh(z);
 
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs);
   }
 };
 
 // Sigmoid operator
 template <typename T>
 struct Sigmoid {
   CUTLASS_HOST_DEVICE
@@ -319,31 +296,31 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct Sigmoid<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Array<T, N> y;
     Sigmoid<T> sigmoid_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = sigmoid_op(value[i]);
+      y[i] = sigmoid_op(rhs[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs);
   }
 };
 
 template <int N>
 struct Sigmoid<Array<half_t, N>> {
   using T = half_t;
 
@@ -364,15 +341,14 @@
                add(cutlass::constants::one<T>(),
                    fast_exp(neg(z))));
 #endif
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
-  CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &z, Params const &params_) const {
     return this->operator()(z);
   }
 };
 
 // SiLu (swish) operator introduced by Elfwing et al. in the following paper
 // "Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning" (2017)
@@ -394,25 +370,25 @@
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct SiLu<Array<T, N>> {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Sigmoid<Array<T, N>> sigmoid_op;
     multiplies<Array<T, N>>     mul;
-    return mul(value, sigmoid_op(value));
+    return mul(rhs, sigmoid_op(rhs));
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs);
   }
 };
 
 // Hardswish operator introduced by Howard et al. in the following paper
 // "Searching for MobileNetV3" (2019)
 // https://arxiv.org/pdf/1905.02244.pdf
 // It is used in models based on MobilenetNetV3.
@@ -454,21 +430,21 @@
     return this->operator()(x);
   }
 };
 
 template <typename T, int N>
 struct HardSwish<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Array<T, N> y;
     HardSwish<T> hardswish_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = hardswish_op(value[i]);
+      y[i] = hardswish_op(rhs[i]);
     }
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
@@ -479,21 +455,21 @@
 };
 
 template <int N>
 struct HardSwish<Array<half_t, N> > {
   using T = half_t;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     minimum<Array<T, N> > mn;
     maximum<Array<T, N> > mx;
     multiplies<Array<T, N> > mul;
     plus<Array<T, N> > add;
-
-    return mul(mul(mn(mx(add(value, T(3)), T(0)), T(6)), value), T(0.16666667f));
+ 
+    return mul(mul(mn(mx(add(rhs, T(3)), T(0)), T(6)), rhs), T(0.16666667f));
   }
 
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
   Array<T, N> operator()(Array<T, N> const &x, Params const &params_) const {
     return this->operator()(x);
@@ -513,15 +489,15 @@
 template <typename T>
 struct GELU {
   CUTLASS_HOST_DEVICE
   T operator()(T const &scalar) const {
     return T(cutlass::constants::half<T>() * scalar *
       (cutlass::constants::one<T>() + (T)erff((float)(scalar / cutlass::constants::root_two<T>()))));
   }
-
+  
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
   T operator()(T const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
@@ -529,15 +505,15 @@
 template <>
 struct GELU<float> {
   CUTLASS_HOST_DEVICE
   float operator()(float const &scalar) const {
     return cutlass::constants::half<float>() * scalar *
       (cutlass::constants::one<float>() + erff( scalar / cutlass::constants::root_two<float>() ));
   }
-
+  
   using Params = LinearCombinationGenericParams<float>;
 
   CUTLASS_HOST_DEVICE
   float operator()(float const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
@@ -545,43 +521,43 @@
 template <>
 struct GELU<double> {
   CUTLASS_HOST_DEVICE
   double operator()(double const &scalar) const {
     return cutlass::constants::half<double>() * scalar *
       (cutlass::constants::one<double>() + erf( scalar / cutlass::constants::root_two<double>() ));
   }
-
+  
   using Params = LinearCombinationGenericParams<double>;
 
   CUTLASS_HOST_DEVICE
   double operator()(double const &scalar, Params const &params_) const {
     return this->operator()(scalar);
   }
 };
 
 template <typename T, int N>
 struct GELU<Array<T, N> > {
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Array<T, N> y;
     GELU<T> gelu_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = gelu_op(value[i]);
+      y[i] = gelu_op(rhs[i]);
     }
 
     return y;
   }
-
+  
   using Params = LinearCombinationGenericParams<T>;
 
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
+  Array<T, N> operator()(Array<T, N> const &rhs, Params const &params_) const {
+    return this->operator()(rhs);
   }
 };
 
 // GELU operator implemented using the Taylor series approximation
 template <typename T>
 struct GELU_taylor {
   static const bool kIsHeavy=true;
@@ -592,19 +568,15 @@
     T k1 = T(0.044715);
 
     return T(cutlass::constants::half<T>() * z *
       (cutlass::constants::one<T>() + fast_tanh(k0 * z * (cutlass::constants::one<T>() + k1 * z * z))));
   }
 
   using Params = LinearCombinationGenericParams<T>;
-
-  CUTLASS_HOST_DEVICE
-  T operator()(T const &scalar, Params const &params_) const {
-    return this->operator()(scalar);
-  }
+  
 };
 
 template <int N>
 struct GELU_taylor<Array<half_t, N> > {
   static const bool kIsHeavy=true;
   CUTLASS_HOST_DEVICE
   Array<half_t, N> operator()(Array<half_t, N> const &z) const {
@@ -625,43 +597,33 @@
 
     y = mul(mul(z, cutlass::constants::half<T>()), add(cutlass::constants::one<T>(), tanh(u)));
 
     return y;
   }
 
   using Params = LinearCombinationGenericParams<half_t>;
-
-  CUTLASS_HOST_DEVICE
-  Array<half_t, N> operator()(Array<half_t, N> const &value, Params const &params_) const {
-    return this->operator()(value);
-  }
 };
 
 template <typename T, int N>
 struct GELU_taylor<Array<T, N> > {
   static const bool kIsHeavy=true;
   CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value) const {
+  Array<T, N> operator()(Array<T, N> const &rhs) const {
     Array<T, N> y;
     GELU_taylor<T> gelu_op;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < N; ++i) {
-      y[i] = gelu_op(value[i]);
+      y[i] = gelu_op(rhs[i]);
     }
 
     return y;
   }
-
+  
   using Params = LinearCombinationGenericParams<T>;
-
-  CUTLASS_HOST_DEVICE
-  Array<T, N> operator()(Array<T, N> const &value, Params const &params_) const {
-    return this->operator()(value);
-  }
 };
 
 /// Computes backwards pass for GELU operator assuming d_t is the layer gradient and
 /// z is computed from the forward pass.
 template <typename T>
 struct dGELU {
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/conversion_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination.h`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -58,93 +58,91 @@
   typename ElementOutput_,                             ///< Data type used to load and store tensors
   int Count,                                           ///< Number of elements computed per operation.
                                                        ///< Usually it is 128/sizeof_bits<ElementOutput_>,
                                                        ///< but we use 64 or 32 sometimes when there are not enough data to store
   typename ElementAccumulator_ = ElementOutput_,       ///< Accumulator data type
   typename ElementCompute_ = ElementOutput_,           ///< Data type used to compute linear combination
   ScaleType::Kind Scale = ScaleType::Default,          ///< Control Alpha and Beta scaling
-  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest,
-  typename ElementSource_ = ElementOutput_
+  FloatRoundStyle Round = FloatRoundStyle::round_to_nearest
 >
 class LinearCombination {
 public:
 
   using ElementOutput = ElementOutput_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
-  using ElementC = ElementSource_;
-  using ElementD = ElementOutput_;
 
   static int const kCount = Count;
   static const ScaleType::Kind kScale = Scale;
   using FragmentOutput = Array<ElementOutput, kCount>;
   using FragmentAccumulator = Array<ElementAccumulator, kCount>;
   using ComputeFragment = Array<ElementCompute, kCount>;
 
   using ParamsBase = LinearCombinationParams;
+  
   static FloatRoundStyle const kRound = Round;
 
   /// Host-constructable parameters structure
   struct Params : ParamsBase{
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
     ElementCompute const *beta_ptr;        ///< pointer to source scalar - if not null, loads it from memory
 
     CUTLASS_HOST_DEVICE
-    Params():
+    Params(): 
       ParamsBase(
-        ElementCompute(1),
+        ElementCompute(1), 
         ElementCompute(0)
       ),
-      alpha(ElementCompute(1)),
-      beta(ElementCompute(0)),
-      alpha_ptr(nullptr),
+      alpha(ElementCompute(1)), 
+      beta(ElementCompute(0)), 
+      alpha_ptr(nullptr), 
       beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha,
       ElementCompute beta
-    ):
+    ): 
       ParamsBase(alpha, beta),
-      alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) { }
+      alpha(alpha), beta(beta), alpha_ptr(nullptr), beta_ptr(nullptr) { } 
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute alpha
-    ):
+    ): 
       ParamsBase(alpha, ElementCompute(0)),
       alpha(alpha), beta(0), alpha_ptr(nullptr), beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr,
       ElementCompute const *beta_ptr
-    ):
+    ): 
       ParamsBase(*alpha_ptr, *beta_ptr),
       alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ElementCompute const *alpha_ptr
     ):
       ParamsBase(*alpha_ptr, ElementCompute(0)),
       alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       ParamsBase const& base
-    ): ParamsBase(base), alpha_ptr(nullptr), beta_ptr(nullptr) {
+    ): ParamsBase(base), alpha_ptr(nullptr), beta_ptr(nullptr) { 
       #if defined(__CUDA_ARCH__)
       alpha = reinterpret_cast<ElementCompute const&>(base.alpha_data);
       beta = reinterpret_cast<ElementCompute const&>(base.beta_data);
       #else
-      memcpy( alpha, base.alpha_data, sizeof(ElementCompute) );
-      memcpy( beta, base.alpha_data, sizeof(ElementCompute) );
+      memcpy( alpha, base.alpha_data, sizeof(ElementCompute) ); 
+      memcpy( beta, base.alpha_data, sizeof(ElementCompute) ); 
       #endif
     }
   };
 
 private:
 
   //
@@ -182,15 +180,15 @@
       beta_ = ElementCompute(1);
     }
   }
 
   /// Computes linear scaling: D = alpha * accumulator + beta * source
   CUTLASS_HOST_DEVICE
   FragmentOutput operator()(
-    FragmentAccumulator const &accumulator,
+    FragmentAccumulator const &accumulator, 
     FragmentOutput const &source) const {
 
     // Convert source to interal compute numeric type
     NumericArrayConverter<ElementCompute, ElementOutput, kCount, Round> source_converter;
     NumericArrayConverter<ElementCompute, ElementAccumulator, kCount, Round> accumulator_converter;
 
     // Convert to destination numeric type
@@ -234,69 +232,16 @@
     if (Scale == ScaleType::Nothing)
       return destination_converter(converted_accumulator);
 
     // Perform binary operations
     ComputeFragment intermediate;
     multiplies<ComputeFragment> mul_accumulator;
 
-    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum
-
-    return destination_converter(intermediate);
-  }
-
-  //
-  // Specializations for scalar (for use with cute::collective::DefaultEpilogue)
-  //
-  CUTLASS_HOST_DEVICE
-  ElementD operator()(ElementAccumulator const accumulator, ElementC const source) const {
-    // Convert everything to Compute type, do compute, and then store to output type
-    NumericConverter<ElementCompute, ElementAccumulator, Round> accumulator_converter;
-    [[maybe_unused]] NumericConverter<ElementCompute, ElementC, Round> source_converter;
-    NumericConverter<ElementD, ElementCompute, Round> destination_converter;
-
-    // Convert to destination numeric type
-
-    ElementCompute converted_accumulator = accumulator_converter(accumulator);
-    if constexpr (Scale == ScaleType::Nothing) {
-      return destination_converter(converted_accumulator);
-    }
-
-    // Perform binary operations
-    ElementCompute intermediate;
-    multiplies<ElementCompute> multiply;
-    multiply_add<ElementCompute> madd;
-
-    if constexpr (Scale == ScaleType::NoBetaScaling) {
-      intermediate = source_converter(source);
-    }
-    else {
-      intermediate = multiply(beta_, source);                            // X =  beta * C + uniform
-    }
-
-    intermediate = madd(alpha_, converted_accumulator, intermediate);    // D = alpha * Accum + X
-    return destination_converter(intermediate);
-  }
-
-  CUTLASS_HOST_DEVICE
-  ElementD operator()(ElementAccumulator const accumulator) const {
-    // Convert everything to Compute type, do compute, and then store to output type
-    NumericConverter<ElementCompute, ElementAccumulator, Round> accumulator_converter;
-    NumericConverter<ElementD, ElementCompute, Round> destination_converter;
-    ElementCompute converted_accumulator = accumulator_converter(accumulator);
-
-    // Convert to destination numeric type
-    if constexpr (Scale == ScaleType::Nothing) {
-      return destination_converter(converted_accumulator);
-    }
-
-    // Perform binary operations
-    ElementCompute intermediate;
-    multiplies<ElementCompute> multiply;
+    intermediate = mul_accumulator(alpha_, converted_accumulator);    // D = alpha * Accum 
 
-    intermediate = multiply(alpha_, accumulator);    // D = alpha * Accum
     return destination_converter(intermediate);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace thread
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_elementwise.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,52 +57,46 @@
   typename ElementC_,
   typename ElementAccumulator_,
   typename ElementCompute_,
   typename ElementZ_,
   typename ElementT_,
   int ElementsPerAccess,
   typename ElementwiseOp_ = Identity<ElementCompute_>,
-  typename BinaryOp_ = plus<ElementCompute_>,
-  bool StoreT_ = true,
-  typename ElementVector_ = ElementC_
+  typename BinaryOp_ = plus<ElementCompute_>
 >
 class LinearCombinationBiasElementwise {
 public:
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
   using ElementZ = ElementZ_;
   using ElementT = ElementT_;
-  using ElementVector = ElementVector_;
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ElementwiseOp_;
   using BinaryOp = BinaryOp_;
 
-  // Indicates that this epilogue applies only one binary operation
-  static bool const kIsSingleSource = true;
-
   using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
   using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
   using FragmentC = Array<ElementOutput, kElementsPerAccess>;
   using FragmentZ = Array<ElementZ, kElementsPerAccess>;
   using FragmentT = Array<ElementT, kElementsPerAccess>;
 
   using FragmentOutput = FragmentZ;
 
   static bool const kIsHeavy = ElementwiseOp::kIsHeavy;
 
   /// If true, the 'Z' tensor is stored
   static bool const kStoreZ = true;
 
   /// If true, the 'T' tensor is stored
-  static bool const kStoreT = StoreT_;
+  static bool const kStoreT = true;
 
   /// Host-constructable parameters structure
   struct Params {
 
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_bias_relu.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -200,49 +200,44 @@
 /// EpilogueWithBroadcast::OutputOp
 template <
   typename ElementC_,
   typename ElementAccumulator_,
   typename ElementCompute_,
   typename ElementZ_,
   int ElementsPerAccess,
-  bool StoreT_ = true,
-  typename ElementVector_ = ElementC_
+  bool StoreT = true
 >
 class LinearCombinationBiasRelu {
 public:
 
   using ElementOutput = ElementC_;
   using ElementC = ElementC_;
   using ElementAccumulator = ElementAccumulator_;
   using ElementCompute = ElementCompute_;
   using ElementZ = ElementZ_;
-  using ElementVector = ElementVector_;
 
   using ElementT = uint1b_t;
 
   static int const kElementsPerAccess = ElementsPerAccess;
   static int const kCount = kElementsPerAccess;
 
   using ElementwiseOp = ReLu<ElementCompute>;
   using BinaryOp = plus<ElementCompute>;
 
-  // Indicates that this epilogue applies only one binary operation
-  static bool const kIsSingleSource = true;
-
   using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
   using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
   using FragmentC = Array<ElementOutput, kElementsPerAccess>;
   using FragmentZ = Array<ElementZ, kElementsPerAccess>;
   using FragmentT = Array<ElementT, kElementsPerAccess>;
 
   /// If true, the 'Z' tensor is stored
   static bool const kStoreZ = true;
 
   /// If true, the 'T' tensor is stored
-  static bool const kStoreT = StoreT_;
+  static bool const kStoreT = StoreT;
 
   /// Host-constructable parameters structure
   struct Params {
 
     ElementCompute alpha;                  ///< scales accumulators
     ElementCompute beta;                   ///< scales source tensor
     ElementCompute const *alpha_ptr;       ///< pointer to accumulator scalar - if not null, loads it from memory
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_clamp.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_dgelu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_drelu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_gelu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_generic.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_hardswish.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /*************************************************************************************************** 
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_leaky_relu.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,15 +33,14 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/epilogue/thread/activation.h"
-#include "cutlass/epilogue/thread/scale_type.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace thread {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_params.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_relu0.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_residual_block.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,285 +24,309 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-  \brief Epilogue functor specialized for residual blocks in deep neural networks.
+  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
+
+  The epilogue rearranges the result of a matrix product through shared memory to match canonical
+  tensor layouts in global memory. Epilogues support conversion and reduction operations.
+
 */
 
 #pragma once
 
+#include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
-#include "cutlass/functional.h"
-#include "cutlass/numeric_conversion.h"
+#include "cutlass/layout/vector.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/tensor_coord.h"
+#include "cutlass/aligned_buffer.h"
+
+#include "cutlass/gemm/gemm.h"
+
+#include "cutlass/transform/pitch_linear_thread_map.h"
+#include "cutlass/transform/threadblock/regular_tile_iterator.h"
+
+#include "cutlass/epilogue/threadblock/epilogue_base.h"
+#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
-namespace thread {
+namespace threadblock {
 
-namespace detail {
+////////////////////////////////////////////////////////////////////////////////
 
-/// Dummy class used to designate that the second binary operator in the epilogue is unsued
-template <typename T>
-class NoOp {};
-
-}
-
-/// Models a residual block of the form: UnaryOp(BinaryOp(BinaryOp(ActivationOp(TensorOp(X) + bias), residual1), residual2))
-template <typename ElementOutput_, typename ElementAccumulator_,
-          typename ElementCompute_, typename ElementC_, int ElementsPerAccess,
-          template <typename T> class ActivationOp_,
-          template <typename T> class BinaryOp1_,
-          template <typename T> class UnaryOp_,
-          template <typename T> class BinaryOp2_ = detail::NoOp,
-          bool StoreT_ = false,
-          typename ElementVector_ = ElementC_>
-class LinearCombinationResidualBlock {
-public:
-  static bool const kIsSingleSource = false;
-
-  using ElementOutput = ElementC_;
-  using ElementC = ElementC_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementCompute = ElementCompute_;
-  using ElementVector = ElementVector_;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kCount = kElementsPerAccess;
-
-  using UnaryOp = UnaryOp_<Array<ElementCompute, kCount>>;
-  using BinaryOp1 = BinaryOp1_<Array<ElementCompute, kCount>>;
-  using BinaryOp2 = BinaryOp2_<Array<ElementCompute, kCount>>;
-  using ActivationOp = ActivationOp_<Array<ElementCompute, kCount>>;
-
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
-  using FragmentC = Array<ElementC, kElementsPerAccess>;
-  using FragmentOutput = Array<ElementOutput, kElementsPerAccess>;
-
-  using ElementZ = ElementOutput_;
-  using ElementT = ElementZ;
-  using FragmentZ = Array<ElementZ, kElementsPerAccess>;
-  using FragmentT = Array<ElementT, kElementsPerAccess>;
-
-  static bool const kIsHeavy = true;
-  static bool const kStoreZ = true;
-  static bool const kStoreT = StoreT_;
-
-  /// Host-constructable parameters structure
-  struct Params {
-
-    ElementCompute alpha;                  ///< scales accumulators
-    ElementCompute beta;                   ///< scales residual input
-    ElementCompute const *alpha_ptr{nullptr};       ///< pointer to accumulator scalar - if not null, loads it from memory
-    ElementCompute const *beta_ptr{nullptr};        ///< pointer to residual scalar - if not null, loads it from memory
-
-    CUTLASS_HOST_DEVICE
-    Params() : alpha(ElementCompute(1)), beta(ElementCompute(1)) {}
-
-    CUTLASS_HOST_DEVICE
-    Params(ElementCompute alpha, ElementCompute beta)
-        : alpha(alpha), beta(beta) {}
-
-    CUTLASS_HOST_DEVICE
-    Params(ElementCompute const *alpha_ptr, ElementCompute const *beta_ptr)
-        : alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {}
-  };
-
-private:
-
-  ElementCompute alpha_;
-  ElementCompute beta_;
-  bool skip_elementwise_;
-
-public:
-
-  /// Constructor from Params
-  CUTLASS_HOST_DEVICE
-  LinearCombinationResidualBlock(Params const &params) {
-    alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
-    beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    skip_elementwise_ = false;
-  }
-
-  /// The "source" tensor corresponds to the residual input
-  CUTLASS_HOST_DEVICE
-  bool is_source_needed() const { return true; }
-
-  /// Functionally required for serial reduction in the epilogue
-  /// IMPORTANT: Split-k is supported only when ActivationOp is Identity.
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {
-    if (k_partition) {
-      beta_ = ElementCompute(1);
+/// Epilogue operator without splitk
+template <
+    /// Shape of threadblock tile (concept: GemmShape)
+    typename Shape_,
+    /// Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
+    typename WarpMmaOperator_,
+    /// Number of partitions of the K dimension
+    int PartitionsK,
+    /// Tile iterator reading and writing output tensors
+    typename OutputTileIterator_,
+    /// Fragment iterator selecting accumulators
+    typename AccumulatorFragmentIterator_,
+    /// Output operator
+    typename OutputOp_,
+    /// Number of interleaved k
+    int InterleavedK>
+class InterleavedEpilogue {
+ public:
+  using Shape = Shape_;
+  using WarpMmaOperator = WarpMmaOperator_;
+  static int const kPartitionsK = PartitionsK;
+  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
+  using OutputTileIterator = OutputTileIterator_;
+  using OutputOp = OutputOp_;
+
+  /// The complete warp-level accumulator tile
+  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
+
+  /// Accumulator element
+  using ElementAccumulator = typename AccumulatorTile::Element;
+
+  /// Output element
+  using ElementOutput = typename OutputTileIterator::Element;
+
+  /// Output access size
+  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
+
+  /// Tensor reference to destination tensor
+  using TensorRef = typename OutputTileIterator::TensorRef;
+
+  /// Tensor reference to sync tensor
+  using SyncTensorRef =
+      typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+
+  /// Const tensor reference to source tensor
+  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
+
+  /// Array type used to output
+  using OutputAccessType = Array<typename OutputTileIterator::Element,
+                                 OutputTileIterator::kElementsPerAccess>;
+
+  /// Array type used by output functor
+  using AccumulatorAccessType =
+      Array<ElementAccumulator, OutputTileIterator::kElementsPerAccess>;
+
+  /// Number of warps
+  using WarpCount =
+      gemm::GemmShape<Shape::kM / WarpMmaOperator::Shape::kM,
+                      Shape::kN / WarpMmaOperator::Shape::kN, kPartitionsK>;
+
+ public:
+  static_assert(OutputTileIterator::kElementsPerAccess,
+                "This must not be zero.");
+
+  static_assert(!(OutputTileIterator::Fragment::kElements %
+                  OutputTileIterator::kElementsPerAccess),
+                "Divisibility");
+
+  /// Shared storage allocation needed by the epilogue
+  struct SharedStorage {};
+
+
+ public:
+  /// Constructor
+  CUTLASS_DEVICE
+  InterleavedEpilogue(
+      SharedStorage &shared_storage,  ///< Shared storage object
+      int thread_idx,                 ///< ID of a thread within the threadblock
+      int warp_idx,                   ///< ID of warp within threadblock
+      int lane_idx                    ///< Id of thread within warp
+    ) {}
+
+  /// Streams the result to global memory
+  CUTLASS_DEVICE
+  void operator()(
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    if (!output_op.is_source_needed()) {
+      compute_source_not_needed_(output_op, destination_iterator, accumulators);  
     }
-
-    if (k_partition != k_partition_count - 1) {
-      skip_elementwise_ = true;
+    else {
+      compute_source_needed_(output_op, destination_iterator, accumulators, source_iterator);
     }
   }
-
-  /// Applies the operation UnaryOp(BinaryOp(BinaryOp(ActivationOp(AB + bias), residual1), residual2))
-  CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &frag_Z, FragmentOutput &, FragmentAccumulator const &AB,
-                  FragmentC const &residual1, FragmentC const &residual2,
-                  FragmentCompute const &bias) const {
-    UnaryOp unary_op;
-    BinaryOp1 binary_op1;
-    BinaryOp2 binary_op2;
-    ActivationOp activation;
-
-    FragmentCompute tmp_Accum =
-        NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
-    FragmentCompute tmp_residual1 =
-        NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(residual1);
-    FragmentCompute tmp_residual2 =
-        NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(residual2);
-
-    FragmentCompute z =
-        binary_op2(binary_op1(activation(alpha_ * tmp_Accum + bias), beta_ * tmp_residual1), beta_ * tmp_residual2);
-    FragmentCompute result_Z = skip_elementwise_ ? z : unary_op(z);
-
-    NumericArrayConverter<ElementOutput, ElementCompute, kElementsPerAccess> convert_z;
-    frag_Z = convert_z(result_Z);
-  }
-
-  /// Should never be called
-  CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &, FragmentOutput &, FragmentAccumulator const &,
-                  FragmentCompute const &) const {}
-};
-
-/// Models a residual block of the form: UnaryOp(BinaryOp(ActivationOp(TensorOp(X) + bias), residual))
-template <typename ElementOutput_, typename ElementAccumulator_,
-          typename ElementCompute_, typename ElementC_, int ElementsPerAccess,
-          template <typename T> class ActivationOp_,
-          template <typename T> class BinaryOp1_,
-          template <typename T> class UnaryOp_,
-          bool StoreT_,
-          typename ElementVector_>
-class LinearCombinationResidualBlock<ElementOutput_, ElementAccumulator_,
-          ElementCompute_, ElementC_, ElementsPerAccess,
-          ActivationOp_, BinaryOp1_, UnaryOp_,
-          detail::NoOp, StoreT_, ElementVector_> {
-public:
-  static bool const kIsSingleSource = true;
-
-  using ElementOutput = ElementC_;
-  using ElementC = ElementC_;
-  using ElementAccumulator = ElementAccumulator_;
-  using ElementCompute = ElementCompute_;
-  using ElementVector = ElementVector_;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kCount = kElementsPerAccess;
-
-  using UnaryOp = UnaryOp_<Array<ElementCompute, kCount>>;
-  using BinaryOp = BinaryOp1_<Array<ElementCompute, kCount>>;
-  using ActivationOp = ActivationOp_<Array<ElementCompute, kCount>>;
-
-  using FragmentAccumulator = Array<ElementAccumulator, kElementsPerAccess>;
-  using FragmentCompute = Array<ElementCompute, kElementsPerAccess>;
-  using FragmentC = Array<ElementC, kElementsPerAccess>;
-  using FragmentOutput = Array<ElementOutput, kElementsPerAccess>;
-
-  using ElementZ = ElementOutput_;
-  using ElementT = ElementZ;
-  using FragmentZ = Array<ElementZ, kElementsPerAccess>;
-  using FragmentT = Array<ElementT, kElementsPerAccess>;
-
-  static bool const kIsHeavy = true;
-  static bool const kStoreZ = true;
-  static bool const kStoreT = StoreT_;
-
-  /// Host-constructable parameters structure
-  struct Params {
-
-    ElementCompute alpha;                  ///< scales accumulators
-    ElementCompute beta;                   ///< scales residual input
-    ElementCompute const *alpha_ptr{nullptr};       ///< pointer to accumulator scalar - if not null, loads it from memory
-    ElementCompute const *beta_ptr{nullptr};        ///< pointer to residual scalar - if not null, loads it from memory
-
-    CUTLASS_HOST_DEVICE
-    Params() : alpha(ElementCompute(1)), beta(ElementCompute(1)) {}
-
-    CUTLASS_HOST_DEVICE
-    Params(ElementCompute alpha, ElementCompute beta)
-        : alpha(alpha), beta(beta) {}
-
-    CUTLASS_HOST_DEVICE
-    Params(ElementCompute const *alpha_ptr, ElementCompute const *beta_ptr)
-        : alpha(0), beta(0), alpha_ptr(alpha_ptr), beta_ptr(beta_ptr) {}
-  };
-
-private:
-
-  ElementCompute alpha_;
-  ElementCompute beta_;
-  bool skip_elementwise_;
-
-public:
-
-  /// Constructor from Params
-  CUTLASS_HOST_DEVICE
-  LinearCombinationResidualBlock(Params const &params) {
-    alpha_ = (params.alpha_ptr ? *params.alpha_ptr : params.alpha);
-    beta_ = (params.beta_ptr ? *params.beta_ptr : params.beta);
-    skip_elementwise_ = false;
-  }
-
-  /// The "source" tensor corresponds to the residual input
-  CUTLASS_HOST_DEVICE
-  bool is_source_needed() const { return true; }
-
-  /// Functionally required for serial reduction in the epilogue
-  /// IMPORTANT: Split-k is supported only when ActivationOp is Identity.
-  CUTLASS_HOST_DEVICE
-  void set_k_partition(int k_partition, int k_partition_count) {
-    if (k_partition) {
-      beta_ = ElementCompute(1);
+   
+  /// Streams the result to global memory
+  CUTLASS_DEVICE
+  void compute_source_not_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators           ///< Complete warp-level accumulator tile
+    ) { 
+
+    //
+    // Iterator over warp-level accumulator fragment
+    //
+
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+
+    //
+    // Iterate over accumulator tile
+    //
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+
+      //
+      // Convert fragment
+      //
+
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+
+      accum_fragment_iterator.load(accum_fragment);
+      ++accum_fragment_iterator;
+
+      //
+      // Compute the output result
+      //
+
+      typename OutputTileIterator::Fragment output_fragment;
+      apply_output_operator_source_not_needed_(output_op, output_fragment, accum_fragment);
+
+      //
+      // Store the final result
+      //
+
+      destination_iterator.set_iteration_index(iter);
+      destination_iterator.store(output_fragment);
+      ++destination_iterator;
     }
+  } 
 
-    if (k_partition != k_partition_count - 1) {
-      skip_elementwise_ = true;
+  /// Streams the result to global memory
+  CUTLASS_DEVICE
+  void compute_source_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    ) { 
+ 
+    //
+    // Predicated tile iterators constructed from members
+    //
+
+    typename OutputTileIterator::Fragment source_fragment;
+
+    source_fragment.clear();
+
+    //
+    // Iterator over warp-level accumulator fragment
+    //
+
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+
+    //
+    // Iterate over accumulator tile
+    //
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+      //
+      // Load the source
+      //
+
+      source_iterator.set_iteration_index(iter);
+      source_iterator.load(source_fragment);
+      ++source_iterator;
+
+      //
+      // Convert fragment
+      //
+
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+
+      accum_fragment_iterator.load(accum_fragment);
+      ++accum_fragment_iterator;
+
+      //
+      // Compute the output result
+      //
+
+      typename OutputTileIterator::Fragment output_fragment;
+      apply_output_operator_source_needed_(output_op, output_fragment, accum_fragment, source_fragment);
+
+      //
+      // Store the final result
+      //
+
+      destination_iterator.set_iteration_index(iter);
+      destination_iterator.store(output_fragment);
+      ++destination_iterator;
     }
   }
 
-  /// Applies the operation UnaryOp(BinaryOp(ActivationOp(AB + bias), residual))
-  CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &frag_Z, FragmentOutput &, FragmentAccumulator const &AB,
-                  FragmentC const &residual,
-                  FragmentCompute const &bias) const {
-    UnaryOp unary_op;
-    BinaryOp binary_op;
-    ActivationOp activation;
-
-    FragmentCompute tmp_Accum =
-        NumericArrayConverter<ElementCompute, ElementAccumulator, kElementsPerAccess>()(AB);
-    FragmentCompute tmp_residual =
-        NumericArrayConverter<ElementCompute, ElementC, kElementsPerAccess>()(residual);
-
-    FragmentCompute z =
-        binary_op(activation(alpha_ * tmp_Accum + bias), beta_ * tmp_residual);
-    FragmentCompute result_Z = skip_elementwise_ ? z : unary_op(z);
-
-    NumericArrayConverter<ElementOutput, ElementCompute, kElementsPerAccess> convert_z;
-    frag_Z = convert_z(result_Z);
+ private:
+  /// Helper to invoke the output functor over each vector of output
+  CUTLASS_DEVICE
+  void apply_output_operator_source_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+      typename OutputTileIterator::Fragment &output_fragment,
+      typename AccumulatorFragmentIterator::Fragment const
+          &aligned_accum_fragment,
+      typename OutputTileIterator::Fragment const &source_fragment) {
+    OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+    AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(
+            &aligned_accum_fragment);
+
+    OutputAccessType const *source_frag_ptr =
+        reinterpret_cast<OutputAccessType const *>(&source_fragment);
+
+    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
+                                    OutputTileIterator::kElementsPerAccess;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kOutputOpIterations; ++i) {
+      // Call the output operator
+      output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+    }
   }
 
-  /// Should never be called
-  CUTLASS_HOST_DEVICE
-  void operator()(FragmentOutput &, FragmentOutput &, FragmentAccumulator const &,
-                  FragmentCompute const &) const {}
+  /// Helper to invoke the output functor over each vector of output
+  CUTLASS_DEVICE
+  void apply_output_operator_source_not_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+      typename OutputTileIterator::Fragment &output_fragment,
+      typename AccumulatorFragmentIterator::Fragment const
+          &aligned_accum_fragment) {
+    OutputAccessType *output_frag_ptr =
+        reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+    AccumulatorAccessType const *compute_frag_ptr =
+        reinterpret_cast<AccumulatorAccessType const *>(
+            &aligned_accum_fragment);
+
+    int const kOutputOpIterations = OutputTileIterator::Fragment::kElements /
+                                    OutputTileIterator::kElementsPerAccess;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kOutputOpIterations; ++i) {
+      // Call the output operator
+      output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
+    }
+  }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-} // namespace thread
+} // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_sigmoid.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_silu.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/linear_combination_with_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/reduction_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/thread/scale_type.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -73,15 +73,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Specialization and defines sensible defaults for epilogues for complex*complex case
 //  4 real-valued mma operations (Complex)
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
-  /// Epilogue Shape
+  /// Epilouge Shape
   typename Shape_,
   /// Warp-level mma operator
   typename WarpMmaTensorOp_,
   /// Number of k partitions
   int PartitionsK,
   /// Epilogue output operator
   typename OutputOp_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -74,15 +74,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Specialization and defines sensible defaults for epilogues for complex*complex case
 //  4 real-valued mma operations (Complex)
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
-  /// Epilogue Shape
+  /// Epilouge Shape
   typename Shape_,
   /// Warp-level mma operator
   typename WarpMmaTensorOp_,
   /// Number of k partitions
   int PartitionsK,
   /// Epilogue output operator
   typename OutputOp_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_direct_store.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_simt.h`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,18 +38,15 @@
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/array.h"
 
-#include "cutlass/arch/mma.h"
-
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/epilogue/thread/linear_combination.h"
 #include "cutlass/epilogue/thread/linear_combination_clamp.h"
 #include "cutlass/epilogue/thread/linear_combination_relu.h"
 #include "cutlass/epilogue/thread/linear_combination_gelu.h"
 #include "cutlass/epilogue/thread/linear_combination_sigmoid.h"
 #include "cutlass/epilogue/thread/linear_combination_planar_complex.h"
@@ -57,24 +54,20 @@
 #include "cutlass/epilogue/thread/reduction_op.h"
 
 #include "cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h"
 
 #include "cutlass/epilogue/warp/fragment_iterator_simt.h"
 #include "cutlass/epilogue/warp/tile_iterator_simt.h"
 #include "cutlass/epilogue/threadblock/default_thread_map_simt.h"
-#include "cutlass/transform/pitch_linear_thread_map.h"
 
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h" 
 #include "cutlass/epilogue/threadblock/shared_load_iterator.h"
-#include "cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h"
 #include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/threadblock/epilogue_depthwise.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
@@ -317,106 +310,12 @@
     SharedLoadIterator,
     OutputOp,
     Padding
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Defines sensible defaults for epilogues for SimtOps.
-template <typename Shape_,        // ThreadBlock Shape
-          typename WarpMmaSimt_,  // mma_depthwise_simt
-          typename OutputOp_,
-          int ElementsPerAccess_,
-          typename ThreadOutputShape_ = cutlass::conv::TensorNHWCShape<1, 1, 1, 1>,
-          typename ThreadBlockOutputShape_ = cutlass::conv::TensorNHWCShape<1, 1, 1, 1> >
-struct DefaultDirectConvEpilogueSimt {
-  using Shape = Shape_;
-  using WarpMmaSimt = WarpMmaSimt_;
-  using WarpShape = typename WarpMmaSimt::Shape;
-  using OutputOp = OutputOp_;
-  using ThreadOutputShape = ThreadOutputShape_;
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
-  static int const kElementsPerAccess = ElementsPerAccess_;
-
-
-  using ElementOutput = typename OutputOp::ElementOutput;
-  using LayoutC = typename WarpMmaSimt::LayoutC;
-  using ElementAccumulator = typename WarpMmaSimt::ElementC;
-
-  /// Number of threads total
-  using WarpCount = gemm::GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN
-  >;
-
-  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
-
-  static int const kThreads = WarpCount::kCount * kWarpSize;
-
-  //
-  // Thread map
-  //
-  
-  using OutputTileThreadMap = cutlass::transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<ThreadBlockOutputShape::kC, ThreadBlockOutputShape::kNHW>,
-    kThreads,
-    kElementsPerAccess
-  >;
-
-
-  using OutputTileIterator = cutlass::epilogue::threadblock::PredicatedTileIteratorDirectConv<
-    OutputTileThreadMap,
-    ElementOutput,
-    ThreadOutputShape,
-    ThreadBlockOutputShape 
-  >;
-
-  using AccumulatorFragmentIterator = cutlass::epilogue::warp::FragmentIteratorSimt<
-    typename WarpMmaSimt::Shape,
-    typename WarpMmaSimt::ThreadMma,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
-  >;
-  
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorSimtDirect2dConv<
-    typename WarpMmaSimt::Shape,
-    ThreadOutputShape,
-    ThreadBlockOutputShape,
-    typename WarpMmaSimt::ThreadMma,
-    ElementAccumulator,
-    layout::RowMajor,
-    typename WarpMmaSimt::Policy
-  >;
-
-  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorPitchLiner<
-    OutputTileThreadMap,
-    ElementAccumulator
-  >;
-
-  /// Hard-coded padding elements added 
-  using Padding = typename WarpTileIterator::Padding;
-  //
-  // Define the epilogue
-  //
-  using Epilogue = cutlass::epilogue::threadblock::EpilogueDepthwise<
-    Shape,
-    ThreadOutputShape,
-    ThreadBlockOutputShape,
-    WarpMmaSimt,
-    OutputTileIterator,
-    AccumulatorFragmentIterator,
-    WarpTileIterator,
-    SharedLoadIterator,
-    OutputOp,
-    Padding
-  >;
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -132,23 +132,22 @@
     ThreadMap,
     float
   >;
 
   static int const kFragmentsPerIteration = 2;
 };
 
-/// Partial specialization for int32_t <= int32_t
+/// Partial specialization for int32_t <= int32_t x 4
 template <
-  int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename ThreadMap
 >
-struct DefaultIteratorsTensorOp<int32_t, int32_t, ElementsPerAccess, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
+struct DefaultIteratorsTensorOp<int32_t, int32_t, 4, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
   
   using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOp<
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
@@ -157,23 +156,22 @@
     ThreadMap,
     int32_t
   >;
 
   static int const kFragmentsPerIteration = 1;
 };
 
-/// Partial specialization for float <= int32_t
+/// Partial specialization for float <= int32_t x 4
 template <
-  int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
   typename InstructionShape,
   typename ThreadMap
 >
-struct DefaultIteratorsTensorOp<float, int32_t, ElementsPerAccess, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
+struct DefaultIteratorsTensorOp<float, int32_t, 4, ThreadblockShape, WarpShape, InstructionShape, ThreadMap> {
 
   using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOp<
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
@@ -220,52 +218,14 @@
     8,
     8
   >;
 
   static int const kFragmentsPerIteration = 2;
 };
 
-/// Partial specialization for half <= int32_t x 8 epilogues avoids shared memory bank conflicts.
-template <
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename ThreadMap
->
-struct DefaultIteratorsTensorOp<
-  half_t, 
-  int32_t, 
-  8, 
-  ThreadblockShape, 
-  WarpShape, 
-  InstructionShape, 
-  ThreadMap> {
-  
-  using WarpTileIterator = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
-    WarpShape,
-    InstructionShape,
-    int32_t,
-    32,
-    16,
-    8,
-    8
-  >;
-
-  using SharedLoadIterator = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
-    ThreadMap,
-    int32_t,
-    32,
-    16,
-    8,
-    8
-  >;
-
-  static int const kFragmentsPerIteration = 2;
-};
-
 /// Partial specialization for int8/int4b_t <= int32 x 16/8 epilogues avoids shared memory bank conflicts.
 /// Threadblock::kN = 256 still has bank conflicts.
 template <
   typename ElementOutput,
   int ElementsPerAccess,
   typename ThreadblockShape,
   typename WarpShape,
@@ -304,15 +264,15 @@
     WarpShape,
     InstructionShape,
     int32_t,
     layout::RowMajor
   >;
 
   using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256),
                              WarpTileIteratorNotMixed,
                              WarpTileIteratorMixed>::type;
 
   using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
     ThreadMap,
     int32_t,
     32,
@@ -323,155 +283,20 @@
 
   using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
     ThreadMap,
     int32_t
   >;
 
   using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
-                             SharedLoadIteratorNotMixed,
-                             SharedLoadIteratorMixed>::type;
-
-  static int const kFragmentsPerIteration = 1;
-};
-
-/// Partial specialization for float_e4m3_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
-/// Threadblock::kN = 256 still has bank conflicts.
-template <
-  int ElementsPerAccess,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename ThreadMap
->
-struct DefaultIteratorsTensorOp<
-  cutlass::float_e4m3_t,
-  float, 
-  ElementsPerAccess,
-  ThreadblockShape, 
-  WarpShape, 
-  InstructionShape, 
-  ThreadMap> {
-
-  using ElementOutput = cutlass::float_e4m3_t;
-
-  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
-              "ElementsPerAccess needs to be 16 or 8.");
-  
-  using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
-    WarpShape,
-    InstructionShape,
-    float,
-    32,
-    cutlass::sizeof_bits<ElementOutput>::value,
-    ElementsPerAccess,
-    8
-  >;
-
-  using WarpTileIteratorNotMixed =  cutlass::epilogue::warp::TileIteratorTensorOp<
-    WarpShape,
-    InstructionShape,
-    float,
-    layout::RowMajor
-  >;
-
-  using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
-                             WarpTileIteratorNotMixed,
-                             WarpTileIteratorMixed>::type;
-
-  using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
-    ThreadMap,
-    float,
-    32,
-    cutlass::sizeof_bits<ElementOutput>::value,
-    ElementsPerAccess,
-    8
-  >;
-
-  using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
-    ThreadMap,
-    float
-  >;
-
-  using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
-                             SharedLoadIteratorNotMixed,
-                             SharedLoadIteratorMixed>::type;
-
-  static int const kFragmentsPerIteration = 1;
-};
-
-/// Partial specialization for float_e5m2_t <= float x 16/8 epilogues avoids shared memory bank conflicts.
-/// Threadblock::kN = 256 still has bank conflicts.
-template <
-  int ElementsPerAccess,
-  typename ThreadblockShape,
-  typename WarpShape,
-  typename InstructionShape,
-  typename ThreadMap
->
-struct DefaultIteratorsTensorOp<
-  cutlass::float_e5m2_t,
-  float, 
-  ElementsPerAccess,
-  ThreadblockShape, 
-  WarpShape, 
-  InstructionShape, 
-  ThreadMap> {
-
-  using ElementOutput = cutlass::float_e5m2_t;
-
-  static_assert((ElementsPerAccess == 16 || ElementsPerAccess == 8),
-              "ElementsPerAccess needs to be 16 or 8.");
-  
-  using WarpTileIteratorMixed = cutlass::epilogue::warp::TileIteratorTensorOpMixed<
-    WarpShape,
-    InstructionShape,
-    float,
-    32,
-    cutlass::sizeof_bits<ElementOutput>::value,
-    ElementsPerAccess,
-    8
-  >;
-
-  using WarpTileIteratorNotMixed =  cutlass::epilogue::warp::TileIteratorTensorOp<
-    WarpShape,
-    InstructionShape,
-    float,
-    layout::RowMajor
-  >;
-
-  using WarpTileIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
-                             WarpTileIteratorNotMixed,
-                             WarpTileIteratorMixed>::type;
-
-  using SharedLoadIteratorMixed = cutlass::epilogue::threadblock::SharedLoadIteratorMixed<
-    ThreadMap,
-    float,
-    32,
-    cutlass::sizeof_bits<ElementOutput>::value,
-    ElementsPerAccess,
-    8
-  >;
-
-  using SharedLoadIteratorNotMixed = cutlass::epilogue::threadblock::SharedLoadIterator<
-    ThreadMap,
-    float
-  >;
-
-  using SharedLoadIterator = typename platform::conditional<
-                             (ThreadblockShape::kN == 256) || (ThreadblockShape::kN == 128 && ElementsPerAccess == 8),
+                             (ThreadblockShape::kN == 256),
                              SharedLoadIteratorNotMixed,
                              SharedLoadIteratorMixed>::type;
 
   static int const kFragmentsPerIteration = 1;
 };
-
 } // namespace detail
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Defines sensible defaults for epilogues for TensorOps.
 template <
   typename Shape_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_simt.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_volta_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/default_thread_map_wmma_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/direct_store_epilogue_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,15 +30,14 @@
  **************************************************************************************************/
 /*! \file
   \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
 
   The epilogue rearranges the result of a matrix product through shared memory to match canonical
   tensor layouts in global memory. Epilogues support conversion and reduction operations.
 
-  The shared memory resource is time-sliced across warps.
 */
 
 #pragma once
 
 #if defined(__CUDACC_RTC__)
 #include <cuda/std/cassert>
 #else
@@ -56,24 +55,23 @@
 
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/transform/pitch_linear_thread_map.h"
 #include "cutlass/transform/threadblock/regular_tile_iterator.h"
 
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
-#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/numeric_types.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
-
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Epilogue operator
 template <
   typename Shape_,                          ///< Shape of threadblock tile (concept: GemmShape)
   typename WarpMmaOperator_,                ///< Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
   int PartitionsK,                          ///< Number of partitions of the K dimension
@@ -83,73 +81,53 @@
   typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
   typename OutputOp_,                       ///< Output operator
   typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
   int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
   int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
     (!IsEpilogueFunctorHeavy<OutputOp_>::value)
 >
-class Epilogue :
+class Epilogue : 
   public EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
+    Shape_, 
+    typename WarpMmaOperator_::Shape, 
+    PartitionsK, 
+    AccumulatorFragmentIterator_, 
+    WarpTileIterator_, 
     Padding_,
-    FragmentsPerPartition>,
-  public EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>
-{
+    FragmentsPerPartition> {
 
 public:
 
   using Base = EpilogueBase<
-    Shape_,
-    typename WarpMmaOperator_::Shape,
-    PartitionsK,
-    AccumulatorFragmentIterator_,
-    WarpTileIterator_,
+    Shape_, 
+    typename WarpMmaOperator_::Shape, 
+    PartitionsK, 
+    AccumulatorFragmentIterator_, 
+    WarpTileIterator_, 
     Padding_,
     FragmentsPerPartition>;
 
-  using BaseStreamK = EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>;
-
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
   using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
   using Padding = Padding_;
+
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
-  /// Number of warps per block
-  using WarpCount = typename Base::WarpCount;
-
-  /// Number of threads per block
-  static int const kBlockThreads = 32 * WarpCount::kCount;
-
-  /// Per-thread accumulator tile type
+  /// The complete warp-level accumulator tile
   using AccumulatorTile = typename Base::AccumulatorTile;
 
-  /// Numerical accumulation element type
-  using ElementAccumulator = typename WarpMmaOperator::ElementC;
-
-  /// Fragment type used by the accumulator tile's fragment iterator
-  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
+  /// Accumulator element
+  using ElementAccumulator = typename WarpTileIterator::Element;
 
   /// Output element
   using ElementOutput = typename OutputTileIterator::Element;
 
   /// Output access size
   static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
@@ -158,286 +136,84 @@
 
   /// Tensor reference to sync tensor
   using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
 
   /// Const tensor reference to source tensor
   using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
 
-  /// Vector type used by the global output iterator
+  /// Array type used to output
   using OutputAccessType = Array<
     typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
 
-  /// Vector type used by the shared output iterator
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
+  /// Array type used by output functor
+  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
+  
+  /// Number of warps
+  using WarpCount = typename Base::WarpCount;
 
   static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
-
   static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
 
-
 public:
 
   static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
     "Mismatch between shared load iterator and output tile iterator.");
 
   static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
 
   static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
     "Divisibility");
 
-  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
-
-
-public:
-
-  /// Aspect for when epilogue source is not needed
-  struct SourceAspectNotNeeded
-  {
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNotNeeded()
-    {}
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
-
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
-      }
-    }
-  };
-
-
-  /// Aspect for when epilogue source is needed
-  struct SourceAspectNeeded
-  {
-    OutputTileIterator source_iterator;
-
-    typename OutputTileIterator::Fragment source_fragment;
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    static void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
-      typename OutputTileIterator::Fragment const &source_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
-
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
-
-      OutputAccessType const *source_frag_ptr =
-        reinterpret_cast<OutputAccessType const *>(&source_fragment);
-
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
-      }
-    }
-
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNeeded(OutputTileIterator source_iterator) :
-      source_iterator(source_iterator)
-    {
-      source_fragment.clear();
-    }
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename SharedLoadIterator::Fragment const &aligned_accum_fragment)
-    {
-      // Load addend source fragment from global memory
-      source_iterator.load(source_fragment);
-      ++source_iterator;
-
-      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
-    }
-  };
-
-
 private:
 
   /// Loads fragment from shared memory aligned with output tensor
   SharedLoadIterator shared_load_iterator_;
 
-  /// Thread index in the threadblock
-  int thread_idx;
-
-  /// Warp index in the threadblock
-  int warp_idx;
-
 public:
 
   /// Constructor
   CUTLASS_DEVICE
   Epilogue(
-      typename Base::SharedStorage &shared_storage,   ///< Shared storage object
-      int thread_idx,                                 ///< ID of a thread within the threadblock
-      int warp_idx,                                   ///< ID of warp within threadblock
-      int lane_idx)                                   ///< Id of thread within warp
-  :
-      Base(shared_storage, thread_idx, warp_idx, lane_idx),
-      BaseStreamK(thread_idx),
-      shared_load_iterator_(shared_storage.reference(), thread_idx),
-      thread_idx(thread_idx),
-      warp_idx(warp_idx)
-  {}
-
-
-  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
-  /// performing epilogue computations, writing to output
-  CUTLASS_DEVICE
-  void reduce(
-      int peer_idx_begin,
-      int peer_idx_end,
-      int reduce_fragment_idx,
-      void *element_workspace,
-      OutputOp const &output_op,                      ///< Output operator
-      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    typename Base::SharedStorage &shared_storage,    ///< Shared storage object    
+    int thread_idx,                   ///< ID of a thread within the threadblock
+    int warp_idx,                     ///< ID of warp within threadblock
+    int lane_idx                     ///< Id of thread within warp
+  ):
+    Base(shared_storage, thread_idx, warp_idx, lane_idx),
+    shared_load_iterator_(shared_storage.reference(), thread_idx) 
   {
-    // Redcuce peer accumulator fragments into one fragment
-    AccumulatorFragment accum_fragment;
-    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
-
-    // Store fragment to shared memory
-    this->warp_tile_iterator_.store(accum_fragment);
-
-    __syncthreads();
-
-    // Initialize/load source-fragment data
-    typename OutputTileIterator::Fragment source_fragment;
-    source_fragment.clear();
-
-    if (output_op.is_source_needed())
-    {
-      source_iterator += reduce_fragment_idx;
-      source_iterator.load(source_fragment);
-    }
-
-    // Load fragment from shared memory
-    typename SharedLoadIterator::Fragment aligned_accum_fragment;
-    shared_load_iterator_.load(aligned_accum_fragment);
-
-    // Add fragments shared by other k partitions
-    if (kPartitionsK > 1)
-    {
-      plus <typename SharedLoadIterator::Fragment> add_fragments;
-
-      CUTLASS_PRAGMA_UNROLL
-      for ( int i = 1; i < kPartitionsK; ++i) {
-        typename SharedLoadIterator::Fragment aligned_addend_fragment;
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        shared_load_iterator_.load(aligned_addend_fragment);
-        aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_addend_fragment);
-      }
-    }
-
-    // Compute the output result
-    typename OutputTileIterator::Fragment output_fragment;
-
-    // Apply the output operator
-    SourceAspectNeeded::apply_output_operator(
-        output_fragment,
-        output_op,
-        aligned_accum_fragment,
-        source_fragment);
-
-    // Store the final result
-    destination_iterator += reduce_fragment_idx;
-    destination_iterator.store(output_fragment);
+    
   }
 
-
-  /// Perform the epilogue computations and stream the result to global memory.
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
-  {
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
-  }
-
-
-  /// Perform the epilogue computations and stream the result to global memory.  Implements
-  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
+  /// Streams the result to global memory
   CUTLASS_DEVICE
   void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (output_op.is_source_needed())
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator) {         ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    
+    if (!output_op.is_source_needed()) {
+      compute_source_not_needed_(output_op, destination_iterator, accumulators);  
     }
-    else
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
+    else {
+      compute_source_needed_(output_op, destination_iterator, accumulators, source_iterator);
     }
   }
 
+private:
 
-  /// Perform the epilogue computations and stream the result to global memory.  Implements a
-  /// single codepath, regardless of whether the output op requires addend data to be loaded
-  CUTLASS_DEVICE
-  void unified(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (!output_op.is_source_needed())
-    {
-      source_iterator.clear_mask();
-      __syncthreads();  // Dummy (CUDA 11.0)
-    }
-
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
-  }
-
-  template<class Seq>
-  struct acc2smem;
+  template <class Seq>
+  struct acc2smem_source_not_needed;
 
   template <size_t... Seq>
-  struct acc2smem<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                      WarpTileIterator &warp_tile_iterator) {
+  struct acc2smem_source_not_needed<cutlass::index_sequence<Seq...>> {
+    template <int Advance>
+    CUTLASS_DEVICE static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
+                                      WarpTileIterator &warp_tile_iterator) {
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < Advance; i++) {
         ++accum_fragment_iterator;
       }
 
       CUTLASS_PRAGMA_UNROLL
       for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
@@ -456,87 +232,99 @@
         warp_tile_iterator.add_pointer_offset(kSmemPointerOffset *
                                               (1 - Base::kFragmentsPerIteration));
       }
     }
 
     CUTLASS_DEVICE
     static void push(size_t pos,
-                    AccumulatorFragmentIterator const &iterator_begin,
-                    WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+                     AccumulatorFragmentIterator const &iterator_begin,
+                     WarpTileIterator &warp_tile_iterator) {
+      int dummy[] = {
+          (pos == (Seq * Base::kFragmentsPerIteration)) &&
+          (helper<Seq * Base::kFragmentsPerIteration>(iterator_begin, warp_tile_iterator), 0)...};
+
+      CUTLASS_UNUSED(dummy[0]);
     }
   };
 
+  static_assert(kPartitionsK == 1 || Base::kFragmentsPerIteration == 1, "One of these must be exactly 1.");
 
   /// Streams the result to global memory
-  template <typename SourceAspect>
   CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    SourceAspect source)
-  {
+  void compute_source_not_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators          ///< Complete warp-level accumulator tile 
+    ) { 
+
+    //
     // Iterator over warp-level accumulator fragment
+    //
+
     AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
 
     //
     // Iterate over accumulator tile
-    //
+    // 
 
     #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration)
-    {
+    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration) {
 
       //
       // Convert and store fragment
       //
-
+      
       __syncthreads();
 
-      acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
-        iter, accum_fragment_iterator, this->warp_tile_iterator_);
+
+      acc2smem_source_not_needed<
+          cutlass::make_index_sequence<OutputTileIterator::kIterations /
+                                   Base::kFragmentsPerIteration>>::push(iter,
+                                                                        accum_fragment_iterator,
+                                                                        this->warp_tile_iterator_);
+
+      __syncthreads();
 
       //
       // Load fragments from shared memory
       //
 
-      __syncthreads();
-
       CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p)
-      {
-        typename SharedLoadIterator::Fragment aligned_accum_fragment;
-        shared_load_iterator_.load(aligned_accum_fragment);
+      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
+
+
+        typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
 
-        if (p < Base::kFragmentsPerIteration - 1)
-        {
+        shared_load_iterator_.load(aligned_accum_fragment[0]);
+
+        if (p < Base::kFragmentsPerIteration - 1) {
           shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
         }
-        else if (kPartitionsK > 1)
-        {
+        else if (kPartitionsK > 1) {
+
           plus <typename SharedLoadIterator::Fragment> add_fragments;
 
           CUTLASS_PRAGMA_UNROLL
           for ( int i = 1; i < kPartitionsK; ++i) {
-            typename SharedLoadIterator::Fragment aligned_accum_fragment_addend;
             shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment_addend);
-            aligned_accum_fragment = add_fragments(aligned_accum_fragment, aligned_accum_fragment_addend);
+            shared_load_iterator_.load(aligned_accum_fragment[i]);
+            aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
           }
 
           shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
         }
 
         //
         // Compute the output result
         //
 
         typename OutputTileIterator::Fragment output_fragment;
-        source.apply_output_operator(output_fragment, output_op, aligned_accum_fragment);
+
+        apply_output_operator_source_not_needed_(output_fragment, output_op, aligned_accum_fragment[0]);
+
 
         //
         // Store the final result
         //
 
         destination_iterator.store(output_fragment);
         ++destination_iterator;
@@ -544,14 +332,178 @@
 
       if (Base::kFragmentsPerIteration > 1) {
         shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
       }
     }
   }
 
+  template<class Seq>
+  struct acc2smem_source_needed;
+
+  template <size_t... Seq>
+  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
+    template<int Advance>
+    CUTLASS_DEVICE
+    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
+                       WarpTileIterator &warp_tile_iterator) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int i = 0; i < Advance; i++) {
+        ++accum_fragment_iterator;
+      }
+
+      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+      accum_fragment_iterator.load(accum_fragment);
+      warp_tile_iterator.store(accum_fragment);
+    }
+
+    CUTLASS_DEVICE
+    static void push(size_t pos,
+                     AccumulatorFragmentIterator const &iterator_begin,
+                     WarpTileIterator &warp_tile_iterator) {
+      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
+    }
+  };
+
+  /// Streams the result to global memory
+  CUTLASS_DEVICE
+  void compute_source_needed_(
+    OutputOp const &output_op,                    ///< Output operator
+    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
+    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
+    OutputTileIterator source_iterator           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
+    ) { 
+    
+    typename OutputTileIterator::Fragment source_fragment;
+
+    source_fragment.clear();
+
+    //
+    // Iterator over warp-level accumulator fragment
+    //
+
+    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
+
+    //
+    // Iterate over accumulator tile
+    // 
+
+    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
+    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
+
+      //
+      // Load the source
+      //
+
+      source_iterator.load(source_fragment);
+      ++source_iterator;
+
+      //
+      // Convert and store fragment
+      //
+      
+      __syncthreads();
+
+      acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
+          iter, accum_fragment_iterator, this->warp_tile_iterator_);
+
+      __syncthreads();
+
+      //
+      // Load fragments from shared memory
+      //
+
+      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
+
+      shared_load_iterator_.load(aligned_accum_fragment[0]);
+
+      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
+      if (kPartitionsK > 1) {
+
+        plus <typename SharedLoadIterator::Fragment> add_fragments;
+
+        CUTLASS_PRAGMA_UNROLL
+        for ( int i = 1; i < kPartitionsK; ++i) {
+          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
+          shared_load_iterator_.load(aligned_accum_fragment[i]);
+          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
+        }
+
+        shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
+      }
+
+      //
+      // Compute the output result
+      //
+     
+      typename OutputTileIterator::Fragment output_fragment;
+
+      apply_output_operator_(output_fragment, output_op, aligned_accum_fragment[0], source_fragment);
+
+
+      //
+      // Store the final result
+      //
+
+      destination_iterator.store(output_fragment);      
+      ++destination_iterator;
+
+    }
+  }
+
+  /// Helper to invoke the output functor over each vector of output
+  CUTLASS_DEVICE
+  void apply_output_operator_(
+    typename OutputTileIterator::Fragment &output_fragment,
+    OutputOp const &output_op,                    ///< Output operator
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment,
+    typename OutputTileIterator::Fragment const &source_fragment) {
+      
+    OutputAccessType *output_frag_ptr = 
+      reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+    AccumulatorAccessType const *compute_frag_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+
+    OutputAccessType const *source_frag_ptr = 
+      reinterpret_cast<OutputAccessType const *>(&source_fragment);
+
+    int const kOutputOpIterations = 
+      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kOutputOpIterations; ++i) {
+
+      // Call the output operator
+      output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
+    }
+  }
+
+  /// Helper to invoke the output functor over each vector of output
+  CUTLASS_DEVICE
+  void apply_output_operator_source_not_needed_(
+    typename OutputTileIterator::Fragment &output_fragment,
+    OutputOp const &output_op,                    ///< Output operator
+    typename SharedLoadIterator::Fragment const &aligned_accum_fragment) {
+    
+    OutputAccessType *output_frag_ptr = 
+      reinterpret_cast<OutputAccessType *>(&output_fragment);
+
+    AccumulatorAccessType const *compute_frag_ptr = 
+      reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+
+    int const kOutputOpIterations = 
+      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int i = 0; i < kOutputOpIterations; ++i) {
+
+      // Call the output operator
+      output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
+    }
+  }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_base_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_depthwise.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_direct_store.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -73,14 +73,15 @@
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   using WarpShape = typename WarpMmaOperator_::Shape;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
   using WarpTileIterator = WarpTileIterator_;
+  using SharedLoadIterator = SharedLoadIterator_;
   using OutputOp = OutputOp_;
   using Padding = MatrixShape<0, 0>;
 
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
   /// The complete warp-level accumulator tile
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_gemm_k_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -145,20 +145,21 @@
       }
 
       Array<ElementOutput, kIterations / 4> source;
       source.clear();
 
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < kIterations / 4; ++i) {
-        ElementOutput *source_ptr = reinterpret_cast<ElementOutput *>(&source);
+        ElementOutput tmp;
         cutlass::arch::global_load<ElementOutput, sizeof(ElementOutput)>(
-                                                  source_ptr[i],
+                                                  tmp,
                                                   (void *)(pointer_ + i * 32),
                                                   guard[i] && LoadForSerialSplitK);
 
+        source[i] = tmp;
       }
 
       FragmentAccumulator sum = gemm_k_with_reduction_accumulation;
 
       CUTLASS_PRAGMA_UNROLL
       for (int i = 0; i < kIterations; ++i) {
         sum[i] += __shfl_xor_sync(0xffffffff, sum[i], 1);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_smem_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_visitor_with_softmax.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -129,16 +129,15 @@
 
   /// Applies the operation when is_source_needed() is true
   CUTLASS_HOST_DEVICE
   void operator()(
     FragmentZ &frag_Z, 
     FragmentT &frag_T, 
     FragmentAccumulator const &AB,
-    FragmentC const &frag_C1,
-    FragmentC const &frag_C2,
+    FragmentC const &frag_C,
     FragmentCompute const &V) const {
 
   }
 
   /// Applies the operation when is_source_needed() is false
   CUTLASS_HOST_DEVICE
   void operator()(
@@ -177,50 +176,17 @@
   typename AccumulatorFragmentIterator_,    ///< Fragment iterator selecting accumulators
   typename WarpTileIterator_,               ///< Warp-scoped tile iterator writing accumulators to SMEM
   typename SharedLoadIterator_,             ///< Threadblock-scoped tile iterator loading from SMEM
   typename OutputOp_,                       ///< Output operator - concept is EpilogueWithBroadcastOp
   typename Padding_,                        ///< Padding added to SMEM allocation to avoid bank conflicts (concept: MatrixShape)
   int FragmentsPerPartition = 1,            ///< Used to coarsten the epilogue granularity
   int IterationsUnroll =                    ///< Used to reduce binary size when epilogue op is large
-    (!IsEpilogueFunctorHeavy<OutputOp_>::value),
-  bool IsSingleSource = OutputOp_::kIsSingleSource
+    (!IsEpilogueFunctorHeavy<OutputOp_>::value)
 >
-class EpilogueWithBroadcast;
-
-template <
-  typename Shape_,
-  typename WarpMmaOperator_,
-  int PartitionsK,
-  typename OutputTileIterator_,
-  typename TensorTileIterator_,
-  typename ElementVector_,
-  typename AccumulatorFragmentIterator_,
-  typename WarpTileIterator_,
-  typename SharedLoadIterator_,
-  typename OutputOp_,
-  typename Padding_,
-  int FragmentsPerPartition,
-  int IterationsUnroll
->
-class EpilogueWithBroadcast<
-  Shape_,
-  WarpMmaOperator_,
-  PartitionsK,
-  OutputTileIterator_,
-  TensorTileIterator_,
-  ElementVector_,
-  AccumulatorFragmentIterator_,
-  WarpTileIterator_,
-  SharedLoadIterator_,
-  OutputOp_,
-  Padding_,
-  FragmentsPerPartition,
-  IterationsUnroll,
-  false
-> : 
+class EpilogueWithBroadcast : 
   public EpilogueBase<
     Shape_, 
     typename WarpMmaOperator_::Shape, 
     PartitionsK, 
     AccumulatorFragmentIterator_, 
     WarpTileIterator_, 
     Padding_,
@@ -233,15 +199,14 @@
     typename WarpMmaOperator_::Shape, 
     PartitionsK, 
     AccumulatorFragmentIterator_, 
     WarpTileIterator_, 
     Padding_,
     FragmentsPerPartition>;
 
-  static bool const kIsSingleSource = false;
   using Shape = Shape_;
   using WarpMmaOperator = WarpMmaOperator_;
   static int const kPartitionsK = PartitionsK;
   using OutputTileIterator = OutputTileIterator_;
   using TensorTileIterator = TensorTileIterator_;
   using ElementVector = ElementVector_;
   using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
@@ -414,695 +379,15 @@
 
   }
 
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void operator()(
     OutputOp const &output_op,                        ///< Output operator
-    ElementVector const * broadcast_ptr,              ///< Broadcast vector
-    OutputTileIterator destination_iterator,          ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator1,              ///< Tile iterator for first source accumulator matrix
-    OutputTileIterator source_iterator2,              ///< Tile iterator for second source accumulator matrix
-    TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additional tensor operand
-    MatrixCoord const &problem_size =                 ///< Problem size needed to guard against out-of-bounds accesses
-        MatrixCoord(Shape::kM, Shape::kN),
-    MatrixCoord const &threadblock_offset =           ///< Threadblock's initial offset within the problem size space
-        MatrixCoord()) {
-    
-    BroadcastFragment broadcast_fragment;
-
-    load_broadcast_fragment_(broadcast_fragment, broadcast_ptr, problem_size, threadblock_offset);
-
-    if (!output_op.is_source_needed()) {
-      compute_source_not_needed_(
-        output_op, 
-        broadcast_fragment, 
-        destination_iterator, 
-        accumulators,
-        tensor_iterator);
-    }
-    else {
-      compute_source_needed_(
-        output_op, 
-        broadcast_fragment, 
-        destination_iterator, 
-        accumulators, 
-        source_iterator1,
-        source_iterator2,
-        tensor_iterator);
-    }
-  }
-
-private:
-
-  CUTLASS_DEVICE
-  void load_broadcast_fragment_(
-    BroadcastFragment & broadcast_fragment,      ///< Fragment containing the accumulated partial reduction over columns
-    ElementVector const * broadcast_ptr,         ///< Broadcast vector
-    MatrixCoord const &problem_size,             ///< Problem size needed to guard against out-of-bounds accesses
-    MatrixCoord const &threadblock_offset        ///< Threadblock's initial offset within the problem size space
-    ) {
-
-    broadcast_fragment.clear();
-    
-    // If no pointer is supplied, set with all zeros and avoid memory accesses
-    if (!broadcast_ptr) {
-      return;
-    }
-
-    int thread_initial_column = ThreadMap::initial_offset(thread_idx_).column();
-
-    int thread_column_idx = threadblock_offset.column() + thread_initial_column;
-    broadcast_ptr += thread_initial_column;
-
-    NumericArrayConverter<ElementCompute, ElementVector, BroadcastDetail::kElementsPerAccess> converter;
-    using AccessType = AlignedArray<ElementVector, BroadcastDetail::kElementsPerAccess>;
-    using ComputeFragmentType = Array<ElementCompute, BroadcastDetail::kElementsPerAccess>;
-
-    ComputeFragmentType *frag_ptr = reinterpret_cast<ComputeFragmentType *>(&broadcast_fragment);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < ThreadMap::Iterations::kColumn; ++j) {
-
-      AccessType loaded;
-
-      loaded.clear();
-
-      if (thread_column_idx < problem_size.column()) {
-        loaded = *reinterpret_cast<AccessType const *>(broadcast_ptr);
-      }
-
-      ComputeFragmentType cvt = converter(loaded);
-      frag_ptr[j] = cvt;
-
-      thread_column_idx += ThreadMap::Delta::kColumn;
-      broadcast_ptr += ThreadMap::Delta::kColumn;
-    }
-  }
-
-  template <class Seq>
-  struct acc2smem_source_not_needed;
-
-  template <size_t... Seq>
-  struct acc2smem_source_not_needed<cutlass::index_sequence<Seq...>> {
-    template <int Advance>
-    CUTLASS_DEVICE static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                                      WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-        typename AccumulatorFragmentIterator::Fragment accum_fragment;
-
-        accum_fragment_iterator.load(accum_fragment);
-        ++accum_fragment_iterator;
-
-        warp_tile_iterator.store(accum_fragment);
-        if (p < Base::kFragmentsPerIteration - 1) {
-          warp_tile_iterator.add_pointer_offset(kSmemPointerOffset);
-        }
-      }
-
-      if (Base::kFragmentsPerIteration > 1) {
-        warp_tile_iterator.add_pointer_offset(kSmemPointerOffset *
-                                              (1 - Base::kFragmentsPerIteration));
-      }
-    }
-
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {
-          (pos == (Seq * Base::kFragmentsPerIteration)) &&
-          (helper<Seq * Base::kFragmentsPerIteration>(iterator_begin, warp_tile_iterator), 0)...};
-
-      CUTLASS_UNUSED(dummy[0]);
-    }
-  };
-
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void compute_source_not_needed_(
-    OutputOp const &output_op,                        ///< Output operator
-    BroadcastFragment const &broadcast_fragment,      ///< Fragment containing the accumulated partial reduction over columns
-    OutputTileIterator destination_iterator,          ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile 
-    TensorTileIterator tensor_iterator                ///< Threadblock tile iterator for additioanl tensor operand
-    ) { 
-
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    // 
-
-    // CUTLASS_PRAGMA_UNROLL
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations / Base::kFragmentsPerIteration : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; iter += Base::kFragmentsPerIteration) {
-
-      //
-      // Convert and store fragment
-      //
-      
-
-      __syncthreads();
-
-      acc2smem_source_not_needed<
-          cutlass::make_index_sequence<OutputTileIterator::kIterations /
-                                   Base::kFragmentsPerIteration>>::push(iter,
-                                                                        accum_fragment_iterator,
-                                                                        this->warp_tile_iterator_);
-
-      __syncthreads();
-
-      //
-      // Load fragments from shared memory
-      //
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int p = 0; p < Base::kFragmentsPerIteration; ++p) {
-
-
-        typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
-
-        shared_load_iterator_.load(aligned_accum_fragment[0]);
-
-        if (p < Base::kFragmentsPerIteration - 1) {
-          shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-        }
-        else if (kPartitionsK > 1) {
-
-          plus <typename SharedLoadIterator::Fragment> add_fragments;
-
-          CUTLASS_PRAGMA_UNROLL
-          for ( int i = 1; i < kPartitionsK; ++i) {
-            shared_load_iterator_.add_pointer_offset(kSmemPointerOffset);
-            shared_load_iterator_.load(aligned_accum_fragment[i]);
-            aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-          }
-
-          shared_load_iterator_.add_pointer_offset((1 - kPartitionsK) * kSmemPointerOffset);
-        }
-
-        //
-        // Apply output operation
-        //
-
-        typename OutputTileIterator::Fragment frag_Z;
-        typename TensorTileIterator::Fragment frag_T;
-
-        apply_output_operator_source_not_needed_(
-          frag_Z,
-          frag_T,
-          output_op,
-          aligned_accum_fragment[0],
-          broadcast_fragment);
-
-        //
-        // Conditionally store fragments
-        //
-
-        if (OutputOp::kStoreZ) {
-          destination_iterator.store(frag_Z);
-          ++destination_iterator;
-        }
-
-        if (OutputOp::kStoreT) {
-          tensor_iterator.store(frag_T);
-          ++tensor_iterator;
-        }
-      }
-
-      if (Base::kFragmentsPerIteration > 1) {
-        shared_load_iterator_.add_pointer_offset(kSmemPointerOffset * (1 - Base::kFragmentsPerIteration));
-      }
-    }
-  }
-
-
-  template<class Seq>
-  struct acc2smem_source_needed;
-
-  template <size_t... Seq>
-  struct acc2smem_source_needed<cutlass::index_sequence<Seq...>> {
-    template<int Advance>
-    CUTLASS_DEVICE
-    static void helper(AccumulatorFragmentIterator accum_fragment_iterator,
-                       WarpTileIterator &warp_tile_iterator) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < Advance; i++) {
-        ++accum_fragment_iterator;
-      }
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
-      accum_fragment_iterator.load(accum_fragment);
-      warp_tile_iterator.store(accum_fragment);
-    }
-
-    CUTLASS_DEVICE
-    static void push(size_t pos,
-                     AccumulatorFragmentIterator const &iterator_begin,
-                     WarpTileIterator &warp_tile_iterator) {
-      int dummy[] = {(pos == Seq) && (helper<Seq>(iterator_begin, warp_tile_iterator), 0)...};
-    }
-  };
-
-  
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void compute_source_needed_(
-    OutputOp const &output_op,                    ///< Output operator
-    BroadcastFragment const &broadcast_fragment,  ///< Fragment containing the accumulated partial reduction over columns
-    OutputTileIterator destination_iterator,      ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator1,          ///< Tile iterator for first source accumulator matrix
-    OutputTileIterator source_iterator2,          ///< Tile iterator for second source accumulator matrix
-    TensorTileIterator tensor_iterator            ///< Threadblock tile iterator for additioanl tensor operand
-    ) { 
-    
-    typename OutputTileIterator::Fragment source_fragment1;
-    source_fragment1.clear();
-    typename OutputTileIterator::Fragment source_fragment2;
-    source_fragment2.clear();
-
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    // 
-
-    #pragma unroll(IterationsUnroll ? OutputTileIterator::kIterations : 1)
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-
-      //
-      // Load the source
-      //
-
-      source_iterator1.load(source_fragment1);
-      ++source_iterator1;
-
-      source_iterator2.load(source_fragment2);
-      ++source_iterator2;
-
-      //
-      // Convert and store fragment
-      //
-      
-      __syncthreads();
-
-      acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
-          iter, accum_fragment_iterator, this->warp_tile_iterator_);
-
-      __syncthreads();
-
-      //
-      // Load fragments from shared memory
-      //
-
-      typename SharedLoadIterator::Fragment aligned_accum_fragment[kPartitionsK];
-
-      shared_load_iterator_.load(aligned_accum_fragment[0]);
-
-      // If the number of k-slices is > 1 - perform a reduction amongst the k-slices
-      if (kPartitionsK > 1)
-      {
-        plus <typename SharedLoadIterator::Fragment> add_fragments;
-        const int tile_row_offset = Base::SharedStorage::StorageShape::kRow / PartitionsK;
-
-        CUTLASS_PRAGMA_UNROLL
-        for ( int i = 1; i < kPartitionsK; ++i) {
-          shared_load_iterator_.add_tile_offset({tile_row_offset , 0});
-          shared_load_iterator_.load(aligned_accum_fragment[i]);
-          aligned_accum_fragment[0] = add_fragments(aligned_accum_fragment[0], aligned_accum_fragment[i]);
-        }
-
-        shared_load_iterator_.add_tile_offset({-1 * (kPartitionsK-1) * tile_row_offset, 0});
-      }
-
-      //
-      // Apply output operation
-      //
-
-      typename OutputTileIterator::Fragment frag_Z;
-      typename TensorTileIterator::Fragment frag_T;
-
-      apply_output_operator_(
-        frag_Z,
-        frag_T,
-        output_op,
-        aligned_accum_fragment[0],
-        source_fragment1,
-        source_fragment2,
-        broadcast_fragment);
-
-      //
-      // Conditionally store fragments
-      //
-
-      if (OutputOp::kStoreZ) {
-        destination_iterator.store(frag_Z);
-        ++destination_iterator;
-      }
-
-      if (OutputOp::kStoreT) {
-        tensor_iterator.store(frag_T);
-        ++tensor_iterator;
-      }
-    }
-  }
-
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_(
-    typename OutputTileIterator::Fragment &frag_Z,
-    typename TensorTileIterator::Fragment &frag_T,
-    OutputOp const &output_op,
-    typename SharedLoadIterator::Fragment const &frag_AB,
-    typename OutputTileIterator::Fragment const &frag_C1,
-    typename OutputTileIterator::Fragment const &frag_C2,
-    BroadcastFragment const &frag_Broadcast) {
-
-    using AccessTypeZ = Array<typename OutputTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeT = Array<typename TensorTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeBroadcast = Array<ElementCompute, kElementsPerAccess>;
-
-    AccessTypeZ *frag_Z_ptr = reinterpret_cast<AccessTypeZ *>(&frag_Z);
-    AccessTypeT *frag_T_ptr = reinterpret_cast<AccessTypeT *>(&frag_T);
-    
-    AccumulatorAccessType const *frag_AB_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&frag_AB);
-
-    OutputAccessType const *frag_C1_ptr =
-      reinterpret_cast<OutputAccessType const *>(&frag_C1);
-
-    OutputAccessType const *frag_C2_ptr =
-      reinterpret_cast<OutputAccessType const *>(&frag_C2);
-
-    AccessTypeBroadcast const *frag_Broadcast_ptr =
-      reinterpret_cast<AccessTypeBroadcast const *>(&frag_Broadcast);
-
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-        output_op(
-          frag_Z_ptr[i],
-          frag_T_ptr[i],
-          frag_AB_ptr[i],
-          frag_C1_ptr[i],
-          frag_C2_ptr[i],
-          frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
-    }
-  }
-
-  /// Helper to invoke the output functor over each vector of output
-  CUTLASS_DEVICE
-  void apply_output_operator_source_not_needed_(
-    typename OutputTileIterator::Fragment &frag_Z,
-    typename TensorTileIterator::Fragment &frag_T,
-    OutputOp const &output_op,
-    typename SharedLoadIterator::Fragment const &frag_AB,
-    BroadcastFragment const &frag_Broadcast) {
-
-    using AccessTypeZ = Array<typename OutputTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeT = Array<typename TensorTileIterator::Element, kElementsPerAccess>;
-    using AccessTypeBroadcast = Array<ElementCompute, kElementsPerAccess>;
-
-    AccessTypeZ *frag_Z_ptr = reinterpret_cast<AccessTypeZ *>(&frag_Z);
-    AccessTypeT *frag_T_ptr = reinterpret_cast<AccessTypeT *>(&frag_T);
-    
-    AccumulatorAccessType const *frag_AB_ptr = 
-      reinterpret_cast<AccumulatorAccessType const *>(&frag_AB);
-
-    AccessTypeBroadcast const *frag_Broadcast_ptr =
-      reinterpret_cast<AccessTypeBroadcast const *>(&frag_Broadcast);
-
-    int const kOutputOpIterations = 
-      OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < kOutputOpIterations; ++i) {
-
-      output_op(
-        frag_Z_ptr[i], 
-        frag_T_ptr[i], 
-        frag_AB_ptr[i], 
-        frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
-    }
-  }
-};
-
-
-template <
-  typename Shape_,
-  typename WarpMmaOperator_,
-  int PartitionsK,
-  typename OutputTileIterator_,
-  typename TensorTileIterator_,
-  typename ElementVector_,
-  typename AccumulatorFragmentIterator_,
-  typename WarpTileIterator_,
-  typename SharedLoadIterator_,
-  typename OutputOp_,
-  typename Padding_,
-  int FragmentsPerPartition,
-  int IterationsUnroll
->
-class EpilogueWithBroadcast<
-  Shape_,
-  WarpMmaOperator_,
-  PartitionsK,
-  OutputTileIterator_,
-  TensorTileIterator_,
-  ElementVector_,
-  AccumulatorFragmentIterator_,
-  WarpTileIterator_,
-  SharedLoadIterator_,
-  OutputOp_,
-  Padding_,
-  FragmentsPerPartition,
-  IterationsUnroll,
-  true
-> : 
-  public EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
-    Padding_,
-    FragmentsPerPartition> {
-
-public:
-
-  using Base = EpilogueBase<
-    Shape_, 
-    typename WarpMmaOperator_::Shape, 
-    PartitionsK, 
-    AccumulatorFragmentIterator_, 
-    WarpTileIterator_, 
-    Padding_,
-    FragmentsPerPartition>;
-
-  static bool const kIsSingleSource = true;
-  using Shape = Shape_;
-  using WarpMmaOperator = WarpMmaOperator_;
-  static int const kPartitionsK = PartitionsK;
-  using OutputTileIterator = OutputTileIterator_;
-  using TensorTileIterator = TensorTileIterator_;
-  using ElementVector = ElementVector_;
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using WarpTileIterator = WarpTileIterator_;
-  using SharedLoadIterator = SharedLoadIterator_;
-  using OutputOp = OutputOp_;
-  using Padding = Padding_;
-
-  using Layout = layout::RowMajor;
-  using LongIndex = typename Layout::LongIndex;
-
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename Base::AccumulatorTile;
-
-  /// Accumulator element
-  using ElementAccumulator = typename WarpTileIterator::Element;
-
-  /// Compute data type produced by the output op
-  using ElementCompute = typename OutputOp::ElementCompute;
-
-  /// Compute fragment
-  using FragmentCompute = Array<ElementCompute, OutputTileIterator::Fragment::kElements>;
-
-  /// Thread map used by output tile iterators
-  using ThreadMap = typename OutputTileIterator::ThreadMap;
-
-  /// Fragment object used to store the broadcast values
-  using BroadcastFragment = Array<
-    ElementCompute, 
-    ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess>;
-
-  /// Output element
-  using ElementOutput = typename OutputTileIterator::Element;
-
-  /// Data type of additional tensor
-  using ElementTensor = typename TensorTileIterator::Element;
-
-  /// Output access size
-  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
-
-  /// Tensor reference to destination tensor
-  using TensorRef = typename OutputTileIterator::TensorRef;
-
-  /// Tensor reference to sync tensor
-  using SyncTensorRef = typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
-
-  /// Const tensor reference to source tensor
-  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
-
-  /// Array type used to output
-  using OutputAccessType = Array<
-    typename OutputTileIterator::Element, OutputTileIterator::kElementsPerAccess>;
-
-  /// Array type used by output functor
-  using AccumulatorAccessType = Array<typename WarpTileIterator::Element, OutputTileIterator::kElementsPerAccess>; 
-
-  /// Array type used by output functor
-  using ComputeAccessType = Array<ElementCompute, OutputTileIterator::kElementsPerAccess>;
-
-  /// Tensor access type
-  using TensorAccessType = Array<ElementTensor, OutputTileIterator::kElementsPerAccess>;
-  
-  /// Number of warps
-  using WarpCount = typename Base::WarpCount;
-
-  /// Shared memory allocation from epilogue base class
-  using BaseSharedStorage = typename Base::SharedStorage;
-
-  static int constexpr kSmemTiles = Base::kFragmentsPerIteration > 1 ? Base::kFragmentsPerIteration : kPartitionsK;
-  static int constexpr kSmemPointerOffset = Base::SharedStorage::StorageShape::kCount / kSmemTiles;
-
-  /// Used for the broadcast
-  struct BroadcastDetail {
-
-    /// Number of threads per warp
-    static int const kWarpSize = 32;
-
-    static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
-
-    /// Number of distinct scalar column indices handled by each thread
-    static int const kColumnsPerThread = ThreadMap::Iterations::kColumn * ThreadMap::kElementsPerAccess;
-
-    /// Number of distinct scalar row indices handled by each thread
-    static int const kRowsPerThread = ThreadMap::Iterations::kCount / ThreadMap::Iterations::kColumn;
-
-    /// Number of threads per threadblock
-    static int const kThreadCount = kWarpSize * WarpCount::kCount;
-
-    /// Number of distinct threads per row of output tile
-    static int const kThreadsPerRow = (Shape::kN / kColumnsPerThread);
-
-    /// Number of distinct threads which must be reduced during the final reduction phase within the threadblock.
-    static int const kThreadRows = kThreadCount / kThreadsPerRow;
-
-    /// I'm not sure what I meant here.
-    static int const kThreadAccessesPerRow = const_max(1, (Shape::kN + kThreadCount - 1) / kThreadCount);
-
-    /// Shape of the shared memory allocation for the epilogue    
-    using StorageShape = MatrixShape<
-      kThreadRows,
-      Shape::kN
-    >;
-
-    /// Debug printing
-    CUTLASS_DEVICE
-    static void print() {
-#if 0
-      printf("BroadcastDetail {\n");
-      printf(
-        "  kColumnsPerThread: %d\nkRowsPerThread: %d\n,kThreadCount: %d\nkThreadsPerRow: %d\n"
-        "kThreadRows: %d\nThreadAccessesPerRow: %d\nStorageShape: %d x %d (count: %d)\n",
-        kColumnsPerThread,
-        kRowsPerThread,
-        kThreadCount,
-        kThreadsPerRow,
-        kThreadRows,
-        kThreadAccessesPerRow,
-        StorageShape::kRow,
-        StorageShape::kColumn,
-        StorageShape::kCount
-      );
-      printf("};\n");
-#endif
-    }
-  };
-
-  /// Shared storage structure (shadows base) with additional SMEM buffer for reduction
-  struct SharedStorage {
-    union {
-      BaseSharedStorage base;
-    };
-
-    CUTLASS_HOST_DEVICE
-    SharedStorage() { }
-  };
-
-public:
-
-
-  static_assert(SharedLoadIterator::Fragment::kElements == OutputTileIterator::Fragment::kElements,
-    "Mismatch between shared load iterator and output tile iterator.");
-
-  static_assert(OutputTileIterator::kElementsPerAccess, "OutputTileIterator::kElementsPerAccess must not be zero.");
-
-  static_assert(!(OutputTileIterator::Fragment::kElements % OutputTileIterator::kElementsPerAccess), 
-    "Divisibility");
-
-private:
-
-  /// Loads fragment from shared memory aligned with output tensor
-  SharedLoadIterator shared_load_iterator_;
-
-  /// Thread index within the threadblock
-  int thread_idx_;
-
-public:
-
-  /// Constructor
-  CUTLASS_DEVICE
-  EpilogueWithBroadcast(
-    SharedStorage &shared_storage,                    ///< Shared storage object    
-    int thread_idx,                                   ///< ID of a thread within the threadblock
-    int warp_idx,                                     ///< ID of warp within threadblock
-    int lane_idx                                      ///< Id of thread within warp
-  ):
-    Base(shared_storage.base, thread_idx, warp_idx, lane_idx),
-    shared_load_iterator_(shared_storage.base.reference(), thread_idx),
-    thread_idx_(thread_idx)
-  {
-
-  }
-
-  /// Streams the result to global memory
-  CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                        ///< Output operator
-    ElementVector const * broadcast_ptr,              ///< Broadcast vector
+    ElementVector const * broadcast_ptr,           ///< Broadcast vector
     OutputTileIterator destination_iterator,          ///< Tile iterator for destination
     AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
     OutputTileIterator source_iterator,               ///< Tile iterator for source accumulator matrix
     TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additional tensor operand
     MatrixCoord const &problem_size =                 ///< Problem size needed to guard against out-of-bounds accesses
         MatrixCoord(Shape::kM, Shape::kN),
     MatrixCoord const &threadblock_offset =           ///< Threadblock's initial offset within the problem size space
@@ -1357,15 +642,15 @@
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void compute_source_needed_(
     OutputOp const &output_op,                    ///< Output operator
     BroadcastFragment const &broadcast_fragment,  ///< Fragment containing the accumulated partial reduction over columns
     OutputTileIterator destination_iterator,      ///< Tile iterator for destination
     AccumulatorTile const &accumulators,          ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator,           ///< Tile iterator for source accumulator matrix
+    OutputTileIterator source_iterator,           ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
     TensorTileIterator tensor_iterator            ///< Threadblock tile iterator for additioanl tensor operand
     ) { 
     
     typename OutputTileIterator::Fragment source_fragment;
     source_fragment.clear();
 
     //
@@ -1470,31 +755,32 @@
 
     AccessTypeZ *frag_Z_ptr = reinterpret_cast<AccessTypeZ *>(&frag_Z);
     AccessTypeT *frag_T_ptr = reinterpret_cast<AccessTypeT *>(&frag_T);
     
     AccumulatorAccessType const *frag_AB_ptr = 
       reinterpret_cast<AccumulatorAccessType const *>(&frag_AB);
 
-    OutputAccessType const *frag_C_ptr =
+    OutputAccessType const *frag_C_ptr = 
       reinterpret_cast<OutputAccessType const *>(&frag_C);
 
     AccessTypeBroadcast const *frag_Broadcast_ptr =
       reinterpret_cast<AccessTypeBroadcast const *>(&frag_Broadcast);
 
     int const kOutputOpIterations = 
       OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
 
     CUTLASS_PRAGMA_UNROLL
     for (int i = 0; i < kOutputOpIterations; ++i) {
-        output_op(
-          frag_Z_ptr[i],
-          frag_T_ptr[i],
-          frag_AB_ptr[i],
-          frag_C_ptr[i],
-          frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
+
+      output_op(
+        frag_Z_ptr[i], 
+        frag_T_ptr[i], 
+        frag_AB_ptr[i], 
+        frag_C_ptr[i], 
+        frag_Broadcast_ptr[i % ThreadMap::Iterations::kColumn]);
     }
   }
 
   /// Helper to invoke the output functor over each vector of output
   CUTLASS_DEVICE
   void apply_output_operator_source_not_needed_(
     typename OutputTileIterator::Fragment &frag_Z,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -120,16 +120,14 @@
   using OutputOp = OutputOp_;
   using ReductionOp = ReductionOp_;
   using Padding = Padding_;
 
   using Layout = layout::RowMajor;
   using LongIndex = typename Layout::LongIndex;
 
-  static bool const kIsSingleSource = true;
-
   /// The complete warp-level accumulator tile
   using AccumulatorTile = typename Base::AccumulatorTile;
 
   /// Accumulator element
   using ElementAccumulator = typename WarpTileIterator::Element;
 
   /// Compute data type produced by the output op
@@ -292,15 +290,15 @@
 
   }
 
   /// Streams the result to global memory
   CUTLASS_DEVICE
   void operator()(
     OutputOp const &output_op,                        ///< Output operator
-    ElementVector * reduction_output_ptr,             ///< Reduction output vector
+    ElementVector * reduction_output_ptr,          ///< Reduction output vector
     OutputTileIterator destination_iterator,          ///< Tile iterator for destination
     AccumulatorTile const &accumulators,              ///< Complete warp-level accumulator tile
     OutputTileIterator source_iterator,               ///< Tile iterator for source accumulator matrix
     TensorTileIterator tensor_iterator,               ///< Threadblock tile iterator for additional tensor operand
     MatrixCoord const &problem_size =                 ///< Problem size needed to guard against out-of-bounds accesses
         MatrixCoord(Shape::kM, Shape::kN),
     MatrixCoord const &threadblock_offset =           ///< Threadblock's initial offset within the problem size space
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/epilogue_workspace.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/interleaved_epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h`

 * *Files 25% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,383 +25,366 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-  \brief Epilogue for threadblock scoped GEMMs using Tensor Ops.
-
-  The epilogue rearranges the result of a matrix product through shared memory to match canonical
-  tensor layouts in global memory. Epilogues support conversion and reduction operations.
-
+    \brief Defines iterators used by warp-level matrix multiply operations targeting Tensor Cores.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
+
 #include "cutlass/array.h"
-#include "cutlass/layout/vector.h"
-#include "cutlass/layout/tensor.h"
-#include "cutlass/tensor_coord.h"
-#include "cutlass/aligned_buffer.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/tensor_ref.h"
+#include "cutlass/matrix_shape.h"
 
+#include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/gemm.h"
 
-#include "cutlass/transform/pitch_linear_thread_map.h"
-#include "cutlass/transform/threadblock/regular_tile_iterator.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/tensor.h"
+#include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm80.h"
+#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
 
-#include "cutlass/epilogue/threadblock/epilogue_base_streamk.h"
-#include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
+#include "cutlass/platform/platform.h"
+#include "cutlass/fast_math.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
-namespace threadblock {
+namespace gemm {
+namespace warp {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+    /// Size of the matrix to load (concept: MatrixShape)
+    typename Shape_,
+    /// Element type
+    typename Element_,
+    /// Layout of operand in memory
+    typename Layout_,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    /// Interval between adjacent *MMA instructions (in units of MMA
+    /// instructions, concept: MatrixShape)
+    typename OpDelta_>
+class MmaTensorOpGaussianComplexAccumulatorTileIterator;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Epilogue operator without splitk
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/// 
+/// Partial specialization for complex<T>
+///
 template <
-    /// Shape of threadblock tile (concept: GemmShape)
+    /// Size of the matrix to load (concept: MatrixShape)
     typename Shape_,
-    /// Warp-level MMA operator (concept: gemm::warp::MmaTensorOp)
-    typename WarpMmaOperator_,
-    /// Number of partitions of the K dimension
-    int PartitionsK,
-    /// Tile iterator reading and writing output tensors
-    typename OutputTileIterator_,
-    /// Fragment iterator selecting accumulators
-    typename AccumulatorFragmentIterator_,
-    /// Output operator
-    typename OutputOp_,
-    /// Number of interleaved k
-    int InterleavedK>
-class InterleavedEpilogue :
-  public EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>
-{
-public:
-
-  using BaseStreamK = EpilogueBaseStreamK<
-    Shape_,
-    PartitionsK,
-    WarpMmaOperator_,
-    AccumulatorFragmentIterator_>;
+    /// Data type of underlying field of reals.
+    typename RealElement,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    /// Interval between adjacent *MMA instructions (in units of MMA
+    /// instructions, concept: MatrixShape)
+    typename OpDelta_>
+class MmaTensorOpGaussianComplexAccumulatorTileIterator<
+    Shape_, complex<RealElement>, cutlass::layout::RowMajor, InstructionShape_, OpDelta_> {
+ public:
 
+  /// Shape of tile to load (concept: MatrixShape)
   using Shape = Shape_;
-  using WarpMmaOperator = WarpMmaOperator_;
-  static int const kPartitionsK = PartitionsK;
-  using AccumulatorFragmentIterator = AccumulatorFragmentIterator_;
-  using OutputTileIterator = OutputTileIterator_;
-  using OutputOp = OutputOp_;
 
-  /// The complete warp-level accumulator tile
-  using AccumulatorTile = typename AccumulatorFragmentIterator::AccumulatorTile;
+  /// Operand tag
+  static Operand const kOperand = Operand::kC;
 
-  /// Fragment type used by the accumulator tile's fragment iterator
-  using AccumulatorFragment = typename AccumulatorFragmentIterator::Fragment;
+  /// Element type
+  using Element = complex<RealElement>;
 
-  /// Accumulator element
-  using ElementAccumulator = typename AccumulatorTile::Element;
+  /// Layout of source tile
+  using Layout = cutlass::layout::RowMajor;
 
-  /// Output element
-  using ElementOutput = typename OutputTileIterator::Element;
+  /// Shape of one matrix product operation (concept: MatrixShape)
+  using InstructionShape = InstructionShape_;
 
-  /// Output access size
-  static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
+  /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
+  using OpDelta = OpDelta_;
 
-  /// Tensor reference to destination tensor
-  using TensorRef = typename OutputTileIterator::TensorRef;
+  /// Number of participating threads
+  static int const kThreads = 32;
 
-  /// Tensor reference to sync tensor
-  using SyncTensorRef =
-      typename cutlass::TensorRef<int, cutlass::layout::PackedVectorLayout>;
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-  /// Const tensor reference to source tensor
-  using ConstTensorRef = typename OutputTileIterator::ConstTensorRef;
+  /// Index type
+  using Index = typename TensorRef::Index;
 
-  /// Array type used to output
-  using OutputAccessType = Array<typename OutputTileIterator::Element,
-                                 OutputTileIterator::kElementsPerAccess>;
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
 
-  /// Array type used by output functor
-  using AccumulatorAccessType =
-      Array<ElementAccumulator, OutputTileIterator::kElementsPerAccess>;
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
-  /// Number of warps
-  using WarpCount =
-      gemm::GemmShape<Shape::kM / WarpMmaOperator::Shape::kM,
-                      Shape::kN / WarpMmaOperator::Shape::kN, kPartitionsK>;
+  /// Internal structure of iterator - made public to enable introspection
+  struct Policy {
+    static_assert(
+        !(Shape::kRow % InstructionShape::kM) &&
+            !(Shape::kColumn % InstructionShape::kN),
+        "Shape of warp-level Mma must be divisible by operator shape.");
 
-public:
-
-  static_assert(OutputTileIterator::kElementsPerAccess,
-                "This must not be zero.");
-
-  static_assert(!(OutputTileIterator::Fragment::kElements %
-                  OutputTileIterator::kElementsPerAccess),
-                "Divisibility");
-
-public:
+    static_assert(platform::is_same<TensorCoord, MatrixCoord>::value,
+      "Layouts must be defined for logical MatrixCoord coordinate space.");
 
-  /// Aspect for when epilogue source is not needed
-  struct SourceAspectNotNeeded
-  {
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNotNeeded()
-    {}
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
+    /// Number of mma operations performed
+    using MmaIterations = MatrixShape<Shape::kRow / InstructionShape::kM,
+                                      Shape::kColumn / InstructionShape::kN>;
+  };
 
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+private:
 
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+  // Assume accumulator tile is an arrangement of 8-by-8 tiles replicated over the entire
+  // shape, with each quad mapped to one row and each thread mapped to 1/4 of the elements
+  // of that row. The accumulators within one row are assumed to be consecutive.
+ static int const kElementsPerAccess = InstructionShape::kN / 4;
+ static int const kRowsPerTile = 8;
+ static int const kAccumulatorRows = InstructionShape::kM / kRowsPerTile;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i]);
-      }
-    }
-  };
+public:
 
+  //
+  // Derived quantities
+  //
+
+  /// Fragment object holding a thread's part of a tile. It is assumed that the accumulators
+  /// are stored in a gaussian complex arrangement with parts 1, 2, and 3 as entirely contiguous
+  /// arranged as [part1, part2, part3]
+  using Fragment = Array<RealElement, (Shape::kCount / kThreads) * 3>;
+
+  static int const kPart1Index = (Shape::kCount / kThreads) * 0;
+  static int const kPart2Index = (Shape::kCount / kThreads) * 1;
+  static int const kPart3Index = (Shape::kCount / kThreads) * 2;
 
-  /// Aspect for when epilogue source is needed
-  struct SourceAspectNeeded
-  {
-    OutputTileIterator source_iterator;
-
-    typename OutputTileIterator::Fragment source_fragment;
-
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    static void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment,
-      typename OutputTileIterator::Fragment const &source_fragment)
-    {
-      OutputAccessType *output_frag_ptr =
-        reinterpret_cast<OutputAccessType *>(&output_fragment);
+private:
 
-      AccumulatorAccessType const *compute_frag_ptr =
-        reinterpret_cast<AccumulatorAccessType const *>(&aligned_accum_fragment);
+  /// Reference to output tensor
+  TensorRef ref_;
 
-      OutputAccessType const *source_frag_ptr =
-        reinterpret_cast<OutputAccessType const *>(&source_fragment);
+public:
+  
+  /// Default ctor constructs null iterator
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator() { }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator(
+    TensorRef const &ref, 
+    int lane_id
+  ):
+    ref_(ref) {
 
-      int const kOutputOpIterations =
-        OutputTileIterator::Fragment::kElements / OutputTileIterator::kElementsPerAccess;
+    int quad = (lane_id >> 2);
+    int lane_in_quad = (lane_id & 3);
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int i = 0; i < kOutputOpIterations; ++i)
-      {
-        // Call the output operator
-        output_frag_ptr[i] = output_op(compute_frag_ptr[i], source_frag_ptr[i]);
-      }
-    }
+    MatrixCoord lane_offset(quad, lane_in_quad * kElementsPerAccess);
 
-    /// Constructor
-    CUTLASS_DEVICE
-    SourceAspectNeeded(OutputTileIterator source_iterator) :
-      source_iterator(source_iterator)
-    {
-      source_fragment.clear();
-    }
+    ref_.add_coord_offset(lane_offset);
+  }
 
-    /// Invoke the output functor over each vector of output
-    CUTLASS_DEVICE
-    void apply_output_operator(
-      typename OutputTileIterator::Fragment &output_fragment,
-      OutputOp const &output_op,
-      typename AccumulatorFragmentIterator::Fragment const &aligned_accum_fragment)
-    {
-      // Load addend source fragment from global memory
-      source_iterator.load(source_fragment);
-      ++source_iterator;
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator &add_pointer_offset(LongIndex offset) {
+    ref_.add_pointer_offset(offset);
+    return *this;
+  }
 
-      apply_output_operator(output_fragment, output_op, aligned_accum_fragment, source_fragment);
-    }
-  };
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator &add_tile_offset(TensorCoord const &tile_offset) {
 
+    ref_.add_coord_offset(tile_offset * make_Coord(Shape::kRow, Shape::kColumn));
 
-  /// Shared storage allocation needed by the epilogue
-  struct SharedStorage {};
+    return *this;
+  }
 
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator++() {
+    // deliberate no-op
+    return *this;
+  }
 
-public:
+  /// Advances the iterator along the advance dimension
+  CUTLASS_HOST_DEVICE
+  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator--() {
+    // deliberate no-op
+    return *this;
+  }
 
-  /// Constructor
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_DEVICE
-  InterleavedEpilogue(
-      SharedStorage &shared_storage,  ///< Shared storage object
-      int thread_idx,                 ///< ID of a thread within the threadblock
-      int warp_idx,                   ///< ID of warp within threadblock
-      int lane_idx)                   ///< Id of thread within warp
-  :
-      BaseStreamK(thread_idx)
-  {}
-
+  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator+=(TensorCoord const &tile_offset) {
+    add_tile_offset(tile_offset);
+    return *this;
+  }
 
-  /// Aggregates the accumulator sets shared by peer blocks in the global workspace,
-  /// performing epilogue computations, writing to output
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_DEVICE
-  void reduce(
-      int peer_idx_begin,
-      int peer_idx_end,
-      int reduce_fragment_idx,
-      void *element_workspace,
-      OutputOp const &output_op,                      ///< Output operator
-      OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-      OutputTileIterator source_iterator)             ///< Threadblock tile coordinate in GEMM (in units of threadblock tiles)
-  {
-    // Redcuce peer accumulator fragments into one fragment
-    AccumulatorFragment accum_fragment;
-    BaseStreamK::reduce(accum_fragment, peer_idx_begin, peer_idx_end, reduce_fragment_idx, element_workspace);
-
-    // Source-fragment data (zero-initialized for scenarios where the
-    // output operator allows us to skip loading it from global input)
-    typename OutputTileIterator::Fragment source_fragment;
-    source_fragment.clear();
-
-    if (output_op.is_source_needed())
-    {
-      source_iterator += reduce_fragment_idx;
-      source_iterator.load(source_fragment);
-    }
+  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator-=(TensorCoord const &tile_offset) {
+    add_tile_offset(-tile_offset);
+    return *this;
+  }
 
-    // Compute the output result
-    typename OutputTileIterator::Fragment output_fragment;
+  /// Loads a fragment from memory at the location pointed to by the iterator.
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
+  }
 
-    // Apply the output operator
-    SourceAspectNeeded::apply_output_operator(output_fragment, output_op, accum_fragment, source_fragment);
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(
+    Fragment &frag,                             ///< fragment to load from the tensor
+    Index pointer_offset) const {               ///< loads a tile with a linear offset
+  
+    TensorRef offset_ref(ref_);
+    offset_ref.add_pointer_offset(pointer_offset);
 
-    // Store the final result
-    destination_iterator += reduce_fragment_idx;
-    destination_iterator.store(output_fragment);
+    CUTLASS_PRAGMA_UNROLL
+    for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
+        
+        int mma_accum_start = kAccumulatorRows * kElementsPerAccess * 
+          (mma_n * Policy::MmaIterations::kRow + mma_m);
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int row = 0; row < kAccumulatorRows; ++row) {
+          CUTLASS_PRAGMA_UNROLL
+          for (int col = 0; col < kElementsPerAccess; ++col) {
+            int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
+                          row * kRowsPerTile;
+            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn + col;
+
+            Element z = offset_ref.at({accum_m, accum_n});
+
+            frag[mma_accum_start + row * kElementsPerAccess + col + kPart1Index] = z.real() + z.imag();
+            frag[mma_accum_start + row * kElementsPerAccess + col + kPart2Index] = -z.real();
+            frag[mma_accum_start + row * kElementsPerAccess + col + kPart3Index] = z.imag();
+          }
+        }
+      }
+    }
   }
 
-
-  /// Perform the epilogue computations and stream the result to global memory.
+  /// Loads a fragment from memory with additional logical offset
   CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators)            ///< Complete warp-level accumulator tile
-  {
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
-  }
+  void load_with_byte_offset(
+    Fragment &frag,                             ///< fragment to load from the tensor
+    Index byte_offset) const {                  ///< loads a tile with a linear offset
 
+    load_with_pointer_offset(byte_offset / sizeof(Element));
+  }
 
-  /// Perform the epilogue computations and stream the result to global memory.  Implements
-  /// two alternative codepaths, depending on whether the output op requires addend data to be loaded.
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
   CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (output_op.is_source_needed())
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
-    }
-    else
-    {
-      operator()(output_op, destination_iterator, accumulators, SourceAspectNotNeeded());
-    }
-  }
+  void load(
+    Fragment &frag,                             ///< fragment to load from the tensor
+    TensorCoord const &tile_offset) const {     ///< loads a tile with a logical offset in units of whole tiles
 
+    load(frag, tile_offset, 0);
+  }
 
-  /// Perform the epilogue computations and stream the result to global memory.  Implements a
-  /// single codepath, regardless of whether the output op requires addend data to be loaded
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
   CUTLASS_DEVICE
-  void unified(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    OutputTileIterator source_iterator )            ///< Tile iterator for addend source
-  {
-    if (!output_op.is_source_needed())
-    {
-      source_iterator.clear_mask();
-      __syncthreads();  // Dummy (CUDA 11.0)
-    }
+  void load(
+    Fragment &frag,                             ///< fragment to load from the tensor
+    TensorCoord const &tile_offset,             ///< loads a tile with a logical offset in units of whole tiles
+    Index pointer_offset) const {               ///< loads a tile with a logical offset AND a pointer offset
 
-    operator()(output_op, destination_iterator, accumulators, SourceAspectNeeded(source_iterator));
+    load_with_pointer_offset(frag, ref_.offset(tile_offset) + pointer_offset);
   }
 
+  /// Stores a fragment to memory
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) const {
+    store_with_pointer_offset(frag, 0);
+  }
 
-  /// Streams the result to global memory
-  template <typename SourceAspect>
+  /// Stores a fragment to memory with additional pointer offset
   CUTLASS_DEVICE
-  void operator()(
-    OutputOp const &output_op,                      ///< Output operator
-    OutputTileIterator destination_iterator,        ///< Tile iterator for destination
-    AccumulatorTile const &accumulators,            ///< Complete warp-level accumulator tile
-    SourceAspect source)
-  {
-    //
-    // Iterator over warp-level accumulator fragment
-    //
-
-    AccumulatorFragmentIterator accum_fragment_iterator(accumulators);
-
-    //
-    // Iterate over accumulator tile
-    //
+  void store_with_pointer_offset(
+    Fragment const &frag,                       ///< fragment to store from the tensor
+    Index pointer_offset) const {               ///< store a tile with a linear offset
+  
+    TensorRef offset_ref(ref_);
+    offset_ref.add_pointer_offset(pointer_offset);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int iter = 0; iter < OutputTileIterator::kIterations; ++iter) {
-
-      //
-      // Convert fragment
-      //
-
-      typename AccumulatorFragmentIterator::Fragment accum_fragment;
+    for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
+        
+        int mma_accum_start = kAccumulatorRows * kElementsPerAccess * 
+          (mma_n * Policy::MmaIterations::kRow + mma_m);
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int row = 0; row < kAccumulatorRows; ++row) {
+          CUTLASS_PRAGMA_UNROLL
+          for (int col = 0; col < kElementsPerAccess; ++col) {
+            int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
+                          row * kRowsPerTile;
+            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn + col;
+            int idx = mma_accum_start + row * kElementsPerAccess + col;
+
+            Element z(frag[kPart1Index + idx] - frag[kPart3Index + idx], 
+                      frag[kPart1Index + idx] + frag[kPart2Index + idx]);
+
+            offset_ref.at({accum_m, accum_n}) = z;
+          }
+        }
+      }
+    }
+  }
 
-      accum_fragment_iterator.load(accum_fragment);
-      ++accum_fragment_iterator;
+  /// Stores a fragment to memory with additional pointer offset
+  CUTLASS_DEVICE
+  void store_with_byte_offset(
+    Fragment const &frag,                       ///< fragment to store from the tensor
+    Index byte_offset) const {                  ///< store a tile with a linear offset
 
-      //
-      // Compute the output result
-      //
+    store_with_pointer_offset(byte_offset / sizeof(Element));
+  }
 
-      typename OutputTileIterator::Fragment output_fragment;
-      source.apply_output_operator(output_fragment, output_op, accum_fragment);
+  /// Stores a fragment to memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void store(
+    Fragment &frag,                             ///< fragment to store to the tensor
+    TensorCoord const &tile_offset) const {     ///< stores a tile with a logical offset in units of whole tiles
 
-      //
-      // Store the final result
-      //
+    store(frag, tile_offset, 0);
+  }
 
-      destination_iterator.set_iteration_index(iter);
-      destination_iterator.store(output_fragment);
-      ++destination_iterator;
-    }
+  /// Stores a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void store(
+      /// fragment to store to the tensor
+      Fragment const &frag,
+      /// stores a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// stores a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    store_with_pointer_offset(frag, ref_.offset(tile_offset) + pointer_offset);
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace epilogue
+} // namespace warp
+} // namespace gemm
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_iterator_parameter.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/output_tile_thread_map.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -676,78 +676,21 @@
         thread_start_row_ += ThreadMap::Count::kGroup * 
           ThreadMap::Shape::kGroup * ThreadMap::Count::kRow * ThreadMap::Shape::kRow;
 
         if (state_[2] == ThreadMap::Count::kCluster) {
           state_[2] = 0;
           byte_pointer_ += params_.advance_tile;
           store_byte_pointer_ += params_.advance_tile;
-
-          thread_start_row_ += ThreadMap::Shape::kGroup * ThreadMap::Shape::kRow
-            * ThreadMap::Shape::kCluster * ThreadMap::Shape::kTile;
         }
       }
     }
 
     return *this;
   }
 
-  /// Advances a number of positions to load or store
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIterator &operator+=(int increment)
-  {
-    // Row
-    state_[0] += increment;
-    int increment_row = state_[0] / ThreadMap::Count::kRow;
-    state_[0] = state_[0] % ThreadMap::Count::kRow;
-
-    byte_pointer_ += (params_.advance_row * increment);
-    store_byte_pointer_ += (params_.advance_row * increment);
-    thread_start_row_ += (ThreadMap::Shape::kRow * increment);
-
-    // Group
-    state_[1] += increment_row;
-    int increment_group = state_[1] / ThreadMap::Count::kGroup;
-    state_[1] = state_[1] % ThreadMap::Count::kGroup;
-
-    byte_pointer_ += (params_.advance_group * increment_row);
-    store_byte_pointer_ += (params_.advance_group * increment_row);
-    thread_start_row_ +=
-        (ThreadMap::Shape::kGroup - 1) *
-        ThreadMap::Shape::kRow *
-        ThreadMap::Count::kRow *
-        increment_row;
-
-
-    // Cluster
-    state_[2] += increment_group;
-    int increment_cluster = state_[2] / ThreadMap::Count::kCluster;
-    state_[2] = state_[2] % ThreadMap::Count::kCluster;
-
-    byte_pointer_ += (params_.advance_cluster * increment_group);
-    store_byte_pointer_ += (params_.advance_cluster * increment_group);
-    thread_start_row_ +=
-        ThreadMap::Count::kGroup *
-        ThreadMap::Shape::kGroup *
-        ThreadMap::Count::kRow *
-        ThreadMap::Shape::kRow *
-        increment_group;
-
-    // Tile
-    byte_pointer_ += (params_.advance_tile * increment_cluster);
-    store_byte_pointer_ += (params_.advance_tile * increment_cluster);
-    thread_start_row_ +=
-        ThreadMap::Shape::kGroup *
-        ThreadMap::Shape::kRow *
-        ThreadMap::Shape::kCluster *
-        ThreadMap::Shape::kTile *
-        increment_cluster;
-
-    return *this;
-  }
-
   ///< Efficiently disables all accesses guarded by mask
   CUTLASS_DEVICE void clear_mask() {
     mask_.clear();
   }
 
   ///< Efficiently enables all accesses guarded by mask
   CUTLASS_DEVICE void enable_mask() {
@@ -997,31 +940,14 @@
         iteration_strided_ = 0;
       }
     }
 
     return *this;
   }
 
-  /// Advances a number of positions to load or store
-  CUTLASS_HOST_DEVICE
-  InterleavedPredicatedTileIterator &operator+=(int increment)
-  {
-    // Contiguous
-    iteration_contiguous_ += increment;
-    int increment_strided = iteration_contiguous_ / ThreadMap::Iterations::kContiguous;
-    iteration_contiguous_ = iteration_contiguous_ % ThreadMap::Iterations::kContiguous;
-    byte_pointer_ += (params_.advance_row * increment);
-
-    // Strided
-    iteration_strided_ += increment_strided;
-    byte_pointer_ += (params_.advance_column * increment_strided);
-
-    return *this;
-  }
-
   ///< Efficiently disables all accesses guarded by mask
   CUTLASS_DEVICE void clear_mask() {
     mask_.clear();
   }
 
   ///< Efficiently enables all accesses guarded by mask
   CUTLASS_DEVICE void enable_mask() {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_affine_layout_params.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_blas3.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_params.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,20 +31,17 @@
 /*! \file
   \brief 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 
-#include "cutlass/conv/conv2d_problem_size.h"
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -244,95 +241,14 @@
   CUTLASS_HOST_DEVICE
   PredicatedTileIteratorParams(LongIndex stride, OutputTileThreadMapDesc thread_map) {
     initialize(stride, thread_map);
   }
 };
 
 
-
-///////////////////////////////////////////////////////////////////////////////
-
-//
-// Parameters struct for PredicatedTileIteratorDirect2dConv
-//
-
-struct PredicatedTileIteratorDirect2dConvParams{
-  using Index = int32_t;
-  using LongIndex = int64_t;
-
-  //
-  // Data members
-  //
-  FastDivmod pq_divmod;
-  FastDivmod q_divmod;
-
-  LongIndex stride;
-  LongIndex stride_n;
-  LongIndex stride_p;
-
-  int N;
-  int P;
-  int Q;
-
-  //
-  // Methods
-  //
-
-  CUTLASS_HOST_DEVICE
-  Status initialize(LongIndex stride_,
-                    cutlass::conv::Conv2dProblemSize const &problem_size,
-                    MatrixCoord threadblock_output_shape) {
-    stride = stride_; // The stride per row of output tensor (bytes)
-    stride_n = problem_size.P * problem_size.Q;
-    stride_p = problem_size.Q ;
-
-    N = problem_size.N;
-    P = problem_size.P;
-    Q = problem_size.Q;
-
-    // Fastdivmod for output O, P, Q
-    if(threadblock_output_shape.row() != 0 && threadblock_output_shape.column() !=0 ){
-      int tiles_p =
-          (problem_size.P + (threadblock_output_shape.row() - 1)) / (threadblock_output_shape.row());
-      int tiles_q = (problem_size.Q + (threadblock_output_shape.column() - 1)) /
-                    (threadblock_output_shape.column());
-
-      pq_divmod = FastDivmod(tiles_p * tiles_q);
-      q_divmod = FastDivmod(tiles_q);
-    }
-
-    return Status::kSuccess;
-  }
-
-  CUTLASS_HOST_DEVICE
-  Status initialize(
-      Index stride_,
-      cutlass::conv::Conv2dProblemSize const &problem_size = cutlass::conv::Conv2dProblemSize(),
-      MatrixCoord threadblock_output_shape = MatrixCoord()) {
-    return initialize(LongIndex(stride_), problem_size, threadblock_output_shape);
-  }
-
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorDirect2dConvParams() { initialize(LongIndex(0)); }
-
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorDirect2dConvParams(Index stride,
-                               cutlass::conv::Conv2dProblemSize const &problem_size,
-                               MatrixCoord threadblock_output_shape) {
-    initialize(stride, problem_size, threadblock_output_shape);
-  }
-
-  CUTLASS_HOST_DEVICE
-  PredicatedTileIteratorDirect2dConvParams(LongIndex stride,
-                               cutlass::conv::Conv2dProblemSize const &problem_size,
-                               MatrixCoord threadblock_output_shape) {
-    initialize(stride, problem_size, threadblock_output_shape);
-  }
-};
-
 ///////////////////////////////////////////////////////////////////////////////
 //  InterleavedPredicatedTileIterator
 ///////////////////////////////////////////////////////////////////////////////
 
 
 /// Predicated tile access iterator descriptor object containing template dependent state
 struct InterleavedPredicatedTileIteratorDesc {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_predicates.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/predicated_tile_iterator_strided_dgrad.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -182,18 +182,18 @@
 
   /// Array of boolean values to contain steady-state predicates
   Mask mask_;
 
   /// Extent of the matrix tile in rows
   Index extent_row_;
 
-  /// Starting Dx h and w dimension for strided dgrad mapping
+  /// Starting Dx h and w dimenstion for strided dgrad mapping
   int start_h_, start_w_;
 
-  /// Effective Dy P and Q dimensions for strided dgrad mapping
+  /// Effective Dy P and Q dimenstions for strided dgrad mapping
   int p_, q_;
 
   /// A thread's starting row position (assuming steady-state predicates have been computed)
   Index thread_start_row_;
 
   /// A thread's starting column position (assuming steady-state predicates have been computed)
   Index thread_start_column_;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -197,19 +197,14 @@
             }
           }
         }
       }
     }
   }
 
-  /// Loads a fragment from memory
-  CUTLASS_DEVICE
-  void set_smem_base_address(Index address) {
-  }
-
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) const {
 
     load_with_pointer_offset(frag, 0);
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_mixed.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -66,31 +66,30 @@
 ///
 template <
   typename ThreadMap_,       ///< Thread map (conept: OutputTileThreadMap)
   typename Element_,         ///< Accumulator data type
   int ElementSizeBits_,      ///< Size of accumulator in bits
   int OutputSizeBits_,       ///< Size of output element in bits
   int ElementsPerAccess,     ///< Vector length of output vector
-  int ContiguousLanes,       ///< Number of lanes in the warp writing to contiguous elements
+  int ContiguousLanes        ///< Number of lanes in the warp writing to contiguous elements
                              ///  in the global memory tensor
-  bool EightBitsOutputOrLess = (OutputSizeBits_ <= 8)
 >
 class SharedLoadIteratorMixed;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Tile iterator used to load output tile from shared memory in epilogue.
 ///
 /// Satisfies: ReadableTileIterator
 ///
 template <
   typename ThreadMap_,       ///< Thread map (conept: OutputTileThreadMap)
   typename Element_          ///< Accumulator data type
 >
-class SharedLoadIteratorMixed<ThreadMap_, Element_, 32, 16, 8, 8, false> {
+class SharedLoadIteratorMixed<ThreadMap_, Element_, 32, 16, 8, 8> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = Element_;
 
   using Layout = layout::RowMajor;
@@ -231,18 +230,14 @@
             }
           }
         }
       }
     }
   }
 
-  /// Set base smem address
-  CUTLASS_DEVICE
-  void set_smem_base_address(Index address) {}
-
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) const {
 
     load_with_pointer_offset(frag, 0);
   }
 };
@@ -250,15 +245,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 16 => int8_t/int4b_t x 16 
 template <
   typename ThreadMap_,      ///< Thread map (conept: OutputTileThreadMap)
   int OutputSizeBits_       ///< Size of output element in bits
 >
-class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 16, 8, true> {
+class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 16, 8> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = int32_t;
 
   using Layout = layout::RowMajor;
@@ -396,18 +391,14 @@
             }
           }
         }
       }
     }
   }
 
-  /// Set base smem address
-  CUTLASS_DEVICE
-  void set_smem_base_address(Index address) {}
-
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) {
 
     load_with_pointer_offset(frag, 0);
   }
 };
@@ -415,15 +406,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 8 => int8_t/int4b_t x 8
 template <
   typename ThreadMap_,      ///< Thread map (conept: OutputTileThreadMap)
   int OutputSizeBits_
 >
-class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 8, 8, true> {
+class SharedLoadIteratorMixed<ThreadMap_, int32_t, 32, OutputSizeBits_, 8, 8> {
 public:
   using ThreadMap = ThreadMap_;
   using Shape = typename ThreadMap::Shape;
 
   using Element = int32_t;
 
   using Layout = layout::RowMajor;
@@ -561,18 +552,14 @@
             }
           }
         }
       }
     }
   }
 
-  /// Set base smem address
-  CUTLASS_DEVICE
-  void set_smem_base_address(Index address) {}
-
   /// Loads a fragment
   CUTLASS_DEVICE
   void load(Fragment &frag) {
 
     load_with_pointer_offset(frag, 0);
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/threadblock/shared_load_iterator_pitch_liner.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_complex_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_gaussian_complex_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_simt.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_volta_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/fragment_iterator_wmma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/simt_policy.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tensor_op_policy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h`

 * *Files 19% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,194 +31,177 @@
 /*! \file
     \brief 
 */
 
 #pragma once
 
 #include "cutlass/array.h"
+#include "cutlass/tensor_ref.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
 
-#include "cutlass/epilogue/warp/simt_policy.h"
-
-#define CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES 1
+#include "cutlass/epilogue/warp/tensor_op_policy.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace epilogue {
 namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
   typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename Operator,      ///< matrix multiply operation (concept: arch::Mma)
+  typename OperatorShape, ///< matrix multiply operation shape (concept: gemm::GemmShape)
   typename Element,       ///< data type of element to be written
-  typename Layout,        ///< target shared memory layout
-  typename MmaSimtPolicy          ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename Layout         ///< target shared memory layout
 >
-class TileIteratorSimt;
+class TileIteratorTensorOp;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
   typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-  typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
-  typename Element_,       ///< data type of element to be written
-  typename MmaSimtPolicy_         ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
+  typename Element_        ///< data type of element to be written
 >
-class TileIteratorSimt<WarpShape_, Operator_, Element_, layout::RowMajor, MmaSimtPolicy_> {
+class TileIteratorTensorOp<WarpShape_, OperatorShape_, Element_, layout::RowMajor> {
 public:
 
   using WarpShape = WarpShape_;
-  using Operator = Operator_;
+  using OperatorShape = OperatorShape_;
   using Element = Element_;
   using Layout = layout::RowMajor;
 
+  using TensorLayout = Layout;
   using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
   using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
   using Index = typename TensorRef::Index;
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
 
   /// Shape of the tile in memory
   using Shape = MatrixShape<
     Policy::kRowsPerIteration,
     WarpShape::kN
   >;
 
   /// This is the fragment size produced by one access of the iterator.
   using Fragment = Array<
-    typename Operator::ElementC, 
-    Policy::kElementsPerIteration>;
+    Element, 
+    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
 
   /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = Array<
-    typename Operator::ElementC, 
-    Policy::kAccumulatorElementCount>;
+  //using AccumulatorTile = typename Operator::FragmentC;
 
   /// Number of times this iterator can be incremented
   static int const kIterations = Policy::kIterations;
 
+  /// Number of times this iterator can be incremented
+  using TileIterations = typename Policy::TileIterations;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+  };
+
   /// Padding quantity
   using Padding = MatrixShape<
     0,
-    4 * Policy::kElementsPerAccess
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
-    + 1
-#endif
-  >;
+    Detail::kLanesInQuad * Policy::kElementsPerAccess>;
 
 private:
 
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
   /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    1
-  >;
-
-#else
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    Policy::kElementsPerAccess
-  >;
-#endif
+  using AccessType = AlignedArray<Element, Policy::kElementsPerAccess>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
   AccessType *pointer_;
 
   /// Internal layout object
   Layout layout_;
 
+  /// Thread offset
+  MatrixCoord thread_offset_;
+
 public:
 
   /// Default constructor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt(): pointer_(nullptr) { }
+  TileIteratorTensorOp(): pointer_(nullptr) { }
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt(
+  TileIteratorTensorOp(
     TensorRef const &ref,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements) { 
+    layout_(ref.stride()[0] / Policy::kElementsPerAccess) {
 
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+    int quad_id = (lane_id / Detail::kLanesInQuad); 
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
-    pointer_ += layout_({
-      lane_offset.row(),
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    thread_offset_ = {
+      quad_id, lane_in_quad * Policy::kElementsPerAccess
+    };
+
+    pointer_ += layout_({thread_offset_.row(), thread_offset_.column() / Policy::kElementsPerAccess});
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
+  TileIteratorTensorOp & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / Policy::kElementsPerAccess;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & add_tile_offset(TensorCoord const &tile_offset) {
+  TileIteratorTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
 
-    pointer_ += layout_({
+    MatrixCoord coord_offset(
       tile_offset.row() * Shape::kRow, 
-      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
+      tile_offset.column() * Shape::kColumn
+    );
+
+    thread_offset_ += coord_offset;
+
+    pointer_ += layout_({
+      coord_offset.row(),
+      coord_offset.column() / Policy::kElementsPerAccess
     });
 
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimt & operator+=(TensorCoord const &tile_offset) {
-
+  TileIteratorTensorOp & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
-    
     return *this;
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
-      // de-vectorized stores
-      using ScalarAccessType = AlignedArray<Element, 1>;
-      ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
-      ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-          scalarPointer[n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s] = scalarFragPtr[n * Policy::kElementsPerAccess + s];
-        }
-      }
-#else
-    // original vector stores
     AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
+      pointer_[n * Detail::kLanesInQuad + pointer_offset / Policy::kElementsPerAccess] = frag_ptr[n];
     }
-#endif
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
@@ -226,155 +209,184 @@
   /// Load
   CUTLASS_HOST_DEVICE
   void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
     AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
+      frag_ptr[n] = pointer_[n * Detail::kLanesInQuad + pointer_offset / Policy::kElementsPerAccess];
     }
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
 
-  /// Set smem base address
   CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
+  TileIteratorTensorOp & operator++() {
+    return add_tile_offset({1, 0});
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Template for reading and writing tiles of accumulators to shared memory
-template <typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-          typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
-          typename Element_,       ///< data type of element to be written
-          typename Layout_,         ///< target shared memory layout
-          typename MmaSimtPolicy_  ///< policy defining lane arrangement (concept: MmaSimtPolicy)
-          >
-class TileIteratorSimtDirectConv {
- public:
+template <
+  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
+  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
+  typename Element_,       ///< data type of element to be written
+  int InterleavedK         ///< number of interleaved k
+>
+class TileIteratorTensorOp<WarpShape_, OperatorShape_, Element_, 
+                            layout::ColumnMajorInterleaved<InterleavedK> > {
+public:
 
   using WarpShape = WarpShape_;
-  using Operator = Operator_;
+  using OperatorShape = OperatorShape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajorInterleaved<InterleavedK>;
+  using TensorLayout = Layout;                ///< shared memory tensor ref layout
 
-  using TensorRef = TensorRef<Element, Layout>;  ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;               ///< Logical coordinate in referenced tensor
+  using TensorRef = TensorRef<Element, TensorLayout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
   using Index = typename TensorRef::Index;
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
 
   /// Shape of the tile in memory
-  using Shape = MatrixShape<Policy::kRowsPerIteration, WarpShape::kN>;
+  using Shape = MatrixShape<
+//    Policy::kRowsPerIteration,
+    WarpShape::kM,
+    InterleavedK
+  >;
 
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = Array<typename Operator::ElementC, Policy::kElementsPerIteration>;
+  /// This is the fragment size produced by one tile
+  using Fragment = Array<
+    Element, 
+    Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction 
+        * Policy::kElementsPerIteration>;
+
+  /// This is the fragment size produced by one iteration
+//  using Fragment = Array<
+//    Element, Policy::kElementsPerIteration >;
 
   /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = Array<typename Operator::ElementC, Policy::kAccumulatorElementCount>;
+  //using AccumulatorTile = typename Operator::FragmentC;
 
   /// Number of times this iterator can be incremented
-  static int const kIterations = Policy::kIterations;
+  using TileIterations = typename Policy::TileIterations;
+
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+  };
 
   /// Padding quantity
-  using Padding = MatrixShape<0,
-                              0
-                              >;
+  using Padding = MatrixShape<
+    0,
+    Detail::kLanesInQuad * Policy::kElementsPerIteration>;
 
 private:
+
   /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    Policy::kElementsPerAccess
-  >;
+  using AccessType = AlignedArray<Element, Policy::kElementsPerAccess>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
   AccessType *pointer_;
 
   /// Internal layout object
-  Layout layout_;
+  TensorLayout layout_;
+
+  /// Thread offset
+  MatrixCoord thread_offset_;
 
-  /// Base smem offset;
-  Index base_smem_address_;
+public:
 
- public:
   /// Default constructor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirectConv() : pointer_(nullptr) {}
+  TileIteratorTensorOp(): pointer_(nullptr) { }
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirectConv(
+  TileIteratorTensorOp(
     TensorRef const &ref,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements) {
+    layout_(ref.stride()[0]) {
 
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+    int quad_id = (lane_id / Detail::kLanesInQuad); 
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
-    pointer_ += layout_({
-      lane_offset.row(),
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    thread_offset_ = {
+      quad_id, lane_in_quad * Policy::kElementsPerIteration
+    };
+
+    pointer_ += (layout_({thread_offset_.row(), thread_offset_.column()}) / Policy::kElementsPerAccess);
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirectConv & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
+  TileIteratorTensorOp & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / Policy::kElementsPerAccess;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirectConv & add_tile_offset(TensorCoord const &tile_offset) {
+  TileIteratorTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
 
-    pointer_ += layout_({
+    MatrixCoord coord_offset(
       tile_offset.row() * Shape::kRow, 
-      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
-    });
+      tile_offset.column() * Shape::kColumn
+    );
+
+    thread_offset_ += coord_offset;
+
+    pointer_ += (layout_({
+      coord_offset.row(),
+      coord_offset.column()
+    }) / Policy::kElementsPerAccess);
 
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirectConv & operator+=(TensorCoord const &tile_offset) {
-
+  TileIteratorTensorOp & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
-    
     return *this;
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    // original vector stores
+      
     AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
-    AccessType * load_pointer_ = reinterpret_cast<AccessType *>(reinterpret_cast<uint8_t *>(pointer_) + base_smem_address_);
+
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      load_pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
+    for (int n = 0; n < Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction; n++ ) {
+
+      AccessType *ptr = pointer_ + layout_({n * Policy::kRowsPerIteration, 0}) / Policy::kElementsPerAccess;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int a = 0; a < Policy::kAccessPerIteration; ++a) {
+        ptr[a + pointer_offset / Policy::kElementsPerAccess] = frag_ptr[n * Policy::kAccessPerIteration + a];
+
+//        printf("store thread %d, address %p, bank %ld\n", threadIdx.x, pointer_+a+n*Detail::kLanesInQuad, 
+//            ((long long)(pointer_+a+n*Detail::kLanesInQuad)>>2)&0x1f);
+      }
     }
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
@@ -383,217 +395,96 @@
   /// Load
   CUTLASS_HOST_DEVICE
   void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
     AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
+    for (int n = 0; n < Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction; n++ ) {
+
+      AccessType *ptr = pointer_ + layout_({n * Policy::kRowsPerIteration, 0}) / Policy::kElementsPerAccess;
+
+      CUTLASS_PRAGMA_UNROLL
+      for (int a = 0; a < Policy::kAccessPerIteration; ++a) {
+        frag_ptr[n * Policy::kAccessPerIteration + a] = ptr[a + pointer_offset / Policy::kElementsPerAccess];
+      }
     }
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
 
-  /// Set smem base address
   CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address){
-    base_smem_address_ = address;
+  TileIteratorTensorOp & operator++() {
+    return add_tile_offset({0, 1});
   }
-
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Template for reading and writing tiles of accumulators to shared memory
-template <typename WarpShape_,               ///< shape of warp-level GEMM (concept: GemmShape)
-          typename ThreadOutputShape_,       /// Size of the matrix to load (concept: TensorNHWC)
-          typename ThreadBlockOutputShape_,  /// Size of the matrix to load (concept: TensorNHWC)
-          typename Operator_,                ///< matrix multi ply operation (concept: arch::Mma)
-          typename Element_,                 ///< data type of element to be written
-          typename Layout_,                  ///< target shared memory layout
-          typename MmaSimtPolicy_            ///< policy defining lane arrangement (concept: MmaSimtPolicy)
-          >
-class TileIteratorSimtDirect2dConv {
- public:
-  using WarpShape = WarpShape_;
-  using ThreadOutputShape = ThreadOutputShape_;
-  using ThreadBlockOutputShape = ThreadBlockOutputShape_;
-  using Operator = Operator_;
-  using Element = Element_;
-  using Layout = layout::RowMajor;
-  using MmaSimtPolicy = MmaSimtPolicy_;
-
-  using TensorRef = TensorRef<Element, Layout>;  ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;               ///< Logical coordinate in referenced tensor
-  using Index = typename TensorRef::Index;
-  using LongIndex = typename TensorRef::LongIndex;
-
-  // Thread-level shape of a fragment
-  using ThreadShape = MatrixShape<ThreadOutputShape::kNHW, ThreadOutputShape::kC>;
-
-  static_assert(!(ThreadShape::kColumn % MmaSimtPolicy::LaneMmaShape::kN),
-                "Thread-level GEMM must be divisible by Policy::LaneMmaShape.");
-
-  using ThreadTileCount = MatrixShape<ThreadBlockOutputShape::kH / ThreadOutputShape::kH,
-                                      ThreadBlockOutputShape::kW / ThreadOutputShape::kW>;
-
-  using Iterations =
-      MatrixShape<ThreadShape::kRow, ThreadShape::kColumn / MmaSimtPolicy::LaneMmaShape::kN>;
-
-  /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = typename Operator::FragmentC;
-
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = AccumulatorTile;
-
-  /// Padding quantity
-  using Padding = MatrixShape<0, 0>;
-
- private:
-  // Storage type for accessing memory
-  using AccessType = AlignedArray<Element, MmaSimtPolicy::LaneMmaShape::kN>;
-  //
-  // Data members
-  //
-
-  /// Internal pointer to memory
-  AccessType *pointer_;
-
-  /// Internal layout object
-  Layout layout_;
-
-  /// Base smem offset;
-  Index base_smem_address_;
-
- public:
-  /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirect2dConv() : pointer_(nullptr) {}
-
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirect2dConv(TensorRef const &ref, unsigned thread_id, unsigned lane_id)
-      : pointer_(reinterpret_cast<AccessType *>(ref.data())),
-        layout_(ref.stride()[0] / AccessType::kElements) {
-  
-    auto lane_layout = MmaSimtPolicy::get_lane_layout();
-
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
-
-    // Get base HW offset of current threads
-    const int threadgroup = thread_id / (ThreadBlockOutputShape::kC / ThreadOutputShape::kC);
-    const int base_p = (threadgroup / (ThreadTileCount::kColumn)) * ThreadOutputShape::kH;
-    const int base_q = (threadgroup % (ThreadTileCount::kColumn)) * ThreadOutputShape::kW;
-
-    const int row_offset = base_p * ThreadBlockOutputShape::kW + base_q;
-
-    pointer_ += layout_(
-        {row_offset,
-         lane_offset.column() * MmaSimtPolicy::LaneMmaShape::kN / int(AccessType::kElements)});
-  }
-
-  /// Adds a pointer offset
-  CUTLASS_HOST_DEVICE
-  TileIteratorSimtDirect2dConv &add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
-    return *this;
-  }
-
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    AccessType *storer_pointer_ =
-        reinterpret_cast<AccessType *>(reinterpret_cast<uint8_t *>(pointer_) + base_smem_address_);
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int h = 0; h < ThreadOutputShape::kH; ++h) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int w = 0; w < ThreadOutputShape::kW; ++w) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int col = 0; col < Iterations::kColumn; ++col) {
-          int offset = (w + h * ThreadBlockOutputShape::kW) *
-                           (ThreadBlockOutputShape::kC / AccessType::kElements) +
-                       col;
-          storer_pointer_[offset + pointer_offset / int(AccessType::kElements)] =
-              frag_ptr[w + h * ThreadOutputShape::kW + col];
-        }
-      }
-    }
-  }
-
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) { store_with_pointer_offset(frag, 0); }
-
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) { base_smem_address_ = address; }
-};
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename WarpShape_,        ///< shape of warp-level GEMM (concept: GemmShape)
-  typename Operator_,         ///< matrix multiply operation (concept: arch::Mma)
-  typename Element_,          ///< data type of element to be written
-  typename Layout_,            ///< target shared memory layout
-  typename MmaSimtPolicy_     ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
+  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
+  typename Element_,       ///< data type of element to be written
+  typename Layout_
 >
-class TileIteratorSimtCanonical {
+class TileIteratorTensorOpCanonical {
 public:
 
   using WarpShape = WarpShape_;
-  using Operator = Operator_;
+  using OperatorShape = OperatorShape_;
   using Element = Element_;
   using Layout = Layout_;
 
   using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
   using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
   using Index = typename TensorRef::Index;
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
+
+  static int const kAccessSize = 1;
+  static int const kAccessCount = Policy::kElementsPerAccess / kAccessSize;
 
   /// Shape of the tile in memory
   using Shape = MatrixShape<
     Policy::kRowsPerIteration,
     WarpShape::kN
   >;
 
   /// This is the fragment size produced by one access of the iterator.
   using Fragment = Array<
-    typename Operator::ElementC, 
-    Policy::kElementsPerIteration>;
+    Element, 
+    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
 
   /// This is the complete warp-level accumulator tile.
-  using AccumulatorTile = Array<
-    typename Operator::ElementC, 
-    Policy::kAccumulatorElementCount>;
+  //using AccumulatorTile = typename Operator::FragmentC;
 
   /// Number of times this iterator can be incremented
   static int const kIterations = Policy::kIterations;
 
+  // Internal constants
+  struct Detail {
+    static int const kLanesInQuad = 4;
+  };
+
   /// Padding quantity
   using Padding = MatrixShape<
     0,
-    4 * Policy::kElementsPerAccess + 1
-  >;
+    Detail::kLanesInQuad * Policy::kElementsPerAccess>;
 
 private:
 
   /// Storage type for accessing memory
-  using AccessType = AlignedArray<
-    Element, 
-    1
-  >;
+  using AccessType = AlignedArray<Element, kAccessSize>;
 
   //
   // Data members
   //
 
   /// Internal pointer to memory
   AccessType *pointer_;
@@ -610,123 +501,110 @@
   /// Thread offset
   MatrixCoord thread_offset_;
 
 public:
 
   /// Default constructor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(): pointer_(nullptr) { }
+  TileIteratorTensorOpCanonical(): pointer_(nullptr) { }
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(
+  TileIteratorTensorOpCanonical(
     TensorRef const &ref,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements),
+    layout_(ref.stride()[0]),
     divisible_(true),
-    extent_(WarpShape::kM, WarpShape::kN) { 
+    extent_(WarpShape::kM, WarpShape::kN) {
 
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+    int quad_id = (lane_id / Detail::kLanesInQuad); 
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
     thread_offset_ = {
-      lane_offset.row() * Shape::kRow, 
-      lane_offset.column() * Policy::kElementsPerAccess
+      quad_id, lane_in_quad * Policy::kElementsPerAccess
     };
 
-    pointer_ += layout_({
-      lane_offset.row() * Shape::kRow,
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    pointer_ += layout_({thread_offset_.row(), thread_offset_.column()});
   }
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical(
+  TileIteratorTensorOpCanonical(
     TensorRef const &ref,
     TensorCoord const &extent,
     unsigned lane_id
   ):
     pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / AccessType::kElements),
+    layout_(ref.stride()[0]),
     divisible_(false),
-    extent_(extent) { 
+    extent_(extent) {
 
-    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
-    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+    int quad_id = (lane_id / Detail::kLanesInQuad); 
+    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
 
     thread_offset_ = {
-      lane_offset.row() * Shape::kRow, 
-      lane_offset.column() * Policy::kElementsPerAccess
+      quad_id, lane_in_quad * Policy::kElementsPerAccess
     };
 
-    pointer_ += layout_({
-      lane_offset.row() * Shape::kRow,
-      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
-    });
+    pointer_ += layout_({thread_offset_.row(), thread_offset_.column()});
   }
 
   /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / AccessType::kElements;
+  TileIteratorTensorOpCanonical & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & add_tile_offset(TensorCoord const &tile_offset) {
+  TileIteratorTensorOpCanonical & add_tile_offset(TensorCoord const &tile_offset) {
 
     MatrixCoord coord_offset(
-      tile_offset.row(), 
+      tile_offset.row() * Shape::kRow, 
       tile_offset.column() * Shape::kColumn
     );
 
     thread_offset_ += coord_offset;
 
     pointer_ += layout_({
-      coord_offset.row(), 
+      coord_offset.row(),
       coord_offset.column()
     });
 
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & operator+=(TensorCoord const &tile_offset) {
-
+  TileIteratorTensorOpCanonical & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
-    
     return *this;
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-    // de-vectorized stores
-    using ScalarAccessType = AlignedArray<Element, 1>;
-    ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
-    ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-        
-        int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
-        int frag_idx = n * Policy::kElementsPerAccess + s;
-        
-        int col = thread_offset_.column() + ptr_idx;
+      for (int a = 0; a < kAccessCount; ++a) {
+
+        int ptr_idx = n * Detail::kLanesInQuad * kAccessCount + pointer_offset + a;
+        int frag_idx = n * kAccessCount + a;
+
+        int col = thread_offset_.column() + n * Detail::kLanesInQuad * Policy::kElementsPerAccess + a;
 
         if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-          scalarPointer[ptr_idx] = scalarFragPtr[frag_idx];
+          pointer_[ptr_idx] = frag_ptr[frag_idx];
         }
       }
     }
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
@@ -734,52 +612,45 @@
     store_with_pointer_offset(frag, 0);
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
-      // de-vectorized loads
-      using ScalarAccessType = AlignedArray<Element, 1>;
-      ScalarAccessType *scalarFragPtr = reinterpret_cast<ScalarAccessType *>(&frag);
-      ScalarAccessType const *scalarPointer = reinterpret_cast<ScalarAccessType const*>(pointer_) + pointer_offset;
-
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
       CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
-        CUTLASS_PRAGMA_UNROLL
-        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
-          
-          int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
-          int frag_idx = n * Policy::kElementsPerAccess + s;
-          
-          int col = thread_offset_.column() + ptr_idx;
-
-          if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-            scalarFragPtr[frag_idx] = scalarPointer[ptr_idx];
-          }
+      for (int a = 0; a < kAccessCount; ++a) {
+
+        int ptr_idx = n * Detail::kLanesInQuad * kAccessCount + pointer_offset + a;
+        int frag_idx = n * kAccessCount + a;
+        
+        int col = thread_offset_.column() + n * Detail::kLanesInQuad * Policy::kElementsPerAccess + a;
+
+        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
+          frag_ptr[frag_idx] = pointer_[ptr_idx];
         }
       }
+    }
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
 
   CUTLASS_HOST_DEVICE
-  TileIteratorSimtCanonical & operator++() {
+  TileIteratorTensorOpCanonical & operator++() {
     return add_tile_offset({1, 0});
   }
-
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,648 +24,551 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief 
+    \brief Defines iterators used by warp-level loading scale and bias vectors.
+   Every scale/bias data only needs to be loaded once for every channel.
 */
 
 #pragma once
 
+#include "cutlass/cutlass.h"
+
 #include "cutlass/array.h"
+#include "cutlass/numeric_types.h"
 #include "cutlass/tensor_ref.h"
+#include "cutlass/matrix_shape.h"
+
+#include "cutlass/arch/memory_sm75.h"
+#include "cutlass/gemm/gemm.h"
+
 #include "cutlass/layout/matrix.h"
+#include "cutlass/layout/tensor.h"
 #include "cutlass/layout/pitch_linear.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
 
-#include "cutlass/epilogue/warp/tensor_op_policy.h"
+#include "cutlass/platform/platform.h"
+#include "cutlass/fast_math.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace epilogue {
+namespace gemm {
 namespace warp {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename OperatorShape, ///< matrix multiply operation shape (concept: gemm::GemmShape)
-  typename Element,       ///< data type of element to be written
-  typename Layout         ///< target shared memory layout
->
-class TileIteratorTensorOp;
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Template for reading and writing tiles of accumulators to shared memory
+    /// Size of the matrix to load (concept: MatrixShape)
+    typename Shape_,
+    /// Data type of A elements
+    typename Element_,
+    /// Layout of operand
+    typename Layout_,
+    /// Shape of one matrix production operation (concept: GemmShape)
+    typename InstructionShape_,
+    /// Policy of the details of LDSM shape and iterations
+    typename Policy_,
+    /// Number of threads participating in one matrix operation
+    int Threads,
+    /// Number of partitions along K dimension
+    int PartitionsK_ = 1>
+class ScaleBiasTileIterator;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// This tile iterator is specialized for 32-thread TensorOps. It uses LDSM to
+/// load from shared memory and therefore must be initialized with a TensorRef
+/// to shared memory.
+///
+/// Satisfies:
+///   ReadableRandomAccessContiguousTileIteratorConcept
+///
 template <
-  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
-  typename Element_        ///< data type of element to be written
->
-class TileIteratorTensorOp<WarpShape_, OperatorShape_, Element_, layout::RowMajor> {
-public:
+    /// Size of the matrix to load (concept: PitchLinearShape)
+    typename Shape_,
+    /// Data type of elements
+    typename Element_,
+    /// Shape of one matrix product operation (concept: PitchLinearShape)
+    typename InstructionShape_,
+    /// Policy of the details of LDSM shape and iterations
+    typename Policy_,
+    /// Number of partitions along K dimension
+    int PartitionsK_>
+class ScaleBiasTileIterator<Shape_, Element_, cutlass::layout::PitchLinear,
+                             InstructionShape_, Policy_, 32, PartitionsK_> {
+ public:
+  /// Shape of tile to load (concept: PitchLinearShape)
+  using Shape = Shape_;
 
-  using WarpShape = WarpShape_;
-  using OperatorShape = OperatorShape_;
+  /// Element type
   using Element = Element_;
-  using Layout = layout::RowMajor;
-
-  using TensorLayout = Layout;
-  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
-  using Index = typename TensorRef::Index;
-  using LongIndex = typename TensorRef::LongIndex;
-
-  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
-
-  /// Shape of the tile in memory
-  using Shape = MatrixShape<
-    Policy::kRowsPerIteration,
-    WarpShape::kN
-  >;
-
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = Array<
-    Element, 
-    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
-
-  /// This is the complete warp-level accumulator tile.
-  //using AccumulatorTile = typename Operator::FragmentC;
-
-  /// Number of times this iterator can be incremented
-  static int const kIterations = Policy::kIterations;
-
-  /// Number of times this iterator can be incremented
-  using TileIterations = typename Policy::TileIterations;
-
-  // Internal constants
-  struct Detail {
-    static int const kLanesInQuad = 4;
-  };
-
-  /// Padding quantity
-  using Padding = MatrixShape<
-    0,
-    Detail::kLanesInQuad * Policy::kElementsPerAccess>;
-
-private:
-
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<Element, Policy::kElementsPerAccess>;
-
-  //
-  // Data members
-  //
-
-  /// Internal pointer to memory
-  AccessType *pointer_;
-
-  /// Internal layout object
-  Layout layout_;
-
-  /// Thread offset
-  MatrixCoord thread_offset_;
-
-public:
-
-  /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp(): pointer_(nullptr) { }
-
-  /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp(
-    TensorRef const &ref,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0] / Policy::kElementsPerAccess) {
-
-    int quad_id = (lane_id / Detail::kLanesInQuad); 
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
-
-    thread_offset_ = {
-      quad_id, lane_in_quad * Policy::kElementsPerAccess
-    };
-
-    pointer_ += layout_({thread_offset_.row(), thread_offset_.column() / Policy::kElementsPerAccess});
-  }
-
-  /// Adds a pointer offset
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / Policy::kElementsPerAccess;
-    return *this;
-  }
-
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
-
-    MatrixCoord coord_offset(
-      tile_offset.row() * Shape::kRow, 
-      tile_offset.column() * Shape::kColumn
-    );
-
-    thread_offset_ += coord_offset;
-
-    pointer_ += layout_({
-      coord_offset.row(),
-      coord_offset.column() / Policy::kElementsPerAccess
-    });
 
-    return *this;
-  }
-
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
-    return *this;
-  }
-
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
-      pointer_[n * Detail::kLanesInQuad + pointer_offset / Policy::kElementsPerAccess] = frag_ptr[n];
-    }
-  }
-
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
-  }
-
-  /// Load
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+  /// Layout of source tile
+  using Layout = cutlass::layout::PitchLinear;
 
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+  /// Shape of one matrix product operation (concept: GemmShape)
+  using InstructionShape = InstructionShape_;
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
-      frag_ptr[n] = pointer_[n * Detail::kLanesInQuad + pointer_offset / Policy::kElementsPerAccess];
-    }
-  }
+  /// Number of participating threads
+  static int const kThreads = 32;
 
-  /// Load
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
 
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & operator++() {
-    return add_tile_offset({1, 0});
-  }
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
-};
+  /// Number of partitions along K dimension
+  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-/// Template for reading and writing tiles of accumulators to shared memory
-template <
-  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
-  typename Element_,       ///< data type of element to be written
-  int InterleavedK         ///< number of interleaved k
->
-class TileIteratorTensorOp<WarpShape_, OperatorShape_, Element_, 
-                            layout::ColumnMajorInterleaved<InterleavedK> > {
-public:
-
-  using WarpShape = WarpShape_;
-  using OperatorShape = OperatorShape_;
-  using Element = Element_;
-  using Layout = layout::ColumnMajorInterleaved<InterleavedK>;
-  using TensorLayout = Layout;                ///< shared memory tensor ref layout
-
-  using TensorRef = TensorRef<Element, TensorLayout>;         ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  /// Index type
   using Index = typename TensorRef::Index;
+
+  /// Long Index type
   using LongIndex = typename TensorRef::LongIndex;
 
-  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
-  /// Shape of the tile in memory
-  using Shape = MatrixShape<
-//    Policy::kRowsPerIteration,
-    WarpShape::kM,
-    InterleavedK
-  >;
-
-  /// This is the fragment size produced by one tile
-  using Fragment = Array<
-    Element, 
-    Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction 
-        * Policy::kElementsPerIteration>;
-
-  /// This is the fragment size produced by one iteration
-//  using Fragment = Array<
-//    Element, Policy::kElementsPerIteration >;
-
-  /// This is the complete warp-level accumulator tile.
-  //using AccumulatorTile = typename Operator::FragmentC;
-
-  /// Number of times this iterator can be incremented
-  using TileIterations = typename Policy::TileIterations;
-
-  // Internal constants
-  struct Detail {
-    static int const kLanesInQuad = 4;
-  };
-
-  /// Padding quantity
-  using Padding = MatrixShape<
-    0,
-    Detail::kLanesInQuad * Policy::kElementsPerIteration>;
+  /// Internal structure of iterator - made public to enable introspection
+  using Policy = Policy_;
 
-private:
+ private:
 
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<Element, Policy::kElementsPerAccess>;
+  /// Pointer type used for accesses
+  using AccessType = Array<Element, kElementsPerAccess>;
 
+ public:
   //
-  // Data members
+  // Derived quantities
   //
 
-  /// Internal pointer to memory
-  AccessType *pointer_;
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = Array<Element, 2 * Policy::kLdsmOpInner *
+                                      InstructionShape::kContiguous / kThreads>;
 
-  /// Internal layout object
-  TensorLayout layout_;
+ private:
 
-  /// Thread offset
-  MatrixCoord thread_offset_;
+  /// Shared memory base pointers - not advanced
+  AccessType const *pointer_;
 
-public:
+  /// Byte offset incremented as iterator advances
+  Index byte_offset_;
 
-  /// Default constructor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp(): pointer_(nullptr) { }
+  /// Internal counter used to determine when to increment byte offset and when
+  /// to XOR it
+  int k_group_idx_;
 
-  /// Constructor from TensorRef
+ public:
+  /// Default ctor constructs null iterator
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp(
-    TensorRef const &ref,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0]) {
-
-    int quad_id = (lane_id / Detail::kLanesInQuad); 
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
-
-    thread_offset_ = {
-      quad_id, lane_in_quad * Policy::kElementsPerIteration
-    };
+  ScaleBiasTileIterator()
+      : pointer_(nullptr),
+        byte_offset_(0),
+        k_group_idx_(0) {}
 
-    pointer_ += (layout_({thread_offset_.row(), thread_offset_.column()}) / Policy::kElementsPerAccess);
+  /// Constructor from TensorRef
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator(TensorRef const &ref_scale_bias,
+                         int lane_id)
+      : byte_offset_(0), k_group_idx_(0) {
+    /// 16816 only
+    pointer_ = reinterpret_cast<AccessType const *>(ref_scale_bias.data()) +
+               ((lane_id >> 3) & 1) * Shape::kContiguous / kElementsPerAccess +
+               (lane_id >> 4);
   }
 
-  /// Adds a pointer offset
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset / Policy::kElementsPerAccess;
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &add_pointer_offset(LongIndex offset) {
+    byte_offset_ += offset * sizeof_bits<Element>::value / 8;
+
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & add_tile_offset(TensorCoord const &tile_offset) {
-
-    MatrixCoord coord_offset(
-      tile_offset.row() * Shape::kRow, 
-      tile_offset.column() * Shape::kColumn
-    );
-
-    thread_offset_ += coord_offset;
-
-    pointer_ += (layout_({
-      coord_offset.row(),
-      coord_offset.column()
-    }) / Policy::kElementsPerAccess);
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &add_tile_offset(
+      TensorCoord const &tile_offset) {
+    int whole_tiles = tile_offset.contiguous() / Policy::kGroupsPerTile;
+    int k_groups_delta = tile_offset.contiguous() % Policy::kGroupsPerTile;
+
+    byte_offset_ += k_groups_delta * sizeof_bits<Element>::value *
+                    kElementsPerAccess * Policy::LdsmShape::kContiguous / 8;
+
+    // Multiply by 2 because scale and bias belonging to the same stage are next
+    // to each other in the shared memory.
+    pointer_ += (2 * whole_tiles * Shape::kContiguous / kElementsPerAccess);
 
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
+  /// Advances the iterator along the advance dimension
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &operator++() {
+    byte_offset_ += Policy::LdsmShape::kContiguous *
+                    sizeof_bits<Element>::value * kElementsPerAccess / 8;
+
+    k_group_idx_++;
+
+    if (k_group_idx_ == (Policy::kGroupsPerTile / kPartitionsK)) {
+      k_group_idx_ = 0;
+      byte_offset_ -= (Policy::kGroupsPerTile / kPartitionsK) *
+                      Policy::LdsmShape::kContiguous *
+                      sizeof_bits<Element>::value * kElementsPerAccess / 8;
+      add_tile_offset({Policy::kGroupsPerTile, 0});
+    }
+
     return *this;
   }
 
-  /// Store
+  /// Advances the iterator along the advance dimension
   CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-      
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction; n++ ) {
+  ScaleBiasTileIterator &operator--() { assert(0); }
 
-      AccessType *ptr = pointer_ + layout_({n * Policy::kRowsPerIteration, 0}) / Policy::kElementsPerAccess;
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int a = 0; a < Policy::kAccessPerIteration; ++a) {
-        ptr[a + pointer_offset / Policy::kElementsPerAccess] = frag_ptr[n * Policy::kAccessPerIteration + a];
-
-//        printf("store thread %d, address %p, bank %ld\n", threadIdx.x, pointer_+a+n*Detail::kLanesInQuad, 
-//            ((long long)(pointer_+a+n*Detail::kLanesInQuad)>>2)&0x1f);
-      }
-    }
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &operator+=(
+      TensorCoord const &tile_offset) {
+    add_tile_offset(tile_offset);
+    return *this;
   }
 
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &operator-=(
+      TensorCoord const &tile_offset) {
+    add_tile_offset(-tile_offset);
+    return *this;
   }
 
-  /// Load
+  /// Loads a fragment from memory at the location pointed to by the iterator.
   CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
+  void load(Fragment &frag) const { load_with_byte_offset(frag, 0); }
 
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset in units of bytes
+      Index byte_offset) const {
+    Array<unsigned, 4> *fetch_ptr =
+        reinterpret_cast<Array<unsigned, 4> *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kRow * Policy::kIterationsPerInstruction; n++ ) {
+    for (int s = 0; s < 1; ++s) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
+        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
 
-      AccessType *ptr = pointer_ + layout_({n * Policy::kRowsPerIteration, 0}) / Policy::kElementsPerAccess;
+        AccessType const *source_ptr =
+            pointer_ + Policy::LdsmShape::kContiguous * c;
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int a = 0; a < Policy::kAccessPerIteration; ++a) {
-        frag_ptr[n * Policy::kAccessPerIteration + a] = ptr[a + pointer_offset / Policy::kElementsPerAccess];
+        char const *source_byte_ptr =
+            reinterpret_cast<char const *>(source_ptr) + byte_offset +
+            byte_offset_;
+
+        cutlass::arch::ldsm<layout::RowMajor, 4>(
+            fetch_ptr[access_idx], source_byte_ptr);
       }
     }
   }
 
-  /// Load
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
-
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOp & operator++() {
-    return add_tile_offset({0, 1});
-  }
-
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset) const {
+    load_with_byte_offset(frag, tile_offset, 0);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index byte_offset) const {
+    Index pointer_offset = tile_offset.contiguous() *
+                               InstructionShape::kContiguous /
+                               kElementsPerAccess;
+
+    byte_offset += sizeof_bits<AccessType>::value * pointer_offset / 8;
+
+    load_with_byte_offset(frag, byte_offset);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    k_group_idx_ = k_group % (Policy::kGroupsPerTile / kPartitionsK);
   }
 };
 
+////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Template for reading and writing tiles of accumulators to shared memory
+/// This tile iterator is specialized for 32-thread TensorOps. It uses LDSM to
+/// load from shared memory and therefore must be initialized with a TensorRef
+/// to shared memory.
+///
+/// Satisfies:
+///   ReadableRandomAccessContiguousTileIteratorConcept
+///
 template <
-  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
-  typename OperatorShape_, ///< matrix multiply operation shape (concept: gemm::GemmShape)
-  typename Element_,       ///< data type of element to be written
-  typename Layout_
->
-class TileIteratorTensorOpCanonical {
-public:
+    /// Size of the matrix to load (concept: MatrixShape)
+    typename Shape_,
+    /// Data type of elements
+    typename Element_,
+    /// Shape of one matrix product operation (concept: MatrixShape)
+    typename InstructionShape_,
+    /// Policy of the details of LDSM shape and iterations
+    typename Policy_,
+    /// Number of partitions along K dimension
+    int PartitionsK_>
+class ScaleBiasTileIterator<Shape_, Element_, cutlass::layout::RowMajor,
+                             InstructionShape_, Policy_, 32, PartitionsK_> {
+ public:
+  /// Shape of tile to load (concept: PitchLinearShape)
+  using Shape = Shape_;
 
-  using WarpShape = WarpShape_;
-  using OperatorShape = OperatorShape_;
+  /// Element type
   using Element = Element_;
-  using Layout = Layout_;
 
-  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
-  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
-  using Index = typename TensorRef::Index;
-  using LongIndex = typename TensorRef::LongIndex;
-
-  using Policy = TensorOpPolicy<WarpShape, OperatorShape, Layout>;
+  /// Layout of source tile
+  using Layout = cutlass::layout::RowMajor;
 
-  static int const kAccessSize = 1;
-  static int const kAccessCount = Policy::kElementsPerAccess / kAccessSize;
+  /// Shape of one matrix product operation (concept: MatrixShape)
+  using InstructionShape = InstructionShape_;
 
-  /// Shape of the tile in memory
-  using Shape = MatrixShape<
-    Policy::kRowsPerIteration,
-    WarpShape::kN
-  >;
-
-  /// This is the fragment size produced by one access of the iterator.
-  using Fragment = Array<
-    Element, 
-    Policy::OperatorCount::kColumn * Policy::kElementsPerAccess>;
-
-  /// This is the complete warp-level accumulator tile.
-  //using AccumulatorTile = typename Operator::FragmentC;
-
-  /// Number of times this iterator can be incremented
-  static int const kIterations = Policy::kIterations;
-
-  // Internal constants
-  struct Detail {
-    static int const kLanesInQuad = 4;
-  };
-
-  /// Padding quantity
-  using Padding = MatrixShape<
-    0,
-    Detail::kLanesInQuad * Policy::kElementsPerAccess>;
+  /// Number of participating threads
+  static int const kThreads = 32;
 
-private:
+  /// TensorRef type for loading element from a tensor
+  using TensorRef = TensorRef<Element, Layout>;
 
-  /// Storage type for accessing memory
-  using AccessType = AlignedArray<Element, kAccessSize>;
+  /// Index type
+  using Index = typename TensorRef::Index;
 
-  //
-  // Data members
-  //
+  /// Long Index type
+  using LongIndex = typename TensorRef::LongIndex;
 
-  /// Internal pointer to memory
-  AccessType *pointer_;
+  /// Coordinate for an element in the tensor
+  using TensorCoord = typename TensorRef::TensorCoord;
 
-  /// Internal layout object
-  Layout layout_;
+  /// Internal structure of iterator - made public to enable introspection
+  using Policy = Policy_;
 
-  /// Guard to indicate whether the shape is divisible
-  bool divisible_;
+  /// Underlying tile iterator implementation
+  using Base = ScaleBiasTileIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::PitchLinear,
+      layout::PitchLinearShape<InstructionShape::kColumn,
+                               InstructionShape::kRow>,
+      Policy, kThreads, PartitionsK_>;
 
-  /// Extent of the output tensor
-  MatrixCoord extent_;
+ public:
+  //
+  // Derived quantities
+  //
 
-  /// Thread offset
-  MatrixCoord thread_offset_;
+  /// Fragment object holding a thread's part of a tile
+  using Fragment = typename Base::Fragment;
 
-public:
+ private:
+  /// Underlying tile iterator
+  Base iterator_;
 
-  /// Default constructor
+ public:
+  /// Default ctor constructs null iterator
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical(): pointer_(nullptr) { }
+  ScaleBiasTileIterator() {}
 
   /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical(
-    TensorRef const &ref,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0]),
-    divisible_(true),
-    extent_(WarpShape::kM, WarpShape::kN) {
-
-    int quad_id = (lane_id / Detail::kLanesInQuad); 
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
-
-    thread_offset_ = {
-      quad_id, lane_in_quad * Policy::kElementsPerAccess
-    };
-
-    pointer_ += layout_({thread_offset_.row(), thread_offset_.column()});
-  }
+  ScaleBiasTileIterator(TensorRef const &ref_scale_bias, int lane_id)
+      : iterator_({ref_scale_bias.data(), ref_scale_bias.stride()}, lane_id) {}
 
-  /// Constructor from TensorRef
+  /// Adds a pointer offset to internal pointer(s) to advance through memory
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical(
-    TensorRef const &ref,
-    TensorCoord const &extent,
-    unsigned lane_id
-  ):
-    pointer_(reinterpret_cast<AccessType *>(ref.data())),
-    layout_(ref.stride()[0]),
-    divisible_(false),
-    extent_(extent) {
-
-    int quad_id = (lane_id / Detail::kLanesInQuad); 
-    int lane_in_quad = (lane_id % Detail::kLanesInQuad);
-
-    thread_offset_ = {
-      quad_id, lane_in_quad * Policy::kElementsPerAccess
-    };
+  ScaleBiasTileIterator &add_pointer_offset(LongIndex offset) {
+    iterator_.add_pointer_offset(offset);
 
-    pointer_ += layout_({thread_offset_.row(), thread_offset_.column()});
-  }
-
-  /// Adds a pointer offset
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical & add_pointer_offset(Index pointer_offset) {
-    pointer_ += pointer_offset;
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical & add_tile_offset(TensorCoord const &tile_offset) {
+  ScaleBiasTileIterator &add_tile_offset(
+      TensorCoord const &tile_offset) {
+    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
 
-    MatrixCoord coord_offset(
-      tile_offset.row() * Shape::kRow, 
-      tile_offset.column() * Shape::kColumn
-    );
-
-    thread_offset_ += coord_offset;
+    return *this;
+  }
 
-    pointer_ += layout_({
-      coord_offset.row(),
-      coord_offset.column()
-    });
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &add_tile_offset_negative(
+      TensorCoord const &tile_offset) {
+    iterator_.add_tile_offset_negative({tile_offset.column(), tile_offset.row()});
 
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  /// Advances the iterator along the advance dimension
   CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical & operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
+  ScaleBiasTileIterator &operator++() {
+    ++iterator_;
+
     return *this;
   }
 
-  /// Store
+  /// Advances the iterator along the advance dimension
   CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int a = 0; a < kAccessCount; ++a) {
+  ScaleBiasTileIterator &operator--() {
+    --iterator_;
 
-        int ptr_idx = n * Detail::kLanesInQuad * kAccessCount + pointer_offset + a;
-        int frag_idx = n * kAccessCount + a;
-
-        int col = thread_offset_.column() + n * Detail::kLanesInQuad * Policy::kElementsPerAccess + a;
-
-        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-          pointer_[ptr_idx] = frag_ptr[frag_idx];
-        }
-      }
-    }
+    return *this;
   }
 
-  /// Store
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    store_with_pointer_offset(frag, 0);
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &operator+=(
+      TensorCoord const &tile_offset) {
+    add_tile_offset(PitchLinearCoord(tile_offset.column(), tile_offset.row()));
+    return *this;
   }
 
-  /// Load
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
-
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
-    
-    CUTLASS_PRAGMA_UNROLL
-    for (int n = 0; n < Policy::OperatorCount::kColumn; ++n) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int a = 0; a < kAccessCount; ++a) {
-
-        int ptr_idx = n * Detail::kLanesInQuad * kAccessCount + pointer_offset + a;
-        int frag_idx = n * kAccessCount + a;
-        
-        int col = thread_offset_.column() + n * Detail::kLanesInQuad * Policy::kElementsPerAccess + a;
-
-        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
-          frag_ptr[frag_idx] = pointer_[ptr_idx];
-        }
-      }
-    }
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
+  CUTLASS_DEVICE
+  ScaleBiasTileIterator &operator-=(
+      TensorCoord const &tile_offset) {
+    add_tile_offset(-PitchLinearCoord(tile_offset.column(), tile_offset.row()));
+    return *this;
   }
 
-  /// Load
+  /// Loads a fragment from memory at the location pointed to by the iterator.
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
+  void load(Fragment &frag) const { iterator_.load(frag); }
 
-  CUTLASS_HOST_DEVICE
-  TileIteratorTensorOpCanonical & operator++() {
-    return add_tile_offset({1, 0});
-  }
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index pointer_offset) const {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
+
+  /// Loads a fragment from memory with additional logical offset
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index byte_offset) const {
+    iterator_.load_with_byte_offset(frag, byte_offset);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset) const {
+    // TODO
+    assert(0);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    // TODO
+    assert(0);
+  }
+
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
+  CUTLASS_DEVICE
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index byte_offset) const {
+    iterator_.load_with_byte_offset(
+        frag, {tile_offset.strided(), tile_offset.contiguous()}, byte_offset);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    iterator_.set_kgroup_index(k_group); 
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
-} // namespace epilogue
+} // namespace gemm 
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_tensor_op_mixed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,16 +60,15 @@
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape)
   typename Element_,              ///< data type of accumulator element
   int ElementSizeBits,            ///< Size of accumulator element in bits
   int OutputSizeBits,             ///< Size of output element in bits
   int OutputElementCount,         ///< number of elements in output vector
-  int ContiguousLanes,            ///< Number of consecutive lanes writing to contiguous memory
-  bool EightBitsOutputOrLess = (OutputSizeBits <= 8)
+  int ContiguousLanes             ///< Number of consecutive lanes writing to contiguous memory
 >
 class TileIteratorTensorOpMixed {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = Element_;
@@ -301,30 +300,25 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 16 => int8_t/int4b_t x 16
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape),
   int OutputSizeBits              ///< Size of output element in bits
 >
-class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 16, 8, true> {
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 16, 8> {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = int32_t;
   using Layout = layout::RowMajor;
   static int const kOutputElementCount = 16;
@@ -508,30 +502,25 @@
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
-
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for int32_t x 8 => int8_t/int4b_t x 8
 template <
   typename WarpShape_,            ///< shape of warp-level GEMM (concept: GemmShape)
   typename OperatorShape_,        ///< matrix multiply operation shape (concept: gemm::GemmShape)
   int OutputSizeBits              ///< Size of output element in bits
 >
-class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 8, 8, true> {
+class TileIteratorTensorOpMixed<WarpShape_, OperatorShape_, int32_t, 32, OutputSizeBits, 8, 8> {
 public:
 
   using WarpShape = WarpShape_;
   using OperatorShape = OperatorShape_;
   using Element = int32_t;
   using Layout = layout::RowMajor;
   static int const kOutputElementCount = 8;
@@ -704,19 +693,14 @@
   }
 
   /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
-
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_volta_tensor_op.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -238,19 +238,14 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment const &frag) {
     load_with_pointer_offset(frag, 0);
   }
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Template for reading and writing tiles of accumulators to shared memory
 template <
   typename WarpShape_         ///< shape of warp-level GEMM (concept: MatrixShape)
@@ -420,19 +415,14 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment const &frag) {
     load_with_pointer_offset(frag, 0);
   }
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_wmma_tensor_op.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -203,20 +203,14 @@
   }
 
   /// Load
   CUTLASS_HOST_DEVICE
   void load(Fragment &frag) const {
     load_with_pointer_offset(frag, 0);
   }
-
-  
-  /// Set smem base address
-  CUTLASS_HOST_DEVICE
-  void set_smem_base_address(Index address) {
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace epilogue
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/volta_tensor_op_policy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/wmma_tensor_op_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,78 +24,113 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Defines basic structures needed for implementing the warp-scoped phase of the epilogue.
-          These quantities assume a 'column-major' arrangement of TensorOp instructions, of which
-          a row-oriented slice is visible per iteration.
+    \brief Unit tests Generic CuTe Layouts
 */
 
-#pragma once
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/layout/layout.h"
+#include "cutlass/matrix_coord.h"
+
+// Cute includes
+#include <cute/layout.hpp>
+#include <cute/int_tuple.hpp>
+
+using namespace cutlass;
+using namespace cute;
+
+namespace test {
+namespace layout {
+
+template <typename GenericLayout, typename Layout> 
+  struct Testbed {
+
+
+    Testbed() {}
+
+    bool run() {
+      GenericLayout generic_layout;
+      Layout layout = Layout::packed({size<0>(generic_layout), size<1>(generic_layout)});
+
+      for (int m = 0; m < size<0>(generic_layout); m++) {
+        for (int n = 0; n < size<1>(generic_layout); n++) {
+          if (generic_layout(m, n) != layout({m, n})) return false;
+        }
+      }
+
+      return true;
+    }
+  };
+
+}
+}
+
+//////////////////////////////////////////////////////////////////////////
+//                      Test Generic CuTe Layouts
+//////////////////////////////////////////////////////////////////////////
+
+/// Canonical Layouts
+
+TEST(GenericLayout, ColumnMajor) {
+  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_1, _8>>;
+  using Layout = cutlass::layout::ColumnMajor;
 
-#include "cutlass/arch/wmma.h"
-#include "cutlass/matrix_shape.h"
-#include "cutlass/layout/matrix.h"
+  test::layout::Testbed<GenericLayout, Layout> testbed;
 
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+  EXPECT_TRUE(testbed.run());
+}
+//////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
+TEST(GenericLayout, RowMajor) {
+  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_4, _1>>;
+  using Layout = cutlass::layout::RowMajor;
 
-namespace cutlass {
-namespace epilogue {
-namespace warp {
+  test::layout::Testbed<GenericLayout, Layout> testbed;
 
-////////////////////////////////////////////////////////////////////////////////
+  EXPECT_TRUE(testbed.run());
+}
+//////////////////////////////////////////////////////////////////////////
 
-/// Policy details related to the epilogue
-template <
-  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename OperatorShape, ///< matrix multiply operation shape (concept: gemm:GemmShape)
-  typename Layout         ///< target shared memory layout
->
-struct WmmaTensorOpPolicy; 
 
-////////////////////////////////////////////////////////////////////////////////
+/// Swizzle Shared Memory layouts
 
-/// Partial specialization for row-major
-template <
-  typename WarpShape,           ///< shape of warp-level GEMM (concept: MatrixShape)
-  typename OperatorShape        ///< matrix multiply operation shape (concept: gemm::GemmShape)
->
-struct WmmaTensorOpPolicy<WarpShape, OperatorShape, layout::RowMajor> {
+TEST(GenericLayout, RowMajorTensorOpMultiplicandCrosswise) {
 
-  /// Number of operations
-  using OperatorCount = MatrixShape<
-    WarpShape::kM / OperatorShape::kM,
-    WarpShape::kN / OperatorShape::kN
-  >;
+  using GenericLayout = decltype(
+        composition(
+          Swizzle<3,3,3>{},
+          Layout<Shape<_128, _64>, Stride<_64, _1>>{})
+  );
 
-  //
-  // Hard-coded constants regarding Tensor Operations
-  //
-  static int const kElementsPerAccess = 2;
-  static int const kRowsPerIteration = OperatorShape::kM;
-  static int const kWmmaFragmentsPerAccess = 1;
+  using Layout = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<
+      cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
 
-  //
-  // Derived quantities
-  //
+  test::layout::Testbed<GenericLayout, Layout> testbed;
 
-  // Number of externally visible iterations
-  static int const kIterations = OperatorCount::kRow;
+  EXPECT_TRUE(testbed.run());
+}
+//////////////////////////////////////////////////////////////////////////
 
-};
+TEST(GenericLayout, ColumnMajorTensorOpMultiplicandCongruous) {
 
-////////////////////////////////////////////////////////////////////////////////
+  using GenericLayout = decltype(
+        composition(
+          Swizzle<3,3,4>{},
+          Layout<Shape<_128, _64>>{})
+  );
 
-} // namespace warp
-} // namespace epilogue
-} // namespace cutlass
+  using Layout = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
+    cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
 
-////////////////////////////////////////////////////////////////////////////////
 
-#endif
+  test::layout::Testbed<GenericLayout, Layout> testbed;
 
+  EXPECT_TRUE(testbed.run());
+}
+//////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/fast_math.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/fast_math.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -276,113 +276,60 @@
 ///
 struct FastDivmod {
 
   int divisor;
   unsigned int multiplier;
   unsigned int shift_right;
 
-  /// Find quotient and remainder using device-side intrinsics
-  CUTLASS_HOST_DEVICE
-  void fast_divmod(int& quotient, int& remainder, int dividend) const {
-
-#if defined(__CUDA_ARCH__)
-    // Use IMUL.HI if divisor != 1, else simply copy the source.
-    quotient = (divisor != 1) ? __umulhi(dividend, multiplier) >> shift_right : dividend;
-#else
-    quotient = int((divisor != 1) ? int(((int64_t)dividend * multiplier) >> 32) >> shift_right : dividend);
-#endif
-
-    // The remainder.
-    remainder = dividend - (quotient * divisor);
-  }
-
-  /// For long int input
-  CUTLASS_HOST_DEVICE
-  void fast_divmod(int& quotient, int64_t& remainder, int64_t dividend) const {
-
-#if defined(__CUDA_ARCH__)
-    // Use IMUL.HI if divisor != 1, else simply copy the source.
-    quotient = (divisor != 1) ? __umulhi(dividend, multiplier) >> shift_right : dividend;
-#else
-    quotient = int((divisor != 1) ? ((dividend * multiplier) >> 32) >> shift_right : dividend);
-#endif
-    // The remainder.
-    remainder = dividend - (quotient * divisor);
-  }
-
-
   /// Construct the FastDivmod object, in host code ideally.
   ///
   /// This precomputes some values based on the divisor and is computationally expensive.
 
   CUTLASS_HOST_DEVICE
   FastDivmod(): divisor(0), multiplier(0), shift_right(0) { }
 
   CUTLASS_HOST_DEVICE
-  FastDivmod(int divisor): divisor(divisor) {
-
-    if (divisor != 1) {
-      unsigned int p = 31 + find_log2(divisor);
-      unsigned m = unsigned(((1ull << p) + unsigned(divisor) - 1) / unsigned(divisor));
-
-      multiplier = m;
-      shift_right = p - 32;
-    } else {
-      multiplier = 0;
-      shift_right = 0;
-    }
+  FastDivmod(int divisor_): divisor(divisor_) {
+    find_divisor(multiplier, shift_right, divisor);
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   void operator()(int &quotient, int &remainder, int dividend) const {
-    fast_divmod(quotient, remainder, dividend);
+    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
   }
 
-  /// Computes integer division using precomputed values. This is computationally
-  /// inexpensive.
-  CUTLASS_HOST_DEVICE
-  int div(int dividend) const {
-    int quotient, remainder;
-    fast_divmod(quotient, remainder, dividend);
-    return quotient;
-  }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   ///
   /// Simply returns the quotient
   CUTLASS_HOST_DEVICE
   int divmod(int &remainder, int dividend) const {
     int quotient;
-    fast_divmod(quotient, remainder, dividend);
+    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
     return quotient;
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   void operator()(int &quotient, int64_t &remainder, int64_t dividend) const {
-    fast_divmod(quotient, remainder, dividend);
+    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
   }
 
   /// Computes integer division and modulus using precomputed values. This is computationally
   /// inexpensive.
   CUTLASS_HOST_DEVICE
   int divmod(int64_t &remainder, int64_t dividend) const {
     int quotient;
-    fast_divmod(quotient, remainder, dividend);
+    fast_divmod(quotient, remainder, dividend, divisor, multiplier, shift_right);
     return quotient;
   }
-
-  /// Returns the divisor when cast to integer
-  CUTLASS_HOST_DEVICE
-  operator int() const { return divisor; }
-
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Object to encapsulate the fast division+modulus operation for 64b integer division.
 ///
 /// This object precomputes two values used to accelerate the computation and is best used
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/float8.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/float8.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/floating_point_nvrtc.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/base_grouped.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -207,15 +207,15 @@
 
     return dim3(args.threadblock_count, 1, 1);
   }
 
   /// Computes the maximum number of active blocks per multiprocessor
   static int maximum_active_blocks(int smem_capacity = -1) {
 
-    CUTLASS_TRACE_HOST("BaseGrouped::maximum_active_blocks()");
+    CUTLASS_TRACE_HOST("GemmUniversalBase::maximum_active_blocks()");
 
     int smem_size = int(sizeof(typename BaseKernel::SharedStorage));
 
     CUTLASS_TRACE_HOST("  smem_size: " << smem_size << " bytes");
 
     cudaError_t result;
     if (smem_size > (48 << 10)) {
@@ -338,22 +338,22 @@
 
     // Choose between the full wave of threadblocks and the tile count. If there
     // are fewer tiles in the group than threadblocks in the full wave, only
     // some threadblocks will be assigned tiles. Those threadblocks
     // which are not assigned tiles still need to perform the work of iterating through
     // problem sizes to determine that they have no work to do. This competes for cycles
     // with those threadblocks that are assigned tiles to compute.
-    return std::min(total_tiles, occupancy_based_block_count);
+    return min(total_tiles, occupancy_based_block_count);
   }
 
 
   /// Initializes GEMM state from arguments.
   Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
-    CUTLASS_TRACE_HOST("BaseGrouped::initialize() - workspace "
+    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace "
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
     // Workspace
     size_t workspace_bytes = get_workspace_size(args);
 
     if (workspace_bytes && !workspace) {
       return Status::kErrorWorkspaceNull;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/default_gemm_configuration.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -761,58 +761,12 @@
 
   using Operator = arch::OpMultiplyAdd;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 ////////////////////////////////////////////////////////////////////////////////
-
-template <typename ElementC,
-          typename ElementAccumulator>
-struct DefaultGemmConfiguration<arch::OpClassTensorOp, arch::Sm90, double,
-                                double, ElementC, ElementAccumulator> {
-
-  static int const kAlignmentA = 1;
-  static int const kAlignmentB = 1;
-  
-  using ThreadblockShape = GemmShape<128, 256, 64>;
-  using WarpShape = GemmShape<64, 64, 64>;
-  using InstructionShape = GemmShape<16, 8, 4>;
-  static int const kStages = 3;
-
-  using EpilogueOutputOp = epilogue::thread::LinearCombination<
-      ElementC, 128 / sizeof_bits<ElementC>::value, ElementAccumulator,
-      ElementAccumulator>;
-
-  using Operator = arch::OpMultiplyAdd;
-};
-
-template <>
-struct DefaultGemmConfiguration<
-    arch::OpClassTensorOp, 
-    arch::Sm90, 
-    complex<double>,
-    complex<double>, 
-    complex<double>,
-    complex<double>
-  > {
-
-  static int const kAlignmentA = 1;
-  static int const kAlignmentB = 1;
-  
-  using ThreadblockShape = GemmShape<64, 64, 16>;
-  using WarpShape = GemmShape<32, 32, 16>;
-  using InstructionShape = GemmShape<16, 8, 4>;
-  static int const kStages = 3;
-
-  using EpilogueOutputOp = epilogue::thread::LinearCombination<
-      complex<double>, 1, complex<double>,
-      complex<double>>;
-
-  using Operator = arch::OpMultiplyAddComplex;
-};
-
 } // namespace device
 } // namespace gemm
 } // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -505,27 +505,27 @@
 
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
     cudaStream_t stream = nullptr) {
     
-    Status status = initialize(args, workspace, stream);
+    Status status = initialize(args, workspace);
     
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_array.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -472,15 +472,15 @@
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
   /// Element type for A matrix operand
   typename ElementA_,
   /// Layout type for A matrix operand
   typename LayoutA_,
   /// Element type for B matrix operand
   typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_batched.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Template for a pipelined batch GEMM kernel.
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
@@ -450,15 +450,15 @@
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
   /// Element type for A matrix operand
   typename ElementA_,
   /// Layout type for A matrix operand
   typename LayoutA_,
   /// Element type for B matrix operand
   typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -471,15 +471,15 @@
 
     return status;
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
   /// Element type for A matrix operand
   typename ElementA_,
   /// Layout type for A matrix operand
   typename LayoutA_,
   /// Element type for B matrix operand
   typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_grouped.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_layernorm_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -190,15 +190,15 @@
 
   using Arguments = typename Base::Arguments;
   using GemmKernel = typename Base::GemmKernel;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_splitk_parallel.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -54,19 +54,14 @@
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /*! 
-  GemmUniversal is a stateful, reusable GEMM handle.  Once initialized for a given GEMM computation
-  (problem geometry and data references), it can be reused across different GEMM problems having the
-  geometry.  (Once initialized, details regarding problem geometry and references to workspace memory
-  cannot be updated.)
-
   The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
   batched array variants.
 */
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
@@ -215,15 +210,15 @@
 
   using Arguments = typename Base::Arguments;
   using GemmKernel = typename Base::GemmKernel;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_adapter.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,526 +24,675 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! 
-  \file
-  \brief The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
-    batched array variants.
+
+/*! \file
+    \brief 
 */
 
 #pragma once
 
-// common
 #include "cutlass/cutlass.h"
-#include "cutlass/trace.h"
-#include "cutlass/cluster_launch.hpp"
-#include "cutlass/device_kernel.h"
+#include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/complex.h"
+#include "cutlass/semaphore.h"
 
-// 2.x
-#include "cutlass/gemm/device/gemm_universal_base.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
+#include "cutlass/trace.h"
 
-// 3.x
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-////////////////////////////////////////////////////////////////////////////////
+namespace cutlass {
+namespace gemm {
+namespace kernel {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+template <
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Epilogue_,             ///! Epilogue
+  typename EpilogueGemmKReduction_,             ///! Epilogue
+  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
+>
+struct GemmWithKReduction {
+public:
 
-namespace cutlass::gemm::device {
+  using Mma = Mma_;
+  using Epilogue = Epilogue_;
+  using EpilogueOutputOp = typename Epilogue::OutputOp;
+  using EpilogueGemmKReduction = EpilogueGemmKReduction_;
+  using ThreadblockSwizzle = ThreadblockSwizzle_;
+
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
+  using ElementC = typename Epilogue::OutputTileIterator::Element;
+  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
+  using LayoutGemmKReduction = cutlass::layout::PitchLinear;
+
+  static ComplexTransform const kTransformA = Mma::kTransformA;
+  static ComplexTransform const kTransformB = Mma::kTransformB;
+  using Operator = typename Mma::Operator;
+
+  using OperatorClass = typename Mma::Operator::OperatorClass;
+  using ThreadblockShape = typename Mma::Shape;
+  using WarpShape = typename Mma::Operator::Shape;
+  using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
+  using ArchTag = typename Mma::ArchTag;
+
+  static int const kStages = Mma::kStages;
+  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
+  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
+  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+
+  /// Warp count (concept: GemmShape)
+  using WarpCount = typename Mma::WarpCount;
+  static int const kThreadCount = 32 * WarpCount::kCount;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Split-K preserves splits that are 128b aligned
+  static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
-/*! 
-  GemmUniversalAdapter is a stateful, reusable GEMM handle built around a kernel
-  of type cutlass::gemm::kernel::Gemm or cutlass::gemm::kernel::GemmUniversal.
+  static int const kReduceKForA = Mma::kReduceKForA;
 
-  It manages the lifetime of the underlying `kernel::Params` struct, and exposes APIs
-  to create it from the host facing arguments. For power users, new static methods
-  are exposed in 3.x APIs that bypass the stateful methods or args->params lowering.
+  //
+  // Structures
+  //
 
-  It supports kernel types that implement both the 2.x and 3.0 APIs,
-  however, this is done by specializing the implementation of GemmUniversalAdapter
-  on the two kernel API types, and thus, GemmUniversalAdapter's behaviour might
-  differ between the two specializations.
-*/
-template <class GemmKernel_, class Enable = void>
-class GemmUniversalAdapter;
+  /// Argument structure
+  struct Arguments {
 
-////////////////////////////////////////////////////////////////////////////////
-////////////////////////////// CUTLASS 3.x API /////////////////////////////////
-////////////////////////////////////////////////////////////////////////////////
-
-template <class GemmKernel_>
-class GemmUniversalAdapter<
-  GemmKernel_,
-  std::enable_if_t<gemm::detail::IsCutlass3GemmKernel<GemmKernel_>::value>>
-{
-public:
-  using GemmKernel = GemmKernel_;
-  using TileShape = typename GemmKernel::TileShape;
-  using ElementA = typename GemmKernel::ElementA;
-  using ElementB = typename GemmKernel::ElementB;
-  using ElementC = typename GemmKernel::ElementC;
-  using ElementAccumulator = typename GemmKernel::TiledMma::ValTypeC;
-  using DispatchPolicy = typename GemmKernel::DispatchPolicy;
-  using CollectiveMainloop = typename GemmKernel::CollectiveMainloop;
-  using CollectiveEpilogue = typename GemmKernel::CollectiveEpilogue;
-
-  // Map back to 2.x type as best as possible
-  using LayoutA = gemm::detail::StrideToLayoutTagA_t<typename GemmKernel::StrideA>;
-  using LayoutB = gemm::detail::StrideToLayoutTagB_t<typename GemmKernel::StrideB>;
-  using LayoutC = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideC>;
-  using LayoutD = gemm::detail::StrideToLayoutTagC_t<typename GemmKernel::StrideD>;
-
-  // NOTE: 3.0 kernels do not support complex transforms for now ...
-  static ComplexTransform const kTransformA = ComplexTransform::kNone;
-  static ComplexTransform const kTransformB = ComplexTransform::kNone;
-
-  // Legacy: Assume MultiplyAdd only since we do not use this tag type in 3.0
-  using MathOperator = cutlass::arch::OpMultiplyAdd;
-
-  // If our TiledMMA's instruction thread layout size is larger than 1, we know its a tensorop!
-  using OperatorClass = std::conditional_t<
-      (cute::size(typename GemmKernel::TiledMma::AtomThrID{}) > 1),
-      cutlass::arch::OpClassTensorOp, cutlass::arch::OpClassSimt>;
-
-  using ArchTag = typename GemmKernel::ArchTag;
-
-  // NOTE: Assume identity swizzle for now
-  static_assert(std::is_void_v<typename GemmKernel::GridSwizzle>,
-    "CUTLASS 3.x kernel types do not support grid swizzle functors yet.");
-  using ThreadblockSwizzle = cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>;
-
-  // Assume TiledMma's ShapeMNK is the same as 2.x's ThreadblockShape
-  using ThreadblockShape = cutlass::gemm::GemmShape<
-      cute::size<0>(TileShape{}),
-      cute::size<1>(TileShape{}),
-      cute::size<2>(TileShape{})>;
-
-  using ClusterShape = cutlass::gemm::GemmShape<
-      cute::size<0>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
-      cute::size<1>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
-      cute::size<2>(typename GemmKernel::DispatchPolicy::ClusterShape{})>;
-
-  // Instruction shape is easy too, since we get that directly from our TiledMma's atom shape
-  using InstructionShape = cutlass::gemm::GemmShape<
-      cute::size<0>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{}),
-      cute::size<1>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{}),
-      cute::size<2>(typename CollectiveMainloop::TiledMma::AtomShape_MNK{})>;
-
-  // Legacy: provide a correct warp count, but no reliable warp shape
-  static int const kThreadCount = GemmKernel::MaxThreadsPerBlock;
-
-  // Warp shape is not a primary API type in 3.x
-  // But we can best approximate it by inspecting the TiledMma::TiledShape_MNK
-  // For this, we make the assumption that we always have 4 warps along M, and rest along N, none along K
-  // We also always round up the warp count to 4 if the tiled mma is smaller than 128 threads
-  static constexpr int WarpsInMma = std::max(4, cute::size(typename GemmKernel::TiledMma{}) / 32);
-  static constexpr int WarpsInMmaM = 4;
-  static constexpr int WarpsInMmaN = cute::ceil_div(WarpsInMma, WarpsInMmaM);
-  using WarpCount = cutlass::gemm::GemmShape<WarpsInMmaM, WarpsInMmaN, 1>;
-  using WarpShape = cutlass::gemm::GemmShape<
-      cute::size<0>(typename CollectiveMainloop::TiledMma::TiledShape_MNK{}) / WarpsInMmaM,
-      cute::size<1>(typename CollectiveMainloop::TiledMma::TiledShape_MNK{}) / WarpsInMmaN,
-      cute::size<2>(typename CollectiveMainloop::TiledMma::TiledShape_MNK{})>;
-
-  static int constexpr kStages = CollectiveMainloop::DispatchPolicy::Stages;
-
-  // Inspect TiledCopy for A and B to compute the alignment size
-  static int constexpr kAlignmentA = gemm::detail::get_alignment_count_from_gmem_tiled_copy<
-      typename CollectiveMainloop::GmemTiledCopyA, ElementA>();
-  static int constexpr kAlignmentB = gemm::detail::get_alignment_count_from_gmem_tiled_copy<
-      typename CollectiveMainloop::GmemTiledCopyB, ElementB>();
-
-  // NOTE: 3.0 DefaultEpilogues don't support vectorized stores (yet)
-  static int constexpr kAlignmentC = 1;
-  static int constexpr kAlignmentD = 1;
-
-  using EpilogueOutputOp = typename CollectiveEpilogue::ThreadEpilogueOp;
-
-  // Split-K preserves splits that are 128b aligned
-  static int constexpr kSplitKAlignment = std::max(
-      128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
-
-  /// Argument structure: User API
-  using Arguments = typename GemmKernel::Arguments;
-  /// Argument structure: Kernel API
-  using Params = typename GemmKernel::Params;
+    //
+    // Data members
+    //
+
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
+    typename EpilogueOutputOp::Params epilogue;
+
+    void const * ptr_A;
+    void const * ptr_B;
+    void const * ptr_C;
+    void * ptr_D;
+    void * ptr_gemm_k_reduction;
+
+    int64_t batch_stride_A;
+    int64_t batch_stride_B;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+    int64_t batch_stride_gemm_k_reduction;
+
+    typename LayoutA::Stride::Index lda;
+    typename LayoutB::Stride::Index ldb;
+    typename LayoutC::Stride::Index ldc;
+    typename LayoutC::Stride::Index ldd;
+    typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction;
+
+    //
+    // Methods
+    //
+    
+    Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr), ptr_gemm_k_reduction(nullptr) { }
+
+    /// constructs an arguments structure
+    Arguments(
+      GemmUniversalMode mode,
+      GemmCoord problem_size,
+      int batch_count,
+      typename EpilogueOutputOp::Params epilogue,
+      void const * ptr_A,
+      void const * ptr_B,
+      void const * ptr_C,
+      void * ptr_D,
+      void * ptr_gemm_k_reduction,
+      int64_t batch_stride_A,
+      int64_t batch_stride_B,
+      int64_t batch_stride_C,
+      int64_t batch_stride_D,
+      int64_t batch_stride_gemm_k_reduction,
+      typename LayoutA::Stride::Index lda,
+      typename LayoutB::Stride::Index ldb,
+      typename LayoutC::Stride::Index ldc,
+      typename LayoutC::Stride::Index ldd,
+      typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
+      epilogue(epilogue), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), ptr_gemm_k_reduction(ptr_gemm_k_reduction), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), batch_stride_gemm_k_reduction(batch_stride_gemm_k_reduction),
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ld_gemm_k_reduction(ld_gemm_k_reduction) {
 
-private:
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
+      }
 
-  /// Kernel API parameters object
-  Params params_;
+    /// Returns arguments for the transposed problem
+    Arguments transposed_problem() const {
+      Arguments args(*this);
+      
+      std::swap(args.problem_size.m(), args.problem_size.n());
+      std::swap(args.ptr_A, args.ptr_B);
+      std::swap(args.lda, args.ldb);
+      std::swap(args.batch_stride_A, args.batch_stride_B);
 
-public:
+      return args;
+    }
+  };
 
-  /// Determines whether the GEMM can execute the given problem.
-  static Status
-  can_implement(Arguments const& args) {
-    if (GemmKernel::can_implement(args)) {
-      return Status::kSuccess;
+  //
+  // Structure for precomputing values in host memory and passing to kernels
+  //
+
+  /// Parameters structure
+  struct Params {
+
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+    
+    typename Mma::IteratorA::Params params_A;
+    typename Mma::IteratorB::Params params_B;
+    typename Epilogue::OutputTileIterator::Params params_C;
+    typename Epilogue::OutputTileIterator::Params params_D;
+    
+    typename EpilogueOutputOp::Params output_op;
+
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
+    void * ptr_A;
+    void * ptr_B;
+    void * ptr_C;
+    void * ptr_D;
+    void * ptr_gemm_k_reduction;
+
+    int64_t batch_stride_A;
+    int64_t batch_stride_B;
+    int64_t batch_stride_C;
+    int64_t batch_stride_D;
+    int64_t batch_stride_gemm_k_reduction;
+
+    int *semaphore;
+
+    //
+    // Methods
+    //
+
+    CUTLASS_HOST_DEVICE
+    Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_B(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_gemm_k_reduction(nullptr),
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      batch_stride_gemm_k_reduction(0),
+      semaphore(nullptr) { }
+
+    CUTLASS_HOST_DEVICE
+    Params(
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
+      params_A(args.lda),
+      params_B(args.ldb),
+      params_C(args.ldc),
+      params_D(args.ldd),
+      output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
+      ptr_A(const_cast<void *>(args.ptr_A)),
+      ptr_B(const_cast<void *>(args.ptr_B)),
+      ptr_C(const_cast<void *>(args.ptr_C)),
+      batch_stride_A(args.batch_stride_A),
+      batch_stride_B(args.batch_stride_B),
+      batch_stride_C(args.batch_stride_C),
+      batch_stride_D(args.batch_stride_D),
+      batch_stride_gemm_k_reduction(args.batch_stride_gemm_k_reduction),
+      semaphore(static_cast<int *>(workspace)) {
+
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::Params() - problem_size: " << problem_size);
+
+      if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {
+        ptr_D = workspace;
+        ptr_gemm_k_reduction = static_cast<uint8_t *>(workspace)
+                 + sizeof(ElementC) * size_t(args.batch_stride_D) * size_t(grid_tiled_shape.k());
+      } else {
+        ptr_D = args.ptr_D;
+        ptr_gemm_k_reduction = args.ptr_gemm_k_reduction;
+      }
     }
-    else {
-      return Status::kInvalid;
+
+    CUTLASS_HOST_DEVICE
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
+
+      ptr_A = const_cast<void *>(args.ptr_A);
+      ptr_B = const_cast<void *>(args.ptr_B);
+      ptr_C = const_cast<void *>(args.ptr_C);
+      ptr_D = args.ptr_D;
+      ptr_gemm_k_reduction = args.ptr_gemm_k_reduction;
+
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_D = args.batch_stride_D;
+      batch_stride_gemm_k_reduction = args.batch_stride_gemm_k_reduction;
+
+      output_op = args.epilogue;
+
+      semaphore = static_cast<int *>(workspace);
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
+    }
+  };
+
+  /// Shared memory storage structure
+  union SharedStorage {
+    typename Mma::SharedStorage main_loop;
+    typename Epilogue::SharedStorage epilogue;
+  };
+
+public:
+
+  //
+  // Methods
+  //
+
+  CUTLASS_DEVICE
+  GemmWithKReduction() { } 
+
+  /// Determines whether kernel satisfies alignment
+  static Status can_implement(
+    cutlass::gemm::GemmCoord const & problem_size) {
+
+    CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
+
+    static int const kAlignmentA = (platform::is_same<typename Mma::IteratorA::Layout,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<typename Mma::IteratorA::Layout,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = (platform::is_same<typename Mma::IteratorB::Layout,
+                                                       layout::RowMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<typename Mma::IteratorB::Layout,
+                                                        layout::RowMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorB::AccessType::kElements;
+    static int const kAlignmentC =  (platform::is_same<LayoutC,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutC,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
+
+    bool isAMisaligned = false;
+    bool isBMisaligned = false;
+    bool isCMisaligned = false;
+
+    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
+      isAMisaligned = problem_size.m() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    }
+
+    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
+      isBMisaligned = problem_size.n() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
+            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    }
+
+    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
+      isCMisaligned = problem_size.m() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    }
+
+    if (isAMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand A");
+      return Status::kErrorMisalignedOperand;
+    }
+
+    if (isBMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand B");
+      return Status::kErrorMisalignedOperand;
+    }
+
+    if (isCMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand C");
+      return Status::kErrorMisalignedOperand;
     }
+
+    CUTLASS_TRACE_HOST("  returning kSuccess");
+
+    return Status::kSuccess;
   }
 
-  /// Gets the workspace size
-  static size_t
-  get_workspace_size(Arguments const& args) {
+  static Status can_implement(Arguments const &args) {
+    return can_implement(args.problem_size);
+  }
+
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
     size_t workspace_bytes = 0;
-    if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {
-      workspace_bytes += sizeof(int) * size_t(cute::size<0>(TileShape{})) * size_t(cute::size<1>(TileShape{}));
-    }
 
-    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
+    if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {                                        
+      
+      // Split-K parallel always requires a temporary workspace                                       
+      workspace_bytes =  
+        sizeof(ElementC) *
+        size_t(args.batch_stride_gemm_k_reduction) * 
+        size_t(grid_tiled_shape.k());                                                                 
+    }
 
-    workspace_bytes += GemmKernel::get_workspace_size(args);
     return workspace_bytes;
   }
+  
+  /// Executes one GEMM
+  CUTLASS_DEVICE
+  void operator()(Params const &params, SharedStorage &shared_storage) {
 
-  /// Computes the grid shape
-  static dim3
-  get_grid_shape(Arguments const& args) {
-    auto tmp_params = GemmKernel::to_underlying_arguments(args);
-    return GemmKernel::get_grid_shape(tmp_params);
-  }
+    // Compute threadblock location
+    ThreadblockSwizzle threadblock_swizzle;
 
-  /// Computes the grid shape
-  static dim3
-  get_grid_shape(Params const& params) {
-    return GemmKernel::get_grid_shape(params);
-  }
+    cutlass::gemm::GemmCoord threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int /* smem_capacity */ = -1) {
-    CUTLASS_TRACE_HOST("GemmUniversal::maximum_active_blocks()");
-    int max_active_blocks = -1;
-    int smem_size = GemmKernel::SharedStorageSize;
-
-    // first, account for dynamic smem capacity if needed
-    cudaError_t result;
-    if (smem_size >= (48 << 10)) {
-      CUTLASS_TRACE_HOST("  Setting smem size to " << smem_size);
-      result = cudaFuncSetAttribute(
-          device_kernel<GemmKernel>,
-          cudaFuncAttributeMaxDynamicSharedMemorySize,
-          smem_size);
-      if (cudaSuccess != result) {
-        result = cudaGetLastError(); // to clear the error bit
-        CUTLASS_TRACE_HOST(
-          "  cudaFuncSetAttribute() returned error: "
-          << cudaGetErrorString(result));
-        return -1;
-      }
-    }
+    // Early exit if CTA is out of range
+    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
+      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
 
-    // query occupancy after setting smem size
-    result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
-        &max_active_blocks,
-        device_kernel<GemmKernel>,
-        GemmKernel::MaxThreadsPerBlock,
-        smem_size);
-
-    if (cudaSuccess != result) {
-      result = cudaGetLastError(); // to clear the error bit
-      CUTLASS_TRACE_HOST(
-        "  cudaOccupancyMaxActiveBlocksPerMultiprocessor() returned error: "
-        << cudaGetErrorString(result));
-      return -1;
+      return;
     }
 
-    CUTLASS_TRACE_HOST("  max_active_blocks: " << max_active_blocks);
-    return max_active_blocks;
-  }
+    int offset_k = 0;
+    int problem_size_k = params.problem_size.k();
 
-  /// Initializes GEMM state from arguments.
-  Status
-  initialize(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
-    CUTLASS_TRACE_HOST("GemmUniversal::initialize() - workspace "
-      << workspace << ", stream: " << (stream ? "non-null" : "null"));
-
-    size_t workspace_bytes = GemmKernel::get_workspace_size(args);
-    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
-
-    if (workspace_bytes) {
-      if (!workspace) {
-        CUTLASS_TRACE_HOST("  error: device workspace must not be null");
-        return Status::kErrorWorkspaceNull;
-      }
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
-      if (args.mode == GemmUniversalMode::kGemm) {
-        CUTLASS_TRACE_HOST("  clearing device workspace");
-        cudaError_t result = cudaMemsetAsync(workspace, 0, workspace_bytes, stream);
-        if (cudaSuccess != result) {
-          result = cudaGetLastError(); // to clear the error bit
-          CUTLASS_TRACE_HOST("  cudaMemsetAsync() returned error " << cudaGetErrorString(result));
-          return Status::kErrorInternal;
-        }
-      }
-    }
+    //
+    // Fetch pointers based on mode.
+    //
+    if (params.mode == GemmUniversalMode::kGemm || 
+      params.mode == GemmUniversalMode::kGemmSplitKParallel) {
 
-    // Initialize the Params structure
-    params_ = GemmKernel::to_underlying_arguments(args, workspace);
+      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
-    // account for dynamic smem capacity if needed
-    int smem_size = GemmKernel::SharedStorageSize;
-    if (smem_size >= (48 << 10)) {
-      CUTLASS_TRACE_HOST("  Setting smem size to " << smem_size);
-      cudaError_t result = cudaFuncSetAttribute(
-          device_kernel<GemmKernel>,
-          cudaFuncAttributeMaxDynamicSharedMemorySize,
-          smem_size);
-      if (cudaSuccess != result) {
-        result = cudaGetLastError(); // to clear the error bit
-        CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error: " << cudaGetErrorString(result));
-        return Status::kErrorInternal;
+        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
       }
+
+      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
+    }
+    else if (params.mode == GemmUniversalMode::kBatched) {
+      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
+      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
+    }
+    else if (params.mode == GemmUniversalMode::kArray) {
+      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
+      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
     }
-    return Status::kSuccess;
-  }
 
-  /// Update API is preserved in 3.0, but does not guarantee a lightweight update of params.
-  Status
-  update(Arguments const& args, void* workspace = nullptr) {
-    CUTLASS_TRACE_HOST("GemmUniversal()::update() - workspace: " << workspace);
+    __syncthreads();
 
-    size_t workspace_bytes = get_workspace_size(args);
-    if (workspace_bytes > 0 && nullptr == workspace) {
-      return Status::kErrorWorkspaceNull;
-    }
+    // Compute initial location in logical coordinates
+    cutlass::MatrixCoord tb_offset_A{
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      offset_k,
+    };
 
-    params_ = GemmKernel::to_underlying_arguments(args, workspace);
-    return Status::kSuccess;
-  }
+    cutlass::MatrixCoord tb_offset_B{
+      offset_k,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    };
 
-  /// Primary run() entry point API that is static allowing users to create and manage their own params.
-  /// Supplied params struct must be construct by calling GemmKernel::to_underling_arguments()
-  static Status
-  run(Params& params, cudaStream_t stream = nullptr) {
-    CUTLASS_TRACE_HOST("GemmUniversal::run()");
-    dim3 constexpr block = GemmKernel::get_block_shape();
-    dim3 const grid = get_grid_shape(params);
-
-    // configure smem size and carveout
-    int smem_size = GemmKernel::SharedStorageSize;
-
-    Status launch_result;
-    // Use extended launch API only for mainloops that use it
-    if constexpr(GemmKernel::ArchTag::kMinComputeCapability >= 90) {
-      dim3 cluster(cute::size<0>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
-                   cute::size<1>(typename GemmKernel::DispatchPolicy::ClusterShape{}),
-                   cute::size<2>(typename GemmKernel::DispatchPolicy::ClusterShape{}));
-      void const* kernel = (void const*) device_kernel<GemmKernel>;
-      void* kernel_params[] = {&params};
-      launch_result = ClusterLauncher::launch(grid, cluster, block, smem_size, stream, kernel, kernel_params);
-    }
-    else {
-      launch_result = Status::kSuccess;
-      device_kernel<GemmKernel><<<grid, block, smem_size, stream>>>(params);
-    }
-
-    cudaError_t result = cudaGetLastError();
-    if (cudaSuccess == result && Status::kSuccess == launch_result) {
-      return Status::kSuccess;
-    }
-    else {
-      CUTLASS_TRACE_HOST("  Kernel launch failed. Reason: " << result);
-      return Status::kErrorInternal;
-    }
-  }
 
-  //
-  // Non-static launch overloads that first create and set the internal params struct of this kernel handle.
-  //
+    // Compute position within threadblock
+    int thread_idx = threadIdx.x;
 
-  /// Launches the kernel after first constructing Params internal state from supplied arguments.
-  Status
-  run(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
-    Status status = initialize(args, workspace, stream);
-    if (Status::kSuccess == status) {
-      status = run(params_, stream);
-    }
-    return status;
-  }
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
+      params.params_A,
+      ptr_A,
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_A);
 
-  /// Launches the kernel after first constructing Params internal state from supplied arguments.
-  Status
-  operator()(Arguments const& args, void* workspace = nullptr, cudaStream_t stream = nullptr) {
-    return run(args, workspace, stream);
-  }
+    typename Mma::IteratorB iterator_B(
+      params.params_B,
+      ptr_B,
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_B);
 
-  /// Overload that allows a user to re-launch the same kernel without updating internal params struct.
-  Status
-  run(cudaStream_t stream = nullptr) {
-    return run(params_, stream);
-  }
+    // Broadcast the warp_id computed by lane 0 to ensure dependent code
+    // is compiled as warp-uniform.
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
-  /// Overload that allows a user to re-launch the same kernel without updating internal params struct.
-  Status
-  operator()(cudaStream_t stream = nullptr) const {
-    return run(params_, stream);
-  }
-};
+    int lane_idx = threadIdx.x % 32;
 
-////////////////////////////////////////////////////////////////////////////////
-////////////////////////////// CUTLASS 2.x API /////////////////////////////////
-////////////////////////////////////////////////////////////////////////////////
-
-template <typename GemmKernel_>
-class GemmUniversalAdapter<
-  GemmKernel_,
-  std::enable_if_t<not gemm::detail::IsCutlass3GemmKernel<GemmKernel_>::value>>
-{
-public:
+    //
+    // Main loop
+    //
 
-  using GemmKernel = GemmKernel_;
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
-  static bool const kInternalTranspose = 
-    platform::is_same<typename GemmKernel::LayoutC, cutlass::layout::RowMajor>::value;
+    typename Mma::FragmentC accumulators;
 
-  using ThreadblockShape = typename GemmKernel::Mma::Shape;
-  using WarpShape = typename GemmKernel::WarpShape;
-  using InstructionShape = typename GemmKernel::InstructionShape;
-
-  // warp-level, arch-level (instruction), math operator 
-  using WarpMmaOperator = typename GemmKernel::Mma::Policy::Operator;
-  using ArchMmaOperator = typename WarpMmaOperator::ArchMmaOperator;
-  using MathOperator = typename WarpMmaOperator::MathOperator;
-  
-  // Operator class and arch tag extract bottom-up 
-  // set it for top-level gemm device-level template
-  using OperatorClass = typename WarpMmaOperator::OperatorClass;
-  using ArchTag = typename WarpMmaOperator::ArchTag;
-
-  // Type, layout, and complex transform deliberately exchanged with B
-  using MapArguments = kernel::detail::MapArguments<
-    typename GemmKernel::ElementA,
-    typename GemmKernel::LayoutA,
-    GemmKernel::kTransformA,
-    GemmKernel::kAlignmentA,
-    typename GemmKernel::ElementB,
-    typename GemmKernel::LayoutB,
-    GemmKernel::kTransformB,
-    GemmKernel::kAlignmentB,
-    typename GemmKernel::LayoutC,
-    kInternalTranspose
-  >;
-
-  using ElementA = typename MapArguments::ElementA;
-  using LayoutA = typename MapArguments::LayoutA;
-  static ComplexTransform const kTransformA = MapArguments::kTransformA;
-  static int const kAlignmentA = MapArguments::kAlignmentA;
-
-  using ElementB = typename MapArguments::ElementB;
-  using LayoutB = typename MapArguments::LayoutB;
-  static ComplexTransform const kTransformB = MapArguments::kTransformB;
-  static int const kAlignmentB = MapArguments::kAlignmentB;
-  
-  using ElementC = typename GemmKernel::ElementC;
-  using LayoutC = typename MapArguments::LayoutC;
-  static int const kAlignmentC = GemmKernel::kAlignmentC;
- 
-  using TensorRefA = TensorRef<ElementA const, LayoutA>;
-  using TensorRefB = TensorRef<ElementB const, LayoutB>;
-  using TensorRefC = TensorRef<ElementC const, LayoutC>;
-  using TensorRefD = TensorRef<ElementC, LayoutC>;
-
-  static int const kStages = GemmKernel::Mma::kStages;
-
-  using EpilogueOutputOp = typename GemmKernel::EpilogueOutputOp;
-  using ElementAccumulator = typename EpilogueOutputOp::ElementAccumulator;
-  using ThreadblockSwizzle = typename GemmKernel::ThreadblockSwizzle;
-  using UnderlyingOperator = GemmUniversalBase<GemmKernel>;
-  using Arguments = typename UnderlyingOperator::Arguments;
+    accumulators.clear();
 
-private:
+    typename Mma::FragmentReduction gemm_k_accumulators;
 
-  UnderlyingOperator underlying_operator_;
+    gemm_k_accumulators.clear();
 
-public:
+    // Compute threadblock-scoped matrix multiply-add
+    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-  /// Constructs the GEMM.
-  GemmUniversalAdapter() { }
+    // Compute threadblock-scoped matrix multiply-add
+    mma(
+      gemm_k_iterations, 
+      accumulators, 
+      iterator_A, 
+      iterator_B, 
+      accumulators,
+      gemm_k_accumulators);
 
-  /// Helper to construct a transposed equivalent for the underying GEMM operator
-  static Arguments to_underlying_arguments(Arguments const &args) {
-    if (kInternalTranspose) {
-      return args.transposed_problem();
-    }
-    else {
-      return args;
-    }
-  }
+    //
+    // Epilogue
+    //
 
-  /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args) {
+    EpilogueOutputOp output_op(params.output_op);
 
-    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
-  }
+    //
+    // Masked tile iterators constructed from members
+    //
 
-  /// Gets the workspace size
-  static size_t get_workspace_size(Arguments const &args) {
-    
-    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
-  }
+    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
-  /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) { 
-    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
-  }
+    //assume identity swizzle
+    MatrixCoord threadblock_offset(
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      threadblock_tile_offset.n() * Mma::Shape::kN
+    );
 
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int smem_capacity = -1) {
-    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
-  }
+    int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
+    ElementC *ptr_gemm_k_reduction = static_cast<ElementC *>(params.ptr_gemm_k_reduction);
 
-    return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
-  }
+    //
+    // Fetch pointers based on mode.
+    //
+    
+    // Construct the semaphore.
+    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-  /// Lightweight update given a subset of arguments.  Problem geometry is assumed to
-  /// remain the same.
-  Status update(Arguments const &args) {
+    if (params.mode == GemmUniversalMode::kGemm) {
 
-    return underlying_operator_.update(to_underlying_arguments(args));
-  }
+      // If performing a reduction via split-K, fetch the initial synchronization
+      if (params.grid_tiled_shape.k() > 1) {
+        
+        // Fetch the synchronization lock initially but do not block.
+        semaphore.fetch();
 
-  /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr) {
+        // Indicate which position in a serial reduction the output operator is currently updating
+        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+      }
+    }
+    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+      ptr_gemm_k_reduction += threadblock_tile_offset.k() * params.batch_stride_gemm_k_reduction;
+    }
+    else if (params.mode == GemmUniversalMode::kBatched) {
+      ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
+      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    }
+    else if (params.mode == GemmUniversalMode::kArray) {
+      ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
+      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
+    }
+
+    // Tile iterator loading from source tensor.
+    typename Epilogue::OutputTileIterator iterator_C(
+      params.params_C,
+      ptr_C,
+      params.problem_size.mn(),
+      thread_idx,
+      threadblock_offset
+    );
+
+    // Tile iterator writing to destination tensor.
+    typename Epilogue::OutputTileIterator iterator_D(
+      params.params_D,
+      ptr_D,
+      params.problem_size.mn(),
+      thread_idx,
+      threadblock_offset
+    );
+
+    Epilogue epilogue(
+      shared_storage.epilogue, 
+      thread_idx, 
+      warp_idx, 
+      lane_idx);
+
+    // Wait on the semaphore - this latency may have been covered by iterator construction
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
+        
+      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+      if (threadblock_tile_offset.k()) {
+        iterator_C = iterator_D;
+      }
 
-    return underlying_operator_.run(stream);
-  }
+      semaphore.wait(threadblock_tile_offset.k());
 
-  /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr) {
-    return run(stream);
-  }
+    }
 
-  /// Runs the kernel using initialized state.
-  Status operator()(
-    Arguments const &args, 
-    void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
-    
-    Status status = initialize(args, workspace, stream);
-    
-    if (status == Status::kSuccess) {
-      status = run(stream);
+    // Execute the epilogue operator to update the destination tensor.
+    epilogue(
+      output_op, 
+      iterator_D, 
+      accumulators, 
+      iterator_C); 
+ 
+    if ((kReduceKForA && threadblock_tile_offset.n() == 0)
+     || (!kReduceKForA && threadblock_tile_offset.m() == 0)) {
+
+      int warp_idx_mn = warp_idx % (Mma::Base::WarpCount::kM * Mma::Base::WarpCount::kN);
+      int warp_idx_m = warp_idx_mn % Mma::Base::WarpCount::kM;
+      int warp_idx_n = warp_idx_mn / Mma::Base::WarpCount::kM;
+ 
+     if ((kReduceKForA && warp_idx_n == 0)
+      || (!kReduceKForA && warp_idx_m == 0)) {
+
+        int reduction_warp_idx = kReduceKForA ? warp_idx_m : warp_idx_n;
+        int reduction_threadblock_offset = kReduceKForA ? threadblock_tile_offset.m() :
+                                                          threadblock_tile_offset.n();
+        int reduction_vector_size = kReduceKForA ? params.problem_size.m()
+                                                 : params.problem_size.n();
+        EpilogueGemmKReduction epilogue_gemm_k_reduction(thread_idx,
+                                                         reduction_warp_idx,
+                                                         lane_idx,
+                                                         reduction_threadblock_offset,
+                                                         ptr_gemm_k_reduction);
+        epilogue_gemm_k_reduction(
+          reduction_vector_size,
+          gemm_k_accumulators,
+          params.mode == GemmUniversalMode::kGemm
+            && (params.grid_tiled_shape.k() > 1)
+            && (threadblock_tile_offset.k() > 0));
+      }
     }
+   
+    //
+    // Release the semaphore
+    //
+
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+
+      int lock = 0;
+      if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
-    return status;
+        // The final threadblock resets the semaphore for subsequent grids.
+        lock = 0;
+      }
+      else {
+        // Otherwise, the semaphore is incremented
+        lock = threadblock_tile_offset.k() + 1;
+      }
+      
+      semaphore.release(lock);
+    }
   }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace cutlass::gemm::device
+} // namespace kernel
+} // namespace gemm
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_base.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,53 +24,54 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*!
+/*! 
   \file
-  \brief The universal GEMM accommodates streamk, batched strided, and batched array variants.
+  \brief The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
+    batched array variants.
 */
 
-
 #pragma once
 
-#include <limits>
+//#include <limits>
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/gemm/kernel/gemm_universal.h"
 
 #include "cutlass/gemm/kernel/default_gemm_universal.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 
 #include "cutlass/trace.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 
 template <typename GemmKernel_>
 class GemmUniversalBase {
 public:
 
   using GemmKernel = GemmKernel_;
   using ThreadblockShape = typename GemmKernel::Mma::Shape;
-
+  
   using ElementA = typename GemmKernel::ElementA;
   using LayoutA = typename GemmKernel::LayoutA;
   using TensorRefA = TensorRef<ElementA const, LayoutA>;
   static ComplexTransform const kTransformA = GemmKernel::kTransformA;
 
   using ElementB = typename GemmKernel::ElementB;
   using LayoutB = typename GemmKernel::LayoutB;
@@ -78,339 +79,343 @@
   static ComplexTransform const kTransformB = GemmKernel::kTransformB;
 
   using ElementC = typename GemmKernel::ElementC;
   using LayoutC = typename GemmKernel::LayoutC;
   using TensorRefC = TensorRef<ElementC const, LayoutC>;
   using TensorRefD = TensorRef<ElementC, LayoutC>;
 
-  /// Numerical accumulation element type
-  using ElementAccumulator = typename GemmKernel::Mma::ElementC;
+  using ElementAccumulator = typename GemmKernel::Mma::Policy::Operator::ElementC;
 
   using EpilogueOutputOp = typename GemmKernel::EpilogueOutputOp;
   using ThreadblockSwizzle = typename GemmKernel::ThreadblockSwizzle;
   using Operator = typename GemmKernel::Operator;
 
   /// Argument structure
   using Arguments = typename GemmKernel::Arguments;
 
 protected:
 
-  //
-  // Device properties (uniform across all instances of the current thread)
-  //
-
-  // Device ordinal
-  thread_local static int device_ordinal_;
-
-  /// Device SM count
-  thread_local static int device_sms_;
-
-  /// Kernel SM occupancy (in thread blocks)
-  thread_local static int sm_occupancy_;
-
-  /// Kernel dynamic shared memory allocation requirement
-  thread_local static int smem_size_;
-
-  /// Initialize static thread-local members for the thread's current device,
-  /// if necessary.
-  static Status init_device_props()
-  {
-    CUTLASS_TRACE_HOST("GemmUniversalBase::init_device_props()");
-
-    cudaError_t cudart_result;
-
-    // Get current device ordinal
-    int current_ordinal;
-    cudart_result = cudaGetDevice(&current_ordinal);
-    if (cudart_result != cudaSuccess) {
-      CUTLASS_TRACE_HOST("  cudaGetDevice() returned error " << cudaGetErrorString(cudart_result));
-      return Status::kErrorInternal;
-    }
-
-    // Done if matches the current static member
-    if (current_ordinal == device_ordinal_) {
-      // Already initialized
-      return Status::kSuccess;
-    }
-
-    // Update SM count member
-    cudart_result = cudaDeviceGetAttribute (&device_sms_, cudaDevAttrMultiProcessorCount, current_ordinal);
-    if (cudart_result != cudaSuccess) {
-      CUTLASS_TRACE_HOST("  cudaDeviceGetAttribute() returned error " << cudaGetErrorString(cudart_result));
-      return Status::kErrorInternal;
-    }
-
-    // Update the kernel function's shared memory configuration for the current device
-    smem_size_ = int(sizeof(typename GemmKernel::SharedStorage));
-
-    // If requires more than 48KB: configure for extended, dynamic shared memory
-    if (smem_size_ >= (48 << 10))
-    {
-      cudart_result = cudaFuncSetAttribute(
-        Kernel2<GemmKernel>,
-        cudaFuncAttributeMaxDynamicSharedMemorySize,
-        smem_size_);
-      if (cudart_result != cudaSuccess) {
-        CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error " << cudaGetErrorString(cudart_result));
-        return Status::kErrorInternal;
-      }
-
-      cudart_result = cudaFuncSetAttribute(
-          Kernel2<GemmKernel>,
-          cudaFuncAttributePreferredSharedMemoryCarveout, 100); // 100% shared memory
-      if (cudart_result != cudaSuccess) {
-        CUTLASS_TRACE_HOST("  cudaFuncSetAttribute() returned error " << cudaGetErrorString(cudart_result));
-        return Status::kErrorInternal;
-      }
-    }
-
-    // Update SM occupancy member
-    cudart_result = cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags(
-      &sm_occupancy_,
-      Kernel2<GemmKernel>,
-      GemmKernel::kThreadCount,
-      smem_size_,
-      cudaOccupancyDisableCachingOverride);
-    if (cudart_result != cudaSuccess) {
-      CUTLASS_TRACE_HOST("  cudaOccupancyMaxActiveBlocksPerMultiprocessorWithFlags() returned error " << cudaGetErrorString(cudart_result));
-      return Status::kErrorInternal;
-    }
-
-    // Update device ordinal member on success
-    device_ordinal_ = current_ordinal;
-
-    CUTLASS_TRACE_HOST("  "
-      "device_ordinal: (" << device_ordinal_ << "), "
-      "device_sms: (" << device_sms_ << "), "
-      "sm_occupancy: (" << sm_occupancy_ << ") "
-      "smem_size: (" << smem_size_ << ") "
-      "GemmKernel::kThreadCount: (" << GemmKernel::kThreadCount << ")");
-
-    return Status::kSuccess;
-  }
-
+  /// Kernel parameters object
+  typename GemmKernel::Params params_;
 
 protected:
 
-  //
-  // Instance data members
-  //
-
-  /// Kernel parameters
-  typename GemmKernel::Params params_;
+  /// Private helper to obtain the grid dimensions with fix-up for split-K
+  static void get_grid_shape_(gemm::GemmCoord &grid_tiled_shape, int &gemm_k_size, Arguments const &args) {
 
+    // Determine grid shape
+    ThreadblockSwizzle threadblock_swizzle;
 
-  /// Initialize params member
-  Status init_params(Arguments const &args)
-  {
-    // Initialize static device properties, if necessary
-    Status result = init_device_props();
-    if (result != Status::kSuccess) {
-      return result;
+    grid_tiled_shape = threadblock_swizzle.get_tiled_shape(
+      args.problem_size, 
+      {ThreadblockShape::kM, ThreadblockShape::kN, ThreadblockShape::kK},
+      args.batch_count);
+    
+    gemm_k_size = args.problem_size.k();
+
+    if (args.mode == GemmUniversalMode::kGemm || args.mode == GemmUniversalMode::kGemmSplitKParallel) {
+
+      int const kAlignK = const_max(const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value), 1);
+
+      gemm_k_size = round_up(ceil_div(args.problem_size.k(), args.batch_count), kAlignK);
+      
+      if (gemm_k_size) {
+        grid_tiled_shape.k() = ceil_div(args.problem_size.k(), gemm_k_size);
+      }
     }
-
-    // Initialize params member
-    params_ = typename GemmKernel::Params(args, device_sms_, sm_occupancy_);
-    return Status::kSuccess;
   }
 
 public:
 
-  //---------------------------------------------------------------------------------------------
-  // Stateless API
-  //---------------------------------------------------------------------------------------------
+  /// Constructs the GEMM.
+  GemmUniversalBase() { }
 
   /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args)
-  {
-    CUTLASS_TRACE_HOST("GemmUniversalBase::can_implement()");
-
-    // Initialize static kernel and device properties, if necessary.
-    Status result = init_device_props();
-    if (result != Status::kSuccess) {
-      return result;
-    }
-
-    dim3 grid = get_grid_shape(args);
+  static Status can_implement(Arguments const &args) {
+    
+    // Determine grid shape
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int gemm_k_size = 0;
+    
+    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
+    
+    ThreadblockSwizzle threadblock_swizzle;
+    dim3 grid = threadblock_swizzle.get_grid_shape(grid_tiled_shape);
+
+    uint32_t const kGridYZMax = ((1 << (sizeof(uint16_t) * 8)) - 1);
+  
+    if (!(grid.y <= kGridYZMax && grid.z <= kGridYZMax)) {
 
-    if (!(grid.y <= std::numeric_limits<uint16_t>::max() &&
-          grid.z <= std::numeric_limits<uint16_t>::max()))
-    {
       return Status::kErrorInvalidProblem;
-    }
+    } 
 
     return GemmKernel::can_implement(args);
   }
 
+  /// Gets the workspace size
+  static size_t get_workspace_size(Arguments const &args) {
 
-  /// Returns the workspace size (in bytes) needed for the problem
-  /// geometry expressed by these arguments
-  static size_t get_workspace_size(Arguments const &args)
-  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::get_workspace_size()");
+ 
+    size_t workspace_bytes = 0;
 
-    // Initialize parameters from args
-    GemmUniversalBase base;
-    if (base.init_params(args) != Status::kSuccess) {
-      return 0;
+    // Determine grid shape
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int gemm_k_size = 0;
+    
+    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
+    
+    if (args.mode == GemmUniversalMode::kGemmSplitKParallel) {
+
+      // Split-K parallel always requires a temporary workspace
+      workspace_bytes = 
+        sizeof(ElementC) *
+        size_t(args.batch_stride_D) *
+        size_t(grid_tiled_shape.k());
+    }
+    else if (args.mode == GemmUniversalMode::kGemm && grid_tiled_shape.k() > 1) {
+
+      // Serial split-K only requires a temporary workspace if the number of partitions along the
+      // GEMM K dimension is greater than one.
+      workspace_bytes = sizeof(int) * size_t(grid_tiled_shape.m()) * size_t(grid_tiled_shape.n());
     }
 
-    // Get size from parameters
-    size_t workspace_bytes = base.params_.get_workspace_size();
-
     CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
+
+    workspace_bytes += GemmKernel::get_extra_workspace_size(args, grid_tiled_shape);
+ 
     return workspace_bytes;
   }
 
+  /// Computes the grid shape
+  static dim3 get_grid_shape(Arguments const &args) {
 
-  /// Returns the grid extents in thread blocks to launch
-  static dim3 get_grid_shape(Arguments const &args)
-  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::get_grid_shape()");
 
-    // Initialize parameters from args
-    GemmUniversalBase base;
-    if (base.init_params(args) != Status::kSuccess) {
-      return dim3(0,0,0);
-    }
+    ThreadblockSwizzle threadblock_swizzle;
 
-    // Get dims from parameters
-    dim3 grid_dims = base.params_.get_grid_dims();
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int gemm_k_size = 0;
 
+    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
+    dim3 result = threadblock_swizzle.get_grid_shape(grid_tiled_shape);    
+    
     CUTLASS_TRACE_HOST(
-         "  tiled_shape: " << base.params_.get_tiled_shape()  << "\n"
-      << "  grid_dims: {" << grid_dims << "}");
+         "  grid_tiled_shape: " << grid_tiled_shape  << "\n"
+      << "  result = {" << result << "}");
 
-    return grid_dims;
+    return result;
   }
 
+  /// Computes the maximum number of active blocks per multiprocessor
+  static int maximum_active_blocks(int smem_capacity = -1) {
 
-  /// Returns the maximum number of active thread blocks per multiprocessor
-  static int maximum_active_blocks()
-  {
     CUTLASS_TRACE_HOST("GemmUniversalBase::maximum_active_blocks()");
 
-    // Initialize static device properties, if necessary
-    if (init_device_props() != Status::kSuccess) {
-      return -1;
+    int max_active_blocks = -1;
+    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
+
+    CUTLASS_TRACE_HOST("  smem_size: " << smem_size << " bytes");
+
+    if (smem_size <= (48 << 10)) {
+
+      cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
+        &max_active_blocks,
+        Kernel<GemmKernel>,
+        GemmKernel::kThreadCount,
+        smem_size);
+
+      if (result == cudaSuccess) {
+        CUTLASS_TRACE_HOST("  max_active_blocks: " << max_active_blocks);
+        return max_active_blocks;
+      }
+    }
+    else {
+
+      // Query assuming zero shared memory then compute occupancy limit based on SMEM
+      cudaError_t result = cudaOccupancyMaxActiveBlocksPerMultiprocessor(
+        &max_active_blocks,
+        Kernel<GemmKernel>,
+        GemmKernel::kThreadCount,
+        0);
+
+      if (result != cudaSuccess) {
+
+        CUTLASS_TRACE_HOST(
+          "  cudaOccupancyMaxActiveBlocksPerMultiprocessor() returned error "
+          << cudaGetErrorString(result));
+
+        return -1;
+      }
+
+      if (smem_capacity < 0) {
+        int device_idx = 0;
+        result = cudaGetDevice(&device_idx);
+
+        if (result != cudaSuccess) {
+          return -1;
+        }
+
+        cudaDeviceProp properties;
+        result = cudaGetDeviceProperties(&properties, device_idx);
+
+        if (result != cudaSuccess) {
+          return -1;
+        }
+
+        smem_capacity = static_cast<int>(properties.sharedMemPerMultiprocessor);
+      }
+
+      int occupancy = std::min(max_active_blocks, smem_capacity / smem_size);
+
+      CUTLASS_TRACE_HOST("  occupancy: " << occupancy);
+
+      return occupancy;
     }
 
-    CUTLASS_TRACE_HOST("  max_active_blocks: " << sm_occupancy_);
-    return sm_occupancy_;
+    CUTLASS_TRACE_HOST("  returning internal error");
+
+    return -1;
   }
 
+  /// Initializes GEMM state from arguments.
+  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
 
-  //---------------------------------------------------------------------------------------------
-  // Stateful API
-  //---------------------------------------------------------------------------------------------
-
-  /// Initializes GEMM state from arguments and workspace memory
-  Status initialize(
-    Arguments const &args,
-    void *workspace = nullptr,
-    cudaStream_t stream = nullptr)
-  {
-    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace "
+    CUTLASS_TRACE_HOST("GemmUniversalBase::initialize() - workspace " 
       << workspace << ", stream: " << (stream ? "non-null" : "null"));
 
-    // Initialize parameters from args
-    Status result = init_params(args);
-    if (result != Status::kSuccess) {
-      return result;
+    size_t workspace_bytes = get_workspace_size(args);
+
+    CUTLASS_TRACE_HOST("  workspace_bytes: " << workspace_bytes);
+
+    if (workspace_bytes) {
+      
+      if (!workspace) {
+        CUTLASS_TRACE_HOST("  error: device workspace must not be null");
+
+        return Status::kErrorWorkspaceNull;
+      }
+
+      if (args.mode == GemmUniversalMode::kGemm) {
+        CUTLASS_TRACE_HOST("  clearing device workspace");
+        cudaError_t result = cudaMemsetAsync(workspace, 0, workspace_bytes, stream);
+
+        if (result != cudaSuccess) {
+          CUTLASS_TRACE_HOST("  cudaMemsetAsync() returned error " << cudaGetErrorString(result));
+
+          return Status::kErrorInternal;
+        }
+      }
     }
 
-    // Assign and prepare workspace memory
-    return params_.init_workspace(workspace, stream);
-  }
+    // Get CUDA grid shape
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int gemm_k_size = 0;
+
+    get_grid_shape_(grid_tiled_shape, gemm_k_size, args);
+
+    // Initialize the Params structure
+    params_ = typename GemmKernel::Params(
+      args,
+      grid_tiled_shape,
+      gemm_k_size,
+      static_cast<int *>(workspace)
+    );
+   
+    // Specify shared memory capacity for kernel. 
+    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
+
+    if (smem_size >= (48 << 10)) {
+      cudaError_t result = cudaFuncSetAttribute(Kernel<GemmKernel>,
+                                    cudaFuncAttributeMaxDynamicSharedMemorySize,
+                                    smem_size);
 
+      if (result != cudaSuccess) {
+        return Status::kErrorInternal;
+      }
+    }
 
-  /// Lightweight update given a subset of arguments.  Problem geometry is assumed to
-  /// remain the same.
-  Status update(Arguments const &args)
-  {
-    CUTLASS_TRACE_HOST("GemmUniversalBase()::update()");
-    params_.update(args);
     return Status::kSuccess;
   }
 
+  /// Lightweight update given a subset of arguments
+  Status update(Arguments const &args, void *workspace = nullptr) {
+
+    CUTLASS_TRACE_HOST("GemmUniversalBase()::update() - workspace: " << workspace);
+
+    size_t workspace_bytes = get_workspace_size(args);
+
+    if (workspace_bytes && !workspace) {
+      return Status::kErrorWorkspaceNull;
+    }
+    
+    params_.update(args, workspace);
+    
+    return Status::kSuccess;
+  }
 
   /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr)
-  {
+  Status run(cudaStream_t stream = nullptr) {
     CUTLASS_TRACE_HOST("GemmUniversalBase::run()");
 
+    //
     // Configure grid and block dimensions
+    //
+
+    ThreadblockSwizzle threadblock_swizzle;
+
+    dim3 grid = threadblock_swizzle.get_grid_shape(params_.grid_tiled_shape);
     dim3 block(GemmKernel::kThreadCount, 1, 1);
-    dim3 grid = params_.get_grid_dims();
 
+    int smem_size = int(sizeof(typename GemmKernel::SharedStorage));
+
+    //
     // Launch kernel
-    CUTLASS_TRACE_HOST("  "
-      "grid: (" << grid << "), "
-      "block: (" << block << "), "
-      "SMEM: (" << smem_size_ << ")");
+    //
+
+    CUTLASS_TRACE_HOST("  grid: (" << grid << "),  block: (" << block 
+      << "),  SMEM: " << smem_size << " bytes");
 
-    Kernel2<GemmKernel><<<grid, block, smem_size_, stream>>>(params_);
+    // Launch
+    cutlass::Kernel<GemmKernel><<<grid, block, smem_size, stream>>>(params_);
 
+    //
     // Query for errors
+    //
     cudaError_t result = cudaGetLastError();
+
     if (result != cudaSuccess) {
       CUTLASS_TRACE_HOST("  grid launch failed with error " << cudaGetErrorString(result));
       return Status::kErrorInternal;
     }
-
+  
     return Status::kSuccess;
   }
 
-
   /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr)
-  {
+  Status operator()(cudaStream_t stream = nullptr) {
     return run(stream);
   }
 
-
   /// Runs the kernel using initialized state.
   Status operator()(
     Arguments const &args, 
     void *workspace = nullptr, 
-    cudaStream_t stream = nullptr)
-  {
+    cudaStream_t stream = nullptr) {
+    
     Status status = initialize(args, workspace, stream);
-
+    
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Static initializers
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Device ordinal
-template <typename GemmKernel_>
-thread_local int GemmUniversalBase<GemmKernel_>::device_ordinal_ = -1;
-
-/// Device SM count
-template <typename GemmKernel_>
-thread_local int GemmUniversalBase<GemmKernel_>::device_sms_ = -1;
-
-/// Kernel SM occupancy (in thread blocks)
-template <typename GemmKernel_>
-thread_local int GemmUniversalBase<GemmKernel_>::sm_occupancy_ = -1;
-
-/// Kernel dynamic shared memory allocation requirement
-template <typename GemmKernel_>
-thread_local int GemmUniversalBase<GemmKernel_>::smem_size_ = -1;
-
-
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_universal_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h`

 * *Files 5% similar despite different names*

```diff
@@ -24,48 +24,47 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Template for a GEMM kernel that can broadcast bias vector in the
-           epigloue.
+    \brief Template for a GEMM kernel that can reduce one of the input matrix
+    into a vector along the K dimension.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/arch/arch.h"
-#include "cutlass/epilogue/thread/linear_combination_bias_elementwise.h"
 #include "cutlass/device_kernel.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-#include "cutlass/gemm/kernel/gemm_universal.h"
+#include "cutlass/gemm/kernel/gemm_with_k_reduction.h"
 
-#include "cutlass/gemm/kernel/default_gemm_universal.h"
-#include "cutlass/gemm/kernel/default_gemm_with_broadcast.h"
+#include "cutlass/gemm/kernel/default_gemm_with_k_reduction.h"
 #include "cutlass/gemm/device/default_gemm_configuration.h"
 #include "cutlass/gemm/device/gemm_universal_base.h"
 
+#include "cutlass/layout/permute.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/*!
-  The universal GEMM with a broadcast epilogue.
-  Supports
+/*! 
+  The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
+  batched array variants.
 */
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
@@ -76,14 +75,16 @@
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_ = ElementC_,
     /// Operator class tag
     typename OperatorClass_ = arch::OpClassSimt,
+    /// Reduce A or B operand along the K dimension
+    bool ReduceKForA_ = true,
     /// Tag indicating architecture to tune for.  This is the minimum SM that
     /// supports the intended feature. The device kernel can be built
     /// targeting any SM larger than this number.
     typename ArchTag_ = arch::Sm70,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
@@ -92,18 +93,18 @@
     typename WarpShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::InstructionShape,
-    /// Epilogue output operator      - must satisfy concept of 'EpilogueWithBroadcastOp'
-    typename EpilogueOutputOp_ = cutlass::epilogue::thread::LinearCombinationBiasElementwise<
-        ElementC_, ElementAccumulator_, ElementAccumulator_,
-        ElementC_, ElementC_, 128 / cutlass::sizeof_bits<ElementC_>::value>,
+    /// Epilogue output operator
+    typename EpilogueOutputOp_ = typename DefaultGemmConfiguration<
+        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
+        ElementAccumulator_>::EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle_ = threadblock::GemmIdentityThreadblockSwizzle<>,
     /// Number of stages used in the pipelined mainloop
     int Stages =
         DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
                                  ElementC_, ElementAccumulator_>::kStages,
     /// Access granularity of A matrix in units of elements
@@ -117,82 +118,94 @@
     /// Operation performed by GEMM
     typename Operator_ = typename DefaultGemmConfiguration<
         OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
         ElementAccumulator_>::Operator,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA = ComplexTransform::kNone,
     /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB = ComplexTransform::kNone
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Gather operand A by using an index array
+    bool GatherA = false,
+    /// Gather operand B by using an index array
+    bool GatherB = false,
+    /// Scatter result D by using an index array
+    bool ScatterD = false,
+    /// Permute result D
+    typename PermuteDLayout = layout::NoPermute
 >
-class GemmUniversalWithBroadcast :
+class GemmWithKReduction : 
   public GemmUniversalBase<
-    typename kernel::DefaultGemmWithBroadcast<
+    typename kernel::DefaultGemmWithKReduction<
       ElementA_,
       LayoutA_,
       TransformA,
       AlignmentA,
       ElementB_,
       LayoutB_,
       TransformB,
       AlignmentB,
       ElementC_,
       LayoutC_,
       ElementAccumulator_,
       OperatorClass_,
+      ReduceKForA_,
       ArchTag_,
       ThreadblockShape_,
       WarpShape_,
       InstructionShape_,
       EpilogueOutputOp_,
       ThreadblockSwizzle_,
       Stages,
-      Operator_
+      Operator_,
+      SharedMemoryClearOption::kNone
     >::GemmKernel
   > {
 
  public:
 
   using ElementAccumulator = ElementAccumulator_;
   using OperatorClass = OperatorClass_;
   using ArchTag = ArchTag_;
   using ThreadblockShape = ThreadblockShape_;
   using WarpShape = WarpShape_;
   using InstructionShape = InstructionShape_;
   using EpilogueOutputOp = EpilogueOutputOp_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
   using Operator = Operator_;
-  static int const kStages = Stages;
-  static int const kAlignmentA = AlignmentA;
-  static int const kAlignmentB = AlignmentB;
-  static int const kAlignmentC = EpilogueOutputOp::kCount;
-  static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
+  static constexpr int kStages = Stages;
+  static constexpr int kAlignmentA = AlignmentA;
+  static constexpr int kAlignmentB = AlignmentB;
+  static constexpr int kAlignmentC = EpilogueOutputOp::kCount;
+  static constexpr ComplexTransform kTransformA = TransformA;
+  static constexpr ComplexTransform kTransformB = TransformB;
 
   using Base = GemmUniversalBase<
-    typename kernel::DefaultGemmWithBroadcast<
+    typename kernel::DefaultGemmWithKReduction<
       ElementA_,
       LayoutA_,
       TransformA,
       AlignmentA,
       ElementB_,
       LayoutB_,
       TransformB,
       AlignmentB,
       ElementC_,
       LayoutC_,
       ElementAccumulator_,
       OperatorClass_,
+      ReduceKForA_,
       ArchTag_,
       ThreadblockShape_,
       WarpShape_,
       InstructionShape_,
       EpilogueOutputOp_,
       ThreadblockSwizzle_,
       Stages,
-      Operator_
+      Operator_,
+      SharedMemoryClearOption::kNone
     >::GemmKernel
   >;
 
   using Arguments = typename Base::Arguments;
   using GemmKernel = typename Base::GemmKernel;
 };
 
@@ -210,14 +223,16 @@
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Element type for internal accumulation
     typename ElementAccumulator_,
     /// Operator class tag
     typename OperatorClass_,
+    /// Reduce A or B operand along the K dimension
+    bool ReduceKForA_,
     /// Tag indicating architecture to tune for.  This is the minimum SM that
     /// supports the intended feature. The device kernel can be built
     /// targeting any SM larger than this number.
     typename ArchTag_,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
@@ -235,21 +250,30 @@
     /// Access granularity of B matrix in units of elements
     int AlignmentB,
     /// Operation performed by GEMM
     typename Operator_,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
     /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB>
-class GemmUniversalWithBroadcast<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
+    ComplexTransform TransformB,
+    /// Gather operand A by using an index array
+    bool GatherA,
+    /// Gather operand B by using an index array
+    bool GatherB,
+    /// Scatter result D by using an index array
+    bool ScatterD,
+    /// Permute result D
+    typename PermuteDLayout
+>
+class GemmWithKReduction<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
            layout::ColumnMajor,  // partially specialized on LayoutC
-           ElementAccumulator_, OperatorClass_, ArchTag_, ThreadblockShape_,
+           ElementAccumulator_, OperatorClass_, ReduceKForA_, ArchTag_, ThreadblockShape_,
            WarpShape_, InstructionShape_, EpilogueOutputOp_,
            ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB,
-           Operator_, TransformA, TransformB> {
+           Operator_, TransformA, TransformB, GatherA, GatherB, ScatterD, PermuteDLayout> {
  public:
 
   using ElementA = ElementA_;
   using LayoutA = LayoutA_;
   using TensorRefA = TensorRef<ElementA const, LayoutA>;
   using ElementB = ElementB_;
   using LayoutB = LayoutB_;
@@ -269,35 +293,40 @@
   using Operator = Operator_;
   static int const kStages = Stages;
   static int const kAlignmentA = AlignmentA;
   static int const kAlignmentB = AlignmentB;
   static ComplexTransform const kTransformA = TransformA;
   static ComplexTransform const kTransformB = TransformB;
 
-  using UnderlyingOperator = typename GemmUniversalWithBroadcast<
+  using UnderlyingOperator = typename GemmWithKReduction< 
     ElementB,
     typename layout::LayoutTranspose<LayoutB>::type,
     ElementA,
     typename layout::LayoutTranspose<LayoutA>::type,
     ElementC,
-    layout::RowMajor,
+    layout::RowMajor,    
     ElementAccumulator,
     OperatorClass,
+    !ReduceKForA_,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
     kAlignmentB,
     kAlignmentA,
     Operator,
     kTransformB,
-    kTransformA
+    kTransformA,
+    GatherB,
+    GatherA,
+    ScatterD,
+    PermuteDLayout
   >::Base;
 
   using GemmKernel = typename UnderlyingOperator::GemmKernel;
   static int const kAlignmentC = EpilogueOutputOp::kCount;
 
   /// Argument structure
   using Arguments = typename UnderlyingOperator::Arguments;
@@ -305,35 +334,35 @@
 private:
 
   UnderlyingOperator underlying_operator_;
 
 public:
 
   /// Constructs the GEMM.
-  GemmUniversalWithBroadcast() { }
+  GemmWithKReduction() = default;
 
   /// Helper to construct a transposed equivalent for the underying GEMM operator
   static Arguments to_underlying_arguments(Arguments const &args) {
     return args.transposed_problem();
   }
 
   /// Determines whether the GEMM can execute the given problem.
   static Status can_implement(Arguments const &args) {
 
     return UnderlyingOperator::can_implement(to_underlying_arguments(args));
   }
 
   /// Gets the workspace size
   static size_t get_workspace_size(Arguments const &args) {
-
+    
     return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
   }
 
   /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) {
+  static dim3 get_grid_shape(Arguments const &args) { 
     return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
   }
 
   /// Computes the maximum number of active blocks per multiprocessor
   static int maximum_active_blocks(int smem_capacity = -1) {
     return UnderlyingOperator::maximum_active_blocks(smem_capacity);
   }
@@ -359,20 +388,20 @@
   /// Runs the kernel using initialized state.
   Status operator()(cudaStream_t stream = nullptr) {
     return run(stream);
   }
 
   /// Runs the kernel using initialized state.
   Status operator()(
-    Arguments const &args,
-    void *workspace = nullptr,
+    Arguments const &args, 
+    void *workspace = nullptr, 
     cudaStream_t stream = nullptr) {
-
+    
     Status status = initialize(args, workspace, stream);
-
+    
     if (status == Status::kSuccess) {
       status = run(stream);
     }
 
     return status;
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemm_with_k_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,392 +24,311 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Template for a GEMM kernel that can reduce one of the input matrix
-    into a vector along the K dimension.
+    \brief 
+      Default kernel-level Rank2K definitions combine threadblock-scoped matrix multiply-add with
+      the appropriate threadblock-scoped epilogue.
+
+  
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/arch/arch.h"
-#include "cutlass/device_kernel.h"
+#include "cutlass/blas3.h"
+
+#include "cutlass/layout/matrix.h"
+#include "cutlass/arch/wmma.h"
+
+#include "cutlass/epilogue/threadblock/epilogue.h"
+#include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/kernel/rank_2k_universal.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/gemm/threadblock/default_mma.h"
+#include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-#include "cutlass/gemm/kernel/gemm_with_k_reduction.h"
 
-#include "cutlass/gemm/kernel/default_gemm_with_k_reduction.h"
-#include "cutlass/gemm/device/default_gemm_configuration.h"
-#include "cutlass/gemm/device/gemm_universal_base.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+#include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
+#endif //CUTLASS_ARCH_WMMA_ENABLED
 
-#include "cutlass/layout/permute.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace device {
+namespace kernel {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-/*! 
-  The universal GEMM accommodates serial reductions, parallel reductions, batched strided, and 
-  batched array variants.
-*/
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC_,
     /// Element type for internal accumulation
-    typename ElementAccumulator_ = ElementC_,
+    typename ElementAccumulator,
     /// Operator class tag
-    typename OperatorClass_ = arch::OpClassSimt,
-    /// Reduce A or B operand along the K dimension
-    bool ReduceKForA_ = true,
-    /// Tag indicating architecture to tune for.  This is the minimum SM that
-    /// supports the intended feature. The device kernel can be built
-    /// targeting any SM larger than this number.
-    typename ArchTag_ = arch::Sm70,
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::ThreadblockShape,
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::InstructionShape,
+    typename InstructionShape,
     /// Epilogue output operator
-    typename EpilogueOutputOp_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::EpilogueOutputOp,
+    typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle_ = threadblock::GemmIdentityThreadblockSwizzle<>,
+    typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
-    int Stages =
-        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
-                                 ElementC_, ElementAccumulator_>::kStages,
-    /// Access granularity of A matrix in units of elements
-    int AlignmentA =
-        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
-                                 ElementC_, ElementAccumulator_>::kAlignmentA,
-    /// Access granularity of B matrix in units of elements
-    int AlignmentB =
-        DefaultGemmConfiguration<OperatorClass_, ArchTag_, ElementA_, ElementB_,
-                                 ElementC_, ElementAccumulator_>::kAlignmentB,
-    /// Operation performed by GEMM
-    typename Operator_ = typename DefaultGemmConfiguration<
-        OperatorClass_, ArchTag_, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator_>::Operator,
+    int Stages,
     /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA = ComplexTransform::kNone,
+    ComplexTransform TransformA,
     /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Gather operand A by using an index array
-    bool GatherA = false,
-    /// Gather operand B by using an index array
-    bool GatherB = false,
-    /// Scatter result D by using an index array
-    bool ScatterD = false,
-    /// Permute result D
-    typename PermuteDLayout = layout::NoPermute
->
-class GemmWithKReduction : 
-  public GemmUniversalBase<
-    typename kernel::DefaultGemmWithKReduction<
-      ElementA_,
-      LayoutA_,
-      TransformA,
-      AlignmentA,
-      ElementB_,
-      LayoutB_,
-      TransformB,
-      AlignmentB,
-      ElementC_,
-      LayoutC_,
-      ElementAccumulator_,
-      OperatorClass_,
-      ReduceKForA_,
-      ArchTag_,
-      ThreadblockShape_,
-      WarpShape_,
-      InstructionShape_,
-      EpilogueOutputOp_,
-      ThreadblockSwizzle_,
-      Stages,
-      Operator_,
-      SharedMemoryClearOption::kNone
-    >::GemmKernel
-  > {
-
- public:
-
-  using ElementAccumulator = ElementAccumulator_;
-  using OperatorClass = OperatorClass_;
-  using ArchTag = ArchTag_;
-  using ThreadblockShape = ThreadblockShape_;
-  using WarpShape = WarpShape_;
-  using InstructionShape = InstructionShape_;
-  using EpilogueOutputOp = EpilogueOutputOp_;
-  using ThreadblockSwizzle = ThreadblockSwizzle_;
-  using Operator = Operator_;
-  static constexpr int kStages = Stages;
-  static constexpr int kAlignmentA = AlignmentA;
-  static constexpr int kAlignmentB = AlignmentB;
-  static constexpr int kAlignmentC = EpilogueOutputOp::kCount;
-  static constexpr ComplexTransform kTransformA = TransformA;
-  static constexpr ComplexTransform kTransformB = TransformB;
-
-  using Base = GemmUniversalBase<
-    typename kernel::DefaultGemmWithKReduction<
-      ElementA_,
-      LayoutA_,
-      TransformA,
-      AlignmentA,
-      ElementB_,
-      LayoutB_,
-      TransformB,
-      AlignmentB,
-      ElementC_,
-      LayoutC_,
-      ElementAccumulator_,
-      OperatorClass_,
-      ReduceKForA_,
-      ArchTag_,
-      ThreadblockShape_,
-      WarpShape_,
-      InstructionShape_,
-      EpilogueOutputOp_,
-      ThreadblockSwizzle_,
-      Stages,
-      Operator_,
-      SharedMemoryClearOption::kNone
-    >::GemmKernel
-  >;
+    ComplexTransform TransformB,
+    /// Operation performed by GEMM
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Blas3 computation mode
+    BlasMode BlasMode_ = BlasMode::kSymmetric>
+struct DefaultRank2KComplex;
+
+
+////////////////////////////////////////////////////////////////////////////////
+namespace detail {
+
+template <
+  /// Layout type for A matrix operand
+  typename LayoutA_,
+  /// Layout type for B matrix operand
+  typename LayoutB_,
+  /// Complex elementwise transformation 
+  ComplexTransform TransformA,
+  /// Complex elementwise transformation 
+  ComplexTransform TransformB,
+  /// Blas3 computation mode (symmetric/hermitian)
+  BlasMode BlasMode_
+  > struct Rank2KTransposedComplexTransform {
+  
+  static ComplexTransform const kTransformA = TransformA;
+  static ComplexTransform const kTransformB = TransformB;
+
+};
+  
+  // partial specializations for HER2K CUBLAS_OP_N layout (ColumMajor)
+template <>
+  struct Rank2KTransposedComplexTransform <
+  layout::ColumnMajor, layout::ColumnMajor, 
+  ComplexTransform::kNone, ComplexTransform::kNone,
+  BlasMode::kHermitian> {
+
+  static ComplexTransform const kTransformA = ComplexTransform::kConjugate;
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
+
+};
+
+  // partial specializations for HER2K CUBLAS_OP_C layout (RowMajor + Complex conjugate) 
+template <>
+  struct Rank2KTransposedComplexTransform <
+  layout::RowMajor, layout::RowMajor, 
+  ComplexTransform::kConjugate, ComplexTransform::kConjugate,
+  BlasMode::kHermitian> {
+
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
+  static ComplexTransform const kTransformB = ComplexTransform::kConjugate;
 
-  using Arguments = typename Base::Arguments;
-  using GemmKernel = typename Base::GemmKernel;
 };
 
+}
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Partial specialization for Ampere Architecture complex datatype (symmetric)
 template <
     /// Element type for A matrix operand
-    typename ElementA_,
+    typename ElementA,
     /// Layout type for A matrix operand
-    typename LayoutA_,
+    typename LayoutA,
     /// Element type for B matrix operand
-    typename ElementB_,
+    typename ElementB,
     /// Layout type for B matrix operand
-    typename LayoutB_,
+    typename LayoutB,
     /// Element type for C and D matrix operands
-    typename ElementC_,
+    typename ElementC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
     /// Element type for internal accumulation
-    typename ElementAccumulator_,
-    /// Operator class tag
-    typename OperatorClass_,
-    /// Reduce A or B operand along the K dimension
-    bool ReduceKForA_,
-    /// Tag indicating architecture to tune for.  This is the minimum SM that
-    /// supports the intended feature. The device kernel can be built
-    /// targeting any SM larger than this number.
-    typename ArchTag_,
+    typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_,
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_,
+    typename InstructionShape,
     /// Epilogue output operator
-    typename EpilogueOutputOp_,
+    typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle_,
+    typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Access granularity of A matrix in units of elements
-    int AlignmentA,
-    /// Access granularity of B matrix in units of elements
-    int AlignmentB,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Operation performed by GEMM
-    typename Operator_,
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial>
+struct DefaultRank2KComplex<
+  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
+  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
+  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kSymmetric> {
+
+  static BlasMode const kBlasMode = BlasMode::kSymmetric;
+  
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementA, LayoutA, 
+      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
+      ThreadblockShape, WarpShape, InstructionShape, Stages, 
+      TransformA, TransformB, Operator>::ThreadblockMma;
+
+  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^T)
+  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementB, LayoutB, 
+      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
+      ThreadblockShape, WarpShape, InstructionShape, Stages, 
+      TransformA, TransformB, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+
+  /// Define the kernel-level Rank2K operator.
+  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
+
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for Ampere Architecture complex datatype (hermitian)
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Element type for C and D matrix operands
+    typename ElementC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Epilogue output operator
+    typename EpilogueOutputOp,
+    /// Threadblock-level swizzling operator
+    typename ThreadblockSwizzle,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
     /// Complex elementwise transformation on B operand
     ComplexTransform TransformB,
-    /// Gather operand A by using an index array
-    bool GatherA,
-    /// Gather operand B by using an index array
-    bool GatherB,
-    /// Scatter result D by using an index array
-    bool ScatterD,
-    /// Permute result D
-    typename PermuteDLayout
->
-class GemmWithKReduction<ElementA_, LayoutA_, ElementB_, LayoutB_, ElementC_,
-           layout::ColumnMajor,  // partially specialized on LayoutC
-           ElementAccumulator_, OperatorClass_, ReduceKForA_, ArchTag_, ThreadblockShape_,
-           WarpShape_, InstructionShape_, EpilogueOutputOp_,
-           ThreadblockSwizzle_, Stages, AlignmentA, AlignmentB,
-           Operator_, TransformA, TransformB, GatherA, GatherB, ScatterD, PermuteDLayout> {
- public:
-
-  using ElementA = ElementA_;
-  using LayoutA = LayoutA_;
-  using TensorRefA = TensorRef<ElementA const, LayoutA>;
-  using ElementB = ElementB_;
-  using LayoutB = LayoutB_;
-  using TensorRefB = TensorRef<ElementB const, LayoutB>;
-  using ElementC = ElementC_;
-  using LayoutC = layout::ColumnMajor;
-  using TensorRefC = TensorRef<ElementC const, LayoutC>;
-  using TensorRefD = TensorRef<ElementC, LayoutC>;
-  using ElementAccumulator = ElementAccumulator_;
-  using OperatorClass = OperatorClass_;
-  using ArchTag = ArchTag_;
-  using ThreadblockShape = ThreadblockShape_;
-  using WarpShape = WarpShape_;
-  using InstructionShape = InstructionShape_;
-  using EpilogueOutputOp = EpilogueOutputOp_;
-  using ThreadblockSwizzle = ThreadblockSwizzle_;
-  using Operator = Operator_;
-  static int const kStages = Stages;
-  static int const kAlignmentA = AlignmentA;
-  static int const kAlignmentB = AlignmentB;
+    /// Operation performed by GEMM
+    typename Operator,
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial>
+struct DefaultRank2KComplex<
+  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
+  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
+  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
+  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kHermitian> {
+
+  static BlasMode const kBlasMode = BlasMode::kHermitian;
+
+  // Complex transform for input A and B matrices (function on input layout)
   static ComplexTransform const kTransformA = TransformA;
   static ComplexTransform const kTransformB = TransformB;
 
-  using UnderlyingOperator = typename GemmWithKReduction< 
-    ElementB,
-    typename layout::LayoutTranspose<LayoutB>::type,
-    ElementA,
-    typename layout::LayoutTranspose<LayoutA>::type,
-    ElementC,
-    layout::RowMajor,    
-    ElementAccumulator,
-    OperatorClass,
-    !ReduceKForA_,
-    ArchTag,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    EpilogueOutputOp,
-    ThreadblockSwizzle,
-    Stages,
-    kAlignmentB,
-    kAlignmentA,
-    Operator,
-    kTransformB,
-    kTransformA,
-    GatherB,
-    GatherA,
-    ScatterD,
-    PermuteDLayout
-  >::Base;
-
-  using GemmKernel = typename UnderlyingOperator::GemmKernel;
-  static int const kAlignmentC = EpilogueOutputOp::kCount;
-
-  /// Argument structure
-  using Arguments = typename UnderlyingOperator::Arguments;
-
-private:
-
-  UnderlyingOperator underlying_operator_;
-
-public:
-
-  /// Constructs the GEMM.
-  GemmWithKReduction() = default;
-
-  /// Helper to construct a transposed equivalent for the underying GEMM operator
-  static Arguments to_underlying_arguments(Arguments const &args) {
-    return args.transposed_problem();
-  }
-
-  /// Determines whether the GEMM can execute the given problem.
-  static Status can_implement(Arguments const &args) {
-
-    return UnderlyingOperator::can_implement(to_underlying_arguments(args));
-  }
-
-  /// Gets the workspace size
-  static size_t get_workspace_size(Arguments const &args) {
-    
-    return UnderlyingOperator::get_workspace_size(to_underlying_arguments(args));
-  }
-
-  /// Computes the grid shape
-  static dim3 get_grid_shape(Arguments const &args) { 
-    return UnderlyingOperator::get_grid_shape(to_underlying_arguments(args));
-  }
-
-  /// Computes the maximum number of active blocks per multiprocessor
-  static int maximum_active_blocks(int smem_capacity = -1) {
-    return UnderlyingOperator::maximum_active_blocks(smem_capacity);
-  }
-
-  /// Initializes GEMM state from arguments.
-  Status initialize(Arguments const &args, void *workspace = nullptr, cudaStream_t stream = nullptr) {
-
-    return underlying_operator_.initialize(to_underlying_arguments(args), workspace, stream);
-  }
-
-  /// Lightweight update given a subset of arguments
-  Status update(Arguments const &args, void *workspace = nullptr) {
-
-    return underlying_operator_.update(to_underlying_arguments(args), workspace);
-  }
-
-  /// Runs the kernel using initialized state.
-  Status run(cudaStream_t stream = nullptr) {
-
-    return underlying_operator_.run(stream);
-  }
-
-  /// Runs the kernel using initialized state.
-  Status operator()(cudaStream_t stream = nullptr) {
-    return run(stream);
-  }
-
-  /// Runs the kernel using initialized state.
-  Status operator()(
-    Arguments const &args, 
-    void *workspace = nullptr, 
-    cudaStream_t stream = nullptr) {
-    
-    Status status = initialize(args, workspace, stream);
-    
-    if (status == Status::kSuccess) {
-      status = run(stream);
-    }
+  using TransposedComplexTransform = detail::Rank2KTransposedComplexTransform<
+                                        LayoutA, LayoutB, 
+                                        TransformA, TransformB,
+                                        kBlasMode>;
+
+  // Complex transform on operandA and operandB (function of blas3 computation)
+  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
+  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
+
+  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^H)
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementA, LayoutA, 
+      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
+      ThreadblockShape, WarpShape, InstructionShape, Stages, 
+      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
+
+  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^H)
+  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+      ElementB, LayoutB, 
+      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
+      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
+      ThreadblockShape, WarpShape, InstructionShape, Stages, 
+      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
+
+  /// Define the epilogue
+  using Epilogue =
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
+          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+
+  /// Define the kernel-level Rank2K operator.
+  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
 
-    return status;
-  }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-} // namespace device
-} // namespace gemm
-} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+}  // namespace kernel
+}  // namespace gemm
+}  // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/gemv.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -344,15 +344,15 @@
     }
 
     return status;
   }
 };
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchange operand.
+/// Parital specialization for column-major output exchange operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for B matrix operand
     typename ElementB_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_2k_grouped.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/rank_k.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -321,15 +321,15 @@
     }
 
     return status;
   }
 };
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for column-major output exchange operand.
+/// Parital specialization for column-major output exchange operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Element type for C and D matrix operands
     typename ElementC_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/symm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -404,15 +404,15 @@
   Case 4 -> Case 1:
       D_col = matrix B x matrix A (RightSide mode) 
    => Transpose(D_col) = Transpose(matrix A) x Transpose(matrix B) (LeftSide mode)
 
    call GEMM mainloop for with RowMajor efficient-epilogue
 ********************************************************************************************************/
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Side Mode for A (kLeft or kRight)
     SideMode SideModeA,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/device/trmm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -559,15 +559,15 @@
   Case 4 -> Case 1:
       D_col = matrix B x matrix A (RightSide mode) 
    => Transpose(D_col) = Transpose(matrix A) x Transpose(matrix B) (LeftSide mode)
 
    call GEMM mainloop for with RowMajor efficient-epilogue
 ********************************************************************************************************/
 
-/// Partial specialization for column-major output exchanges problem size and operand.
+/// Parital specialization for column-major output exchanges problem size and operand.
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
     /// Side Mode for A 
     SideMode SideModeA,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -131,85 +131,14 @@
     typename PermuteDLayout = layout::NoPermute,
     ///
     typename Enable = void
 >
 struct DefaultGemm;
 
 ////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear,
-    /// Gather operand A by using an index array
-    bool GatherA,
-    /// Gather operand B by using an index array
-    bool GatherB,
-    /// Scatter result D by using an index array
-    bool ScatterD,
-    /// Permute result D
-    typename PermuteDLayout
->
-struct DefaultGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
-                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-                   arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
-                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                   Operator, SharedMemoryClear, GatherA, GatherB, ScatterD, PermuteDLayout> {
-  /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator, false, SharedMemoryClear, GatherA, GatherB>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, ScatterD, PermuteDLayout>::Epilogue;
-
-  /// Define the kernel-level GEMM operator.
-  using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
-};
-
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
@@ -258,16 +187,16 @@
 >
 struct DefaultGemm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, ElementC,
                    LayoutC, ElementAccumulator, arch::OpClassTensorOp,
                    arch::Sm80, ThreadblockShape, WarpShape, InstructionShape,
                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
                    Operator, SharedMemoryClear, GatherA, GatherB, ScatterD, PermuteDLayout> {
 
-  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
+  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
       ElementAccumulator, LayoutC, arch::OpClassTensorOp, arch::Sm80,
       ThreadblockShape, WarpShape, InstructionShape, Stages,
@@ -710,16 +639,16 @@
     SharedMemoryClear,
     GatherA,
     GatherB,
     ScatterD,
     PermuteDLayout,
     typename platform::enable_if< ! platform::is_same<ArchTag, arch::Sm80>::value >::type > {
 
-  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
+  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA,
       LayoutA,
       kAlignmentA,
@@ -837,16 +766,16 @@
                    Operator,
                    SharedMemoryClear,
                    GatherA,
                    GatherB,
                    ScatterD,
                    PermuteDLayout> {
 
-  static_assert((platform::is_same<LayoutC, layout::RowMajor>::value
-             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value),
+  static_assert(platform::is_same<LayoutC, layout::RowMajor>::value
+             || platform::is_same<LayoutC, layout::AffineRankN<2>>::value,
              "Epilogue in the kernel level must be row major");
 
   /// Define the threadblock-scoped matrix multiply-accumulate
   using Mma = typename cutlass::gemm::threadblock::DefaultMma<
       ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
       ElementAccumulator, LayoutC, arch::OpClassSimt, arch::Sm80,
       ThreadblockShape, WarpShape, GemmShape<1, 1, 1>, Stages,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_complex.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -115,74 +115,14 @@
   /// If true, kernel is configured to support serial reduction in the epilogue
   bool SplitKSerial
 >
 struct DefaultGemmComplex;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
-    /// Multiply-add operator 
-    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the epilogue
-    bool SplitKSerial
-  >
-struct DefaultGemmComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC,
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
-
-  /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementA, LayoutA, ElementB, LayoutB, ElementAccumulator,
-      layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, ThreadblockShape,
-      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
-
-  /// Define the kernel-level GEMM operator.
-  using GemmKernel = kernel::Gemm<Mma, Epilogue, ThreadblockSwizzle, SplitKSerial>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Element type for B matrix operand
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -44,19 +44,17 @@
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/kernel/gemm_grouped.h"
-#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
+#include "cutlass/gemm/kernel/gemm_universal.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
 #include "cutlass/gemm/kernel/default_gemm_complex.h"
-#include "cutlass/gemm/device/default_gemm_configuration.h"
 
 #include "cutlass/layout/permute.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
@@ -99,28 +97,30 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
     /// Operation performed by GEMM
-    typename Operator = typename device::DefaultGemmConfiguration<
-        OperatorClass, ArchTag, ElementA_, ElementB_, ElementC_,
-        ElementAccumulator>::Operator,
+    typename Operator,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
+    /// Gather operand A by using an index array
+    bool GatherA = false,
+    /// Gather operand B by using an index array
+    bool GatherB = false,
+    /// Scatter result D by using an index array
+    bool ScatterD = false,
     /// Permute result D
     typename PermuteDLayout = layout::NoPermute,
     ///
     typename Enable = void
     >
-struct DefaultGemmGrouped;
+struct DefaultGemmUniversal;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // Real-valued GEMM kernels
 //
 
 template <
@@ -154,24 +154,28 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_,
     /// Operation performed by GEMM
     typename Operator,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear,
+    /// Gather operand A by using an index array
+    bool GatherA,
+    /// Gather operand B by using an index array
+    bool GatherB,
+    /// Scatter result D by using an index array
+    bool ScatterD,
     /// Permute result D
     typename PermuteDLayout
 >
-struct DefaultGemmGrouped<
+struct DefaultGemmUniversal<
   ElementA,
   LayoutA,
   ComplexTransform::kNone,   // transform A
   kAlignmentA,
   ElementB,
   LayoutB,
   ComplexTransform::kNone,   // transform B
@@ -183,72 +187,55 @@
   ArchTag,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
-  GroupScheduleMode_,
   Operator,
   SharedMemoryClear,
+  GatherA,
+  GatherB,
+  ScatterD,
   PermuteDLayout,
   typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
-  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
-  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
-
-  using MapArguments = kernel::detail::MapArguments<
+  using DefaultGemmKernel = typename kernel::DefaultGemm<
     ElementA,
     LayoutA,
-    ComplexTransform::kNone,
     kAlignmentA,
     ElementB,
     LayoutB,
-    ComplexTransform::kNone,
     kAlignmentB,
-    LayoutC,
-    kInternalTranspose
-  >;
-
-  // Define the default GEMM kernel
-  using DefaultGemmKernel = typename kernel::DefaultGemm<
-    typename MapArguments::ElementA,
-    typename MapArguments::LayoutA,
-    MapArguments::kAlignmentA,
-    typename MapArguments::ElementB,
-    typename MapArguments::LayoutB,
-    MapArguments::kAlignmentB,
     ElementC,
-    typename MapArguments::LayoutC,
+    LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
     true,
     Operator,
     SharedMemoryClear,
-    false, /*GatherA*/
-    false, /*GatherB*/
-    false, /*ScatterD*/
+    GatherA,
+    GatherB,
+    ScatterD,
     PermuteDLayout
   >::GemmKernel;
 
     /// Define the kernel in terms of the default kernel
-  using GemmKernel = kernel::GemmGrouped<
+  using GemmKernel = kernel::GemmUniversal<
     typename DefaultGemmKernel::Mma,
     typename DefaultGemmKernel::Epilogue,
-    ThreadblockSwizzle,
-    GroupScheduleMode_,
-    kInternalTranspose
+    ThreadblockSwizzle
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 //
 // Complex-valued GEMM kernels
@@ -289,22 +276,20 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_,
     /// Operation performed by GEMM
     typename Operator,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear
   >
-struct DefaultGemmGrouped<
+struct DefaultGemmUniversal<
   ElementA,
   LayoutA,
   TransformA,
   kAlignmentA,
   ElementB,
   LayoutB,
   TransformB,
@@ -316,66 +301,50 @@
   ArchTag,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
-  GroupScheduleMode_,
   Operator,
   SharedMemoryClear,
-  layout::NoPermute, /*PermuteDLayout*/
+  false,
+  false,
+  false,
+  layout::NoPermute,
   typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
-  // If true, we must construct a 'transposed-and-exchanged' Mma operator.
-  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
-
-  using MapArguments = kernel::detail::MapArguments<
+  using DefaultGemmKernel = typename kernel::DefaultGemmComplex<
     ElementA,
     LayoutA,
-    TransformA,
-    kAlignmentA,
     ElementB,
     LayoutB,
-    TransformB,
-    kAlignmentB,
-    LayoutC,
-    kInternalTranspose
-  >;
-
-  using DefaultGemmKernel = typename kernel::DefaultGemmComplex<
-    typename MapArguments::ElementA,
-    typename MapArguments::LayoutA,
-    typename MapArguments::ElementB,
-    typename MapArguments::LayoutB,
     ElementC,
-    typename MapArguments::LayoutC,
+    LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
-    MapArguments::kTransformA,
-    MapArguments::kTransformB,
+    TransformA,
+    TransformB,
     Operator,
     false
   >::GemmKernel;
 
   /// Define the kernel in terms of the default kernel
-  using GemmKernel = kernel::GemmGrouped<
+  using GemmKernel = kernel::GemmUniversal<
     typename DefaultGemmKernel::Mma,
     typename DefaultGemmKernel::Epilogue, 
-    ThreadblockSwizzle,
-    GroupScheduleMode_,
-    kInternalTranspose
+    ThreadblockSwizzle
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_layernorm_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_planar_complex_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_sparse.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_splitk_parallel.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,68 +26,61 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
-      Default kernel-level GEMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
-  
-      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
-      accommodated by exchanging A and B operands and assuming transposed layouts. Partial
-      specializations here choose 'device::GemmTransposed' to implement this functionality.
-
+    \brief
+      Default kernel-level grouped Rank2K.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/kernel/gemm_universal.h"
-#include "cutlass/gemm/kernel/gemm_universal_streamk.h"
-#include "cutlass/gemm/kernel/default_gemm.h"
-#include "cutlass/gemm/kernel/default_gemm_complex.h"
-
-#include "cutlass/layout/permute.h"
+#include "cutlass/gemm/kernel/rank_2k_transpose_operands.h"
+#include "cutlass/gemm/kernel/default_rank_2k.h"
+#include "cutlass/gemm/kernel/default_rank_2k_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
-    typename ElementA_,
+    typename ElementA,
     /// Layout type for A matrix operand
-    typename LayoutA_,
+    typename LayoutA,
     /// Complex elementwise transformation on A operand
     ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
-    typename ElementB_,
+    typename ElementB,
     /// Layout type for B matrix operand
-    typename LayoutB_,
+    typename LayoutB,
     /// Complex elementwise transformation on B operand
     ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
-    typename ElementC_,
+    typename ElementC,
     /// Layout type for C and D matrix operands
-    typename LayoutC_,
+    typename LayoutC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -100,51 +93,51 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Operation performed by GEMM
     typename Operator,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
-    /// Gather operand A by using an index array
-    bool GatherA = false,
-    /// Gather operand B by using an index array
-    bool GatherB = false,
-    /// Scatter result D by using an index array
-    bool ScatterD = false,
-    /// Permute result D
-    typename PermuteDLayout = layout::NoPermute,
+    /// Blas3 computation mode
+    BlasMode BlasMode_ = BlasMode::kSymmetric,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
     ///
     typename Enable = void
     >
-struct DefaultGemmUniversal;
+struct DefaultRank2KGrouped;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Real-valued GEMM kernels
+// Real-valued grouped Rank2K
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -157,107 +150,87 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Operation performed by GEMM
     typename Operator,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear,
-    /// Gather operand A by using an index array
-    bool GatherA,
-    /// Gather operand B by using an index array
-    bool GatherB,
-    /// Scatter result D by using an index array
-    bool ScatterD,
-    /// Permute result D
-    typename PermuteDLayout
->
-struct DefaultGemmUniversal<
-  ElementA,
-  LayoutA,
-  ComplexTransform::kNone,   // transform A
-  kAlignmentA,
-  ElementB,
-  LayoutB,
-  ComplexTransform::kNone,   // transform B
-  kAlignmentB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  OperatorClass,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  Stages,
-  Operator,
-  SharedMemoryClear,
-  GatherA,
-  GatherB,
-  ScatterD,
-  PermuteDLayout,
-  typename platform::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+    /// Blas3 computation mode
+    BlasMode BlasMode_,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_
+    >
+struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
+          ElementB, LayoutB, TransformB, kAlignmentB,
+          ElementC, LayoutC,
+          FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
+          WarpShape, InstructionShape, EpilogueOutputOp,
+          ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
+          typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
+  // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
+  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  using DefaultGemmKernel = typename kernel::DefaultGemm<
+  using MapArguments = kernel::detail::Rank2KMapArguments<
     ElementA,
     LayoutA,
+    TransformA,
     kAlignmentA,
     ElementB,
     LayoutB,
+    TransformB,
     kAlignmentB,
-    ElementC,
     LayoutC,
+    FillModeC,
+    kInternalTranspose
+  >;
+
+  // Define the default grouped Rank2K kernel
+  using DefaultRank2Kkernel = typename kernel::DefaultRank2K<
+    typename MapArguments::ElementA,
+    typename MapArguments::LayoutA,
+    MapArguments::kAlignmentA,
+    typename MapArguments::ElementB,
+    typename MapArguments::LayoutB,
+    MapArguments::kAlignmentB,
+    ElementC,
+    typename MapArguments::LayoutC,
+    MapArguments::kFillModeC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
-    true,
+    false,                  // SplitKSerial
     Operator,
-    SharedMemoryClear,
-    GatherA,
-    GatherB,
-    ScatterD,
-    PermuteDLayout
-  >::GemmKernel;
-
-  /// Universal kernel without StreamkFeature member type
-  template <class SwizzleT, class Enable = void>
-  class SelectBase :
-    public kernel::GemmUniversal<
-      typename DefaultGemmKernel::Mma,
-      typename DefaultGemmKernel::Epilogue,
-      SwizzleT>
-  {};
-
-  /// Universal kernel with StreamkFeature member type
-  template <class SwizzleT>
-  class SelectBase<SwizzleT, typename SwizzleT::StreamkFeature> :
-    public kernel::GemmUniversalStreamk<
-      typename DefaultGemmKernel::Mma,
-      typename DefaultGemmKernel::Epilogue,
-      SwizzleT>
-  {};
+    BlasMode_
+  >::Rank2Kkernel;
 
-  /// Select kernel by ThreadblockSwizzle's support for StreamkFeature
-  using GemmKernel = SelectBase<ThreadblockSwizzle>;
+  /// Define the kernel in terms of the default kernel
+  using Rank2Kkernel = kernel::Rank2KGrouped<
+    typename DefaultRank2Kkernel::Mma1,
+    typename DefaultRank2Kkernel::Mma2,
+    typename DefaultRank2Kkernel::Epilogue,
+    ThreadblockSwizzle,
+    TransformA,
+    TransformB,
+    DefaultRank2Kkernel::kFillModeC,
+    DefaultRank2Kkernel::kBlasMode,
+    GroupScheduleMode_,
+    kInternalTranspose
+  >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
 //
-// Complex-valued GEMM kernels
+// Complex-valued grouped Rank2K
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
@@ -273,14 +246,16 @@
     ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
+    /// Fill Mode for C (kLower or kUpper)
+    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -293,88 +268,86 @@
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// Operation performed by GEMM
     typename Operator,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear
-  >
-struct DefaultGemmUniversal<
-  ElementA,
-  LayoutA,
-  TransformA,
-  kAlignmentA,
-  ElementB,
-  LayoutB,
-  TransformB,
-  kAlignmentB,
-  ElementC,
-  LayoutC,
-  ElementAccumulator,
-  OperatorClass,
-  ArchTag,
-  ThreadblockShape,
-  WarpShape,
-  InstructionShape,
-  EpilogueOutputOp,
-  ThreadblockSwizzle,
-  Stages,
-  Operator,
-  SharedMemoryClear,
-  false,
-  false,
-  false,
-  layout::NoPermute,
-  typename platform::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+    /// Blas3 computation mode
+    BlasMode BlasMode_,
+    /// Whether the schedule of problems to visit has been precomputed
+    GroupScheduleMode GroupScheduleMode_
+    >
+struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
+          ElementB, LayoutB, TransformB, kAlignmentB,
+          ElementC, LayoutC,
+          FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
+          WarpShape, InstructionShape, EpilogueOutputOp,
+          ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
+          typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
+  // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
+  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  using DefaultGemmKernel = typename kernel::DefaultGemmComplex<
+  using MapArguments = kernel::detail::Rank2KMapArguments<
     ElementA,
     LayoutA,
+    TransformA,
+    kAlignmentA,
     ElementB,
     LayoutB,
-    ElementC,
+    TransformB,
+    kAlignmentB,
     LayoutC,
+    FillModeC,
+    kInternalTranspose
+  >;
+
+  // Define the default grouped Rank2K kernel
+  using DefaultRank2Kkernel = typename kernel::DefaultRank2KComplex<
+    typename MapArguments::ElementA,
+    typename MapArguments::LayoutA,
+    typename MapArguments::ElementB,
+    typename MapArguments::LayoutB,
+    ElementC,
+    typename MapArguments::LayoutC,
+    MapArguments::kFillModeC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
+    MapArguments::kTransformA,
+    MapArguments::kTransformB,
+    Operator,
+    false,                  // SplitKSerial
+    BlasMode_
+  >::Rank2Kkernel;
+
+  /// Define the kernel in terms of the default kernel
+  /// Pass through the user-provided TransformA and TransformB so as to
+  /// correctly set public-facing TransformA and TransformB in kernel::Rank2KGrouped.
+  /// This is needed because kernel::DefaultRank2KComplex may change TransformA and
+  /// TransformB that become template arguments to Mma1 and Mma2.
+  using Rank2Kkernel = kernel::Rank2KGrouped<
+    typename DefaultRank2Kkernel::Mma1,
+    typename DefaultRank2Kkernel::Mma2,
+    typename DefaultRank2Kkernel::Epilogue,
+    ThreadblockSwizzle,
     TransformA,
     TransformB,
-    Operator,
-    false
-  >::GemmKernel;
-
-  /// Universal kernel without StreamkFeature member type
-  template <class SwizzleT, class Enable = void>
-  class SelectBase :
-    public kernel::GemmUniversal<
-      typename DefaultGemmKernel::Mma,
-      typename DefaultGemmKernel::Epilogue,
-      SwizzleT>
-  {};
-
-  /// Universal kernel with StreamkFeature member type
-  template <class SwizzleT>
-  class SelectBase<SwizzleT, typename SwizzleT::StreamkFeature> :
-    public kernel::GemmUniversalStreamk<
-      typename DefaultGemmKernel::Mma,
-      typename DefaultGemmKernel::Epilogue,
-      SwizzleT>
-  {};
-
-  /// Select kernel by ThreadblockSwizzle's support for StreamkFeature
-  using GemmKernel = SelectBase<ThreadblockSwizzle>;
+    DefaultRank2Kkernel::kFillModeC,
+    DefaultRank2Kkernel::kBlasMode,
+    GroupScheduleMode_,
+    kInternalTranspose
+  >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_broadcast.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -117,15 +117,15 @@
   // Replace epilogue
   using Epilogue = typename cutlass::epilogue::threadblock::DefaultEpilogueWithBroadcastTensorOp<
     typename GemmBase::Epilogue::Shape,
     typename GemmBase::Epilogue::WarpMmaOperator,
     GemmBase::Epilogue::kPartitionsK,
     ElementC_,
     typename EpilogueOutputOp::ElementT,
-    typename EpilogueOutputOp::ElementVector,
+    ElementC_,
     EpilogueOutputOp,
     GemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Compose the GEMM kernel
   using GemmKernel = GemmWithFusedEpilogue<
     typename GemmBase::Mma,
@@ -133,15 +133,15 @@
     ThreadblockSwizzle
   >;
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization: ArchTag = cutlass::arch::Sm70
+/// Parital specialization: ArchTag = cutlass::arch::Sm70
 ///
 ///
 template <
   /// Element type for A matrix operand
   typename ElementA_,
   /// Layout type for A matrix operand
   typename LayoutA_,
@@ -217,15 +217,15 @@
   // Replace epilogue
   using Epilogue = typename cutlass::epilogue::threadblock::DefaultEpilogueWithBroadcastVoltaTensorOp<
     typename GemmBase::Epilogue::Shape,
     typename GemmBase::Epilogue::WarpMmaOperator,
     GemmBase::Epilogue::kPartitionsK,
     ElementC_,
     typename EpilogueOutputOp::ElementT,
-    typename EpilogueOutputOp::ElementVector,
+    ElementC_,
     EpilogueOutputOp,
     GemmBase::Epilogue::kElementsPerAccess
   >::Epilogue;
 
   // Compose the GEMM kernel
   using GemmKernel = GemmWithFusedEpilogue<
     typename GemmBase::Mma,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_k_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -87,15 +87,15 @@
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
-    /// Reduce A or B along the K dimension
+    ///
     bool ReduceKForA_,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemm_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -134,15 +134,15 @@
     Epilogue,
     ThreadblockSwizzle
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization: ArchTag = cutlass::arch::Sm70
+/// Parital specialization: ArchTag = cutlass::arch::Sm70
 ///
 ///
 template <
   /// Element type for A matrix operand
   typename ElementA_,
   /// Layout type for A matrix operand
   typename LayoutA_,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_gemv.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -116,92 +116,14 @@
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
 struct DefaultRank2K;
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultRank2K<
-                    ElementA, LayoutA, kAlignmentA, 
-                    ElementB, LayoutB, kAlignmentB, 
-                    ElementC,layout::RowMajor, FillModeC, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x BT)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, 
-      kAlignmentA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      kAlignmentB,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-  
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x AT)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementB, LayoutB, 
-      kAlignmentB, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      kAlignmentA,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
-
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, BlasMode::kSymmetric>;
-};
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level Rank2K definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
 
   
 */
 
 #pragma once
 
@@ -44,21 +44,22 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_2k_universal.h"
+#include "cutlass/gemm/kernel/symm_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
+#include "cutlass/gemm/threadblock/default_multistage_trmm_complex.h"
 #include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op_blas3.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
 
 
@@ -71,24 +72,26 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode kSideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -99,398 +102,213 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRank2KComplex;
-
-
-////////////////////////////////////////////////////////////////////////////////
-namespace detail {
-
-template <
-  /// Layout type for A matrix operand
-  typename LayoutA_,
-  /// Layout type for B matrix operand
-  typename LayoutB_,
-  /// Complex elementwise transformation 
-  ComplexTransform TransformA,
-  /// Complex elementwise transformation 
-  ComplexTransform TransformB,
-  /// Blas3 computation mode (symmetric/hermitian)
-  BlasMode BlasMode_
-  > struct Rank2KTransposedComplexTransform {
-  
-  static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
-
-};
-  
-  // partial specializations for HER2K CUBLAS_OP_N layout (ColumMajor)
-template <>
-  struct Rank2KTransposedComplexTransform <
-  layout::ColumnMajor, layout::ColumnMajor, 
-  ComplexTransform::kNone, ComplexTransform::kNone,
-  BlasMode::kHermitian> {
-
-  static ComplexTransform const kTransformA = ComplexTransform::kConjugate;
-  static ComplexTransform const kTransformB = ComplexTransform::kNone;
-
-};
-
-  // partial specializations for HER2K CUBLAS_OP_C layout (RowMajor + Complex conjugate) 
-template <>
-  struct Rank2KTransposedComplexTransform <
-  layout::RowMajor, layout::RowMajor, 
-  ComplexTransform::kConjugate, ComplexTransform::kConjugate,
-  BlasMode::kHermitian> {
-
-  static ComplexTransform const kTransformA = ComplexTransform::kNone;
-  static ComplexTransform const kTransformB = ComplexTransform::kConjugate;
-
-};
-
-}
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture complex datatype (symmetric)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^T)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementB, LayoutB, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture complex datatype (hermitian)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
-  // Complex transform for input A and B matrices (function on input layout)
-  static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
-
-  using TransposedComplexTransform = detail::Rank2KTransposedComplexTransform<
-                                        LayoutA, LayoutB, 
-                                        TransformA, TransformB,
-                                        kBlasMode>;
-
-  // Complex transform on operandA and operandB (function of blas3 computation)
-  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
-  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^H)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^H)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementB, LayoutB, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
-
-};
+struct DefaultSymmComplex;
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture complex datatype (symmetric)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode kSideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
     /// Element type for C and D matrix operands
     typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+struct DefaultSymmComplex<
+  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
+  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
   arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
   EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kSymmetric> {
+  Operator, SplitKSerial, BlasMode::kSymmetric> {
 
   static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+  // Complex Transform don't appply to A or B for SYMM
+  static ComplexTransform const TransformA = ComplexTransform::kNone; 
+  static ComplexTransform const TransformB = ComplexTransform::kNone; 
+
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
+	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
       ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^T)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
       ElementB, LayoutB, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformB, Operator>::ThreadblockMma;
+      kSideModeA, kFillModeA, kDiagTypeMma1, 
+      ElementAccumulator, layout::RowMajor, 
+      arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape,
+      Stages, TransformA, TransformB, Operator>::ThreadblockMma;
+
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
+	static const DiagType kDiagTypeMma2 = DiagType::kZero;
+  using LayoutAMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                typename layout::LayoutTranspose<LayoutA>::type, 
+                                LayoutA
+                              >::type;
+  using LayoutBMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                LayoutB, 
+                                typename layout::LayoutTranspose<LayoutB>::type
+                              >::type; 
+	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
+			ElementA, LayoutAMma2, 
+			ElementB, LayoutBMma2, 
+			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
+			ElementAccumulator, layout::RowMajor, 
+			arch::OpClassTensorOp, arch::Sm80,
+			ThreadblockShape, WarpShape, InstructionShape,
+			Stages, TransformA, TransformB, Operator>::ThreadblockMma;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
           ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+          EpilogueOutputOp::kCount, Operator>::Epilogue;
 
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
+  /// Define the kernel-level Symm operator.
+  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture complex datatype (hermitian)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode kSideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
     /// Element type for C and D matrix operands
     typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Operation performed by GEMM
     typename Operator,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial>
-struct DefaultRank2KComplex<
-  ElementA, LayoutA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
+struct DefaultSymmComplex<
+  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
+  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
   arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
   EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, TransformB, Operator, SplitKSerial, BlasMode::kHermitian> {
+  Operator, SplitKSerial, BlasMode::kHermitian> {
 
   static BlasMode const kBlasMode = BlasMode::kHermitian;
 
-  // Complex transform for input A and B matrices (function on input layout)
-  static ComplexTransform const kTransformA = TransformA;
-  static ComplexTransform const kTransformB = TransformB;
-
-  using TransposedComplexTransform = detail::Rank2KTransposedComplexTransform<
-                                        LayoutA, LayoutB, 
-                                        TransformA, TransformB,
-                                        kBlasMode>;
-
-  // Complex transform on operandA and operandB (function of blas3 computation)
-  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
-  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
 
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^H)
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
+	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
+  static ComplexTransform const TransformAMma1 = ComplexTransform::kNone; 
+  static ComplexTransform const TransformBMma1 = ComplexTransform::kNone; 
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
       ElementA, LayoutA, 
-      ElementB, typename layout::LayoutTranspose<LayoutB>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (B x A^H)
-  using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
       ElementB, LayoutB, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
+      kSideModeA, kFillModeA, kDiagTypeMma1, 
+      ElementAccumulator, layout::RowMajor, 
+      arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape,
+      Stages, TransformAMma1, TransformBMma1, Operator, BlasMode::kHermitian>::ThreadblockMma;
+
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// TRMM - withOUT diagonal - with conjugate transpose: alpha * AT * B or alpha * B * AT
+	static const DiagType kDiagTypeMma2 = DiagType::kZero;
+  using LayoutAMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                typename layout::LayoutTranspose<LayoutA>::type, 
+                                LayoutA
+                              >::type;
+  using LayoutBMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                LayoutB, 
+                                typename layout::LayoutTranspose<LayoutB>::type
+                              >::type;
+  static ComplexTransform const TransformAMma2 = (kSideModeA == SideMode::kLeft) ? 
+                                              ComplexTransform::kConjugate : ComplexTransform::kNone;
+  static ComplexTransform const TransformBMma2 = (kSideModeA == SideMode::kLeft) ? 
+                                              ComplexTransform::kNone : ComplexTransform::kConjugate;
+
+	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
+			ElementA, LayoutAMma2, 
+			ElementB, LayoutBMma2, 
+			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
+			ElementAccumulator, layout::RowMajor, 
+			arch::OpClassTensorOp, arch::Sm80,
+			ThreadblockShape, WarpShape, InstructionShape,
+			Stages, TransformAMma2, TransformBMma2, Operator>::ThreadblockMma;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
+      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
           ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
+          EpilogueOutputOp::kCount, Operator>::Epilogue;
 
-  /// Define the kernel-level Rank2K operator.
-  using Rank2Kkernel = kernel::Rank2KUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, FillModeC, kBlasMode>;
+  /// Define the kernel-level Symm operator.
+  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
 
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
 
 }  // namespace kernel
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,61 +26,64 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief
-      Default kernel-level grouped Rank2K.
+    \brief 
+      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
+      the appropriate threadblock-scoped epilogue.
+  
+      Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
+      accommodated by exchanging A and B operands and assuming transposed layouts.
+
+  
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
+#include "cutlass/blas3.h"
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
-#include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/kernel/rank_2k_transpose_operands.h"
-#include "cutlass/gemm/kernel/default_rank_2k.h"
-#include "cutlass/gemm/kernel/default_rank_2k_complex.h"
+#include "cutlass/gemm/kernel/symm_universal.h"
+#include "cutlass/gemm/kernel/default_symm.h"
+#include "cutlass/gemm/kernel/default_symm_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
-    typename ElementA,
+    typename ElementA_,
     /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
+    typename LayoutA_,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode SideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode FillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
-    typename ElementB,
+    typename ElementB_,
     /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
+    typename LayoutB_,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
-    typename ElementC,
+    typename ElementC_,
     /// Layout type for C and D matrix operands
-    typename LayoutC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
+    typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -91,53 +94,52 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by SYRK
     typename Operator,
-    /// Blas3 computation mode
+    /// Blas3 computation mode (symmetric/hermitian)
     BlasMode BlasMode_ = BlasMode::kSymmetric,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_ = GroupScheduleMode::kDeviceOnly,
     ///
     typename Enable = void
     >
-struct DefaultRank2KGrouped;
+struct DefaultSymmUniversal;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Real-valued grouped Rank2K
+// Real-valued SYMM/HEMM update kernels
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode SideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode FillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -148,114 +150,108 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// Blas3 computation mode
-    BlasMode BlasMode_,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_
-    >
-struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
-          ElementB, LayoutB, TransformB, kAlignmentB,
-          ElementC, LayoutC,
-          FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
-          WarpShape, InstructionShape, EpilogueOutputOp,
-          ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
-          typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by SYMM/HEMM
+    typename Operator>
+struct DefaultSymmUniversal<
+  ElementA,
+  LayoutA,
+  SideModeA,
+  FillModeA,
+  kAlignmentA,
+  ElementB,
+  LayoutB,
+  kAlignmentB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  OperatorClass,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  SplitKSerial,
+  Operator,
+  BlasMode::kSymmetric,
+  typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
-  // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
-  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  using MapArguments = kernel::detail::Rank2KMapArguments<
+  using DefaultSymmkernel = typename kernel::DefaultSymm<
     ElementA,
     LayoutA,
-    TransformA,
+    SideModeA,
+    FillModeA,
     kAlignmentA,
     ElementB,
     LayoutB,
-    TransformB,
     kAlignmentB,
-    LayoutC,
-    FillModeC,
-    kInternalTranspose
-  >;
-
-  // Define the default grouped Rank2K kernel
-  using DefaultRank2Kkernel = typename kernel::DefaultRank2K<
-    typename MapArguments::ElementA,
-    typename MapArguments::LayoutA,
-    MapArguments::kAlignmentA,
-    typename MapArguments::ElementB,
-    typename MapArguments::LayoutB,
-    MapArguments::kAlignmentB,
     ElementC,
-    typename MapArguments::LayoutC,
-    MapArguments::kFillModeC,
+    LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
-    false,                  // SplitKSerial
+    SplitKSerial,
     Operator,
-    BlasMode_
-  >::Rank2Kkernel;
+    BlasMode::kSymmetric
+  >::SymmKernel;
 
-  /// Define the kernel in terms of the default kernel
-  using Rank2Kkernel = kernel::Rank2KGrouped<
-    typename DefaultRank2Kkernel::Mma1,
-    typename DefaultRank2Kkernel::Mma2,
-    typename DefaultRank2Kkernel::Epilogue,
+    /// Define the kernel in terms of the default kernel
+  using SymmKernel = kernel::SymmUniversal<
+    typename DefaultSymmkernel::Mma1,
+    typename DefaultSymmkernel::Mma2,
+    typename DefaultSymmkernel::Epilogue, 
     ThreadblockSwizzle,
-    TransformA,
-    TransformB,
-    DefaultRank2Kkernel::kFillModeC,
-    DefaultRank2Kkernel::kBlasMode,
-    GroupScheduleMode_,
-    kInternalTranspose
+    SideModeA,
+    FillModeA
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
 //
-// Complex-valued grouped Rank2K
+// Complex-valued SYMM/HEMM update kernels
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode SideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode FillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -266,90 +262,81 @@
     typename InstructionShape,
     /// Epilogue output operator
     typename EpilogueOutputOp,
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Operation performed by GEMM
+    /// If true, kernel is configured to support serial reduction in the
+    /// epilogue
+    bool SplitKSerial,
+    /// Operation performed by SYRK
     typename Operator,
-    /// Blas3 computation mode
-    BlasMode BlasMode_,
-    /// Whether the schedule of problems to visit has been precomputed
-    GroupScheduleMode GroupScheduleMode_
-    >
-struct DefaultRank2KGrouped<ElementA, LayoutA, TransformA, kAlignmentA,
-          ElementB, LayoutB, TransformB, kAlignmentB,
-          ElementC, LayoutC,
-          FillModeC, ElementAccumulator, OperatorClass, ArchTag, ThreadblockShape,
-          WarpShape, InstructionShape, EpilogueOutputOp,
-          ThreadblockSwizzle, Stages, Operator, BlasMode_, GroupScheduleMode_,
-          typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
+    // BlasMode
+    BlasMode kBlasMode
+  >
+
+struct DefaultSymmUniversal<
+  ElementA,
+  LayoutA,
+  SideModeA,
+  FillModeA, 
+  kAlignmentA,
+  ElementB,
+  LayoutB,
+  kAlignmentB,
+  ElementC,
+  LayoutC,
+  ElementAccumulator,
+  OperatorClass,
+  ArchTag,
+  ThreadblockShape,
+  WarpShape,
+  InstructionShape,
+  EpilogueOutputOp,
+  ThreadblockSwizzle,
+  Stages,
+  SplitKSerial,
+  Operator,
+  kBlasMode,
+  typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
-  // If true, we must construct a 'transposed-and-exchanged' Rank2K operator.
-  static bool const kInternalTranspose = platform::is_same<LayoutC, layout::ColumnMajor>::value;
 
-  using MapArguments = kernel::detail::Rank2KMapArguments<
+  using DefaultSymmkernel = typename kernel::DefaultSymmComplex<
     ElementA,
     LayoutA,
-    TransformA,
-    kAlignmentA,
+    SideModeA,
+    FillModeA,
     ElementB,
     LayoutB,
-    TransformB,
-    kAlignmentB,
-    LayoutC,
-    FillModeC,
-    kInternalTranspose
-  >;
-
-  // Define the default grouped Rank2K kernel
-  using DefaultRank2Kkernel = typename kernel::DefaultRank2KComplex<
-    typename MapArguments::ElementA,
-    typename MapArguments::LayoutA,
-    typename MapArguments::ElementB,
-    typename MapArguments::LayoutB,
     ElementC,
-    typename MapArguments::LayoutC,
-    MapArguments::kFillModeC,
+    LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
-    MapArguments::kTransformA,
-    MapArguments::kTransformB,
     Operator,
-    false,                  // SplitKSerial
-    BlasMode_
-  >::Rank2Kkernel;
-
-  /// Define the kernel in terms of the default kernel
-  /// Pass through the user-provided TransformA and TransformB so as to
-  /// correctly set public-facing TransformA and TransformB in kernel::Rank2KGrouped.
-  /// This is needed because kernel::DefaultRank2KComplex may change TransformA and
-  /// TransformB that become template arguments to Mma1 and Mma2.
-  using Rank2Kkernel = kernel::Rank2KGrouped<
-    typename DefaultRank2Kkernel::Mma1,
-    typename DefaultRank2Kkernel::Mma2,
-    typename DefaultRank2Kkernel::Epilogue,
+    SplitKSerial,
+    kBlasMode
+  >::SymmKernel;
+
+    /// Define the kernel in terms of the default kernel
+  using SymmKernel = kernel::SymmUniversal<
+    typename DefaultSymmkernel::Mma1,
+    typename DefaultSymmkernel::Mma2,
+    typename DefaultSymmkernel::Epilogue, 
     ThreadblockSwizzle,
-    TransformA,
-    TransformB,
-    DefaultRank2Kkernel::kFillModeC,
-    DefaultRank2Kkernel::kBlasMode,
-    GroupScheduleMode_,
-    kInternalTranspose
+    SideModeA,
+    FillModeA
   >;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_2k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level RankK definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
 
   
 */
 
 #pragma once
 
@@ -44,23 +44,24 @@
 #include "cutlass/layout/matrix.h"
 #include "cutlass/arch/wmma.h"
 
 #include "cutlass/epilogue/threadblock/epilogue.h"
 #include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/rank_k_universal.h"
+#include "cutlass/gemm/kernel/symm_universal.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+#include "cutlass/gemm/threadblock/default_trmm.h"
 #include "cutlass/gemm/threadblock/default_mma.h"
 #include "cutlass/gemm/threadblock/default_mma_core_simt.h"
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op_blas3.h"
+#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
 #include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator.h"
 
 #if defined(CUTLASS_ARCH_WMMA_ENABLED)
 #include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
 #endif //CUTLASS_ARCH_WMMA_ENABLED
@@ -75,22 +76,30 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode kSideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode kFillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB_,
+    /// Layout type for B matrix operand
+    typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
@@ -108,93 +117,39 @@
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator,
     /// Blas3 computation mode
     BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultRankK;
+struct DefaultSymm;
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultRankK<
-                    ElementA, LayoutA, kAlignmentA, 
-                    ElementC,layout::RowMajor, FillModeC, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x AT)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, 
-      kAlignmentA, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      kAlignmentA,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-  
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
-
-  /// Define the kernel-level Rank2 operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
-};
 
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Side Mode for A (kLeft or kRight)
+    SideMode kSideModeA,
+    /// Fill Mode for A (kLower or kUpper)
+    FillMode kFillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentB,
     /// Element type for C and D matrix operands
     typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Warp-level tile size (concept: GemmShape)
@@ -206,42 +161,67 @@
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator>
-struct DefaultRankK<
-                    ElementA, LayoutA, kAlignmentA, 
-                    ElementC,layout::RowMajor, FillModeC, 
+struct DefaultSymm<
+                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
+                    ElementB, LayoutB, kAlignmentB, 
+                    ElementC,layout::RowMajor, 
                     ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
                     ThreadblockShape, WarpShape, InstructionShape,
                     EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
                     Operator> {
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x AT)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMma<
-      ElementA, LayoutA, 
-      kAlignmentA, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      kAlignmentA,
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-  
+
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate
+  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
+	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
+  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
+      ElementA, LayoutA, kAlignmentA, 
+      ElementB, LayoutB, kAlignmentB,
+      kSideModeA, kFillModeA, kDiagTypeMma1, 
+      ElementAccumulator, layout::RowMajor, 
+      arch::OpClassTensorOp, arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape,
+      Stages, Operator>::ThreadblockMma;
+
+  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
+  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
+	static const DiagType kDiagTypeMma2 = DiagType::kZero;
+  using LayoutAMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                typename layout::LayoutTranspose<LayoutA>::type, 
+                                LayoutA
+                              >::type;
+  using LayoutBMma2 = typename platform::conditional<
+                                (kSideModeA == SideMode::kLeft), 
+                                LayoutB, 
+                                typename layout::LayoutTranspose<LayoutB>::type
+                              >::type; 
+	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
+			ElementA, LayoutAMma2, kAlignmentA, 
+			ElementB, LayoutBMma2, kAlignmentB,
+			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
+			ElementAccumulator, layout::RowMajor, 
+			arch::OpClassTensorOp, arch::Sm80,
+			ThreadblockShape, WarpShape, InstructionShape,
+			Stages, Operator>::ThreadblockMma;
 
   static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
 
   /// Define the epilogue
   using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, BlasMode::kSymmetric>::Epilogue;
+      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
+          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
+          EpilogueOutputOp::kCount>::Epilogue;
 
-  /// Define the kernel-level Rank2 operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
+  /// Define the kernel-level SYMM/HEMM operator.
+  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
 };
 ////////////////////////////////////////////////////////////////////////////////
 
 
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_complex.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -151,148 +151,14 @@
   static ComplexTransform const kTransformB = ComplexTransform::kConjugate;
 
 };
 
 }
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture complex datatype (symmetric)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultRankKComplex<
-  ElementA, LayoutA, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x B^T)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementA, LayoutA, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      TransformA, TransformA, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level RankK operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture complex datatype (hermitian)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Fill Mode for C (kLower or kUpper)
-    FillMode FillModeC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultRankKComplex<
-  ElementA, LayoutA, ElementC, 
-  layout::RowMajor, FillModeC, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  TransformA, Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
-  // Complex transform for input A and B matrices (function on input layout)
-  static ComplexTransform const kTransformA = TransformA;
-
-  using TransposedComplexTransform = detail::RankKTransposedComplexTransform<
-                                        LayoutA, 
-                                        TransformA,
-                                        kBlasMode>;
-
-  // Complex transform on operandA and operandB (function of blas3 computation)
-  static ComplexTransform const kTransformOperandA = TransposedComplexTransform::kTransformA;
-  static ComplexTransform const kTransformOperandB = TransposedComplexTransform::kTransformB;
-
-  /// Define the threadblock-scoped matrix multiply-accumulate (A x A^H)
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplex<
-      ElementA, LayoutA, 
-      ElementA, typename layout::LayoutTranspose<LayoutA>::type, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, 
-      ThreadblockShape, WarpShape, InstructionShape, Stages, 
-      kTransformOperandA, kTransformOperandB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOpBlas3<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator, kBlasMode>::Epilogue;
-
-  /// Define the kernel-level RankK operator.
-  using RankKkernel = kernel::RankKUniversal<Mma, Epilogue, ThreadblockSwizzle, FillModeC>;
-
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
 /// Partial specialization for Ampere Architecture complex datatype (symmetric)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Element type for C and D matrix operands
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_rank_k_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h`

 * *Files 19% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,298 +24,305 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief 
-      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
+    \brief Defines basic properties needed by CTA-level GEMMs assuming expectations about data
+      layout of the global memory fragments, data types, and internal tile sizes.
 
-  
+      Partial specializations for threadblock::Mma operations targeting simt instructions.
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/arch/wmma.h"
-
-#include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/array.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/symm_universal.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_trmm.h"
-#include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-#include "cutlass/epilogue/threadblock/default_epilogue_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_volta_tensor_op.h"
-#include "cutlass/epilogue/threadblock/default_epilogue_simt.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
+#include "cutlass/numeric_types.h"
+#include "cutlass/matrix_shape.h"
 
+#include "cutlass/gemm/warp/mma.h"
+#include "cutlass/gemm/threadblock/mma_pipelined.h"
+#include "cutlass/gemm/threadblock/mma_singlestage.h"
+#include "cutlass/arch/cache_operation.h" 
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace kernel {
-
-////////////////////////////////////////////////////////////////////////////////
+namespace threadblock {
 
 template <
-    /// Element type for A matrix operand
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Element type for C and D matrix operands
-    typename ElementC_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Operator class tag
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Shape of threadblock-scoped matrix multiply operator
+    typename Shape,
+    /// Shape of warp-level matrix multiply operator
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Shape of one matrix production operation (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// Blas3 computation mode
-    BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultSymm;
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
+    /// Element data type of A operand
     typename ElementA,
-    /// Layout type for A matrix operand
+    /// Layout of operand A
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
+    /// Element data type of B operand
     typename ElementB,
-    /// Layout type for B matrix operand
+    /// Layout of operand B
     typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
-    /// Element type for C and D matrix operands
+    /// Data type of accumulator
     typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultSymm<
-                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
-                    ElementB, LayoutB, kAlignmentB, 
-                    ElementC,layout::RowMajor, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm90, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
-      ElementA, LayoutA, kAlignmentA, 
-      ElementB, LayoutB, kAlignmentB,
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
-  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
-			ElementA, LayoutAMma2, kAlignmentA, 
-			ElementB, LayoutBMma2, kAlignmentB,
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm90,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
-
-  /// Define the kernel-level SYMM/HEMM operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
+    /// Layout of accumulator
+    typename LayoutC,
+    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
+    typename OperatorClass,
+    /// Size of a threadblock-scoped access
+    int kAccessSizeInBits = -1, // -1 denoting the default
+    /// Number of stages
+    int Stages = 2,
+    /// Operation performed by MMA
+    typename Operator = typename platform::conditional<
+        (platform::is_same<OperatorClass,
+                           cutlass::arch::OpClassTensorOp>::value) &&
+            (platform::is_same<ElementA, int8_t>::value ||
+             platform::is_same<ElementA, int4b_t>::value ||
+             platform::is_same<ElementA, uint8_t>::value ||
+             platform::is_same<ElementA, uint4b_t>::value),
+        cutlass::arch::OpMultiplyAddSaturate,
+        cutlass::arch::OpMultiplyAdd>::type,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false,
+    /// Cache operation of operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA =
+        cutlass::arch::CacheOperation::Global,
+    /// Cache operation of operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB =
+        cutlass::arch::CacheOperation::Global,
+    /// per-element transformation for elements of A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// per-element transformation for elements of B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
+>
+struct DefaultMmaCoreWithAccessSize;
 
-/// Partial specialization for Ampere Architecture
 template <
-    /// Element type for A matrix operand
+    /// Shape of threadblock-scoped matrix multiply operator
+    typename Shape,
+    /// Shape of warp-level matrix multiply operator
+    typename WarpShape,
+    /// Shape of one matrix production operation (concept: GemmShape)
+    typename InstructionShape,
+    /// Element data type of A operand
     typename ElementA,
-    /// Layout type for A matrix operand
+    /// Layout of operand A
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
+    /// Element data type of B operand
     typename ElementB,
-    /// Layout type for B matrix operand
+    /// Layout of operand B
     typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
-    /// Element type for C and D matrix operands
+    /// Data type of accumulator
     typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Layout of accumulator
+    typename LayoutC,
+    /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
+    typename OperatorClass,
+    /// Number of stages
     int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
+    /// Operation performed by MMA
+    typename Operator,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor,
+    /// Cache operation of operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA,
+    /// Cache operation of operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB,
+    /// per-element transformation for elements of A
+    ComplexTransform TransformA,
+    /// per-element transformation for elements of B
+    ComplexTransform TransformB,
+    bool IsComplex
+>
+struct DefaultMmaCoreWithAccessSize<
+    Shape, WarpShape, InstructionShape,
+    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
+    OperatorClass, -1, Stages, Operator, AccumulatorsInRowMajor,
+    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
+> : DefaultMmaCore<
+    Shape, WarpShape, InstructionShape,
+    ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
+    OperatorClass, Stages, Operator, AccumulatorsInRowMajor,
+    CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
+> {};
+
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization:
+///
+///   A: column-major
+///   B: row-major
+///   Operator: simt class
+///
+/// This uses the default warp-level operator given tile sizes
+template <
+    /// Shape of threadblock-scoped matrix multiply operator (concept:
+    /// GemmShape)
+    typename Shape_,
+    /// Shape of warp-level matrix multiply operator (concept: GemmShape)
+    typename WarpShape_,
+    /// Data type of A operand
+    typename ElementA_,
+    /// Data type of B operand
+    typename ElementB_,
+    /// Data type of accumulator
+    typename ElementC_,
+    /// Layout of accumulator
+    typename LayoutC_,
+    /// Size of a threadblock-scoped access (a value of -1 indicates the default)
+    int kAccessSizeInBits_,
     /// Operation performed by GEMM
-    typename Operator>
-struct DefaultSymm<
-                    ElementA, LayoutA, kSideModeA, kFillModeA, kAlignmentA, 
-                    ElementB, LayoutB, kAlignmentB, 
-                    ElementC,layout::RowMajor, 
-                    ElementAccumulator, arch::OpClassTensorOp, arch::Sm80, 
-                    ThreadblockShape, WarpShape, InstructionShape,
-                    EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                    Operator> {
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultTrmm<
-      ElementA, LayoutA, kAlignmentA, 
-      ElementB, LayoutB, kAlignmentB,
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate 
-  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultTrmm<
-			ElementA, LayoutAMma2, kAlignmentA, 
-			ElementB, LayoutBMma2, kAlignmentB,
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm80,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma1::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
-
-  /// Define the kernel-level SYMM/HEMM operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
+    typename Operator_>
+struct DefaultMmaCoreWithAccessSize<Shape_, WarpShape_, typename std::enable_if<kAccessSizeInBits_ != -1, GemmShape<1, 1, 1>>::type, ElementA_,
+                      layout::ColumnMajor, ElementB_, layout::RowMajor,
+                      ElementC_, LayoutC_, arch::OpClassSimt, kAccessSizeInBits_, 2, Operator_
+                     > {
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using InstructionShape = GemmShape<1, 1, 1>;
+  using ElementA = ElementA_;
+  using LayoutA = layout::ColumnMajor;
+  using ElementB = ElementB_;
+  using LayoutB = layout::RowMajor;
+  using ElementC = ElementC_;
+  using LayoutC = LayoutC_;
+  using OperatorClass = arch::OpClassSimt;
+  static int const PartitionsK = Shape::kK / WarpShape::kK;
+
+  /// Default Operator
+  using Operator = Operator_;
+
+  /// Number of warps present
+  using WarpCount = GemmShape<
+    Shape::kM / WarpShape::kM,
+    Shape::kN / WarpShape::kN,
+    PartitionsK
+  >;
+
+  // Divisility requirements
+  static_assert(
+    !(Shape::kM % WarpShape::kM) &&
+    !(Shape::kN % WarpShape::kN),
+    "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
+  );
+
+  /// Number of threads per warp
+  static int const kWarpSize = warp::WarpSize<arch::OpClassSimt>::value;
+
+  /// Number of threads total
+  static int const kThreads = WarpCount::kCount * kWarpSize;
+
+  static int const kElementsPerAccessDefault = 1;
+  static_assert(kAccessSizeInBits_ == -1 ||
+          sizeof_bits<ElementA>::value == sizeof_bits<ElementB>::value ||
+          kAccessSizeInBits_ / sizeof_bits<ElementA>::value == kElementsPerAccessDefault,
+          "Non-default value for kAccessSizeInBits_ is only allowed if size(elementA) == sizeof(elementB)");
+  static int const kElementsPerAccess = (kAccessSizeInBits_ != -1) ? kAccessSizeInBits_ / sizeof_bits<ElementA>::value : kElementsPerAccessDefault;
+
+  //
+  // Shared memory layouts
+  //
+
+  using SmemLayoutA = layout::ColumnMajor;
+  using SmemLayoutB = layout::RowMajor;
+
+  //
+  // Iterators to write to shared memory
+  //
+
+  /// ThreadMap of iterator A
+  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
+    layout::PitchLinearShape<Shape::kM, Shape::kK>,
+    kThreads,
+    kElementsPerAccess
+  >;
+
+  /// Shared memory iterator to A operand
+  using SmemIteratorA = transform::threadblock::RegularTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>, 
+    ElementA, 
+    SmemLayoutA,
+    1,
+    IteratorThreadMapA
+  >;
+
+  /// Policy of iterator B
+  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
+    layout::PitchLinearShape<Shape::kN, Shape::kK>,
+    kThreads,
+    kElementsPerAccess
+  >;
+
+  /// Shared memory iterator to B operand
+  using SmemIteratorB = transform::threadblock::RegularTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>, 
+    ElementB, 
+    SmemLayoutB,
+    0,
+    IteratorThreadMapB
+  >;
+
+  //
+  // Warp-level matrix multiply operator
+  //
+
+  // Define the warp-level op
+  static const int WarpNumThreadsM = detail::simt_get_warp_threads_m<WarpShape>();
+  static const int WarpNumThreadsN = kWarpSize / WarpNumThreadsM;
+  static const int ThreadTileM = WarpShape::kM / WarpNumThreadsM;
+  static const int ThreadTileN = WarpShape::kN / WarpNumThreadsN;
+  static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
+      "WarpShape must be divisible by ThreadTile shape.");
+  static const int LaneLayout = ThreadTileM > 4 && ThreadTileN > 4 ? 2 : 1;
+  static const int numElementsA = 128 / sizeof_bits<ElementA>::value;
+  static const int numElementsB = 128 / sizeof_bits<ElementB>::value;
+  static const int LaneM = cutlass::const_min(numElementsA, ThreadTileM);
+  static const int LaneN = cutlass::const_min(numElementsB, ThreadTileN);
+  // these should have max of thread tile also
+  using LaneMmaShape = cutlass::gemm::GemmShape<
+      LaneM,
+      LaneN,
+      1>;
+  using Policy = cutlass::gemm::warp::MmaSimtPolicy<
+      cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
+      cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
+      LaneMmaShape
+  >;
+
+  using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
+    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,     /// Data type of A elements
+    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,     /// Data type of B elements
+    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,     /// Element type of C matrix
+    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
+    Policy        /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
+    >;            /// Used for partial specialization
+
+  /// Policy used to define MmaPipelined
+  using MmaPolicy = MmaPolicy<
+    MmaWarpSimt,
+    MatrixShape<0, 0>,
+    MatrixShape<0, 0>,
+    WarpCount::kK
+  >;
 };
-////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
-}  // namespace gemm
-}  // namespace cutlass
+/////////////////////////////////////////////////////////////////////////////////////////////////
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,483 +26,531 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief 
-      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
-      the appropriate threadblock-scoped epilogue.
+    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
 
   
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/arch/wmma.h"
-
-#include "cutlass/epilogue/threadblock/epilogue.h"
-#include "cutlass/epilogue/thread/linear_combination.h"
-
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/symm_universal.h"
+#include "cutlass/arch/arch.h"
 #include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/gemm/threadblock/default_mma.h"
-#include "cutlass/gemm/threadblock/default_multistage_trmm_complex.h"
-#include "cutlass/gemm/threadblock/default_multistage_mma_complex.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle.h"
-
-#include "cutlass/epilogue/threadblock/default_epilogue_complex_tensor_op.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/epilogue/threadblock/default_epilogue_wmma_tensor_op.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
-
+#include "cutlass/numeric_types.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h"
+#include "cutlass/gemm/threadblock/mma_blas3_multistage.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace kernel {
+namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
-    /// Element type for C and D matrix operands
-    typename ElementC_,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
+    /// Element type for internal accumulation
+    typename ElementAccumulator_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
+    /// Operator class tag
+    typename OperatorClass_,
+    /// Tag indicating architecture to tune for
+    typename ArchTag_,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape_,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape_,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape_,
+    /// Number of stages used in the pipelined mainloop
+    int Stages,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB = ComplexTransform::kNone,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator = arch::OpMultiplyAddComplex,
+    /// Blas3 computation mode
+    BlasMode BlasMode_ = BlasMode::kTriangular,
+    /// Store the accumulators in row major or column major.  Row major is used
+    /// when output layout is interleaved.
+    bool AccumulatorsInRowMajor = false>
+struct DefaultMultistageTrmmComplex;
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization for row-major output
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for internal accumulation
     typename ElementAccumulator,
-    /// Operator class tag
+    /// Tag indicating architecture to tune for
     typename OperatorClass,
     /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Blas3 computation mode
-    BlasMode BlasMode_ = BlasMode::kSymmetric>
-struct DefaultSymmComplex;
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            kSideMode, kFillMode, kDiagType,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          kSideMode, kFillMode, kDiagType, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          kSideMode, FillMode::kFull, DiagType::kInvalid,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
+};
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture complex datatype (symmetric)
+/// Specialization for row-major output and right-side mode
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  // Complex Transform don't appply to A or B for SYMM
-  static ComplexTransform const TransformA = ComplexTransform::kNone; 
-  static ComplexTransform const TransformB = ComplexTransform::kNone; 
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm90,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
-
-  /// Define the kernel-level Symm operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            SideMode::kRight, kFillMode, kDiagType,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          SideMode::kRight, kFillMode, kDiagType,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture complex datatype (hermitian)
+/// Specialization for row-major output with unit diagonal
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  static ComplexTransform const TransformAMma1 = ComplexTransform::kNone; 
-  static ComplexTransform const TransformBMma1 = ComplexTransform::kNone; 
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformAMma1, TransformBMma1, Operator, BlasMode::kHermitian>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - withOUT diagonal - with conjugate transpose: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type;
-  static ComplexTransform const TransformAMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kConjugate : ComplexTransform::kNone;
-  static ComplexTransform const TransformBMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kNone : ComplexTransform::kConjugate;
-
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm90,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformAMma2, TransformBMma2, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
-
-  /// Define the kernel-level Symm operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            kSideMode, kFillMode, DiagType::kUnit,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          kSideMode, kFillMode, DiagType::kUnit, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          kSideMode, FillMode::kFull, DiagType::kInvalid,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (symmetric)
+/// Specialization for row-major output and right-side mode, unit diagonal
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kSymmetric> {
-
-  static BlasMode const kBlasMode = BlasMode::kSymmetric;
-  // Complex Transform don't appply to A or B for SYMM
-  static ComplexTransform const TransformA = ComplexTransform::kNone; 
-  static ComplexTransform const TransformB = ComplexTransform::kNone; 
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - withOUT diagonal: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type; 
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm80,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
-
-  /// Define the kernel-level Symm operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
-
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            SideMode::kRight, kFillMode, DiagType::kUnit,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          SideMode::kRight, kFillMode, DiagType::kUnit,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
+
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Ampere Architecture complex datatype (hermitian)
+/// Specialization for row-major output (for TRMM where diagonal imag part is ignored - used by HEMM)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode kSideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode kFillModeA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Element type for C and D matrix operands
-    typename ElementC,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
     /// Element type for internal accumulation
     typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
+    /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
+    /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Operation performed by GEMM
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial>
-struct DefaultSymmComplex<
-  ElementA, LayoutA, kSideModeA, kFillModeA, ElementB, LayoutB, ElementC, 
-  layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm80, ThreadblockShape, WarpShape, InstructionShape, 
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, 
-  Operator, SplitKSerial, BlasMode::kHermitian> {
-
-  static BlasMode const kBlasMode = BlasMode::kHermitian;
-
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - with diagonal: alpha * A * B or alpha * B * A
-	static const DiagType kDiagTypeMma1 = DiagType::kNonUnit;
-  static ComplexTransform const TransformAMma1 = ComplexTransform::kNone; 
-  static ComplexTransform const TransformBMma1 = ComplexTransform::kNone; 
-  using Mma1 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, 
-      ElementB, LayoutB, 
-      kSideModeA, kFillModeA, kDiagTypeMma1, 
-      ElementAccumulator, layout::RowMajor, 
-      arch::OpClassTensorOp, arch::Sm80,
-      ThreadblockShape, WarpShape, InstructionShape,
-      Stages, TransformAMma1, TransformBMma1, Operator, BlasMode::kHermitian>::ThreadblockMma;
-
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  /// TRMM - withOUT diagonal - with conjugate transpose: alpha * AT * B or alpha * B * AT
-	static const DiagType kDiagTypeMma2 = DiagType::kZero;
-  using LayoutAMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                typename layout::LayoutTranspose<LayoutA>::type, 
-                                LayoutA
-                              >::type;
-  using LayoutBMma2 = typename platform::conditional<
-                                (kSideModeA == SideMode::kLeft), 
-                                LayoutB, 
-                                typename layout::LayoutTranspose<LayoutB>::type
-                              >::type;
-  static ComplexTransform const TransformAMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kConjugate : ComplexTransform::kNone;
-  static ComplexTransform const TransformBMma2 = (kSideModeA == SideMode::kLeft) ? 
-                                              ComplexTransform::kNone : ComplexTransform::kConjugate;
-
-	using Mma2 = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-			ElementA, LayoutAMma2, 
-			ElementB, LayoutBMma2, 
-			kSideModeA, InvertFillMode<kFillModeA>::mode, kDiagTypeMma2, 
-			ElementAccumulator, layout::RowMajor, 
-			arch::OpClassTensorOp, arch::Sm80,
-			ThreadblockShape, WarpShape, InstructionShape,
-			Stages, TransformAMma2, TransformBMma2, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma1::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            kSideMode, kFillMode, DiagType::kNonUnit,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator, BlasMode::kHermitian> {
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  // PredicatedTileAccessIteratorTriangularMatrix only tracks diagonal elements,
+  // when DiagType is kUnit
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          kSideMode, kFillMode, DiagType::kUnit, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          kSideMode, FillMode::kFull, DiagType::kInvalid,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill,
+      BlasMode::kHermitian>;
+};
 
-  /// Define the kernel-level Symm operator.
-  using SymmKernel = kernel::SymmUniversal<Mma1, Mma2, Epilogue, ThreadblockSwizzle, kSideModeA, kFillModeA>;
+////////////////////////////////////////////////////////////////////////////////
 
+/// Specialization for row-major output and right-side mode (for TRMM where diagonal imag part is ignored - used by HEMM)
+template <
+    /// Element type for A matrix operand
+    typename ElementA,
+    /// Layout type for A matrix operand
+    typename LayoutA,
+    /// Element type for B matrix operand
+    typename ElementB,
+    /// Layout type for B matrix operand
+    typename LayoutB,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Element type for internal accumulation
+    typename ElementAccumulator,
+    /// Tag indicating architecture to tune for
+    typename OperatorClass,
+    /// Tag indicating architecture to tune for
+    typename ArchTag,
+    /// Threadblock-level tile size (concept: GemmShape)
+    typename ThreadblockShape,
+    /// Warp-level tile size (concept: GemmShape)
+    typename WarpShape,
+    /// Instruction-level tile size (concept: GemmShape)
+    typename InstructionShape,
+    /// Number of stages used in the multistage mainloop
+    int Stages,
+    /// Complex transformation on operand A
+    ComplexTransform TransformA,
+    /// Complex transformation on operand B
+    ComplexTransform TransformB,
+    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
+    typename Operator>
+struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
+                            SideMode::kRight, kFillMode, DiagType::kNonUnit,
+                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
+                            InstructionShape, Stages, TransformA, TransformB, Operator, BlasMode::kHermitian> {
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
+      Stages, TransformA, TransformB, Operator>;
+
+  // Define iterators over tiles from the A operand
+  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
+  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using IteratorA =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
+          ElementA, LayoutA, 1, ThreadMapA, 
+          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
+          AccessTypeA>;
+
+  // Define iterators over tiles from the B operand
+  // PredicatedTileAccessIteratorTriangularMatrix only tracks diagonal elements,
+  // when DiagType is kUnit
+  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
+  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using IteratorB =
+      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
+          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
+          ElementB, LayoutB, 0, ThreadMapB, 
+          SideMode::kRight, kFillMode, DiagType::kUnit,
+          AccessTypeB>;
+
+  // Define the threadblock-scoped multistage matrix multiply
+  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
+      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
+      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
+      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill,
+      BlasMode::kHermitian>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace kernel
+}  // namespace threadblock
 }  // namespace gemm
 }  // namespace cutlass
+
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_symm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_universal.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,15 +27,15 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-      Default kernel-level SYMM/HEMM definitions combine threadblock-scoped matrix multiply-add with
+      Default kernel-level TRMM definitions combine threadblock-scoped matrix multiply-add with
       the appropriate threadblock-scoped epilogue.
   
       Note, CUTLASS epilogues universally target row-major outputs. Column-major outputs are
       accommodated by exchanging A and B operands and assuming transposed layouts.
 
   
 */
@@ -43,43 +43,49 @@
 #pragma once
 
 #include "cutlass/blas3.h"
 
 #include "cutlass/complex.h"
 #include "cutlass/layout/matrix.h"
 
-#include "cutlass/gemm/kernel/symm_universal.h"
-#include "cutlass/gemm/kernel/default_symm.h"
-#include "cutlass/gemm/kernel/default_symm_complex.h"
+#include "cutlass/gemm/kernel/trmm_universal.h"
+#include "cutlass/gemm/kernel/default_trmm.h"
+#include "cutlass/gemm/kernel/default_trmm_complex.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode SideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode FillModeA,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
     typename ElementC_,
     /// Layout type for C and D matrix operands
     typename LayoutC_,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
@@ -97,45 +103,45 @@
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
-    /// Operation performed by SYRK
+    /// Operation performed by TRMM
     typename Operator,
-    /// Blas3 computation mode (symmetric/hermitian)
-    BlasMode BlasMode_ = BlasMode::kSymmetric,
     ///
     typename Enable = void
     >
-struct DefaultSymmUniversal;
+struct DefaultTrmmUniversal;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
-// Real-valued SYMM/HEMM update kernels
+// Real-valued TRMM kernels
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode SideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode FillModeA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
@@ -153,101 +159,109 @@
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
-    /// Operation performed by SYMM/HEMM
+    /// Operation performed by TRMM
     typename Operator>
-struct DefaultSymmUniversal<
+struct DefaultTrmmUniversal<
   ElementA,
   LayoutA,
-  SideModeA,
-  FillModeA,
+  ComplexTransform::kNone,   // transform A
   kAlignmentA,
   ElementB,
   LayoutB,
+  ComplexTransform::kNone,   // transform B
   kAlignmentB,
+  kSideMode,
+  kFillMode,
+  kDiagType,
   ElementC,
   LayoutC,
   ElementAccumulator,
   OperatorClass,
   ArchTag,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
-  BlasMode::kSymmetric,
   typename std::enable_if< ! cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
-  using DefaultSymmkernel = typename kernel::DefaultSymm<
+  using DefaultTrmmKernel = typename kernel::DefaultTrmm<
     ElementA,
     LayoutA,
-    SideModeA,
-    FillModeA,
     kAlignmentA,
     ElementB,
     LayoutB,
     kAlignmentB,
+    kSideMode,
+    kFillMode,
+    kDiagType,
     ElementC,
     LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
     SplitKSerial,
-    Operator,
-    BlasMode::kSymmetric
-  >::SymmKernel;
+    Operator
+  >::TrmmKernel;
 
     /// Define the kernel in terms of the default kernel
-  using SymmKernel = kernel::SymmUniversal<
-    typename DefaultSymmkernel::Mma1,
-    typename DefaultSymmkernel::Mma2,
-    typename DefaultSymmkernel::Epilogue, 
+  using TrmmKernel = kernel::TrmmUniversal<
+    typename DefaultTrmmKernel::Mma,
+    typename DefaultTrmmKernel::Epilogue, 
     ThreadblockSwizzle,
-    SideModeA,
-    FillModeA
+    kSideMode,
+    kFillMode,
+    kDiagType
   >;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 //
-// Complex-valued SYMM/HEMM update kernels
+// Complex-valued TRMM kernels
 //
 
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
-    /// Side Mode for A (kLeft or kRight)
-    SideMode SideModeA,
-    /// Fill Mode for A (kLower or kUpper)
-    FillMode FillModeA,
+    /// Complex elementwise transformation on A operand
+    ComplexTransform TransformA,
     /// Access granularity of A matrix in units of elements
     int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Complex elementwise transformation on B operand
+    ComplexTransform TransformB,
     /// Access granularity of B matrix in units of elements
     int kAlignmentB,
+    /// Side Mode for the kernel
+    SideMode kSideMode,
+    /// Fill Mode for the triangular matrix
+    FillMode kFillMode,
+    /// Diag Type for the triangular matrix
+    DiagType kDiagType,
     /// Element type for C and D matrix operands
     typename ElementC,
     /// Layout type for C and D matrix operands
     typename LayoutC,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Operator class tag
@@ -265,78 +279,81 @@
     /// Threadblock-level swizzling operator
     typename ThreadblockSwizzle,
     /// Number of stages used in the pipelined mainloop
     int Stages,
     /// If true, kernel is configured to support serial reduction in the
     /// epilogue
     bool SplitKSerial,
-    /// Operation performed by SYRK
-    typename Operator,
-    // BlasMode
-    BlasMode kBlasMode
+    /// Operation performed by TRMM
+    typename Operator
   >
-
-struct DefaultSymmUniversal<
+struct DefaultTrmmUniversal<
   ElementA,
   LayoutA,
-  SideModeA,
-  FillModeA, 
+  TransformA,
   kAlignmentA,
   ElementB,
   LayoutB,
+  TransformB,
   kAlignmentB,
+  kSideMode,
+  kFillMode,
+  kDiagType,
   ElementC,
   LayoutC,
   ElementAccumulator,
   OperatorClass,
   ArchTag,
   ThreadblockShape,
   WarpShape,
   InstructionShape,
   EpilogueOutputOp,
   ThreadblockSwizzle,
   Stages,
   SplitKSerial,
   Operator,
-  kBlasMode,
   typename std::enable_if<cutlass::is_complex<ElementAccumulator>::value>::type
 > {
 
-  using DefaultSymmkernel = typename kernel::DefaultSymmComplex<
+  using DefaultTrmmKernel = typename kernel::DefaultTrmmComplex<
     ElementA,
     LayoutA,
-    SideModeA,
-    FillModeA,
     ElementB,
     LayoutB,
+    kSideMode,
+    kFillMode,
+    kDiagType,
     ElementC,
     LayoutC,
     ElementAccumulator,
     OperatorClass,
     ArchTag,
     ThreadblockShape,
     WarpShape,
     InstructionShape,
     EpilogueOutputOp,
     ThreadblockSwizzle,
     Stages,
+    TransformA,
+    TransformB,
     Operator,
-    SplitKSerial,
-    kBlasMode
-  >::SymmKernel;
+    SplitKSerial
+  >::TrmmKernel;
 
-    /// Define the kernel in terms of the default kernel
-  using SymmKernel = kernel::SymmUniversal<
-    typename DefaultSymmkernel::Mma1,
-    typename DefaultSymmkernel::Mma2,
-    typename DefaultSymmkernel::Epilogue, 
+  /// Define the kernel in terms of the default kernel
+  using TrmmKernel = kernel::TrmmUniversal<
+    typename DefaultTrmmKernel::Mma,
+    typename DefaultTrmmKernel::Epilogue, 
     ThreadblockSwizzle,
-    SideModeA,
-    FillModeA
+    kSideMode,
+    kFillMode,
+    kDiagType
   >;
 };
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 }  // namespace kernel
 }  // namespace gemm
 }  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm.h`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -120,84 +120,14 @@
     bool SplitKSerial,
     /// Operation performed by GEMM
     typename Operator>
 struct DefaultTrmm;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// If true, kernel is configured to support serial reduction in the
-    /// epilogue
-    bool SplitKSerial,
-    /// Operation performed by GEMM
-    typename Operator>
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-                   kSideMode, kFillMode, kDiagType, ElementC,
-                   layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-                   arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
-                   EpilogueOutputOp, ThreadblockSwizzle, Stages, SplitKSerial,
-                   Operator> {
-                    
-  /// Define the threadblock-scoped triagular matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultTrmm<
-      ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB,
-      kSideMode, kFillMode, kDiagType, 
-      ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp, arch::Sm90,
-      ThreadblockShape, WarpShape, InstructionShape, Stages,
-      Operator>::ThreadblockMma;
-
-  static const int kPartitionsK = ThreadblockShape::kK / WarpShape::kK;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueTensorOp<
-          ThreadblockShape, typename Mma::Operator, kPartitionsK, EpilogueOutputOp,
-          EpilogueOutputOp::kCount>::Epilogue;
-
-  /// Define the kernel-level TRMM operator.
-  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Access granularity of A matrix in units of elements
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/default_trmm_complex.h`

 * *Files 14% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -118,82 +118,14 @@
   /// If true, kernel is configured to support serial reduction in the epilogue
   bool SplitKSerial
 >
 struct DefaultTrmmComplex;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Hopper Architecture
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for C and D matrix operands
-    typename ElementC,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Epilogue output operator
-    typename EpilogueOutputOp,
-    /// Threadblock-level swizzling operator
-    typename ThreadblockSwizzle,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Complex elementwise transformation on A operand
-    ComplexTransform TransformA,
-    /// Complex elementwise transformation on B operand
-    ComplexTransform TransformB,
-    /// Multiply-add operator 
-    // (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator,
-    /// If true, kernel is configured to support serial reduction in the epilogue
-    bool SplitKSerial
-  >
-struct DefaultTrmmComplex<
-  ElementA, LayoutA, ElementB, LayoutB, 
-  kSideMode, kFillMode, kDiagType,
-  ElementC, layout::RowMajor, ElementAccumulator, arch::OpClassTensorOp,
-  arch::Sm90, ThreadblockShape, WarpShape, InstructionShape,
-  EpilogueOutputOp, ThreadblockSwizzle, Stages, TransformA, TransformB, Operator, SplitKSerial> {
-
-  /// Define the threadblock-scoped matrix multiply-accumulate
-  using Mma = typename cutlass::gemm::threadblock::DefaultMultistageTrmmComplex<
-      ElementA, LayoutA, ElementB, LayoutB, 
-      kSideMode, kFillMode, kDiagType,
-      ElementAccumulator,layout::RowMajor, arch::OpClassTensorOp, arch::Sm90, ThreadblockShape,
-      WarpShape, InstructionShape, Stages, TransformA, TransformB, Operator>::ThreadblockMma;
-
-  /// Define the epilogue
-  using Epilogue =
-      typename cutlass::epilogue::threadblock::DefaultEpilogueComplexTensorOp<
-          ThreadblockShape, typename Mma::Operator, 1, EpilogueOutputOp,
-          EpilogueOutputOp::kCount, Operator>::Epilogue;
-
-  /// Define the kernel-level TRMM operator.
-  using TrmmKernel = kernel::TrmmUniversal<Mma, Epilogue, ThreadblockSwizzle, kSideMode, kFillMode, kDiagType>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
 /// Partial specialization for Ampere Architecture
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
     /// Element type for B matrix operand
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/ell_gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -252,15 +252,15 @@
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B,
       params.gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_array.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -189,15 +189,15 @@
 
       //
       // Main loop
       //
       
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = canonical_warp_idx();
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
       
       Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
       typename Mma::FragmentC accumulators;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_batched.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -200,15 +200,15 @@
 
       //
       // Main loop
       //
 
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = canonical_warp_idx();
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
       
       Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
       typename Mma::FragmentC accumulators;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -311,14 +311,21 @@
   static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
+
+  static size_t get_extra_workspace_size(
+    Arguments const &args,
+    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
  
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
@@ -391,15 +398,15 @@
 
       typename Mma::FragmentC accumulators;
 
       accumulators.clear();
       
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = canonical_warp_idx();
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
 
       //
       // Matrix multiply phase
       //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_problem_visitor.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,31 +46,20 @@
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace detail {
 // Helper for correctly representing problem sizes in grouped kernels 
-template <
-  typename ThreadblockShape,
-  bool Transposed
->
+template <bool Transposed>
 struct GemmGroupedProblemSizeHelper {
 
   static bool const kTransposed = Transposed;
 
   CUTLASS_HOST_DEVICE
-  static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
-    return cutlass::gemm::GemmCoord(
-      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
-      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
-      1);
-  }
-
-  CUTLASS_HOST_DEVICE
   static void possibly_transpose_problem(cutlass::gemm::GemmCoord& problem) {
     if (kTransposed) {
       swap(problem.m(), problem.n());
     }
   }
 
   CUTLASS_HOST_DEVICE
@@ -84,23 +73,23 @@
 /// Visitor class to abstract away the algorithm for iterating over tiles
 template <typename ThreadblockShape,
           GroupScheduleMode GroupScheduleMode_,
           int PrefetchTileCount,
           int ThreadCount,
           bool Transposed = false>
 struct GemmGroupedProblemVisitor : public GroupedProblemVisitor<
-                                            detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transposed>,
+                                            detail::GemmGroupedProblemSizeHelper<Transposed>,
                                             ThreadblockShape,
                                             GroupScheduleMode_,
                                             PrefetchTileCount,
                                             ThreadCount> {
 
   static bool const kTransposed = Transposed;
 
-  using ProblemSizeHelper = detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transposed>;
+  using ProblemSizeHelper = detail::GemmGroupedProblemSizeHelper<Transposed>;
   using Base = GroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape, GroupScheduleMode_, PrefetchTileCount, ThreadCount>;
   using Params = typename Base::Params;
   using SharedStorage = typename Base::SharedStorage;
 
   //
   // Methods
   //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_grouped_softmax_mainloop_fusion.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -331,14 +331,21 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
 
+  static size_t get_extra_workspace_size(
+    Arguments const &args,
+    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // These types shadow the type-level definitions and support the ability to implement
     // a 'transposed' GEMM that computes the transposed problems.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_layernorm_mainloop_fusion.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,15 +37,14 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/layout/matrix.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -101,20 +100,24 @@
   static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
+  struct Arguments {
+
     //
     // Data members
     //
 
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_var;
     void const * ptr_mean;
     void const * ptr_gamma;
@@ -125,14 +128,15 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_var;
     int64_t batch_stride_mean;
     int64_t batch_stride_gamma;
     int64_t batch_stride_beta;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
 
     typename LayoutA::Stride stride_a;
     typename LayoutB::Stride stride_b;
     typename LayoutScaleBias::Stride stride_var;
     typename LayoutScaleBias::Stride stride_mean;
     typename LayoutScaleBias::Stride stride_gamma;
     typename LayoutScaleBias::Stride stride_beta;
@@ -153,21 +157,22 @@
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
     
     Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_var(nullptr), ptr_mean(nullptr),
       ptr_gamma(nullptr), ptr_beta(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr)
-    {}
+      ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -193,35 +198,39 @@
       typename LayoutScaleBias::Stride stride_mean,
       typename LayoutScaleBias::Stride stride_gamma,
       typename LayoutScaleBias::Stride stride_beta,
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       ptr_var(ptr_var), ptr_mean(ptr_mean), 
       ptr_gamma(ptr_gamma), ptr_beta(ptr_beta), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
-      batch_stride_var(batch_stride_var), batch_stride_mean(batch_stride_mean),
-      batch_stride_gamma(batch_stride_gamma), batch_stride_beta(batch_stride_beta),
-      lda(0), ldb(0), ldc(0), ldd(0),
-      ld_var(0), ld_mean(0),
-      ld_gamma(0), ld_beta(0),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       stride_var(stride_var), stride_mean(stride_mean),
       stride_gamma(stride_gamma), stride_beta(stride_beta),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices)
-    {
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      lda = 0;
+      ldb = 0;
+      ldc = 0;
+      ldd = 0;
+      ld_var = 0;
+      ld_mean = 0;
+      ld_gamma = 0;
+      ld_beta = 0;
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-    }
+      }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -247,40 +256,41 @@
       typename LayoutScaleBias::Stride::LongIndex ld_mean,
       typename LayoutScaleBias::Stride::LongIndex ld_gamma,
       typename LayoutScaleBias::Stride::LongIndex ld_beta,
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
       ptr_var(ptr_var), ptr_mean(ptr_mean), 
       ptr_gamma(ptr_gamma), ptr_beta(ptr_beta), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
       batch_stride_var(batch_stride_var), batch_stride_mean(batch_stride_mean),
       batch_stride_gamma(batch_stride_gamma), batch_stride_beta(batch_stride_beta),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ld_var(ld_var), ld_mean(ld_mean),
       ld_gamma(ld_gamma), ld_beta(ld_beta),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices)
-    {
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       stride_var = make_Coord(ld_var);
       stride_mean = make_Coord(ld_mean);
       stride_gamma = make_Coord(ld_gamma);
       stride_beta = make_Coord(ld_beta);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-    }
+      }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
@@ -289,45 +299,36 @@
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
-
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    //
-    // Data members
-    //
+  struct Params {
 
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+    
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     
     typename EpilogueOutputOp::Params output_op;
 
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
     void * ptr_A;
     void * ptr_B;
     void * ptr_var;
     void * ptr_mean;
     void * ptr_gamma;
     void * ptr_beta;
     void * ptr_C;
@@ -336,38 +337,73 @@
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_var;
     int64_t batch_stride_mean;
     int64_t batch_stride_gamma;
     int64_t batch_stride_beta;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
 
     int * ptr_gather_A_indices;
     int * ptr_gather_B_indices;
     int * ptr_scatter_D_indices;
 
+    int *semaphore;
+
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_B(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_var(nullptr),
+      ptr_mean(nullptr),
+      ptr_gamma(nullptr),
+      ptr_beta(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_var(0),
+      batch_stride_mean(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr),
+      semaphore(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
       params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
       params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
       params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
       output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_var(const_cast<void *>(args.ptr_var)),
       ptr_mean(const_cast<void *>(args.ptr_mean)),
       ptr_gamma(const_cast<void *>(args.ptr_gamma)),
       ptr_beta(const_cast<void *>(args.ptr_beta)),
       ptr_C(const_cast<void *>(args.ptr_C)),
@@ -375,55 +411,71 @@
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_var(args.batch_stride_var),
       batch_stride_mean(args.batch_stride_mean),
       batch_stride_gamma(args.batch_stride_gamma),
       batch_stride_beta(args.batch_stride_beta),
       batch_stride_C(args.batch_stride_C),
+      batch_stride_D(args.batch_stride_D),
       ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
       ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices))
-    {}
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
+      semaphore(static_cast<int *>(workspace)) {
+
+    }
+
+    CUTLASS_HOST_DEVICE
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
 
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    void update(Arguments const &args)
-    {
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_var = const_cast<void *>(args.ptr_var);
       ptr_mean = const_cast<void *>(args.ptr_mean);
       ptr_gamma = const_cast<void *>(args.ptr_gamma);
       ptr_beta = const_cast<void *>(args.ptr_beta);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
       ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
       ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_var = args.batch_stride_var;
+      batch_stride_mean = args.batch_stride_mean;
+      batch_stride_gamma = args.batch_stride_gamma;
+      batch_stride_beta = args.batch_stride_beta;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_D = args.batch_stride_D;
+
       output_op = args.epilogue;
       
+      semaphore = static_cast<int *>(workspace);
       CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
-
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  GemmLayernormMainloopFusion() { } 
+
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
 
     static int const kAlignmentA = (platform::is_same<LayoutA,
@@ -499,31 +551,20 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-public:
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmLayernormMainloopFusion op;
-    op(params, shared_storage);
+    return 0;
   }
  
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_params.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_pipelined.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -107,15 +107,15 @@
   typename Mma::IteratorB iterator_B(
     params_B,
     ref_B.data(),
     {problem_size.k(), problem_size.n()},
     tb_thread_id,
     tb_offset_B);
 
-  int warp_id = canonical_warp_idx();
+  int warp_id = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
   int lane_id = threadIdx.x % 32;
 
   //
   // Main loop
   //
 
   // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,15 +37,14 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -102,20 +101,24 @@
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   //
   // Arguments structure
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
+  struct Arguments {
+
     //
     // Data members
     //
 
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A_real;
     void const * ptr_A_imag;
 
     void const * ptr_B_real;
     void const * ptr_B_imag;
@@ -137,30 +140,34 @@
     
     int64_t batch_stride_A;
     int64_t batch_stride_A_imag;
     int64_t batch_stride_B;
     int64_t batch_stride_B_imag;
     int64_t batch_stride_C;
     int64_t batch_stride_C_imag;
+    int64_t batch_stride_D;
     int64_t batch_stride_D_imag;
 
+
     //
     // Methods
     //
     
-    Arguments() :
+    Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
       ptr_A_real(nullptr), 
       ptr_A_imag(nullptr), 
       ptr_B_real(nullptr), 
       ptr_B_imag(nullptr), 
       ptr_C_real(nullptr), 
       ptr_C_imag(nullptr), 
       ptr_D_real(nullptr),
       ptr_D_imag(nullptr)
-    {}
+      { }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -183,17 +190,19 @@
       int64_t batch_stride_A = 0,
       int64_t batch_stride_A_imag = 0,
       int64_t batch_stride_B = 0,
       int64_t batch_stride_B_imag = 0,
       int64_t batch_stride_C = 0,
       int64_t batch_stride_C_imag = 0,
       int64_t batch_stride_D = 0,
-      int64_t batch_stride_D_imag = 0)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      int64_t batch_stride_D_imag = 0
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A_real(ptr_A_real), 
       ptr_A_imag(ptr_A_imag), 
       ptr_B_real(ptr_B_real),
       ptr_B_imag(ptr_B_imag),
       ptr_C_real(ptr_C_real),
       ptr_C_imag(ptr_C_imag),
@@ -209,16 +218,18 @@
       ldd_imag(ldd_imag),
       batch_stride_A(batch_stride_A),
       batch_stride_A_imag(batch_stride_A_imag),
       batch_stride_B(batch_stride_B),
       batch_stride_B_imag(batch_stride_B_imag),
       batch_stride_C(batch_stride_C),
       batch_stride_C_imag(batch_stride_C_imag),
-      batch_stride_D_imag(batch_stride_D_imag)
-    {}
+      batch_stride_D(batch_stride_D),
+      batch_stride_D_imag(batch_stride_D_imag) {
+
+      }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A_real, args.ptr_B_real);
@@ -228,157 +239,178 @@
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.batch_stride_A_imag, args.batch_stride_B_imag);
 
       return args;
     }
   };
 
-
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    //
-    // Data members
-    //
-
+  struct Params {
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+    
     typename Mma::IteratorA::Params params_A_real;
     typename Mma::IteratorA::Params params_A_imag;
     typename Mma::IteratorB::Params params_B_real;
     typename Mma::IteratorB::Params params_B_imag;
     typename Epilogue::OutputTileIterator::Params params_C_real;
     typename Epilogue::OutputTileIterator::Params params_C_imag;
     typename Epilogue::OutputTileIterator::Params params_D_real;
     typename Epilogue::OutputTileIterator::Params params_D_imag;
     
     typename EpilogueOutputOp::Params output_op;
 
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
     void * ptr_A_real;
     void * ptr_A_imag;
     void * ptr_B_real;
     void * ptr_B_imag;
     void * ptr_C_real;
     void * ptr_C_imag;
     void * ptr_D_real;
     void * ptr_D_imag;
 
     int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-
     int64_t batch_stride_A_imag;
+    int64_t batch_stride_B;
     int64_t batch_stride_B_imag;
+    int64_t batch_stride_C;
     int64_t batch_stride_C_imag;
+    int64_t batch_stride_D;
     int64_t batch_stride_D_imag;
 
+    int *semaphore;
+
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      batch_count(0),
+      gemm_k_size(0),
+      swizzle_log_tile(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A_real(nullptr),
+      ptr_A_imag(nullptr),
+      ptr_B_real(nullptr),
+      ptr_B_imag(nullptr),
+      ptr_C_real(nullptr),
+      ptr_C_imag(nullptr),
+      ptr_D_real(nullptr),
+      ptr_D_imag(nullptr),
+      batch_stride_A(0),
+      batch_stride_A_imag(0),
+      batch_stride_B(0),
+      batch_stride_B_imag(0),
+      batch_stride_C(0),
+      batch_stride_C_imag(0),
+      batch_stride_D(0),
+      batch_stride_D_imag(0),
+      semaphore(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A_real(args.lda_real),
       params_A_imag(args.lda_imag),
       params_B_real(args.ldb_real),
       params_B_imag(args.ldb_imag),
       params_C_real(args.ldc_real),
       params_C_imag(args.ldc_imag),
       params_D_real(args.ldd_real),
       params_D_imag(args.ldd_imag),
       output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
       ptr_A_real(const_cast<void *>(args.ptr_A_real)),
       ptr_A_imag(const_cast<void *>(args.ptr_A_imag)),
       ptr_B_real(const_cast<void *>(args.ptr_B_real)),
       ptr_B_imag(const_cast<void *>(args.ptr_B_imag)),
       ptr_C_real(const_cast<void *>(args.ptr_C_real)),
       ptr_C_imag(const_cast<void *>(args.ptr_C_imag)),
       ptr_D_real(args.ptr_D_real),
       ptr_D_imag(args.ptr_D_imag),
       batch_stride_A(args.batch_stride_A),
-      batch_stride_B(args.batch_stride_B),
-      batch_stride_C(args.batch_stride_C),
       batch_stride_A_imag(args.batch_stride_A_imag),
+      batch_stride_B(args.batch_stride_B),
       batch_stride_B_imag(args.batch_stride_B_imag),
+      batch_stride_C(args.batch_stride_C),
       batch_stride_C_imag(args.batch_stride_C_imag),
-      batch_stride_D_imag(args.batch_stride_D_imag)
-    {}
+      batch_stride_D(args.batch_stride_D),
+      batch_stride_D_imag(args.batch_stride_D_imag),
+      semaphore(static_cast<int *>(workspace)) {
 
-    /// Returns the workspace size (in bytes) needed for this problem geometry
-    size_t get_workspace_size() const
-    {
-      size_t workspace_bytes = ParamsBase::get_workspace_size();
-      if (this->mode == GemmUniversalMode::kGemmSplitKParallel)
-      {
-        // Double the size returned by the base class because we need to
-        // accumulate two ElementC components
-        workspace_bytes *= 2;
-      }
-
-      return workspace_bytes;
     }
 
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    void update(Arguments const &args)
-    {
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
+
       ptr_A_real = const_cast<void *>(args.ptr_A_real);
       ptr_A_imag = const_cast<void *>(args.ptr_A_imag);
 
       ptr_B_real = const_cast<void *>(args.ptr_B_real);
       ptr_B_imag = const_cast<void *>(args.ptr_B_imag);
 
       ptr_C_real = const_cast<void *>(args.ptr_C_real);
       ptr_C_imag = const_cast<void *>(args.ptr_C_imag);
 
       ptr_D_real = const_cast<void *>(args.ptr_D_real);
       ptr_D_imag = const_cast<void *>(args.ptr_D_imag);
 
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_A_imag = args.batch_stride_A_imag;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_B_imag = args.batch_stride_B_imag;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_C_imag = args.batch_stride_C_imag;
+      batch_stride_D = args.batch_stride_D;
+      batch_stride_D_imag = args.batch_stride_D_imag;
+
       output_op = args.epilogue;
+      
+      semaphore = static_cast<int *>(workspace);
     }
   };
 
-
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  GemmPlanarComplex() { } 
+
   /// Determines whether kernel satisfies alignment
-  static Status can_implement(Arguments const &args)
-  {
+  static Status can_implement(Arguments const &args) {
+
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
     static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
     static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
     bool isAMisaligned = false;
     bool isBMisaligned = false;
     bool isCMisaligned = false;
@@ -404,31 +436,20 @@
     if (isAMisaligned || isBMisaligned || isCMisaligned) {
       return Status::kErrorMisalignedOperand;
     }
 
     return Status::kSuccess;
   }
 
-public:
-
-  //
-  // Device-only API
-  //
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmPlanarComplex op;
-    op(params, shared_storage);
+    return 0;
   }
 
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
@@ -521,15 +542,15 @@
       ptr_B_imag,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_planar_complex_array.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,15 +37,14 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
@@ -102,20 +101,24 @@
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
   //
   // Arguments structure
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
+  struct Arguments {
+
     //
     // Data members
     //
 
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
     typename EpilogueOutputOp::Params epilogue;
 
     int const *ptr_M;
     int const *ptr_N;
     int const *ptr_K;
 
     void const * const * ptr_A_real;
@@ -135,31 +138,36 @@
     typename LayoutB::Stride::Index ldb_real;
     typename LayoutB::Stride::Index ldb_imag;
     typename LayoutC::Stride::Index ldc_real;
     typename LayoutC::Stride::Index ldc_imag;
     typename LayoutC::Stride::Index ldd_real;
     typename LayoutC::Stride::Index ldd_imag;
 
+    int64_t batch_stride_D;    // unused
+
     //
     // Methods
     //
     
     Arguments(): 
+      mode(GemmUniversalMode::kArray),
+      batch_count(1),
       ptr_M(nullptr),
       ptr_N(nullptr),
       ptr_K(nullptr),
       ptr_A_real(nullptr), 
       ptr_A_imag(nullptr), 
       ptr_B_real(nullptr), 
       ptr_B_imag(nullptr), 
       ptr_C_real(nullptr), 
       ptr_C_imag(nullptr), 
       ptr_D_real(nullptr),
-      ptr_D_imag(nullptr)
-    {}
+      ptr_D_imag(nullptr),
+      batch_stride_D(0)
+      { }
 
     /// constructs an arguments structure
     Arguments(
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
       int const *ptr_M,
@@ -176,17 +184,19 @@
       typename LayoutA::Stride::Index lda_real,
       typename LayoutA::Stride::Index lda_imag,
       typename LayoutB::Stride::Index ldb_real,
       typename LayoutB::Stride::Index ldb_imag,
       typename LayoutC::Stride::Index ldc_real,
       typename LayoutC::Stride::Index ldc_imag,
       typename LayoutC::Stride::Index ldd_real,
-      typename LayoutC::Stride::Index ldd_imag)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      typename LayoutC::Stride::Index ldd_imag
+    ):
+      mode(GemmUniversalMode::kArray),
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue),
       ptr_M(ptr_M),
       ptr_N(ptr_N),
       ptr_K(ptr_K),
       ptr_A_real(ptr_A_real), 
       ptr_A_imag(ptr_A_imag), 
       ptr_B_real(ptr_B_real),
@@ -198,16 +208,18 @@
       lda_real(lda_real),
       lda_imag(lda_imag),
       ldb_real(ldb_real),
       ldb_imag(ldb_imag),
       ldc_real(ldc_real),
       ldc_imag(ldc_imag),
       ldd_real(ldd_real),
-      ldd_imag(ldd_imag)
-    {}
+      ldd_imag(ldd_imag),
+      batch_stride_D(0) {
+
+      }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
       Arguments args(*this);
       
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_M, args.ptr_N);
@@ -216,102 +228,107 @@
       std::swap(args.lda_real, args.ldb_real);
       std::swap(args.lda_imag, args.ldb_imag);
 
       return args;
     }
   };
 
-
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    //
-    // Data members
-    //
-
+  struct Params {
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
     typename Mma::IteratorA::Params params_A_real;
     typename Mma::IteratorA::Params params_A_imag;
     typename Mma::IteratorB::Params params_B_real;
     typename Mma::IteratorB::Params params_B_imag;
     typename Epilogue::OutputTileIterator::Params params_C_real;
     typename Epilogue::OutputTileIterator::Params params_C_imag;
     typename Epilogue::OutputTileIterator::Params params_D_real;
     typename Epilogue::OutputTileIterator::Params params_D_imag;
-
+    
     typename EpilogueOutputOp::Params output_op;
 
+    int batch_count;
+    
     int const *ptr_M;
     int const *ptr_N;
     int const *ptr_K;
 
     void const * const * ptr_A_real;
     void const * const * ptr_A_imag;
     void const * const * ptr_B_real;
     void const * const * ptr_B_imag;
     void const * const * ptr_C_real;
     void const * const * ptr_C_imag;
     void * const * ptr_D_real;
     void * const * ptr_D_imag;
 
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      batch_count(0),
+      swizzle_log_tile(0),
+      ptr_M(nullptr),
+      ptr_N(nullptr),
+      ptr_K(nullptr),
+      ptr_A_real(nullptr),
+      ptr_A_imag(nullptr),
+      ptr_B_real(nullptr),
+      ptr_B_imag(nullptr),
+      ptr_C_real(nullptr),
+      ptr_C_imag(nullptr),
+      ptr_D_real(nullptr),
+      ptr_D_imag(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size = 0,                                    // ignored
+      void *workspace = nullptr                               // ignored
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       ptr_M(args.ptr_M),
       ptr_N(args.ptr_N),
       ptr_K(args.ptr_K),
       params_A_real(args.lda_real),
       params_A_imag(args.lda_imag),
       params_B_real(args.ldb_real),
       params_B_imag(args.ldb_imag),
       params_C_real(args.ldc_real),
       params_C_imag(args.ldc_imag),
       params_D_real(args.ldd_real),
       params_D_imag(args.ldd_imag),
       output_op(args.epilogue),
+      batch_count(args.batch_count),
       ptr_A_real(args.ptr_A_real),
       ptr_A_imag(args.ptr_A_imag),
       ptr_B_real(args.ptr_B_real),
       ptr_B_imag(args.ptr_B_imag),
       ptr_C_real(args.ptr_C_real),
       ptr_C_imag(args.ptr_C_imag),
       ptr_D_real(args.ptr_D_real),
-      ptr_D_imag(args.ptr_D_imag)
-    {}
+      ptr_D_imag(args.ptr_D_imag) {
+
+    }
+
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
 
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    void update(Arguments const &args)
-    {
       ptr_M = args.ptr_M;
       ptr_N = args.ptr_N;
       ptr_K = args.ptr_K;
 
       ptr_A_real = args.ptr_A_real;
       ptr_A_imag = args.ptr_A_imag;
 
@@ -324,27 +341,29 @@
       ptr_D_real = args.ptr_D_real;
       ptr_D_imag = args.ptr_D_imag;
 
       output_op = args.epilogue;
     }
   };
 
-
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  GemmPlanarComplexArray() { } 
+
   /// Determines whether kernel satisfies alignment
   static Status can_implement(Arguments const &args) {
 
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
     static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
     static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
 
@@ -373,32 +392,20 @@
     if (isAMisaligned || isBMisaligned || isCMisaligned) {
       return Status::kErrorMisalignedOperand;
     }
 
     return Status::kSuccess;
   }
 
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-public:
-
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmPlanarComplexArray op;
-    op(params, shared_storage);
+    return 0;
   }
-
-
+ 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
@@ -463,15 +470,15 @@
         //
         // Compute indices within threadblock and warp.
         //
         int thread_idx = threadIdx.x;
 
         // Broadcast the warp_id computed by lane 0 to ensure dependent code
         // is compiled as warp-uniform.
-        int warp_idx = canonical_warp_idx();
+        int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
         int lane_idx = threadIdx.x % 32;
     
         //
         // Proceed with regular GEMM logic.
         //
 
         // Compute initial location in logical coordinates
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_splitk_parallel.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_transpose_operands.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,50 +32,38 @@
 /*! \file
     \brief 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
-#include "cutlass/arch/arch.h"
 #include "cutlass/fast_math.h"
+#include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
 
 #include "cutlass/layout/matrix.h"
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-class GemmUniversal<
-  Mma_,
-  Epilogue_,
-  ThreadblockSwizzle_,
-  void,
-  // 3.x kernels use the first template argument to define the ProblemShape tuple
-  // We use this invariant to SFINAE dispatch against either the 2.x API or the 3.x API
-  std::enable_if_t<not cute::is_tuple<Mma_>::value>
-> {
+struct GemmUniversal {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
@@ -109,30 +97,35 @@
   static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
+  struct Arguments {
+
     //
     // Data members
     //
 
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
 
     typename LayoutA::Stride stride_a;
     typename LayoutB::Stride stride_b;
     typename LayoutC::Stride stride_c;
     typename LayoutC::Stride stride_d;
 
     typename LayoutA::Stride::LongIndex lda;
@@ -143,21 +136,22 @@
     int const * ptr_gather_A_indices;
     int const * ptr_gather_B_indices;
     int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
-
-    Arguments():
+    
+    Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
       ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
       ptr_gather_A_indices(nullptr),
       ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr)
-    {}
+      ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -171,30 +165,31 @@
       int64_t batch_stride_D,
       typename LayoutA::Stride stride_a,
       typename LayoutB::Stride stride_b,
       typename LayoutC::Stride stride_c,
       typename LayoutC::Stride stride_d,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
       stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices)
-    {
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
       lda = 0;
       ldb = 0;
       ldc = 0;
       ldd = 0;
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-    }
+      }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -210,158 +205,188 @@
       typename LayoutB::Stride::LongIndex ldb,
       typename LayoutC::Stride::LongIndex ldc,
       typename LayoutC::Stride::LongIndex ldd,
       int const *ptr_gather_A_indices = nullptr,
       int const *ptr_gather_B_indices = nullptr,
       int const *ptr_scatter_D_indices = nullptr
     ):
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue),
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
+      epilogue(epilogue), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
       ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices)
-    {
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
       stride_a = make_Coord(lda);
       stride_b = make_Coord(ldb);
       stride_c = make_Coord(ldc);
       stride_d = make_Coord(ldd);
       CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-    }
+      }
 
     /// Returns arguments for the transposed problem
-    Arguments transposed_problem() const
-    {
+    Arguments transposed_problem() const {
       Arguments args(*this);
-
+      
       std::swap(args.problem_size.m(), args.problem_size.n());
       std::swap(args.ptr_A, args.ptr_B);
       std::swap(args.lda, args.ldb);
       std::swap(args.stride_a, args.stride_b);
       std::swap(args.batch_stride_A, args.batch_stride_B);
       std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
 
       return args;
     }
   };
 
-
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    //
-    // Data members
-    //
+  struct Params {
 
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
+    
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
-
+    
     typename EpilogueOutputOp::Params output_op;
 
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
 
     int * ptr_gather_A_indices;
     int * ptr_gather_B_indices;
     int * ptr_scatter_D_indices;
 
+    int *semaphore;
+
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_B(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr),
+      semaphore(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
       params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
       params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
       params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
       output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
+      batch_stride_D(args.batch_stride_D),
       ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
       ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices))
-    {}
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
+      semaphore(static_cast<int *>(workspace)) {
 
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    void update(Arguments const &args)
-    {
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
+    }
+
+    CUTLASS_HOST_DEVICE
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
 
-      // Update input/output pointers
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
       ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
       ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
       ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
 
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_D = args.batch_stride_D;
+
       output_op = args.epilogue;
+      
+      semaphore = static_cast<int *>(workspace);
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
   };
 
-
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
-
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  GemmUniversal() { } 
+
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
-    cutlass::gemm::GemmCoord const & problem_size)
-  {
+    cutlass::gemm::GemmCoord const & problem_size) {
+
     CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
 
     static int const kAlignmentA = (platform::is_same<LayoutA,
                                                       layout::ColumnMajorInterleaved<32>>::value)
                                    ? 32
                                    : (platform::is_same<LayoutA,
                                                         layout::ColumnMajorInterleaved<64>>::value)
@@ -433,38 +458,23 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-public:
-
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmUniversal op;
-    op(params, shared_storage);
+    return 0;
   }
-
-
+ 
   /// Executes one GEMM
   CUTLASS_DEVICE
-  void operator()(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
+  void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord threadblock_tile_offset =
         threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
@@ -534,15 +544,15 @@
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B,
       params.ptr_gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
@@ -663,15 +673,15 @@
       accumulators, 
       iterator_C); 
     
     //
     // Release the semaphore
     //
 
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
+    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
 
       int lock = 0;
       if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
 
         // The final threadblock resets the semaphore for subsequent grids.
         lock = 0;
       }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_universal_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_fused_epilogue.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,46 +32,35 @@
     \brief Gemm kernel with fused reduction operation.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
-#include "cutlass/layout/layout.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 
 #include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
   typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
-  bool IsSingleSource = Epilogue_::kIsSingleSource
->
-struct GemmWithFusedEpilogue;
-
-// GemmWithFusedEpilogue with two sources
-template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
-  typename Epilogue_,             ///! Epilogue
   typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct GemmWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, false> {
+struct GemmWithFusedEpilogue {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
@@ -108,761 +97,38 @@
   );
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase{
+  struct Arguments {
 
     //
     // Data members
     //
 
-    typename EpilogueOutputOp::Params epilogue;
-
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C1;
-    void const * ptr_C2;
-    void * ptr_D;
-
-    void * ptr_Vector;
-    void * ptr_Tensor;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C1;
-    int64_t batch_stride_C2;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
-
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc1;
-    typename LayoutC::Stride::Index ldc2;
-    typename LayoutC::Stride::Index ldd;
-    typename LayoutC::Stride::Index ldr;
-    typename LayoutC::Stride::Index ldt;
-
-    //
-    // Methods
-    //
-    
-    Arguments(): 
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C1(nullptr),
-      ptr_C2(nullptr),
-      ptr_D(nullptr)
-    {}
-
-    /// constructs an arguments structure
-    Arguments(
-      GemmUniversalMode mode,
-      GemmCoord problem_size,
-      int batch_count,
-      typename EpilogueOutputOp::Params epilogue,
-      void const * ptr_A,
-      void const * ptr_B,
-      void const * ptr_C1,
-      void const * ptr_C2,
-      void * ptr_D,
-      void * ptr_Vector,
-      void * ptr_Tensor,
-      int64_t batch_stride_A,
-      int64_t batch_stride_B,
-      int64_t batch_stride_C1,
-      int64_t batch_stride_C2,
-      int64_t batch_stride_D,
-      int64_t batch_stride_Vector,
-      int64_t batch_stride_Tensor,
-      typename LayoutA::Stride::Index lda,
-      typename LayoutB::Stride::Index ldb,
-      typename LayoutC::Stride::Index ldc1,
-      typename LayoutC::Stride::Index ldc2,
-      typename LayoutC::Stride::Index ldd,
-      typename LayoutC::Stride::Index ldr,
-      typename LayoutC::Stride::Index ldt)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue), 
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C1(ptr_C1), ptr_C2(ptr_C2), ptr_D(ptr_D), 
-      ptr_Vector(ptr_Vector), 
-      ptr_Tensor(ptr_Tensor),
-      batch_stride_A(batch_stride_A), 
-      batch_stride_B(batch_stride_B), 
-      batch_stride_C1(batch_stride_C1), 
-      batch_stride_C2(batch_stride_C2), 
-      batch_stride_Vector(batch_stride_Vector),
-      batch_stride_Tensor(batch_stride_Tensor),
-      lda(lda), ldb(ldb), ldc1(ldc1), ldc2(ldc2), ldd(ldd), ldr(ldr), ldt(ldt)
-    {
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
-      CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
-    }
-
-    /// Returns arguments for the transposed problem
-    Arguments transposed_problem() const {
-      Arguments args(*this);
-      
-      std::swap(args.problem_size.m(), args.problem_size.n());
-      std::swap(args.ptr_A, args.ptr_B);
-      std::swap(args.lda, args.ldb);
-      std::swap(args.batch_stride_A, args.batch_stride_B);
-
-      return args;
-    }
-  };
-
-
-  //
-  // Structure for precomputing values in host memory and passing to kernels
-  //
-
-  /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    //
-    // Data members
-    //
-
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
-    typename Epilogue::OutputTileIterator::Params params_C1;
-    typename Epilogue::OutputTileIterator::Params params_C2;
-    typename Epilogue::OutputTileIterator::Params params_D;
-    typename Epilogue::TensorTileIterator::Params params_Tensor;
-    typename EpilogueOutputOp::Params output_op;
-
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_C1;
-    void * ptr_C2;
-    void * ptr_D;
-
-    void * ptr_Vector;
-    typename LayoutC::Stride::Index ldr;
-
-    void * ptr_Tensor;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C1;
-    int64_t batch_stride_C2;
-    int64_t batch_stride_Vector;
-    int64_t batch_stride_Tensor;
-
-    //
-    // Host dispatch API
-    //
-
-    /// Default constructor
-    Params() = default;
-
-    /// Constructor
-    Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
-      params_A(args.lda),
-      params_B(args.ldb),
-      params_C1(args.ldc1),
-      params_C2(args.ldc2),
-      params_D(args.ldd),
-      params_Tensor(args.ldt),
-      output_op(args.epilogue),
-      ptr_A(const_cast<void *>(args.ptr_A)),
-      ptr_B(const_cast<void *>(args.ptr_B)),
-      ptr_C1(const_cast<void *>(args.ptr_C1)),
-      ptr_C2(const_cast<void *>(args.ptr_C2)),
-      ptr_D(args.ptr_D),
-      ptr_Vector(args.ptr_Vector),
-      ldr(args.ldr),
-      ptr_Tensor(args.ptr_Tensor),
-      batch_stride_A(args.batch_stride_A),
-      batch_stride_B(args.batch_stride_B),
-      batch_stride_C1(args.batch_stride_C1),
-      batch_stride_C2(args.batch_stride_C2),
-      batch_stride_Vector(args.batch_stride_Vector),
-      batch_stride_Tensor(args.batch_stride_Tensor)
-    {
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
-      CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
-    }
-
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    CUTLASS_HOST_DEVICE
-    void update(Arguments const &args)
-    {
-      ptr_A = const_cast<void *>(args.ptr_A);
-      ptr_B = const_cast<void *>(args.ptr_B);
-      ptr_C1 = const_cast<void *>(args.ptr_C1);
-      ptr_C2 = const_cast<void *>(args.ptr_C2);
-      ptr_D = args.ptr_D;
-
-      ptr_Vector = args.ptr_Vector;
-      ldr = args.ldr;
-      ptr_Tensor = args.ptr_Tensor;
-
-      output_op = args.epilogue;
-
-      CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::update()");
-      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
-      CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
-      CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
-    }
-  };
-
-
-  /// Shared memory storage structure
-  union SharedStorage {
-    typename Mma::SharedStorage main_loop;
-    typename Epilogue::SharedStorage epilogue;
-  };
-
-public:
-
-  //
-  // Host dispatch API
-  //
-
-  /// Determines whether kernel satisfies alignment
-  static Status can_implement(
-    cutlass::gemm::GemmCoord const & problem_size) {
-
-    CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::can_implement()");
-
-    static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
-
-    bool isAMisaligned = false;
-    bool isBMisaligned = false;
-    bool isCMisaligned = false;
-
-    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
-      isAMisaligned = problem_size.m() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    }
-
-    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
-      isBMisaligned = problem_size.n() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
-            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    }
-
-    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
-      isCMisaligned = problem_size.m() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    }
-
-    if (isAMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for A operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    if (isBMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for B operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    if (isCMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for C operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    CUTLASS_TRACE_HOST("  returning kSuccess");
-
-    return Status::kSuccess;
-  }
-
-  static Status can_implement(Arguments const &args) {
-    return can_implement(args.problem_size);
-  }
-
-public:
-
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmWithFusedEpilogue op;
-    op(params, shared_storage);
-  }
-
-  #define SPLIT_K_ENABLED 1
-
-  /// Executes one GEMM
-  CUTLASS_DEVICE
-  void operator()(Params const &params, SharedStorage &shared_storage) {
-
-    // Compute threadblock location
-    ThreadblockSwizzle threadblock_swizzle;
-
-    cutlass::gemm::GemmCoord threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-    // Early exit if CTA is out of range
-    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
-      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
-
-      return;
-    }
-
-    int offset_k = 0;
-    int problem_size_k = params.problem_size.k();
-
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
-    ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
-
-
-    #if SPLIT_K_ENABLED
-    //
-    // Fetch pointers based on mode.
-    //
-    if (params.mode == GemmUniversalMode::kGemm || 
-      params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-
-      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
-
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
-      }
-
-      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
-      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
-      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
-    }
-    #endif
-
-    // Compute initial location in logical coordinates
-    cutlass::MatrixCoord tb_offset_A{
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      offset_k,
-    };
-
-    cutlass::MatrixCoord tb_offset_B{
-      offset_k,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    };
-
-    // Compute position within threadblock
-    int thread_idx = threadIdx.x;
-
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.params_A,
-      ptr_A,
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_A);
-
-    typename Mma::IteratorB iterator_B(
-      params.params_B,
-      ptr_B,
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_B);
-
-    // Broadcast the warp_id computed by lane 0 to ensure dependent code
-    // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-
-    int lane_idx = threadIdx.x % 32;
-
-    //
-    // Main loop
-    //
-
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
-
-    typename Mma::FragmentC accumulators;
-
-    accumulators.clear();
-
-    // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
-
-    // Compute threadblock-scoped matrix multiply-add
-    mma(
-      gemm_k_iterations, 
-      accumulators, 
-      iterator_A, 
-      iterator_B, 
-      accumulators);
-
-    //
-    // Epilogue
-    //
-
-    EpilogueOutputOp output_op(params.output_op);
-
-    //
-    // Masked tile iterators constructed from members
-    //
-
-    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-    //assume identity swizzle
-    MatrixCoord threadblock_offset(
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    );
-
-    int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
-
-    ElementC *ptr_C1 = static_cast<ElementC *>(params.ptr_C1);
-    ElementC *ptr_C2 = static_cast<ElementC *>(params.ptr_C2);
-    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
-    typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
-
-    // Define the reduction output pointer and move to the appropriate place
-    typename Epilogue::ElementVector *ptr_Vector = 
-      static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
-
-    //
-    // Fetch pointers based on mode.
-    //
-    
-    //
-    // Special path when split-K not enabled.
-    // 
-
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() == 1) {
-
-      // Tile iterators loading from source tensors.
-      typename Epilogue::OutputTileIterator iterator_C1(
-        params.params_C1,
-        ptr_C1,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      typename Epilogue::OutputTileIterator iterator_C2(
-        params.params_C2,
-        ptr_C2,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      // Tile iterator writing to destination tensor.
-      typename Epilogue::OutputTileIterator iterator_D(
-        params.params_D,
-        ptr_D,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset
-      );
-
-      // Additional tensor to load from
-      typename Epilogue::TensorTileIterator tensor_iterator(
-          params.params_Tensor,
-          // Only the final block outputs Tensor
-          ptr_Tensor,
-          params.problem_size.mn(),
-          thread_idx,
-          threadblock_offset);
-
-      // Construct the epilogue
-      Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
-        lane_idx);
-
-      // Move to appropriate location for this output tile
-      if (ptr_Vector) {
-        ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
-      }
-
-      // Execute the epilogue operator to update the destination tensor.
-      epilogue(output_op,
-               ptr_Vector,
-               iterator_D,
-               accumulators,
-               iterator_C1,
-               iterator_C2,
-               tensor_iterator,
-               params.problem_size.mn(),
-               threadblock_offset);
-
-      return;
-    }
-
-    //
-    // Slower path when split-K or batching is needed
-    //
-
-      
-    #if SPLIT_K_ENABLED
-    // Construct the semaphore.
-    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
-
-    if (params.mode == GemmUniversalMode::kGemm) {
-
-      // If performing a reduction via split-K, fetch the initial synchronization
-      if (params.grid_tiled_shape.k() > 1) {
-        
-        // Fetch the synchronization lock initially but do not block.
-        semaphore.fetch();
-
-        // Indicate which position in a serial reduction the output operator is currently updating
-        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_C1 += threadblock_tile_offset.k() * params.batch_stride_C1;
-      if (ptr_C2) {
-        ptr_C2 += threadblock_tile_offset.k() * params.batch_stride_C2;
-      }
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-      if (ptr_Tensor) {
-        ptr_Tensor += threadblock_tile_offset.k() * params.batch_stride_Tensor;
-      }
-      if (ptr_Vector) {
-        ptr_Vector += threadblock_tile_offset.k() * params.batch_stride_Vector;
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_C1 = static_cast<ElementC * const *>(params.ptr_C1)[threadblock_tile_offset.k()];
-      if (ptr_C2) {
-        ptr_C2 = static_cast<ElementC * const *>(params.ptr_C2)[threadblock_tile_offset.k()];
-      }
-      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
-      if (ptr_Tensor) {
-        ptr_Tensor = static_cast<typename Epilogue::ElementTensor * const *>(params.ptr_Tensor)[threadblock_tile_offset.k()];
-      }
-      if (ptr_Vector) {
-        ptr_Vector = static_cast<typename Epilogue::ElementVector * const *>(params.ptr_Vector)[threadblock_tile_offset.k()];
-      }
-    }
-    #endif
-
-    // Tile iterators loading from source tensors.
-    typename Epilogue::OutputTileIterator iterator_C1(
-      params.params_C1,
-      ptr_C1,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
-
-    typename Epilogue::OutputTileIterator iterator_C2(
-      params.params_C2,
-      ptr_C2,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
-
-    // Tile iterator writing to destination tensor.
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.params_D,
-      ptr_D,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset
-    );
-
-    // Additional tensor to load from
-    typename Epilogue::TensorTileIterator tensor_iterator(
-        params.params_Tensor,
-        // Only the final block outputs Tensor
-        ((params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) &&
-         (params.grid_tiled_shape.k() != threadblock_tile_offset.k() + 1))
-            ? nullptr
-            : ptr_Tensor,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset);
-
-    // Construct the epilogue
-    Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
-      lane_idx);
-
-    #if SPLIT_K_ENABLED
-    // Wait on the semaphore - this latency may have been covered by iterator construction
-    if ((params.mode == GemmUniversalMode::kGemm) && params.grid_tiled_shape.k() > 1) {
-        
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_offset.k()) {
-        iterator_C1 = iterator_D;
-      }
-
-      semaphore.wait(threadblock_tile_offset.k());
-
-    }
-    #endif
-
-    // Move to appropriate location for this output tile
-    if (ptr_Vector) {
-      ptr_Vector += threadblock_offset.column() + threadblock_tile_offset.m() * params.ldr;
-    }
-
-    // Execute the epilogue operator to update the destination tensor.
-    epilogue(output_op,
-             // Only the final block uses Vector
-             ((params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) &&
-              (params.grid_tiled_shape.k() != threadblock_tile_offset.k() + 1))
-                 ? nullptr
-                 : ptr_Vector,
-             iterator_D,
-             accumulators,
-             iterator_C1,
-             iterator_C2,
-             tensor_iterator,
-             params.problem_size.mn(),
-             threadblock_offset);
-
-    //
-    // Release the semaphore
-    //
-
-    #if SPLIT_K_ENABLED
-    if ((params.mode == GemmUniversalMode::kGemm)  && params.grid_tiled_shape.k() > 1) { 
-
-      int lock = 0;
-      if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
-
-        // The final threadblock resets the semaphore for subsequent grids.
-        lock = 0;
-      }
-      else {
-        // Otherwise, the semaphore is incremented
-        lock = threadblock_tile_offset.k() + 1;
-      }
-      
-      semaphore.release(lock);
-    }
-    #endif
-  }
-};
-
-// GemmWithFusedEpilogue with one source
-template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
-  typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
->
-struct GemmWithFusedEpilogue<Mma_, Epilogue_, ThreadblockSwizzle_, true> {
-public:
-
-  using Mma = Mma_;
-  using Epilogue = Epilogue_;
-  using EpilogueOutputOp = typename Epilogue::OutputOp;
-  using ThreadblockSwizzle = ThreadblockSwizzle_;
-
-  using ElementA = typename Mma::IteratorA::Element;
-  using LayoutA = typename Mma::IteratorA::Layout;
-  using ElementB = typename Mma::IteratorB::Element;
-  using LayoutB = typename Mma::IteratorB::Layout;
-  using ElementC = typename Epilogue::OutputTileIterator::Element;
-  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-
-  static ComplexTransform const kTransformA = Mma::kTransformA;
-  static ComplexTransform const kTransformB = Mma::kTransformB;
-  using Operator = typename Mma::Operator;
-
-  using OperatorClass = typename Mma::Operator::OperatorClass;
-  using ThreadblockShape = typename Mma::Shape;
-  using WarpShape = typename Mma::Operator::Shape;
-  using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
-  using ArchTag = typename Mma::ArchTag;
-
-  static int const kStages = Mma::kStages;
-  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
-  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
-
-  /// Warp count (concept: GemmShape)
-  using WarpCount = typename Mma::WarpCount;
-  static int const kThreadCount = 32 * WarpCount::kCount;
-
-  /// Split-K preserves splits that are 128b aligned
-  static int const kSplitKAlignment = const_max(
-    128 / sizeof_bits<ElementA>::value, 
-    128 / sizeof_bits<ElementB>::value
-  );
-
-  //
-  // Structures
-  //
-
-  /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
-    //
-    // Data members
-    //
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
 
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
     void * ptr_Vector;
     void * ptr_Tensor;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
     int64_t batch_stride_Vector;
     int64_t batch_stride_Tensor;
 
     typename LayoutA::Stride::Index lda;
     typename LayoutB::Stride::Index ldb;
     typename LayoutC::Stride::Index ldc;
     typename LayoutC::Stride::Index ldd;
@@ -870,19 +136,17 @@
     typename LayoutC::Stride::Index ldt;
 
     //
     // Methods
     //
     
     Arguments(): 
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr)
-    {}
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
@@ -899,30 +163,33 @@
       int64_t batch_stride_Vector,
       int64_t batch_stride_Tensor,
       typename LayoutA::Stride::Index lda,
       typename LayoutB::Stride::Index ldb,
       typename LayoutC::Stride::Index ldc,
       typename LayoutC::Stride::Index ldd,
       typename LayoutC::Stride::Index ldr,
-      typename LayoutC::Stride::Index ldt)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
+      typename LayoutC::Stride::Index ldt
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
       epilogue(epilogue), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
       ptr_Vector(ptr_Vector), 
       ptr_Tensor(ptr_Tensor),
       batch_stride_A(batch_stride_A), 
       batch_stride_B(batch_stride_B), 
       batch_stride_C(batch_stride_C), 
+      batch_stride_D(batch_stride_D), 
       batch_stride_Vector(batch_stride_Vector),
       batch_stride_Tensor(batch_stride_Tensor),
       lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ldr(ldr), ldt(ldt)
     {
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Arguments::Arguments() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
+      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << this->ldt);
     }
 
     /// Returns arguments for the transposed problem
     Arguments transposed_problem() const {
@@ -933,138 +200,176 @@
       std::swap(args.lda, args.ldb);
       std::swap(args.batch_stride_A, args.batch_stride_B);
 
       return args;
     }
   };
 
-
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
+  struct Params {
 
-    //
-    // Data members
-    //
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
 
     typename Mma::IteratorA::Params params_A;
     typename Mma::IteratorB::Params params_B;
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     typename Epilogue::TensorTileIterator::Params params_Tensor;
-
+    
     typename EpilogueOutputOp::Params output_op;
 
+
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
-
+    
     void * ptr_Vector;
     typename LayoutC::Stride::Index ldr;
 
     void * ptr_Tensor;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
+    int64_t batch_stride_D;
     int64_t batch_stride_Vector;
     int64_t batch_stride_Tensor;
 
+    int *semaphore;
+
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_B(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_Vector(nullptr),
+      ldr(0),
+      ptr_Tensor(nullptr),
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      batch_stride_Vector(0),
+      batch_stride_Tensor(0),
+      semaphore(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A(args.lda),
       params_B(args.ldb),
       params_C(args.ldc),
       params_D(args.ldd),
       params_Tensor(args.ldt),
       output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
       ptr_D(args.ptr_D),
-      ptr_Vector(args.ptr_Vector),
+      ptr_Vector(args.ptr_Vector), 
       ldr(args.ldr),
       ptr_Tensor(args.ptr_Tensor),
+
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
+      batch_stride_D(args.batch_stride_D),
       batch_stride_Vector(args.batch_stride_Vector),
-      batch_stride_Tensor(args.batch_stride_Tensor)
-    {
+      batch_stride_Tensor(args.batch_stride_Tensor),
+
+      semaphore(static_cast<int *>(workspace)) {
+
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::Params() - problem_size: " << problem_size);
-      CUTLASS_TRACE_HOST("  ptr_Vector: " << (void *)this->ptr_Vector);
+      CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
       CUTLASS_TRACE_HOST("  ldt: " << args.ldt);
     }
 
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
     CUTLASS_HOST_DEVICE
-    void update(Arguments const &args)
-    {
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
+
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
       ptr_Vector = args.ptr_Vector;
       ldr = args.ldr;
       ptr_Tensor = args.ptr_Tensor;
 
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_D = args.batch_stride_D;
+      batch_stride_Vector = args.batch_stride_Vector;
+      batch_stride_Tensor = args.batch_stride_Tensor;
+
       output_op = args.epilogue;
 
+      semaphore = static_cast<int *>(workspace);
+
       CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::Params::update()");
       CUTLASS_TRACE_HOST("  ptr_Reduction: " << (void *)this->ptr_Reduction);
       CUTLASS_TRACE_HOST("  ptr_Tensor: " << (void *)this->ptr_Tensor);
       CUTLASS_TRACE_HOST("  ldr: " << this->ldr);
     }
   };
 
-
   /// Shared memory storage structure
   union SharedStorage {
     typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  GemmWithFusedEpilogue() { } 
+
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
     CUTLASS_TRACE_HOST("GemmWithFusedEpilogue::can_implement()");
 
     static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
@@ -1122,28 +427,18 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-public:
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
 
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmWithFusedEpilogue op;
-    op(params, shared_storage);
+    return 0;
   }
 
   #define SPLIT_K_ENABLED 1
 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
@@ -1218,15 +513,15 @@
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
@@ -1264,15 +559,15 @@
     MatrixCoord threadblock_offset(
       threadblock_tile_offset.m() * Mma::Shape::kM,
       threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
     typename Epilogue::ElementTensor *ptr_Tensor = static_cast<typename Epilogue::ElementTensor *>(params.ptr_Tensor);
 
     // Define the reduction output pointer and move to the appropriate place
     typename Epilogue::ElementVector *ptr_Vector = 
       static_cast<typename Epilogue::ElementVector *>(params.ptr_Vector);
 
@@ -1282,15 +577,15 @@
     
     //
     // Special path when split-K not enabled.
     // 
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() == 1) {
 
-      // Tile iterators loading from source tensors.
+      // Tile iterator loading from source tensor.
       typename Epilogue::OutputTileIterator iterator_C(
         params.params_C,
         ptr_C,
         params.problem_size.mn(),
         thread_idx,
         threadblock_offset
       );
@@ -1380,15 +675,15 @@
       }
       if (ptr_Vector) {
         ptr_Vector = static_cast<typename Epilogue::ElementVector * const *>(params.ptr_Vector)[threadblock_tile_offset.k()];
       }
     }
     #endif
 
-    // Tile iterators loading from source tensors.
+    // Tile iterator loading from source tensor.
     typename Epilogue::OutputTileIterator iterator_C(
       params.params_C,
       ptr_C,
       params.problem_size.mn(),
       thread_idx,
       threadblock_offset
     );
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemm_with_k_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,425 +27,363 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
+
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
+#include "cutlass/blas3.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
-
-#include "cutlass/trace.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
+  typename Mma1_,                 ///! Threadblock-scoped matrix multiply-accumulate (A*B^T)
+  typename Mma2_,                 ///! Threadblock-scoped matrix multiply-accumulate (B*A^T)
   typename Epilogue_,             ///! Epilogue
-  typename EpilogueGemmKReduction_,             ///! Epilogue
-  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
+  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
+  FillMode FillModeC_,            ///! Fill Mode for C (kLower or kUpper)
+  BlasMode BlasMode_              ///! Blas3 computation mode
 >
-struct GemmWithKReduction {
+struct Rank2KUniversal {
 public:
 
-  using Mma = Mma_;
+  using Mma1 = Mma1_;
+  using Mma2 = Mma2_;
   using Epilogue = Epilogue_;
   using EpilogueOutputOp = typename Epilogue::OutputOp;
-  using EpilogueGemmKReduction = EpilogueGemmKReduction_;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
-  using ElementA = typename Mma::IteratorA::Element;
-  using LayoutA = typename Mma::IteratorA::Layout;
-  using ElementB = typename Mma::IteratorB::Element;
-  using LayoutB = typename Mma::IteratorB::Layout;
+  using ElementA = typename Mma1::IteratorA::Element;
+  using ElementB = typename Mma1::IteratorB::Element;
+
+  // Mma1 (A x B^T)
+  using LayoutA = typename Mma1::IteratorA::Layout;
+  using LayoutBT = typename Mma1::IteratorB::Layout;
+  static ComplexTransform const kMma1TransformA = Mma1::kTransformA;
+  static ComplexTransform const kMma1TransformB = Mma1::kTransformB;
+
+  // Mma2 (B x A^T)
+  using LayoutB = typename Mma2::IteratorA::Layout;
+  using LayoutAT = typename Mma2::IteratorB::Layout;
+  static ComplexTransform const kMma2TransformA = Mma2::kTransformA;
+  static ComplexTransform const kMma2TransformB = Mma2::kTransformB;
+
+  // Common type definitions for Mma1 and Mma2
+  using Operator = typename Mma1::Operator;
+  using OperatorClass = typename Mma1::Operator::OperatorClass;
+  using ThreadblockShape = typename Mma1::Shape;
+  using WarpShape = typename Mma1::Operator::Shape;
+  using InstructionShape = typename Mma1::Policy::Operator::InstructionShape;
+  using ArchTag = typename Mma1::ArchTag;
+
+  static int const kStages = Mma1::kStages;
+  static int const kAlignmentA = Mma1::IteratorA::AccessType::kElements;
+  static int const kAlignmentB = Mma1::IteratorB::AccessType::kElements;
+
+  // Output related typedefinitions
   using ElementC = typename Epilogue::OutputTileIterator::Element;
   using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-  using LayoutGemmKReduction = cutlass::layout::PitchLinear;
-
-  static ComplexTransform const kTransformA = Mma::kTransformA;
-  static ComplexTransform const kTransformB = Mma::kTransformB;
-  using Operator = typename Mma::Operator;
-
-  using OperatorClass = typename Mma::Operator::OperatorClass;
-  using ThreadblockShape = typename Mma::Shape;
-  using WarpShape = typename Mma::Operator::Shape;
-  using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
-  using ArchTag = typename Mma::ArchTag;
-
-  static int const kStages = Mma::kStages;
-  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
+  static FillMode const kFillModeC = FillModeC_;
   static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+  static BlasMode const kBlasMode = BlasMode_;
+
 
   /// Warp count (concept: GemmShape)
-  using WarpCount = typename Mma::WarpCount;
+  using WarpCount = typename Mma1::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
-  /// Split-K preserves splits that are 128b aligned
-  static int const kSplitKAlignment = const_max(128 / sizeof_bits<ElementA>::value, 128 / sizeof_bits<ElementB>::value);
-
-  static int const kReduceKForA = Mma::kReduceKForA;
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase
-  {
+  struct Arguments {
+
     //
     // Data members
     //
 
+    GemmUniversalMode mode;
+    GemmCoord problem_size;
+    int batch_count;
+
     typename EpilogueOutputOp::Params epilogue;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
-    void * ptr_gemm_k_reduction;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_gemm_k_reduction;
+    int64_t batch_stride_D;
 
     typename LayoutA::Stride::Index lda;
     typename LayoutB::Stride::Index ldb;
     typename LayoutC::Stride::Index ldc;
     typename LayoutC::Stride::Index ldd;
-    typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction;
 
     //
     // Methods
     //
-
-    Arguments() :
-      ptr_A(nullptr),
-      ptr_B(nullptr),
-      ptr_C(nullptr),
-      ptr_D(nullptr),
-      ptr_gemm_k_reduction(nullptr)
-    {}
+    
+    Arguments(): 
+      mode(GemmUniversalMode::kGemm), 
+      batch_count(1), 
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
       typename EpilogueOutputOp::Params epilogue,
       void const * ptr_A,
       void const * ptr_B,
       void const * ptr_C,
       void * ptr_D,
-      void * ptr_gemm_k_reduction,
       int64_t batch_stride_A,
       int64_t batch_stride_B,
       int64_t batch_stride_C,
       int64_t batch_stride_D,
-      int64_t batch_stride_gemm_k_reduction,
       typename LayoutA::Stride::Index lda,
       typename LayoutB::Stride::Index ldb,
       typename LayoutC::Stride::Index ldc,
-      typename LayoutC::Stride::Index ldd,
-      typename LayoutGemmKReduction::Stride::Index ld_gemm_k_reduction)
-    :
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue(epilogue),
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), ptr_gemm_k_reduction(ptr_gemm_k_reduction),
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_gemm_k_reduction(batch_stride_gemm_k_reduction),
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd), ld_gemm_k_reduction(ld_gemm_k_reduction)
-    {
-      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-    }
-
-    /// Returns arguments for the transposed problem
-    Arguments transposed_problem() const {
-      Arguments args(*this);
-
-      std::swap(args.problem_size.m(), args.problem_size.n());
-      std::swap(args.ptr_A, args.ptr_B);
-      std::swap(args.lda, args.ldb);
-      std::swap(args.batch_stride_A, args.batch_stride_B);
+      typename LayoutC::Stride::Index ldd
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
+      epilogue(epilogue), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd) {
 
-      return args;
-    }
-  };
+      }
+
+      /// Returns arguments for a the transposed problem
+      Arguments transposed_problem() const {
+        Arguments args(*this);
+        
+        std::swap(args.ptr_A, args.ptr_B);
+        std::swap(args.lda, args.ldb);
+        std::swap(args.batch_stride_A, args.batch_stride_B);
+
+        return args;
+      }
 
+  };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC>
-  {
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
+  struct Params {
 
-    //
-    // Data members
-    //
+    cutlass::gemm::GemmCoord problem_size;
+    cutlass::gemm::GemmCoord grid_tiled_shape;
+    int swizzle_log_tile;
     
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
+    // Mma1 Iterator A and B params
+    typename Mma1::IteratorA::Params params_A;
+    typename Mma1::IteratorB::Params params_BT;
+
+    // Mma2 Iterator A and B params 
+    typename Mma2::IteratorA::Params params_B;
+    typename Mma2::IteratorB::Params params_AT;
+
     typename Epilogue::OutputTileIterator::Params params_C;
     typename Epilogue::OutputTileIterator::Params params_D;
     
     typename EpilogueOutputOp::Params output_op;
 
+    GemmUniversalMode mode;
+    int batch_count;
+    int gemm_k_size;
+
     void * ptr_A;
     void * ptr_B;
     void * ptr_C;
     void * ptr_D;
-    void * ptr_gemm_k_reduction;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
-    int64_t batch_stride_gemm_k_reduction;
+    int64_t batch_stride_D;
+
+    int *semaphore;
 
     //
-    // Host dispatch API
+    // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      swizzle_log_tile(0),
+      params_A(0),
+      params_BT(0),
+      params_B(0),
+      params_AT(0),
+      params_C(0),
+      params_D(0),
+      batch_count(0),
+      gemm_k_size(0),
+      mode(cutlass::gemm::GemmUniversalMode::kGemm),
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      batch_stride_A(0),
+      batch_stride_B(0),
+      batch_stride_C(0),
+      batch_stride_D(0),
+      semaphore(nullptr) { }
 
-    /// Constructor
+    CUTLASS_HOST_DEVICE
     Params(
-      Arguments const &args,  /// GEMM application arguments
-      int device_sms,         /// Number of SMs on the device
-      int sm_occupancy)       /// Kernel SM occupancy (in thread blocks)
-    :
-      ParamsBase(args, device_sms, sm_occupancy),
+      Arguments const &args,
+      cutlass::gemm::GemmCoord const & grid_tiled_shape,
+      int gemm_k_size,
+      void *workspace = nullptr
+    ):
+      problem_size(args.problem_size),
+      grid_tiled_shape(grid_tiled_shape),
+      swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
       params_A(args.lda),
+      params_BT(args.ldb),
       params_B(args.ldb),
+      params_AT(args.lda),
       params_C(args.ldc),
       params_D(args.ldd),
       output_op(args.epilogue),
+      mode(args.mode),
+      batch_count(args.batch_count),
+      gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
+      ptr_D(const_cast<void *>(args.ptr_D)),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
-      batch_stride_gemm_k_reduction(args.batch_stride_gemm_k_reduction),
-      ptr_D(args.ptr_D),
-      ptr_gemm_k_reduction(args.ptr_gemm_k_reduction)
-    {}
-
-    /// Assign and initialize the specified workspace buffer.  Assumes
-    /// the memory allocated to workspace is at least as large as get_workspace_size().
-    Status init_workspace(
-      void *workspace,
-      cudaStream_t stream = nullptr)
-    {
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::Params() - problem_size: " << this->problem_size);
-
-      if (this->mode == GemmUniversalMode::kGemmSplitKParallel) {
-        ptr_D = workspace;
-        ptr_gemm_k_reduction = static_cast<uint8_t *>(workspace)
-                 + sizeof(ElementC) * size_t(this->batch_stride_D) * size_t(this->grid_tiled_shape.k());
-
-        return Status::kSuccess;
-      }
-
-      return ParamsBase::init_workspace(workspace, stream);
+      batch_stride_D(args.batch_stride_D),
+      semaphore(static_cast<int *>(workspace)) {
     }
 
-    /// Returns the workspace size (in bytes) needed for this problem geometry
-    size_t get_workspace_size() const
-    {
-      size_t workspace_bytes = ParamsBase::get_workspace_size();
-
-      if (this->mode == GemmUniversalMode::kGemmSplitKParallel)
-      {
-        // Split-K parallel always requires a temporary workspace
-        workspace_bytes +=
-          sizeof(ElementC) *
-          size_t(batch_stride_gemm_k_reduction) *
-          size_t(this->grid_tiled_shape.k());
-      }
+    CUTLASS_HOST_DEVICE
+    void update(
+      Arguments const &args,
+      void *workspace = nullptr) {
 
-      return workspace_bytes;
-    }
-
-    /// Lightweight update given a subset of arguments.  Problem geometry is assumed
-    /// to remain the same.
-    void update(Arguments const &args)
-    {
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
-      ptr_gemm_k_reduction = args.ptr_gemm_k_reduction;
 
       output_op = args.epilogue;
 
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
+      semaphore = static_cast<int *>(workspace);
     }
+
   };
 
   /// Shared memory storage structure
   union SharedStorage {
-    typename Mma::SharedStorage main_loop;
+    typename Mma1::SharedStorage mma1_main_loop;
+    typename Mma2::SharedStorage mma2_main_loop;
     typename Epilogue::SharedStorage epilogue;
   };
 
-
 public:
 
   //
-  // Host dispatch API
+  // Methods
   //
 
+  CUTLASS_DEVICE
+  Rank2KUniversal() { } 
+
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
-    CUTLASS_TRACE_HOST("GemmUniversal::can_implement()");
-
-    static int const kAlignmentA = (platform::is_same<typename Mma::IteratorA::Layout,
-                                                      layout::ColumnMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<typename Mma::IteratorA::Layout,
-                                                        layout::ColumnMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Mma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = (platform::is_same<typename Mma::IteratorB::Layout,
-                                                       layout::RowMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<typename Mma::IteratorB::Layout,
-                                                        layout::RowMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Mma::IteratorB::AccessType::kElements;
-    static int const kAlignmentC =  (platform::is_same<LayoutC,
-                                                      layout::ColumnMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<LayoutC,
-                                                        layout::ColumnMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
-
-    bool isAMisaligned = false;
-    bool isBMisaligned = false;
-    bool isCMisaligned = false;
-
-    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
-      isAMisaligned = problem_size.m() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    }
-
-    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
-      isBMisaligned = problem_size.n() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
-            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    }
-
-    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
-      isCMisaligned = problem_size.m() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    }
-
-    if (isAMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand A");
-      return Status::kErrorMisalignedOperand;
-    }
+    static int const kAlignmentA = Mma1::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = Mma1::IteratorB::AccessType::kElements;
+    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
+
+    if ((problem_size.m() % kAlignmentA) || (problem_size.k() % kAlignmentA) ||
+      (problem_size.n() % kAlignmentB) || (problem_size.k() % kAlignmentB) ||
+      (problem_size.m() % kAlignmentC) || (problem_size.n() % kAlignmentC)) {
 
-    if (isBMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand B");
       return Status::kErrorMisalignedOperand;
     }
 
-    if (isCMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for operand C");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    CUTLASS_TRACE_HOST("  returning kSuccess");
-
     return Status::kSuccess;
   }
 
-
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
-
-public:
-
-  //
-  // Device-only API
-  //
-
-  // Factory invocation
-  CUTLASS_DEVICE
-  static void invoke(
-    Params const &params,
-    SharedStorage &shared_storage)
-  {
-    GemmWithKReduction op;
-    op(params, shared_storage);
-  }
-
-
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
     cutlass::gemm::GemmCoord threadblock_tile_offset =
         threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
-
       return;
     }
+   
+    // Early exit if Fill Mode is Lower and
+    // if the entire tile is above the main diagonal (bottom-left corner is at or above the diagonal)
+    if (kFillModeC == cutlass::FillMode::kLower &&
+        (threadblock_tile_offset.m() + 1) * Mma1::Shape::kM <= threadblock_tile_offset.n() * Mma1::Shape::kN) {
+      return;
+    }    
+    
+    // Early exit if Fill Mode is Upper and
+    // if the entire tile is below the main diagonal (top-right corner is at or below the diagonal)
+    if (kFillModeC == cutlass::FillMode::kUpper &&
+        threadblock_tile_offset.m() * Mma1::Shape::kM >= (threadblock_tile_offset.n() + 1) * Mma1::Shape::kN) {
+      return;
+    }    
+    
+    bool tile_on_diagonal = false;
+    // Mark tiles that are being crossed by the main diagonal
+    // (top-right and bottom-left corners are on either side of the diagonal)
+    if ((threadblock_tile_offset.m() + 1) * Mma1::Shape::kM > threadblock_tile_offset.n() * Mma1::Shape::kN
+        && threadblock_tile_offset.m() * Mma1::Shape::kM < (threadblock_tile_offset.n() + 1) * Mma1::Shape::kN) {
+      tile_on_diagonal = true;
+    }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
     ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
@@ -458,111 +396,267 @@
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
         problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
-      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
-      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
-    }
 
     __syncthreads();
 
     // Compute initial location in logical coordinates
-    cutlass::MatrixCoord tb_offset_A{
-      threadblock_tile_offset.m() * Mma::Shape::kM,
+    cutlass::MatrixCoord tb_offset_MxK{
+      threadblock_tile_offset.m() * Mma1::Shape::kM,
       offset_k,
     };
 
-    cutlass::MatrixCoord tb_offset_B{
+    cutlass::MatrixCoord tb_offset_KxN{
       offset_k,
-      threadblock_tile_offset.n() * Mma::Shape::kN
+      threadblock_tile_offset.n() * Mma1::Shape::kN
     };
 
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
+    // Construct iterators to A and B operands for Mma1
+    typename Mma1::IteratorA iterator_A(
       params.params_A,
       ptr_A,
       {params.problem_size.m(), problem_size_k},
       thread_idx,
-      tb_offset_A);
+      tb_offset_MxK);
+
+    typename Mma1::IteratorB iterator_BT(
+      params.params_BT,
+      ptr_B,
+      {problem_size_k, params.problem_size.n()},
+      thread_idx,
+      tb_offset_KxN);
 
-    typename Mma::IteratorB iterator_B(
+    // Construct iterators to A and B operands for Mma2
+    typename Mma2::IteratorA iterator_B(
       params.params_B,
       ptr_B,
+      {params.problem_size.m(), problem_size_k},
+      thread_idx,
+      tb_offset_MxK);
+
+    typename Mma2::IteratorB iterator_AT(
+      params.params_AT,
+      ptr_A,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
-      tb_offset_B);
+      tb_offset_KxN);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+    // Construct thread-scoped matrix multiply for Mma1 (A x BT)
+    Mma1 mma1(shared_storage.mma1_main_loop, thread_idx, warp_idx, lane_idx);
 
-    typename Mma::FragmentC accumulators;
+    // Construct thread-scoped matrix multiply for Mma2 (B x AT)
+    Mma2 mma2(shared_storage.mma2_main_loop, thread_idx, warp_idx, lane_idx);
 
-    accumulators.clear();
+    typename Mma1::FragmentC accumulators;
 
-    typename Mma::FragmentReduction gemm_k_accumulators;
-
-    gemm_k_accumulators.clear();
+    accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
+    int gemm_k_iterations = (problem_size_k - offset_k + Mma1::Shape::kK - 1) / Mma1::Shape::kK;
 
-    // Compute threadblock-scoped matrix multiply-add
-    mma(
+    // Compute threadblock-scoped matrix multiply-add (A x BT)
+    mma1(
       gemm_k_iterations, 
       accumulators, 
       iterator_A, 
+      iterator_BT, 
+      accumulators);
+
+    // HER2K kernel needs Alpha to be complex and is conj(Alpha) is applied to the second HERK.
+    if (kBlasMode == BlasMode::kHermitian) {
+
+      //
+      // Epilogue
+      //
+
+      EpilogueOutputOp output_op(params.output_op);
+
+      //
+      // Masked tile iterators constructed from members
+      //
+
+      threadblock_tile_offset =
+          threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+
+      //assume identity swizzle
+      MatrixCoord threadblock_offset(
+        threadblock_tile_offset.m() * Mma1::Shape::kM,
+        threadblock_tile_offset.n() * Mma1::Shape::kN
+      );
+
+      int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
+
+      ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+      ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
+
+      //
+      // Fetch pointers based on mode.
+      //
+      
+      // Construct the semaphore.
+      Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+
+      if (params.mode == GemmUniversalMode::kGemm) {
+
+        // If performing a reduction via split-K, fetch the initial synchronization
+        if (params.grid_tiled_shape.k() > 1) {
+          
+          // Fetch the synchronization lock initially but do not block.
+          semaphore.fetch();
+
+          // Indicate which position in a serial reduction the output operator is currently updating
+          output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+        }
+      }
+      else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+        ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+      }
+      else if (params.mode == GemmUniversalMode::kBatched) {
+        ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
+        ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+      }
+      else if (params.mode == GemmUniversalMode::kArray) {
+        ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
+        ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
+      }
+
+      
+      // If CTA not on diagonal, FillMode doesn't apply. 
+      FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
+
+      // Tile iterator loading from source tensor.
+      typename Epilogue::OutputTileIterator iterator_C(
+        params.params_C,
+        ptr_C,
+        params.problem_size.mn(),
+        thread_idx,
+        threadblock_offset,
+        kFillModeCTA
+      );
+
+      // Tile iterator writing to destination tensor.
+      typename Epilogue::OutputTileIterator iterator_D(
+        params.params_D,
+        ptr_D,
+        params.problem_size.mn(),
+        thread_idx,
+        threadblock_offset,
+        kFillModeCTA
+      );
+
+      Epilogue epilogue(
+        shared_storage.epilogue, 
+        thread_idx, 
+        warp_idx, 
+        lane_idx);
+
+      // Wait on the semaphore - this latency may have been covered by iterator construction
+      if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
+          
+        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
+        if (threadblock_tile_offset.k()) {
+          iterator_C = iterator_D;
+        }
+
+        semaphore.wait(threadblock_tile_offset.k());
+
+        __threadfence();
+      }
+
+      // Execute the epilogue operator to update the destination tensor.
+      epilogue(
+        output_op, 
+        iterator_D, 
+        accumulators, 
+        iterator_C); 
+      
+      //
+      // Release the semaphore
+      //
+
+      if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
+
+        int lock = 0;
+        if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
+
+          // The final threadblock resets the semaphore for subsequent grids.
+          lock = 0;
+        }
+        else {
+          // Otherwise, the semaphore is incremented
+          lock = threadblock_tile_offset.k() + 1;
+        }
+        
+        semaphore.release(lock);
+      }
+
+      __syncthreads();
+
+      accumulators.clear();
+    }
+
+    // Compute threadblock-scoped matrix multiply-add (B x AT)
+    mma2(
+      gemm_k_iterations, 
+      accumulators, 
       iterator_B, 
-      accumulators,
-      gemm_k_accumulators);
+      iterator_AT, 
+      accumulators);
 
     //
     // Epilogue
     //
 
     EpilogueOutputOp output_op(params.output_op);
 
+    /* Needed for HER2K where the second HERK is multiplied by conj(alpha) */
+    typename EpilogueOutputOp::Params second_her2k_params(conj(params.output_op.alpha), 1);
+    EpilogueOutputOp output_op_her2k(second_her2k_params);
+
     //
     // Masked tile iterators constructed from members
     //
 
-    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    threadblock_tile_offset =
+        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     //assume identity swizzle
     MatrixCoord threadblock_offset(
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      threadblock_tile_offset.n() * Mma::Shape::kN
+      threadblock_tile_offset.m() * Mma1::Shape::kM,
+      threadblock_tile_offset.n() * Mma1::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
+
+    // HER2K kernel needs Alpha to be complex and is conj(Alpha) is applied to the second HERK.
+    if (kBlasMode == BlasMode::kHermitian) {
+      ptr_C = static_cast<ElementC *>(params.ptr_D);
+    }
+
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
-    ElementC *ptr_gemm_k_reduction = static_cast<ElementC *>(params.ptr_gemm_k_reduction);
 
     //
     // Fetch pointers based on mode.
     //
     
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
@@ -572,46 +666,55 @@
       // If performing a reduction via split-K, fetch the initial synchronization
       if (params.grid_tiled_shape.k() > 1) {
         
         // Fetch the synchronization lock initially but do not block.
         semaphore.fetch();
 
         // Indicate which position in a serial reduction the output operator is currently updating
-        output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+        if (kBlasMode == BlasMode::kSymmetric) {
+          output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+        } else {
+          output_op_her2k.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+        }
       }
     }
     else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
       ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-      ptr_gemm_k_reduction += threadblock_tile_offset.k() * params.batch_stride_gemm_k_reduction;
     }
     else if (params.mode == GemmUniversalMode::kBatched) {
       ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
       ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
     }
     else if (params.mode == GemmUniversalMode::kArray) {
       ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
       ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
     }
 
+    
+    // If CTA not on diagonal, FillMode doesn't apply. 
+    FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
+
     // Tile iterator loading from source tensor.
     typename Epilogue::OutputTileIterator iterator_C(
       params.params_C,
       ptr_C,
       params.problem_size.mn(),
       thread_idx,
-      threadblock_offset
+      threadblock_offset,
+      kFillModeCTA
     );
 
     // Tile iterator writing to destination tensor.
     typename Epilogue::OutputTileIterator iterator_D(
       params.params_D,
       ptr_D,
       params.problem_size.mn(),
       thread_idx,
-      threadblock_offset
+      threadblock_offset,
+      kFillModeCTA
     );
 
     Epilogue epilogue(
       shared_storage.epilogue, 
       thread_idx, 
       warp_idx, 
       lane_idx);
@@ -622,52 +725,32 @@
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
       if (threadblock_tile_offset.k()) {
         iterator_C = iterator_D;
       }
 
       semaphore.wait(threadblock_tile_offset.k());
 
+      __threadfence();
     }
 
     // Execute the epilogue operator to update the destination tensor.
-    epilogue(
-      output_op, 
-      iterator_D, 
-      accumulators, 
-      iterator_C); 
- 
-    if ((kReduceKForA && threadblock_tile_offset.n() == 0)
-     || (!kReduceKForA && threadblock_tile_offset.m() == 0)) {
-
-      int warp_idx_mn = warp_idx % (Mma::Base::WarpCount::kM * Mma::Base::WarpCount::kN);
-      int warp_idx_m = warp_idx_mn % Mma::Base::WarpCount::kM;
-      int warp_idx_n = warp_idx_mn / Mma::Base::WarpCount::kM;
- 
-     if ((kReduceKForA && warp_idx_n == 0)
-      || (!kReduceKForA && warp_idx_m == 0)) {
-
-        int reduction_warp_idx = kReduceKForA ? warp_idx_m : warp_idx_n;
-        int reduction_threadblock_offset = kReduceKForA ? threadblock_tile_offset.m() :
-                                                          threadblock_tile_offset.n();
-        int reduction_vector_size = kReduceKForA ? params.problem_size.m()
-                                                 : params.problem_size.n();
-        EpilogueGemmKReduction epilogue_gemm_k_reduction(thread_idx,
-                                                         reduction_warp_idx,
-                                                         lane_idx,
-                                                         reduction_threadblock_offset,
-                                                         ptr_gemm_k_reduction);
-        epilogue_gemm_k_reduction(
-          reduction_vector_size,
-          gemm_k_accumulators,
-          params.mode == GemmUniversalMode::kGemm
-            && (params.grid_tiled_shape.k() > 1)
-            && (threadblock_tile_offset.k() > 0));
-      }
+    if (kBlasMode == BlasMode::kSymmetric) {
+      epilogue(
+        output_op,
+        iterator_D,
+        accumulators,
+        iterator_C);
+    } else {
+      epilogue(
+        output_op_her2k,
+        iterator_D,
+        accumulators,
+        iterator_C);
     }
-   
+    
     //
     // Release the semaphore
     //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
 
       int lock = 0;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/gemv_batched_strided.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/grouped_problem_visitor.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -122,15 +122,19 @@
   problem_tile_start(0),
   problem_idx(0)
   {}
 
   /// Get the grid shape
   CUTLASS_HOST_DEVICE
   static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
-    return ProblemSizeHelper::grid_shape(problem);
+
+    return cutlass::gemm::GemmCoord(
+      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
+      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
+      1);
   }
 
   /// Gets the global tile index
   CUTLASS_HOST_DEVICE
   int32_t tile_index() const {
     return tile_idx;
   }
@@ -338,15 +342,15 @@
           int ThreadCount>
 struct GroupedProblemVisitor<ProblemSizeHelper,
                              ThreadblockShape,
                              GroupScheduleMode::kHostPrecompute,
                              PrefetchTileCount,
                              ThreadCount> : public BaseGroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape> {
   static_assert(PrefetchTileCount > 0,
-                "GroupedProblemVisitor with GroupScheduleMode `kHostPrecompute` currently requires prefetching to shared memory");
+                "GroupedProblemVisitor with GroupScheduleMode `kHost` currently requires prefetching to shared memory");
 
   using Base = BaseGroupedProblemVisitor<ProblemSizeHelper, ThreadblockShape>;
   using Params = typename Base::Params;
   using ProblemInfo = typename Base::ProblemInfo;
   static bool const kRequiresPrecomputation = true;
 
   static int const kPrefetchTileCount = PrefetchTileCount;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/params_universal_base.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -380,14 +380,21 @@
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return Status::kSuccess;
   }
 
+  static size_t get_extra_workspace_size(
+    Arguments const &args,
+    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     //
     // Problem visitor.
     //
@@ -521,15 +528,15 @@
         ptr_A,
         {problem_size_k, problem_size.n()},
         thread_idx,
         tb_offset_KxN);
 
       // Broadcast the warp_id computed by lane 0 to ensure dependent code
       // is compiled as warp-uniform.
-      int warp_idx = canonical_warp_idx();
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
       int lane_idx = threadIdx.x % 32;
 
       //
       // Main loop
       //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_grouped_problem_visitor.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -134,15 +134,15 @@
             r = ThreadblockShape::N / ThreadblockShape::M
             i = (i_macro * r) + (t % r)
             j = j_macro
         else:
             i = i_macro
             j = j_macro
 
-    Handling cases with grid dimensions that aren't multiples of each other
+    Handling cases with grid dimensions that aren't multiples of eachother
     ----------------------------------------------------------------------
     Even though threadblock shapes M and N are typically multiples of one another, the grid
     for a given problem may not have dimensions of the same ratio as that of the threadblock.
     For example, a problem of size 132x132 using a threadblock of shape 64x32 will result
     in a grid of 3x5 tiles. In this case, there is not an integer number of "true tiles"
     per "macro tile."
 
@@ -277,22 +277,14 @@
 
 // Helper for correctly representing problem sizes in grouped kernels 
 template <typename ThreadblockShape>
 struct Rank2KGroupedProblemSizeHelper {
   using OffsetHelper = Rank2KGroupedProblemVisitorOffsetHelper<ThreadblockShape>;
 
   CUTLASS_HOST_DEVICE
-  static cutlass::gemm::GemmCoord grid_shape(const cutlass::gemm::GemmCoord& problem) {
-    return cutlass::gemm::GemmCoord(
-      ((problem.m() - 1 + ThreadblockShape::kM) / ThreadblockShape::kM),
-      ((problem.n() - 1 + ThreadblockShape::kN) / ThreadblockShape::kN),
-      1);
-  }
-
-  CUTLASS_HOST_DEVICE
   static int32_t tile_count(const cutlass::gemm::GemmCoord& grid) {
     // Return the number of tiles at or below the diagonal (or at and above
     // for mode kUpper). We do this by first calculating this value assuming
     // we have a square matrix of tiles of size `dim x dim` where `dim` is the
     // minimum among {grid.m(), grid.n()}. We then multiply the resulting value
     // by OffsetHelper::kThreadblockSkewRatio to account for cases in which there
     // are more tiles in one dimension than the other.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_transpose_operands.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_2k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -27,90 +27,81 @@
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
     \brief 
-
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
+#include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
 
+#include "cutlass/layout/matrix.h"
+
+#include "cutlass/trace.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma1_,                 ///! Threadblock-scoped matrix multiply-accumulate (A*B^T)
-  typename Mma2_,                 ///! Threadblock-scoped matrix multiply-accumulate (B*A^T)
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
   typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
-  FillMode FillModeC_,            ///! Fill Mode for C (kLower or kUpper)
-  BlasMode BlasMode_              ///! Blas3 computation mode
+  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
 >
-struct Rank2KUniversal {
+struct GemmUniversalwithEpilogueVisitor {
 public:
 
-  using Mma1 = Mma1_;
-  using Mma2 = Mma2_;
+  using Mma = Mma_;
   using Epilogue = Epilogue_;
-  using EpilogueOutputOp = typename Epilogue::OutputOp;
+  using EpilogueVisitor = typename Epilogue::Visitor;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
-  using ElementA = typename Mma1::IteratorA::Element;
-  using ElementB = typename Mma1::IteratorB::Element;
-
-  // Mma1 (A x B^T)
-  using LayoutA = typename Mma1::IteratorA::Layout;
-  using LayoutBT = typename Mma1::IteratorB::Layout;
-  static ComplexTransform const kMma1TransformA = Mma1::kTransformA;
-  static ComplexTransform const kMma1TransformB = Mma1::kTransformB;
-
-  // Mma2 (B x A^T)
-  using LayoutB = typename Mma2::IteratorA::Layout;
-  using LayoutAT = typename Mma2::IteratorB::Layout;
-  static ComplexTransform const kMma2TransformA = Mma2::kTransformA;
-  static ComplexTransform const kMma2TransformB = Mma2::kTransformB;
-
-  // Common type definitions for Mma1 and Mma2
-  using Operator = typename Mma1::Operator;
-  using OperatorClass = typename Mma1::Operator::OperatorClass;
-  using ThreadblockShape = typename Mma1::Shape;
-  using WarpShape = typename Mma1::Operator::Shape;
-  using InstructionShape = typename Mma1::Policy::Operator::InstructionShape;
-  using ArchTag = typename Mma1::ArchTag;
-
-  static int const kStages = Mma1::kStages;
-  static int const kAlignmentA = Mma1::IteratorA::AccessType::kElements;
-  static int const kAlignmentB = Mma1::IteratorB::AccessType::kElements;
-
-  // Output related typedefinitions
-  using ElementC = typename Epilogue::OutputTileIterator::Element;
-  using LayoutC = typename Epilogue::OutputTileIterator::Layout;
-  static FillMode const kFillModeC = FillModeC_;
-  static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
-  static BlasMode const kBlasMode = BlasMode_;
-
+  using ElementA = typename Mma::IteratorA::Element;
+  using LayoutA = typename Mma::IteratorA::Layout;
+  using ElementB = typename Mma::IteratorB::Element;
+  using LayoutB = typename Mma::IteratorB::Layout;
+  using ElementC = typename EpilogueVisitor::ElementOutput;
+  using LayoutC = typename EpilogueVisitor::OutputTileIterator::Layout;
+
+  static ComplexTransform const kTransformA = Mma::kTransformA;
+  static ComplexTransform const kTransformB = Mma::kTransformB;
+  using Operator = typename Mma::Operator;
+
+  using OperatorClass = typename Mma::Operator::OperatorClass;
+  using ThreadblockShape = typename Mma::Shape;
+  using WarpShape = typename Mma::Operator::Shape;
+  using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
+  using ArchTag = typename Mma::ArchTag;
+
+  static int const kStages = Mma::kStages;
+  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
+  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
+  static int const kAlignmentC = EpilogueVisitor::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
-  using WarpCount = typename Mma1::WarpCount;
+  using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
+  /// Split-K preserves splits that are 128b aligned
+  static int const kSplitKAlignment = const_max(
+    128 / sizeof_bits<ElementA>::value,
+    128 / sizeof_bits<ElementB>::value
+  );
 
   //
   // Structures
   //
 
   /// Argument structure
   struct Arguments {
@@ -119,105 +110,160 @@
     // Data members
     //
 
     GemmUniversalMode mode;
     GemmCoord problem_size;
     int batch_count;
 
-    typename EpilogueOutputOp::Params epilogue;
+    typename EpilogueVisitor::Arguments epilogue_visitor;
 
     void const * ptr_A;
     void const * ptr_B;
     void const * ptr_C;
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
     int64_t batch_stride_D;
 
-    typename LayoutA::Stride::Index lda;
-    typename LayoutB::Stride::Index ldb;
-    typename LayoutC::Stride::Index ldc;
-    typename LayoutC::Stride::Index ldd;
+    typename LayoutA::Stride stride_a;
+    typename LayoutB::Stride stride_b;
+    typename LayoutC::Stride stride_c;
+    typename LayoutC::Stride stride_d;
+
+    typename LayoutA::Stride::LongIndex lda;
+    typename LayoutB::Stride::LongIndex ldb;
+    typename LayoutC::Stride::LongIndex ldc;
+    typename LayoutC::Stride::LongIndex ldd;
+
+    int const * ptr_gather_A_indices;
+    int const * ptr_gather_B_indices;
+    int const * ptr_scatter_D_indices;
 
     //
     // Methods
     //
     
     Arguments(): 
       mode(GemmUniversalMode::kGemm), 
       batch_count(1), 
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr) { }
+      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr) {}
 
     /// constructs an arguments structure
     Arguments(
       GemmUniversalMode mode,
       GemmCoord problem_size,
       int batch_count,
-      typename EpilogueOutputOp::Params epilogue,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
       void const * ptr_A,
       void const * ptr_B,
       void const * ptr_C,
       void * ptr_D,
       int64_t batch_stride_A,
       int64_t batch_stride_B,
       int64_t batch_stride_C,
       int64_t batch_stride_D,
-      typename LayoutA::Stride::Index lda,
-      typename LayoutB::Stride::Index ldb,
-      typename LayoutC::Stride::Index ldc,
-      typename LayoutC::Stride::Index ldd
+      typename LayoutA::Stride stride_a,
+      typename LayoutB::Stride stride_b,
+      typename LayoutC::Stride stride_c,
+      typename LayoutC::Stride stride_d,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
     ):
       mode(mode), 
       problem_size(problem_size), 
       batch_count(batch_count),
-      epilogue(epilogue), 
+      epilogue_visitor(epilogue_visitor), 
       ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
-      batch_stride_A(batch_stride_A), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd) {
-
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D), 
+      stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      lda = 0;
+      ldb = 0;
+      ldc = 0;
+      ldd = 0;
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
       }
 
-      /// Returns arguments for a the transposed problem
-      Arguments transposed_problem() const {
-        Arguments args(*this);
-        
-        std::swap(args.ptr_A, args.ptr_B);
-        std::swap(args.lda, args.ldb);
-        std::swap(args.batch_stride_A, args.batch_stride_B);
-
-        return args;
+    /// constructs an arguments structure
+    Arguments(
+      GemmUniversalMode mode,
+      GemmCoord problem_size,
+      int batch_count,
+      typename EpilogueVisitor::Arguments epilogue_visitor,
+      void const * ptr_A,
+      void const * ptr_B,
+      void const * ptr_C,
+      void * ptr_D,
+      int64_t batch_stride_A,
+      int64_t batch_stride_B,
+      int64_t batch_stride_C,
+      int64_t batch_stride_D,
+      typename LayoutA::Stride::LongIndex lda,
+      typename LayoutB::Stride::LongIndex ldb,
+      typename LayoutC::Stride::LongIndex ldc,
+      typename LayoutC::Stride::LongIndex ldd,
+      int const *ptr_gather_A_indices = nullptr,
+      int const *ptr_gather_B_indices = nullptr,
+      int const *ptr_scatter_D_indices = nullptr
+    ):
+      mode(mode), 
+      problem_size(problem_size), 
+      batch_count(batch_count),
+      epilogue_visitor(epilogue_visitor), 
+      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D), 
+      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C), batch_stride_D(batch_stride_D),
+      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
+      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
+      ptr_scatter_D_indices(ptr_scatter_D_indices) {
+      stride_a = make_Coord(lda);
+      stride_b = make_Coord(ldb);
+      stride_c = make_Coord(ldc);
+      stride_d = make_Coord(ldd);
+      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
       }
 
+    /// Returns arguments for the transposed problem
+    Arguments transposed_problem() const {
+      Arguments args(*this);
+      
+      std::swap(args.problem_size.m(), args.problem_size.n());
+      std::swap(args.ptr_A, args.ptr_B);
+      std::swap(args.lda, args.ldb);
+      std::swap(args.stride_a, args.stride_b);
+      std::swap(args.batch_stride_A, args.batch_stride_B);
+      std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
+
+      return args;
+    }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
   struct Params {
 
     cutlass::gemm::GemmCoord problem_size;
     cutlass::gemm::GemmCoord grid_tiled_shape;
     int swizzle_log_tile;
-    
-    // Mma1 Iterator A and B params
-    typename Mma1::IteratorA::Params params_A;
-    typename Mma1::IteratorB::Params params_BT;
-
-    // Mma2 Iterator A and B params 
-    typename Mma2::IteratorA::Params params_B;
-    typename Mma2::IteratorB::Params params_AT;
 
-    typename Epilogue::OutputTileIterator::Params params_C;
-    typename Epilogue::OutputTileIterator::Params params_D;
+    typename Mma::IteratorA::Params params_A;
+    typename Mma::IteratorB::Params params_B;
+    typename EpilogueVisitor::OutputTileIterator::Params params_C;
+    typename EpilogueVisitor::OutputTileIterator::Params params_D;
     
-    typename EpilogueOutputOp::Params output_op;
+    typename EpilogueVisitor::Params epilogue_visitor;
 
     GemmUniversalMode mode;
     int batch_count;
     int gemm_k_size;
 
     void * ptr_A;
     void * ptr_B;
@@ -225,170 +271,231 @@
     void * ptr_D;
 
     int64_t batch_stride_A;
     int64_t batch_stride_B;
     int64_t batch_stride_C;
     int64_t batch_stride_D;
 
+    int * ptr_gather_A_indices;
+    int * ptr_gather_B_indices;
+    int * ptr_scatter_D_indices;
+
     int *semaphore;
 
     //
     // Methods
     //
 
     CUTLASS_HOST_DEVICE
     Params():
       swizzle_log_tile(0),
       params_A(0),
-      params_BT(0),
       params_B(0),
-      params_AT(0),
       params_C(0),
       params_D(0),
       batch_count(0),
       gemm_k_size(0),
       mode(cutlass::gemm::GemmUniversalMode::kGemm),
       ptr_A(nullptr),
       ptr_B(nullptr),
       ptr_C(nullptr),
       ptr_D(nullptr),
       batch_stride_A(0),
       batch_stride_B(0),
       batch_stride_C(0),
       batch_stride_D(0),
+      ptr_gather_A_indices(nullptr),
+      ptr_gather_B_indices(nullptr),
+      ptr_scatter_D_indices(nullptr),
       semaphore(nullptr) { }
 
     CUTLASS_HOST_DEVICE
     Params(
       Arguments const &args,
       cutlass::gemm::GemmCoord const & grid_tiled_shape,
       int gemm_k_size,
       void *workspace = nullptr
     ):
       problem_size(args.problem_size),
       grid_tiled_shape(grid_tiled_shape),
       swizzle_log_tile(ThreadblockSwizzle().get_log_tile(grid_tiled_shape)),
-      params_A(args.lda),
-      params_BT(args.ldb),
-      params_B(args.ldb),
-      params_AT(args.lda),
-      params_C(args.ldc),
-      params_D(args.ldd),
-      output_op(args.epilogue),
+      params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
+      params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
+      params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
+      params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
+      epilogue_visitor(args.epilogue_visitor),
       mode(args.mode),
       batch_count(args.batch_count),
       gemm_k_size(gemm_k_size),
       ptr_A(const_cast<void *>(args.ptr_A)),
       ptr_B(const_cast<void *>(args.ptr_B)),
       ptr_C(const_cast<void *>(args.ptr_C)),
-      ptr_D(const_cast<void *>(args.ptr_D)),
+      ptr_D(args.ptr_D),
       batch_stride_A(args.batch_stride_A),
       batch_stride_B(args.batch_stride_B),
       batch_stride_C(args.batch_stride_C),
       batch_stride_D(args.batch_stride_D),
+      ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
+      ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
+      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)),
       semaphore(static_cast<int *>(workspace)) {
+
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
       void *workspace = nullptr) {
 
       ptr_A = const_cast<void *>(args.ptr_A);
       ptr_B = const_cast<void *>(args.ptr_B);
       ptr_C = const_cast<void *>(args.ptr_C);
       ptr_D = args.ptr_D;
 
-      output_op = args.epilogue;
+      ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
+      ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
+      ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
+
+      batch_stride_A = args.batch_stride_A;
+      batch_stride_B = args.batch_stride_B;
+      batch_stride_C = args.batch_stride_C;
+      batch_stride_D = args.batch_stride_D;
 
+      epilogue_visitor = args.epilogue_visitor;
+      
       semaphore = static_cast<int *>(workspace);
+      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
     }
-
   };
 
   /// Shared memory storage structure
   union SharedStorage {
-    typename Mma1::SharedStorage mma1_main_loop;
-    typename Mma2::SharedStorage mma2_main_loop;
+    typename Mma::SharedStorage main_loop;
     typename Epilogue::SharedStorage epilogue;
+    typename EpilogueVisitor::SharedStorage visitor;
   };
 
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  Rank2KUniversal() { } 
+  GemmUniversalwithEpilogueVisitor() { } 
 
   /// Determines whether kernel satisfies alignment
   static Status can_implement(
     cutlass::gemm::GemmCoord const & problem_size) {
 
-    static int const kAlignmentA = Mma1::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = Mma1::IteratorB::AccessType::kElements;
-    static int const kAlignmentC = Epilogue::OutputTileIterator::kElementsPerAccess;
-
-    if ((problem_size.m() % kAlignmentA) || (problem_size.k() % kAlignmentA) ||
-      (problem_size.n() % kAlignmentB) || (problem_size.k() % kAlignmentB) ||
-      (problem_size.m() % kAlignmentC) || (problem_size.n() % kAlignmentC)) {
+    CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
+
+    static int const kAlignmentA = (platform::is_same<LayoutA,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutA,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorA::AccessType::kElements;
+    static int const kAlignmentB = (platform::is_same<LayoutB,
+                                                      layout::RowMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutB,
+                                                        layout::RowMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Mma::IteratorB::AccessType::kElements;
+    static int const kAlignmentC = (platform::is_same<LayoutC,
+                                                      layout::ColumnMajorInterleaved<32>>::value)
+                                   ? 32
+                                   : (platform::is_same<LayoutC,
+                                                        layout::ColumnMajorInterleaved<64>>::value)
+                                     ? 64
+                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
+
+    bool isAMisaligned = false;
+    bool isBMisaligned = false;
+    bool isCMisaligned = false;
+
+    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
+      isAMisaligned = problem_size.m() % kAlignmentA;
+    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
+      isAMisaligned = problem_size.k() % kAlignmentA;
+    }
+
+    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
+      isBMisaligned = problem_size.n() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
+            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
+      isBMisaligned = problem_size.k() % kAlignmentB;
+    }
+
+    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
+      isCMisaligned = problem_size.m() % kAlignmentC;
+    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
+            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
+      isCMisaligned = problem_size.n() % kAlignmentC;
+    }
+
+    if (isAMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for A operand");
+      return Status::kErrorMisalignedOperand;
+    }
+
+    if (isBMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for B operand");
+      return Status::kErrorMisalignedOperand;
+    }
 
+    if (isCMisaligned) {
+      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for C operand");
       return Status::kErrorMisalignedOperand;
     }
 
+    CUTLASS_TRACE_HOST("  returning kSuccess");
+
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
     return can_implement(args.problem_size);
   }
 
+  static size_t get_extra_workspace_size(Arguments const &args,
+                                         cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
     // Compute threadblock location
     ThreadblockSwizzle threadblock_swizzle;
 
-    cutlass::gemm::GemmCoord threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    cutlass::gemm::GemmCoord threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     // Early exit if CTA is out of range
     if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
       params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
+
       return;
     }
-   
-    // Early exit if Fill Mode is Lower and
-    // if the entire tile is above the main diagonal (bottom-left corner is at or above the diagonal)
-    if (kFillModeC == cutlass::FillMode::kLower &&
-        (threadblock_tile_offset.m() + 1) * Mma1::Shape::kM <= threadblock_tile_offset.n() * Mma1::Shape::kN) {
-      return;
-    }    
-    
-    // Early exit if Fill Mode is Upper and
-    // if the entire tile is below the main diagonal (top-right corner is at or below the diagonal)
-    if (kFillModeC == cutlass::FillMode::kUpper &&
-        threadblock_tile_offset.m() * Mma1::Shape::kM >= (threadblock_tile_offset.n() + 1) * Mma1::Shape::kN) {
-      return;
-    }    
-    
-    bool tile_on_diagonal = false;
-    // Mark tiles that are being crossed by the main diagonal
-    // (top-right and bottom-left corners are on either side of the diagonal)
-    if ((threadblock_tile_offset.m() + 1) * Mma1::Shape::kM > threadblock_tile_offset.n() * Mma1::Shape::kN
-        && threadblock_tile_offset.m() * Mma1::Shape::kM < (threadblock_tile_offset.n() + 1) * Mma1::Shape::kN) {
-      tile_on_diagonal = true;
-    }
 
     int offset_k = 0;
     int problem_size_k = params.problem_size.k();
 
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A); 
+    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
     ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
 
     //
     // Fetch pointers based on mode.
     //
     if (params.mode == GemmUniversalMode::kGemm || 
       params.mode == GemmUniversalMode::kGemmSplitKParallel) {
@@ -396,360 +503,167 @@
       if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
 
         problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size; 
       }
 
       offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
     }
+    else if (params.mode == GemmUniversalMode::kBatched) {
+      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
+      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
+    }
+    else if (params.mode == GemmUniversalMode::kArray) {
+      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
+      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
+    }
 
     __syncthreads();
 
     // Compute initial location in logical coordinates
-    cutlass::MatrixCoord tb_offset_MxK{
-      threadblock_tile_offset.m() * Mma1::Shape::kM,
+    cutlass::MatrixCoord tb_offset_A{
+      threadblock_tile_offset.m() * Mma::Shape::kM,
       offset_k,
     };
 
-    cutlass::MatrixCoord tb_offset_KxN{
+    cutlass::MatrixCoord tb_offset_B{
       offset_k,
-      threadblock_tile_offset.n() * Mma1::Shape::kN
+      threadblock_tile_offset.n() * Mma::Shape::kN
     };
 
-
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
-    // Construct iterators to A and B operands for Mma1
-    typename Mma1::IteratorA iterator_A(
+    // Construct iterators to A and B operands
+    typename Mma::IteratorA iterator_A(
       params.params_A,
       ptr_A,
       {params.problem_size.m(), problem_size_k},
       thread_idx,
-      tb_offset_MxK);
-
-    typename Mma1::IteratorB iterator_BT(
-      params.params_BT,
-      ptr_B,
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_KxN);
+      tb_offset_A,
+      params.ptr_gather_A_indices);
 
-    // Construct iterators to A and B operands for Mma2
-    typename Mma2::IteratorA iterator_B(
+    typename Mma::IteratorB iterator_B(
       params.params_B,
       ptr_B,
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_MxK);
-
-    typename Mma2::IteratorB iterator_AT(
-      params.params_AT,
-      ptr_A,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
-      tb_offset_KxN);
+      tb_offset_B,
+      params.ptr_gather_B_indices);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
-    // Construct thread-scoped matrix multiply for Mma1 (A x BT)
-    Mma1 mma1(shared_storage.mma1_main_loop, thread_idx, warp_idx, lane_idx);
-
-    // Construct thread-scoped matrix multiply for Mma2 (B x AT)
-    Mma2 mma2(shared_storage.mma2_main_loop, thread_idx, warp_idx, lane_idx);
+    // Construct thread-scoped matrix multiply
+    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
 
-    typename Mma1::FragmentC accumulators;
+    typename Mma::FragmentC accumulators;
 
     accumulators.clear();
 
     // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - offset_k + Mma1::Shape::kK - 1) / Mma1::Shape::kK;
+    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-    // Compute threadblock-scoped matrix multiply-add (A x BT)
-    mma1(
+    // Compute threadblock-scoped matrix multiply-add
+    mma(
       gemm_k_iterations, 
       accumulators, 
       iterator_A, 
-      iterator_BT, 
-      accumulators);
-
-    // HER2K kernel needs Alpha to be complex and is conj(Alpha) is applied to the second HERK.
-    if (kBlasMode == BlasMode::kHermitian) {
-
-      //
-      // Epilogue
-      //
-
-      EpilogueOutputOp output_op(params.output_op);
-
-      //
-      // Masked tile iterators constructed from members
-      //
-
-      threadblock_tile_offset =
-          threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-      //assume identity swizzle
-      MatrixCoord threadblock_offset(
-        threadblock_tile_offset.m() * Mma1::Shape::kM,
-        threadblock_tile_offset.n() * Mma1::Shape::kN
-      );
-
-      int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
-
-      ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
-      ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
-
-      //
-      // Fetch pointers based on mode.
-      //
-      
-      // Construct the semaphore.
-      Semaphore semaphore(params.semaphore + block_idx, thread_idx);
-
-      if (params.mode == GemmUniversalMode::kGemm) {
-
-        // If performing a reduction via split-K, fetch the initial synchronization
-        if (params.grid_tiled_shape.k() > 1) {
-          
-          // Fetch the synchronization lock initially but do not block.
-          semaphore.fetch();
-
-          // Indicate which position in a serial reduction the output operator is currently updating
-          output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-        }
-      }
-      else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-        ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-      }
-      else if (params.mode == GemmUniversalMode::kBatched) {
-        ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
-        ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-      }
-      else if (params.mode == GemmUniversalMode::kArray) {
-        ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
-        ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
-      }
-
-      
-      // If CTA not on diagonal, FillMode doesn't apply. 
-      FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
-
-      // Tile iterator loading from source tensor.
-      typename Epilogue::OutputTileIterator iterator_C(
-        params.params_C,
-        ptr_C,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset,
-        kFillModeCTA
-      );
-
-      // Tile iterator writing to destination tensor.
-      typename Epilogue::OutputTileIterator iterator_D(
-        params.params_D,
-        ptr_D,
-        params.problem_size.mn(),
-        thread_idx,
-        threadblock_offset,
-        kFillModeCTA
-      );
-
-      Epilogue epilogue(
-        shared_storage.epilogue, 
-        thread_idx, 
-        warp_idx, 
-        lane_idx);
-
-      // Wait on the semaphore - this latency may have been covered by iterator construction
-      if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-          
-        // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-        if (threadblock_tile_offset.k()) {
-          iterator_C = iterator_D;
-        }
-
-        semaphore.wait(threadblock_tile_offset.k());
-
-        __threadfence();
-      }
-
-      // Execute the epilogue operator to update the destination tensor.
-      epilogue(
-        output_op, 
-        iterator_D, 
-        accumulators, 
-        iterator_C); 
-      
-      //
-      // Release the semaphore
-      //
-
-      if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) { 
-
-        int lock = 0;
-        if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
-
-          // The final threadblock resets the semaphore for subsequent grids.
-          lock = 0;
-        }
-        else {
-          // Otherwise, the semaphore is incremented
-          lock = threadblock_tile_offset.k() + 1;
-        }
-        
-        semaphore.release(lock);
-      }
-
-      __syncthreads();
-
-      accumulators.clear();
-    }
-
-    // Compute threadblock-scoped matrix multiply-add (B x AT)
-    mma2(
-      gemm_k_iterations, 
-      accumulators, 
       iterator_B, 
-      iterator_AT, 
       accumulators);
 
     //
     // Epilogue
     //
 
-    EpilogueOutputOp output_op(params.output_op);
-
-    /* Needed for HER2K where the second HERK is multiplied by conj(alpha) */
-    typename EpilogueOutputOp::Params second_her2k_params(conj(params.output_op.alpha), 1);
-    EpilogueOutputOp output_op_her2k(second_her2k_params);
+    // EpilogueOutputOp output_op(params.output_op);
 
     //
     // Masked tile iterators constructed from members
     //
 
-    threadblock_tile_offset =
-        threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
 
     //assume identity swizzle
     MatrixCoord threadblock_offset(
-      threadblock_tile_offset.m() * Mma1::Shape::kM,
-      threadblock_tile_offset.n() * Mma1::Shape::kN
+      threadblock_tile_offset.m() * Mma::Shape::kM,
+      threadblock_tile_offset.n() * Mma::Shape::kN
     );
 
     int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
-
-    // HER2K kernel needs Alpha to be complex and is conj(Alpha) is applied to the second HERK.
-    if (kBlasMode == BlasMode::kHermitian) {
-      ptr_C = static_cast<ElementC *>(params.ptr_D);
-    }
-
+    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C); 
     ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
 
     //
     // Fetch pointers based on mode.
     //
     
     // Construct the semaphore.
     Semaphore semaphore(params.semaphore + block_idx, thread_idx);
 
-    if (params.mode == GemmUniversalMode::kGemm) {
+    // if (params.mode == GemmUniversalMode::kGemm) {
 
-      // If performing a reduction via split-K, fetch the initial synchronization
-      if (params.grid_tiled_shape.k() > 1) {
+    //   // TODO: fix this order
+    //   // If performing a reduction via split-K, fetch the initial synchronization
+    //   if (params.grid_tiled_shape.k() > 1) {
         
-        // Fetch the synchronization lock initially but do not block.
-        semaphore.fetch();
-
-        // Indicate which position in a serial reduction the output operator is currently updating
-        if (kBlasMode == BlasMode::kSymmetric) {
-          output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-        } else {
-          output_op_her2k.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
-        }
-      }
-    }
-    else if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_C += threadblock_tile_offset.k() * params.batch_stride_C;
-      ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_C = static_cast<ElementC * const *>(params.ptr_C)[threadblock_tile_offset.k()];
-      ptr_D = static_cast<ElementC * const *>(params.ptr_D)[threadblock_tile_offset.k()];
-    }
+    //     // Fetch the synchronization lock initially but do not block.
+    //     semaphore.fetch();
 
+    //     // Indicate which position in a serial reduction the output operator is currently updating
+    //     output_op.set_k_partition(threadblock_tile_offset.k(), params.grid_tiled_shape.k());
+    //   }
+    // }
     
-    // If CTA not on diagonal, FillMode doesn't apply. 
-    FillMode kFillModeCTA = tile_on_diagonal ? kFillModeC : FillMode::kNone;
-
     // Tile iterator loading from source tensor.
-    typename Epilogue::OutputTileIterator iterator_C(
-      params.params_C,
-      ptr_C,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset,
-      kFillModeCTA
-    );
 
-    // Tile iterator writing to destination tensor.
-    typename Epilogue::OutputTileIterator iterator_D(
-      params.params_D,
-      ptr_D,
-      params.problem_size.mn(),
-      thread_idx,
-      threadblock_offset,
-      kFillModeCTA
+    EpilogueVisitor epilogue_visitor(
+        params.epilogue_visitor,
+        shared_storage.visitor,
+        threadblock_offset,
+        threadblock_tile_offset,
+        thread_idx,
+        params.problem_size.mn()
     );
 
+    // if (params.mode == GemmUniversalMode::kGemmSplitKParallel) {
+    //   ptr_D += threadblock_tile_offset.k() * params.batch_stride_D;
+    // }
+    if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
+      epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
+    }
+
     Epilogue epilogue(
-      shared_storage.epilogue, 
-      thread_idx, 
-      warp_idx, 
+      shared_storage.epilogue,
+      thread_idx,
+      warp_idx,
       lane_idx);
 
     // Wait on the semaphore - this latency may have been covered by iterator construction
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
         
       // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      if (threadblock_tile_offset.k()) {
-        iterator_C = iterator_D;
-      }
+      // TODO: ???
+      // if (threadblock_tile_offset.k()) {
+      //   iterator_C = iterator_D;
+      // }
 
       semaphore.wait(threadblock_tile_offset.k());
-
-      __threadfence();
     }
 
+
     // Execute the epilogue operator to update the destination tensor.
-    if (kBlasMode == BlasMode::kSymmetric) {
-      epilogue(
-        output_op,
-        iterator_D,
-        accumulators,
-        iterator_C);
-    } else {
-      epilogue(
-        output_op_her2k,
-        iterator_D,
-        accumulators,
-        iterator_C);
-    }
+    epilogue(epilogue_visitor, accumulators); 
     
     //
     // Release the semaphore
     //
 
     if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/rank_k_universal.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -399,15 +399,15 @@
       ptr_B,
       {problem_size_k, params.problem_size.n()},
       thread_idx,
       tb_offset_B);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/sparse_gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -273,15 +273,15 @@
         params.params_E, params.ref_E.data(),
         {params.problem_size.m(),
          problem_size_k / kSparse / kElementsPerElementE},
         thread_idx, tb_offset_E);
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
 
     // Construct thread-scoped matrix multiply
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/symm_universal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -411,15 +411,15 @@
     };
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/kernel/trmm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -376,15 +376,15 @@
     };
 
     // Compute position within threadblock
     int thread_idx = threadIdx.x;
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = canonical_warp_idx();
+    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
     int lane_idx = threadIdx.x % 32;
 
     //
     // Main loop
     //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm50.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -140,18 +140,15 @@
     // Copy accumulators
     D = C;
 
     // Compute matrix product
     CUTLASS_PRAGMA_UNROLL
     for (int k = 0; k < Shape::kK; ++k) {
       #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 860)
-      if (kMultipleOf2 &&
-        platform::is_same<ElementA, float>::value &&
-        platform::is_same<ElementB, float>::value &&
-        platform::is_same<ElementC, float>::value) {
+      if (kMultipleOf2 && platform::is_same<ElementA, float>::value && platform::is_same<ElementB, float>::value && platform::is_same<ElementC, float>::value) {
 
         //2x2 zigzag - m and n loops to increment by 2. Inner loop to process 4 multiply-adds in a 2x2 tile.
         CUTLASS_PRAGMA_UNROLL
         for (int n = 0; n < Shape::kN; n+=2) {
   
           CUTLASS_PRAGMA_UNROLL
           for (int m = 0; m < Shape::kM; m+=2) {
@@ -251,192 +248,14 @@
     }
   }
 };
 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-namespace detail {
-
-/// Matrix multiply-add operation - assumes operand B is not changing
-struct MmaComplexF32_Column {
-
-  using Shape = gemm::GemmShape<1, 1, 1>;
-  using ElementC = complex<float>;
-
-  CUTLASS_HOST_DEVICE
-  void operator()(
-    Array<complex<float>, 1> &d,
-    Array<complex<float>, 1> const &a,
-    Array<complex<float>, 1> const &b,
-    Array<complex<float>, 1> const &c
-  ) {
-
-    d[0].real() =  a[0].real() * b[0].real() + c[0].real();
-    d[0].imag() =  a[0].real() * b[0].imag() + d[0].imag();
-    d[0].real() = -a[0].imag() * b[0].imag() + d[0].real();
-    d[0].imag() =  a[0].imag() * b[0].real() + c[0].imag();
-  }
-};
-
-/// Matrix multiply-add operation - assumes operand A is not changing
-struct MmaComplexF32_Corner {
-
-  using Shape = gemm::GemmShape<1, 1, 1>;
-  using ElementC = complex<float>;
-
-  CUTLASS_HOST_DEVICE
-  void operator()(
-    Array<complex<float>, 1> &d,
-    Array<complex<float>, 1> const &a,
-    Array<complex<float>, 1> const &b,
-    Array<complex<float>, 1> const &c
-  ) {
-
-    d[0].real() = -a[0].imag() * b[0].imag() + d[0].real();
-    d[0].imag() =  a[0].real() * b[0].imag() + d[0].imag();
-    d[0].real() =  a[0].real() * b[0].real() + c[0].real();
-    d[0].imag() =  a[0].imag() * b[0].real() + c[0].imag();
-  }
-};
-
-} // namespace detail
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Gemplate that handles all packed matrix layouts
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
-  typename Shape_,
-  /// Layout of A matrix (concept: layout::MapFunc)
-  typename LayoutA_,
-  /// Layout of B matrix (concept: layout::MapFunc)
-  typename LayoutB_,
-  /// Layout of C matrix (concept: layout::MapFunc)
-  typename LayoutC_
->
-struct MmaGeneric<
-  Shape_,
-  complex<float>,
-  LayoutA_,
-  complex<float>,
-  LayoutB_,
-  complex<float>,
-  LayoutC_,
-  arch::OpMultiplyAdd> {
-
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
-  using Shape = Shape_;
-
-  /// Data type of operand A
-  using ElementA = complex<float>;
-
-  /// Layout of A matrix (concept: layout::MapFunc)
-  using LayoutA = LayoutA_;
-
-  /// Data type of operand B
-  using ElementB = complex<float>;
-
-  /// Layout of B matrix (concept: layout::MapFunc)
-  using LayoutB = LayoutB_;
-
-  /// Element type of operand C
-  using ElementC = complex<float>;
-
-  /// Layout of C matrix (concept: layout::MapFunc)
-  using LayoutC = LayoutC_;
-
-  /// Underlying mathematical operator
-  using Operator = arch::OpMultiplyAdd;
-
-  /// A operand storage
-  using FragmentA = Array<ElementA, Shape::kMK>;
-
-  /// B operand storage
-  using FragmentB = Array<ElementB, Shape::kKN>;
-
-  /// C operand storage
-  using FragmentC = Array<ElementC, Shape::kMN>;
-
-  /// Instruction
-  using MmaOp = arch::Mma<
-    gemm::GemmShape<1,1,1>,
-    1,
-    ElementA, LayoutA,
-    ElementB, LayoutB,
-    ElementC, LayoutC,
-    Operator>;
-
-  //
-  // Methods
-  //
-
-  /// Computes a matrix product D = A * B + C
-  CUTLASS_HOST_DEVICE
-  void operator()(
-    FragmentC & D,
-    FragmentA const & A,
-    FragmentB const & B,
-    FragmentC const & C) {
-
-    TensorRef<ElementA const, LayoutA> a_ref(
-      reinterpret_cast<ElementA const *>(&A), LayoutA::packed({Shape::kM, Shape::kK}));
-
-    TensorRef<ElementB const, LayoutB> b_ref(
-      reinterpret_cast<ElementB const *>(&B), LayoutB::packed({Shape::kK, Shape::kN}));
-
-    TensorRef<ElementC, LayoutC> d_ref(
-      reinterpret_cast<ElementC *>(&D), LayoutC::packed(make_Coord(Shape::kM, Shape::kN)));
-
-    detail::MmaComplexF32_Column mma_column;
-    detail::MmaComplexF32_Corner mma_corner;
-
-    // Copy accumulators
-    D = C;
-
-    // Compute matrix product
-    CUTLASS_PRAGMA_UNROLL
-    for (int k = 0; k < Shape::kK; ++k) {
-
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < Shape::kN; ++n) {
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int m = 0; m < Shape::kM; ++m) {
-
-          int m_serpentine = (n % 2) ? (Shape::kM - 1 - m) : m;
-
-          MatrixCoord mn(m_serpentine, n);
-          MatrixCoord mk(m_serpentine, k);
-          MatrixCoord kn(k, n);
-
-          Array<ElementC, 1> d;
-          Array<ElementA, 1> a;
-          Array<ElementB, 1> b;
-
-          d[0] = d_ref.at(mn);
-          a[0] = a_ref.at(mk);
-          b[0] = b_ref.at(kn);
-
-          if ((m == 0 && n) || m == Shape::kM - 1) {
-            mma_corner(d, a, b, d);
-          }
-          else {
-            mma_column(d, a, b, d);
-          }
-
-          d_ref.at(mn) = d[0];
-        }
-      }
-    }
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Gemplate that handles conventional layouts for FFMA and DFMA GEMM
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Data type of A elements
   typename ElementA_,
   /// Layout of A matrix (concept: layout::MapFunc)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm60.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/thread/mma_sm61.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_ell_mma.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_gemv_core.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_simt.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,17 +32,14 @@
 /*! \file
     \brief Defines basic properties needed by CTA-level GEMMs assuming
    expectations about data layout of the global memory fragments, data types,
    and internal tile sizes.
 
       Partial specializations for threadblock::Mma operations targeting TensorOp
    instructions.
-
-      SM80 Multi stage kernel expects stage number to be larger or equal to 3
-   to use asyncronous copy.
 */
 
 #pragma once
 
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_access_size.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/conv/threadblock/depthwise_mma_core_with_lane_access_size.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,34 +28,38 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Defines basic properties needed by CTA-level GEMMs assuming expectations about data
       layout of the global memory fragments, data types, and internal tile sizes.
 
-      Partial specializations for threadblock::Mma operations targeting simt instructions.
+      Partial specializations for threadblock::Mma operations targeting depthwise related simt instructions.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/warp/mma.h"
 #include "cutlass/gemm/threadblock/mma_pipelined.h"
 #include "cutlass/gemm/threadblock/mma_singlestage.h"
+
+#include "cutlass/gemm/threadblock/mma_base.h"
+#include "cutlass/conv/warp/mma_depthwise_simt.h"
+
 #include "cutlass/arch/cache_operation.h" 
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
+namespace conv {
 namespace threadblock {
 
 template <
     /// Shape of threadblock-scoped matrix multiply operator
     typename Shape,
     /// Shape of warp-level matrix multiply operator
     typename WarpShape,
@@ -71,16 +75,18 @@
     typename LayoutB,
     /// Data type of accumulator
     typename ElementC,
     /// Layout of accumulator
     typename LayoutC,
     /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
     typename OperatorClass,
-    /// Size of a threadblock-scoped access
-    int kAccessSizeInBits = -1, // -1 denoting the default
+    /// Size of a warp-scoped per thread access
+    int kLaneAccessSizeA_ = 0,
+    /// Size of a warp-scoped per thread access 
+    int kLaneAccessSizeB_ = 0,
     /// Number of stages
     int Stages = 2,
     /// Operation performed by MMA
     typename Operator = typename platform::conditional<
         (platform::is_same<OperatorClass,
                            cutlass::arch::OpClassTensorOp>::value) &&
             (platform::is_same<ElementA, int8_t>::value ||
@@ -100,15 +106,17 @@
         cutlass::arch::CacheOperation::Global,
     /// per-element transformation for elements of A
     ComplexTransform TransformA = ComplexTransform::kNone,
     /// per-element transformation for elements of B
     ComplexTransform TransformB = ComplexTransform::kNone,
     bool IsComplex = false // (is_complex<ElementA>::value || is_complex<ElementB>::value)
 >
-struct DefaultMmaCoreWithAccessSize;
+struct DepthwiseMmaCoreWithLaneAccessSize;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Shape of threadblock-scoped matrix multiply operator
     typename Shape,
     /// Shape of warp-level matrix multiply operator
     typename WarpShape,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -140,35 +148,32 @@
     cutlass::arch::CacheOperation::Kind CacheOpB,
     /// per-element transformation for elements of A
     ComplexTransform TransformA,
     /// per-element transformation for elements of B
     ComplexTransform TransformB,
     bool IsComplex
 >
-struct DefaultMmaCoreWithAccessSize<
+struct DepthwiseMmaCoreWithLaneAccessSize<
     Shape, WarpShape, InstructionShape,
     ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-    OperatorClass, -1, Stages, Operator, AccumulatorsInRowMajor,
+    OperatorClass, -1, -1, Stages, Operator, AccumulatorsInRowMajor,
     CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
-> : DefaultMmaCore<
+> : cutlass::gemm::threadblock::DefaultMmaCore<
     Shape, WarpShape, InstructionShape,
     ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
     OperatorClass, Stages, Operator, AccumulatorsInRowMajor,
     CacheOpA, CacheOpB, TransformA, TransformB, IsComplex
 > {};
 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization:
 ///
-///   A: column-major
-///   B: row-major
+///   A: row-major
+///   B: column-major
 ///   Operator: simt class
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -178,151 +183,155 @@
     typename ElementA_,
     /// Data type of B operand
     typename ElementB_,
     /// Data type of accumulator
     typename ElementC_,
     /// Layout of accumulator
     typename LayoutC_,
-    /// Size of a threadblock-scoped access (a value of -1 indicates the default)
-    int kAccessSizeInBits_,
+    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
+    int kLaneAccessSizeA_,
+    /// Size of a warp-scoped per thread access (a value of -1 indicates the default)
+    int kLaneAccessSizeB_,
     /// Operation performed by GEMM
     typename Operator_>
-struct DefaultMmaCoreWithAccessSize<Shape_, WarpShape_, typename std::enable_if<kAccessSizeInBits_ != -1, GemmShape<1, 1, 1>>::type, ElementA_,
-                      layout::ColumnMajor, ElementB_, layout::RowMajor,
-                      ElementC_, LayoutC_, arch::OpClassSimt, kAccessSizeInBits_, 2, Operator_
-                     > {
+struct DepthwiseMmaCoreWithLaneAccessSize<Shape_,
+                                        WarpShape_,
+                                        cutlass::gemm::GemmShape<1, 1, 1>,
+                                        ElementA_,
+                                        layout::RowMajor,
+                                        ElementB_,
+                                        layout::ColumnMajor,
+                                        ElementC_,
+                                        LayoutC_,
+                                        arch::OpClassSimt,
+                                        kLaneAccessSizeA_,
+                                        kLaneAccessSizeB_,
+                                        2,
+                                        Operator_> : public cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
+                                                                           WarpShape_,
+                                                                           cutlass::gemm::GemmShape<1, 1, 1>,
+                                                                           ElementA_,
+                                                                           layout::RowMajor,
+                                                                           ElementB_,
+                                                                           layout::ColumnMajor,
+                                                                           ElementC_,
+                                                                           LayoutC_,
+                                                                           arch::OpClassSimt,
+                                                                           2,
+                                                                           Operator_> {
+  using Base = cutlass::gemm::threadblock::DefaultMmaCore<Shape_,
+                              WarpShape_,
+                              cutlass::gemm::GemmShape<1, 1, 1>,
+                              ElementA_,
+                              layout::RowMajor,
+                              ElementB_,
+                              layout::ColumnMajor,
+                              ElementC_,
+                              LayoutC_,
+                              arch::OpClassSimt,
+                              2,
+                              Operator_>;
+
   using Shape = Shape_;
   using WarpShape = WarpShape_;
-  using InstructionShape = GemmShape<1, 1, 1>;
+  using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
   using ElementA = ElementA_;
-  using LayoutA = layout::ColumnMajor;
+  using LayoutA = layout::RowMajor;
   using ElementB = ElementB_;
-  using LayoutB = layout::RowMajor;
+  using LayoutB = layout::ColumnMajor;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
   using OperatorClass = arch::OpClassSimt;
-  static int const PartitionsK = Shape::kK / WarpShape::kK;
+
+  static int const kLaneAccessSizeA = kLaneAccessSizeA_;
+  static int const kLaneAccessSizeB = kLaneAccessSizeB_;
+
+  // Divisility requirements
+  static_assert( kLaneAccessSizeA > 0 && kLaneAccessSizeB > 0,
+    "Size of a warp-scoped per thread access should be larger then ZERO" );
 
   /// Default Operator
   using Operator = Operator_;
 
   /// Number of warps present
-  using WarpCount = GemmShape<
-    Shape::kM / WarpShape::kM,
-    Shape::kN / WarpShape::kN,
-    PartitionsK
-  >;
+  using WarpCount = typename Base::WarpCount;
 
   // Divisility requirements
   static_assert(
     !(Shape::kM % WarpShape::kM) &&
     !(Shape::kN % WarpShape::kN),
     "Threadblock-scoped GEMM should be divisible by warp-scoped GEMM size."
   );
 
   /// Number of threads per warp
-  static int const kWarpSize = warp::WarpSize<arch::OpClassSimt>::value;
-
-  /// Number of threads total
-  static int const kThreads = WarpCount::kCount * kWarpSize;
+  static int const kWarpSize = cutlass::gemm::warp::WarpSize<arch::OpClassSimt>::value;
 
-  static int const kElementsPerAccessDefault = 1;
-  static_assert(kAccessSizeInBits_ == -1 ||
-          sizeof_bits<ElementA>::value == sizeof_bits<ElementB>::value ||
-          kAccessSizeInBits_ / sizeof_bits<ElementA>::value == kElementsPerAccessDefault,
-          "Non-default value for kAccessSizeInBits_ is only allowed if size(elementA) == sizeof(elementB)");
-  static int const kElementsPerAccess = (kAccessSizeInBits_ != -1) ? kAccessSizeInBits_ / sizeof_bits<ElementA>::value : kElementsPerAccessDefault;
+  static int const kElementsPerAccess = 1;
 
   //
   // Shared memory layouts
   //
 
   using SmemLayoutA = layout::ColumnMajor;
   using SmemLayoutB = layout::RowMajor;
 
   //
-  // Iterators to write to shared memory
+  // Iterators to write to shared memory are same as base class
   //
 
-  /// ThreadMap of iterator A
-  using IteratorThreadMapA = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kM, Shape::kK>,
-    kThreads,
-    kElementsPerAccess
-  >;
-
-  /// Shared memory iterator to A operand
-  using SmemIteratorA = transform::threadblock::RegularTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>, 
-    ElementA, 
-    SmemLayoutA,
-    1,
-    IteratorThreadMapA
-  >;
-
-  /// Policy of iterator B
-  using IteratorThreadMapB = transform::PitchLinearStripminedThreadMap<
-    layout::PitchLinearShape<Shape::kN, Shape::kK>,
-    kThreads,
-    kElementsPerAccess
-  >;
-
-  /// Shared memory iterator to B operand
-  using SmemIteratorB = transform::threadblock::RegularTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>, 
-    ElementB, 
-    SmemLayoutB,
-    0,
-    IteratorThreadMapB
-  >;
-
   //
   // Warp-level matrix multiply operator
   //
 
   // Define the warp-level op
-  static const int WarpNumThreadsM = detail::simt_get_warp_threads_m<WarpShape>();
+  static const int WarpNumThreadsM = cutlass::gemm::threadblock::detail::simt_get_warp_threads_m<WarpShape>(); 
   static const int WarpNumThreadsN = kWarpSize / WarpNumThreadsM;
   static const int ThreadTileM = WarpShape::kM / WarpNumThreadsM;
   static const int ThreadTileN = WarpShape::kN / WarpNumThreadsN;
   static_assert(!(WarpShape::kM % WarpNumThreadsM) && !(WarpShape::kN % WarpNumThreadsN),
       "WarpShape must be divisible by ThreadTile shape.");
   static const int LaneLayout = ThreadTileM > 4 && ThreadTileN > 4 ? 2 : 1;
-  static const int numElementsA = 128 / sizeof_bits<ElementA>::value;
-  static const int numElementsB = 128 / sizeof_bits<ElementB>::value;
+  static const int numElementsA = kLaneAccessSizeA / sizeof_bits<ElementA>::value;
+  static const int numElementsB = kLaneAccessSizeB / sizeof_bits<ElementB>::value;
   static const int LaneM = cutlass::const_min(numElementsA, ThreadTileM);
   static const int LaneN = cutlass::const_min(numElementsB, ThreadTileN);
+
+  static int const kPaddingM = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementA>::value);
+  static int const kPaddingN = cutlass::gemm::threadblock::detail::simt_transpose_padding(kWarpSize, Shape::kK, sizeof_bits<ElementB>::value);
+
+  static_assert(!(kPaddingM % LaneM) && !(kPaddingN % LaneN),
+                "Padding must be divisible by Lane");
+
   // these should have max of thread tile also
   using LaneMmaShape = cutlass::gemm::GemmShape<
       LaneM,
       LaneN,
       1>;
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
-  using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy        /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
-    >;            /// Used for partial specialization
+  using MmaWarpSimt = cutlass::conv::warp::MmaDepthwiseSimt<
+      WarpShape,      /// Size of the Gemm problem - concept: gemm::GemmShape<>
+      ElementA,       /// Data type of A elements
+      SmemLayoutA,    /// Layout of A matrix (concept: MatrixLayout)
+      ElementB,       /// Data type of B elements
+      SmemLayoutB,    /// Layout of B matrix (concept: MatrixLayout)
+      ElementC,       /// Element type of C matrix
+      LayoutC,        /// Layout of C matrix (concept: MatrixLayout)
+      Policy          /// Policy describing warp-level MmaSimtOp (concept: MmaSimtOp policy)
+  >;
 
-  /// Policy used to define MmaPipelined
-  using MmaPolicy = MmaPolicy<
+  /// Policy used to define MmaPipelined 
+  using MmaPolicy = cutlass::gemm::threadblock::MmaPolicy<
     MmaWarpSimt,
-    MatrixShape<0, 0>,
-    MatrixShape<0, 0>,
+    MatrixShape<kPaddingM, 0>,    // skew for A matrix to avoid SMEM bank conflicts
+    MatrixShape<0, kPaddingN>,    // skew for B matrix to avoid SMEM bank conflicts
     WarpCount::kK
   >;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 } // namespace threadblock
-} // namespace gemm
+} // namespace conv
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_with_reduction.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -86,15 +86,15 @@
     typename LayoutB,
     /// Data type of accumulator
     typename ElementC,
     /// Layout of accumulator
     typename LayoutC,
     /// Indicates type of math operator (arch::OpClassSimt or arch::OpClassTensorOp)
     typename OperatorClass,
-    /// Reduce operand A or B along K dimension
+    ///                                                                                               
     bool ReduceKForA_,
     /// Number of stages
     int Stages = 2,
     /// Operation performed by MMA
     typename Operator = typename platform::conditional<
         (platform::is_same<OperatorClass,
                            cutlass::arch::OpClassTensorOp>::value) &&
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_core_wmma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_layernorm_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_planar_complex_pipelined.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_softmax_mainloop_fusion.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_mma_with_reduction.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_mma_complex_core_sm80.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -596,15 +596,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: column-major
 ///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+///   Math Instruction: MMA.1688.F32.TF32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -726,15 +726,15 @@
 
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: column-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+///   Math Instruction: MMA.1688.F32.TF32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -857,15 +857,15 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: row-major
 ///   B: column-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+///   Math Instruction: MMA.1688.F32.TF32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -988,15 +988,15 @@
 ////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for complex floating-point
 ///
 ///   A: row-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex
-///   Math Instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+///   Math Instruction: MMA.1688.F32.TF32
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
     /// Shape of warp-level matrix multiply operator (concept: GemmShape)
@@ -1114,18 +1114,18 @@
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<MmaTensorOp, MatrixShape<0, 0>,
                                         MatrixShape<0, 0>, WarpCount::kK>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex SIMT operation
+/// Partial specialization for complex double-precision
 ///
 ///   A: column-major
-///   B: column-major
+///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -1263,36 +1263,33 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    1,            /// 1 partition along K dimension
-    kTransformA,  /// Transform for A
-    kTransformB   /// Transform for B
-    >;            /// Used for partial specialization
+    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,  /// Data type of A elements
+    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,  /// Data type of B elements
+    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,  /// Element type of C matrix
+    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
+    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    >;         /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<0, 0>,
     MatrixShape<0, Shape::kK / 32>,
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex SIMT operation
+/// Partial specialization for complex double-precision
 ///
 ///   A: column-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
@@ -1430,39 +1427,36 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    1,            /// 1 partition along K dimension
-    kTransformA,  /// Transform for A
-    kTransformB   /// Transform for B
-    >;            /// Used for partial specialization
+    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,  /// Data type of A elements
+    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,  /// Data type of B elements
+    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,  /// Element type of C matrix
+    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
+    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    >;         /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<0, 0>,
     MatrixShape<0, 0>,    // or Shape::kK / 32
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex SIMT operation
+/// Partial specialization for complex double-precision
 ///
-///   A: row-major
-///   B: column-major
+///   A: column-major
+///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
     typename Shape_,
@@ -1603,38 +1597,35 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    1,            /// 1 partition along K dimension
-    kTransformA,  /// Transform for A
-    kTransformB   /// Transform for B
-    >;            /// Used for partial specialization
+    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,  /// Data type of A elements
+    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,  /// Data type of B elements
+    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,  /// Element type of C matrix
+    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
+    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    >;         /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<Shape::kK / 32, 0>,
     MatrixShape<0, Shape::kK / 32>,
     WarpCount::kK>;
 };
 
-/// Partial specialization for complex SIMT operation
+/// Partial specialization for complex double-precision
 ///
-///   A: row-major
+///   A: column-major
 ///   B: row-major
 ///   Operator: arch::OpMultiplyAddComplex or arch::OpMultiplyGaussianComplex
 ///
 /// This uses the default warp-level operator given tile sizes
 template <
     /// Shape of threadblock-scoped matrix multiply operator (concept:
     /// GemmShape)
@@ -1773,26 +1764,23 @@
   using Policy = cutlass::gemm::warp::MmaSimtPolicy<
       cutlass::MatrixShape<WarpNumThreadsM, WarpNumThreadsN>,   // WarpShape
       cutlass::layout::RowMajorInterleaved<LaneLayout>,         // LaneLayout
       LaneMmaShape
   >;
 
   using MmaWarpSimt = cutlass::gemm::warp::MmaSimt<
-    WarpShape,    /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
-    ElementA,     /// Data type of A elements
-    SmemLayoutA,  /// Layout of A matrix (concept: MatrixLayout)
-    ElementB,     /// Data type of B elements
-    SmemLayoutB,  /// Layout of B matrix (concept: MatrixLayout)
-    ElementC,     /// Element type of C matrix
-    LayoutC,      /// Layout of C matrix (concept: MatrixLayout)
-    Policy,       /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-    1,            /// 1 partition along K dimension
-    kTransformA,  /// Transform for A
-    kTransformB   /// Transform for B
-    >;            /// Used for partial specialization
+    WarpShape, /// Size of the Gemm problem - concept: gemm::GemmShape<> 128, 128, 8
+    ElementA,  /// Data type of A elements
+    SmemLayoutA,   /// Layout of A matrix (concept: MatrixLayout)
+    ElementB,  /// Data type of B elements
+    SmemLayoutB,   /// Layout of B matrix (concept: MatrixLayout)
+    ElementC,  /// Element type of C matrix
+    LayoutC,   /// Layout of C matrix (concept: MatrixLayout)
+    Policy     /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+    >;         /// Used for partial specialization
 
   /// Policy used to define MmaPipelined
   using MmaPolicy = MmaPolicy<
     MmaWarpSimt,
     MatrixShape<Shape::kK / 32, 0>,
     MatrixShape<0, 0>,    // or Shape::kK / 32
     WarpCount::kK>;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_multistage_trmm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,47 +24,60 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
+// 
 /*! \file
-    \brief Template for a multistage GEMM kernel. Does not compute batching or support split-K.
-
-  
+    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
 */
 
 #pragma once
 
 #include "cutlass/blas3.h"
 #include "cutlass/arch/arch.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-#include "cutlass/numeric_types.h"
+#include "cutlass/arch/wmma.h"
+
+#include "cutlass/layout/matrix.h"
 #include "cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h"
 #include "cutlass/gemm/threadblock/mma_blas3_multistage.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
+#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
+#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
+#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
+
+#if defined(CUTLASS_ARCH_WMMA_ENABLED)
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
+#endif //CUTLASS_ARCH_WMMA_ENABLED
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Element type for A matrix operand
     typename ElementA_,
     /// Layout type for A matrix operand
     typename LayoutA_,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB_,
     /// Layout type for B matrix operand
     typename LayoutB_,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Side Mode for the kernel
     SideMode kSideMode,
     /// Fill Mode for the triangular matrix
     FillMode kFillMode,
     /// Diag Type for the triangular matrix
     DiagType kDiagType,
     /// Element type for internal accumulation
@@ -79,478 +92,354 @@
     typename ThreadblockShape_,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape_,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape_,
     /// Number of stages used in the pipelined mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB = ComplexTransform::kNone,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator = arch::OpMultiplyAddComplex,
-    /// Blas3 computation mode
-    BlasMode BlasMode_ = BlasMode::kTriangular,
+    /// Operation perfomed by GEMM
+    typename Operator,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false>
-struct DefaultMultistageTrmmComplex;
+    bool AccumulatorsInRowMajor = false
+    >
+struct DefaultTrmm;
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output
+/// Specialization for row-major output (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Side Mode for the kernel
     SideMode kSideMode,
     /// Fill Mode for the triangular matrix
     FillMode kFillMode,
     /// Diag Type for the triangular matrix
     DiagType kDiagType,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            kSideMode, kFillMode, kDiagType,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
+                  kSideMode, kFillMode, kDiagType, 
+                  ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          kSideMode, kFillMode, kDiagType, 
-          AccessTypeA>;
+          ElementA, LayoutA, 1, ThreadMapA, kSideMode, kFillMode, kDiagType, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          kSideMode, FillMode::kFull, DiagType::kInvalid,
-          AccessTypeB>;
-
+          ElementB, LayoutB, 0, ThreadMapB, kSideMode, FillMode::kFull, DiagType::kInvalid, AccessTypeB>;
+  
   // Define the threadblock-scoped multistage matrix multiply
   using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
       typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output and right-side mode
+/// Specialization for row-major output, right side mode (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Fill Mode for the triangular matrix
     FillMode kFillMode,
     /// Diag Type for the triangular matrix
     DiagType kDiagType,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            SideMode::kRight, kFillMode, kDiagType,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
+                  SideMode::kRight, kFillMode, kDiagType, 
+                  ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
-          AccessTypeA>;
+          ElementA, LayoutA, 1, ThreadMapA, SideMode::kRight, FillMode::kFull, DiagType::kInvalid, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          SideMode::kRight, kFillMode, kDiagType,
-          AccessTypeB>;
+          ElementB, LayoutB, 0, ThreadMapB, SideMode::kRight, kFillMode, kDiagType, AccessTypeB>;
 
   // Define the threadblock-scoped multistage matrix multiply
   using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
       typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output with unit diagonal
+/// Specialization for row-major output with unit diagonal (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Side Mode for the kernel
     SideMode kSideMode,
     /// Fill Mode for the triangular matrix
     FillMode kFillMode,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            kSideMode, kFillMode, DiagType::kUnit,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          kSideMode, kFillMode, DiagType::kUnit, 
-          AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          kSideMode, FillMode::kFull, DiagType::kInvalid,
-          AccessTypeB>;
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
+                  kSideMode, kFillMode, DiagType::kUnit, 
+                  ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
 
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization for row-major output and right-side mode, unit diagonal
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            SideMode::kRight, kFillMode, DiagType::kUnit,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator> {
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
+
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
-          AccessTypeA>;
+          ElementA, LayoutA, 1, ThreadMapA, kSideMode, kFillMode, DiagType::kUnit, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          SideMode::kRight, kFillMode, DiagType::kUnit,
-          AccessTypeB>;
-
+          ElementB, LayoutB, 0, ThreadMapB, kSideMode, FillMode::kFull, DiagType::kInvalid, AccessTypeB>;
+  
   // Define the threadblock-scoped multistage matrix multiply
   using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
       typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
-
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output (for TRMM where diagonal imag part is ignored - used by HEMM)
+/// Specialization for row-major output, right side mode, unit diagonal (OperatorClass TensorOp)
 template <
     /// Element type for A matrix operand
     typename ElementA,
     /// Layout type for A matrix operand
     typename LayoutA,
+    /// Access granularity of A matrix in units of elements
+    int kAlignmentA,
     /// Element type for B matrix operand
     typename ElementB,
     /// Layout type for B matrix operand
     typename LayoutB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
+    /// Access granularity of B matrix in units of elements
+    int kAlignmentB,
     /// Fill Mode for the triangular matrix
     FillMode kFillMode,
     /// Element type for internal accumulation
     typename ElementAccumulator,
     /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
     typename ArchTag,
     /// Threadblock-level tile size (concept: GemmShape)
     typename ThreadblockShape,
     /// Warp-level tile size (concept: GemmShape)
     typename WarpShape,
     /// Instruction-level tile size (concept: GemmShape)
     typename InstructionShape,
     /// Number of stages used in the multistage mainloop
     int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            kSideMode, kFillMode, DiagType::kNonUnit,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator, BlasMode::kHermitian> {
+    /// Operation perfomed by GEMM
+    typename Operator
+    >
+struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
+                  SideMode::kRight, kFillMode, DiagType::kUnit, 
+                  ElementAccumulator, layout::RowMajor,
+                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
+                  InstructionShape, Stages, Operator, false> {
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpA =
+      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
+
+  static cutlass::arch::CacheOperation::Kind const CacheOpB =
+      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
+          ? cutlass::arch::CacheOperation::Global
+          : cutlass::arch::CacheOperation::Always;
 
   // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
+      Stages, Operator, false, CacheOpA, CacheOpB>;
 
   // Define iterators over tiles from the A operand
-  // PredicatedTileAccessIteratorTriangularMatrix only tracks diagonal elements,
-  // when DiagType is kUnit
   using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          kSideMode, kFillMode, DiagType::kUnit, 
-          AccessTypeA>;
+  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
 
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          kSideMode, FillMode::kFull, DiagType::kInvalid,
-          AccessTypeB>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill,
-      BlasMode::kHermitian>;
-};
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization for row-major output and right-side mode (for TRMM where diagonal imag part is ignored - used by HEMM)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename OperatorClass,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Complex transformation on operand A
-    ComplexTransform TransformA,
-    /// Complex transformation on operand B
-    ComplexTransform TransformB,
-    /// Multiply-add operator (arch::OpMultiplyAddComplex, arch::OpMultiplyGaussianComplex)
-    typename Operator>
-struct DefaultMultistageTrmmComplex<ElementA, LayoutA, ElementB, LayoutB,
-                            SideMode::kRight, kFillMode, DiagType::kNonUnit,
-                            ElementAccumulator, layout::RowMajor, OperatorClass, ArchTag, ThreadblockShape, WarpShape,
-                            InstructionShape, Stages, TransformA, TransformB, Operator, BlasMode::kHermitian> {
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMultistageMmaComplexCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA, 
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, OperatorClass,
-      Stages, TransformA, TransformB, Operator>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, ThreadMapA::kElementsPerAccess>;
   using IteratorA =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, 
-          SideMode::kRight, FillMode::kFull, DiagType::kInvalid, 
-          AccessTypeA>;
+          ElementA, LayoutA, 1, ThreadMapA, SideMode::kRight, FillMode::kFull, DiagType::kInvalid, AccessTypeA>;
 
   // Define iterators over tiles from the B operand
-  // PredicatedTileAccessIteratorTriangularMatrix only tracks diagonal elements,
-  // when DiagType is kUnit
   using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, ThreadMapB::kElementsPerAccess>;
+  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
+
   using IteratorB =
       cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
           cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, 
-          SideMode::kRight, kFillMode, DiagType::kUnit,
-          AccessTypeB>;
+          ElementB, LayoutB, 0, ThreadMapB, SideMode::kRight, kFillMode, DiagType::kUnit, AccessTypeB>;
 
   // Define the threadblock-scoped multistage matrix multiply
   using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
       typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
       MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
       MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill,
-      BlasMode::kHermitian>;
+      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace gemm
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace gemm
+} // namespace cutlass 
 
 ////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_sparse_mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/default_trmm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,422 +24,640 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-// 
+
 /*! \file
-    \brief Template for a pipelined GEMM kernel. Does not compute batching or support split-K.
+    \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
+      Tensor Cores.
 */
 
 #pragma once
 
-#include "cutlass/blas3.h"
-#include "cutlass/arch/arch.h"
-#include "cutlass/arch/wmma.h"
-
-#include "cutlass/layout/matrix.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h"
-#include "cutlass/gemm/threadblock/mma_blas3_multistage.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator.h"
-#include "cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h"
-#include "cutlass/gemm/threadblock/default_mma_core_simt.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm70.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm75.h"
-#include "cutlass/gemm/threadblock/default_mma_core_sm80.h"
-
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
-#endif //CUTLASS_ARCH_WMMA_ENABLED
+#include "cutlass/cutlass.h"
+
+#include "cutlass/array.h"
+#include "cutlass/complex.h"
+#include "cutlass/numeric_types.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/functional.h"
+
+#include "cutlass/arch/memory_sm75.h"
+#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm80.h"
+
+#include "cutlass/gemm/gemm.h"
+#include "cutlass/gemm/warp/mma.h"
 
-////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/gemm/warp/mma_tensor_op_policy.h"
+#include "cutlass/gemm/warp/mma_tensor_op.h"
+
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
+#include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
-namespace threadblock {
+namespace warp {
 
-////////////////////////////////////////////////////////////////////////////////
+namespace detail {
 
 template <
-    /// Element type for A matrix operand
-    typename ElementA_,
-    /// Layout type for A matrix operand
-    typename LayoutA_,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB_,
-    /// Layout type for B matrix operand
-    typename LayoutB_,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for internal accumulation
-    typename ElementAccumulator_,
-    /// Layout type for C and D matrix operands
-    typename LayoutC_,
-    /// Operator class tag
-    typename OperatorClass_,
-    /// Tag indicating architecture to tune for
-    typename ArchTag_,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape_,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape_,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape_,
-    /// Number of stages used in the pipelined mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator,
-    /// Store the accumulators in row major or column major.  Row major is used
-    /// when output layout is interleaved.
-    bool AccumulatorsInRowMajor = false
-    >
-struct DefaultTrmm;
-
-////////////////////////////////////////////////////////////////////////////////
+  /// Data type of real & imag members of complex numbers in the SourceFragment
+  typename RealElement,
+  /// Destination fragment required by the mma operation 
+  typename DestinationFragment,
+  /// Source fragment holding complex<RealElement> elements
+  typename SourceFragment,
+  /// Number of mma operations performed
+  typename MmaIterations,
+  /// Shape of operand elements
+  typename MmaOperandShape,
+  /// Complex transform on A operand
+  ComplexTransform Transform_,
+  /// Operand A or Operand B
+  Operand Operand_,
+  /// Floating-point rounding style for big part
+  FloatRoundStyle RoundBig_,
+  /// Floating-point rounding style for small part
+  FloatRoundStyle RoundSmall_>
+struct UnpackComplexConvertAndPackForMmaFastF32;
 
-/// Specialization for row-major output (OperatorClass TensorOp)
+// Partial specialization for OperandA and Congruous smem layout
 template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
-                  kSideMode, kFillMode, kDiagType, 
-                  ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, kSideMode, kFillMode, kDiagType, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, kSideMode, FillMode::kFull, DiagType::kInvalid, AccessTypeB>;
+  typename RealElement,
+  typename DestinationFragment, 
+  typename SourceFragment,
+  typename MmaIterations,
+  typename MmaOperandShape,
+  ComplexTransform Transform_,
+  FloatRoundStyle RoundBig_,
+  FloatRoundStyle RoundSmall_>
+struct UnpackComplexConvertAndPackForMmaFastF32 <
+  RealElement,
+  DestinationFragment,
+  SourceFragment,
+  MmaIterations,
+  MmaOperandShape,
+  Transform_,
+  Operand::kA,
+  RoundBig_,
+  RoundSmall_> {
   
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
+  //
+  // Type definitions
+  //
+  static Operand const kOperand = Operand::kA;
+  static ComplexTransform const kTransform = Transform_;
+  static FloatRoundStyle const kRoundBig = RoundBig_;
+  static FloatRoundStyle const kRoundSmall = RoundSmall_;
+
+  // Data type of elements in the destination fragment
+  using MmaElement = typename DestinationFragment::Element;
+
+  // Numeric convertor MmaElementBig, MmaElementSmall <= RealElement
+  using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
+
+  // Operand layout parameters
+  using SourceFragmentLayout = layout::ColumnMajor;
+  static int const kLdm = MmaIterations::kRow * MmaOperandShape::kRow;
+
+  // BigSmall Fragment holding two TF32 elements (big, small) for every float
+  using BigSmallFragment = Array<MmaElement, 2>;
+
+  /// Index in fargments for the big and small part
+  static int const kBigIndex = 0;
+  static int const kSmallIndex = 1;
+
+  /// Ctor
+  CUTLASS_DEVICE
+  UnpackComplexConvertAndPackForMmaFastF32() {}
+
+  CUTLASS_DEVICE
+  void operator()(DestinationFragment *dest, SourceFragment const &source) {
+    
+    Converter convert_op;
+    SourceFragmentLayout layout(kLdm);
+
+    DestinationFragment *dest_big_ = reinterpret_cast<DestinationFragment*>(dest);
+    DestinationFragment *dest_small_ = reinterpret_cast<DestinationFragment*>(&dest[MmaIterations::kRow * 2]);
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int i=0; i<MmaIterations::kRow; i++) {
+      int pos = 0;
+      CUTLASS_PRAGMA_UNROLL
+      for(int c=0; c<MmaOperandShape::kColumn; c++) {
+        CUTLASS_PRAGMA_UNROLL
+        for(int r=0; r<MmaOperandShape::kRow; r++) {
+          // Logical position of element in source fragment
+          int row = r + i * MmaOperandShape::kRow;
+          int col = c;
+
+          // Access complex<RealElement> and apply rounding on real and imag parts
+          BigSmallFragment a = convert_op(source[layout(MatrixCoord{row,col})].real());
+          BigSmallFragment b = convert_op(source[layout(MatrixCoord{row,col})].imag());
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest_big_[i][pos] = a[kBigIndex];
+          dest_big_[i+MmaIterations::kRow][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kBigIndex] : b[kBigIndex]);
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest_small_[i][pos] = a[kSmallIndex];
+          dest_small_[i+MmaIterations::kRow][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kSmallIndex] : b[kSmallIndex]);
+
+          // Next position
+          pos++;
+        }
+      }
+    }
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Specialization for row-major output, right side mode (OperatorClass TensorOp)
+// Partial specialization for OperandB and Congruous smem layout
 template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Diag Type for the triangular matrix
-    DiagType kDiagType,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
-                  SideMode::kRight, kFillMode, kDiagType, 
-                  ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, SideMode::kRight, FillMode::kFull, DiagType::kInvalid, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, SideMode::kRight, kFillMode, kDiagType, AccessTypeB>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaMultistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
+  typename RealElement,
+  typename DestinationFragment, 
+  typename SourceFragment,
+  typename MmaIterations,
+  typename MmaOperandShape,
+  ComplexTransform Transform_,
+  FloatRoundStyle RoundBig_,
+  FloatRoundStyle RoundSmall_>
+struct UnpackComplexConvertAndPackForMmaFastF32 <
+  RealElement,
+  DestinationFragment,
+  SourceFragment,
+  MmaIterations,
+  MmaOperandShape,
+  Transform_,
+  Operand::kB,
+  RoundBig_,
+  RoundSmall_> {
+  
+  //
+  // Type definitions
+  //
+  static Operand const kOperand = Operand::kB;
+  static ComplexTransform const kTransform = Transform_;
+  static FloatRoundStyle const kRoundBig = RoundBig_;
+  static FloatRoundStyle const kRoundSmall = RoundSmall_;
+
+  // Data type of elements in the destination fragment
+  using MmaElement = typename DestinationFragment::Element;
+
+  // Numeric convertor MmaElementBig, MmaElementSmall <= RealElement
+  using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
+
+  // Operand layout parameters
+  using SourceFragmentLayout = layout::RowMajor;
+  static int const kLdm = MmaIterations::kColumn * MmaOperandShape::kColumn;
+
+  // BigSmall Fragment holding two TF32 elements (big, small) for every float
+  using BigSmallFragment = Array<MmaElement, 2>;
+
+  /// Index in fargments for the big and small part
+  static int const kBigIndex = 0;
+  static int const kSmallIndex = 1;
+
+  /// Ctor
+  CUTLASS_DEVICE
+  UnpackComplexConvertAndPackForMmaFastF32() {}
+
+  CUTLASS_HOST_DEVICE
+  void operator()(DestinationFragment *dest, SourceFragment const &source) {
+    
+    Converter convert_op;
+    SourceFragmentLayout layout(kLdm);
+
+    DestinationFragment *dest_big_ = reinterpret_cast<DestinationFragment*>(dest);
+    DestinationFragment *dest_small_ = reinterpret_cast<DestinationFragment*>(&dest[MmaIterations::kColumn * 2]);
+
+    CUTLASS_PRAGMA_UNROLL
+    for(int i=0; i<MmaIterations::kColumn; i++) {
+      int pos = 0;
+      CUTLASS_PRAGMA_UNROLL
+      for(int c=0; c<MmaOperandShape::kColumn; c++) {
+        CUTLASS_PRAGMA_UNROLL
+        for(int r=0; r<MmaOperandShape::kRow; r++) {
+          // Logical position of element in source fragment
+          int row = r;
+          int col = c + i * MmaOperandShape::kColumn;
+
+          // Access complex<RealElement> apply rounding on real and imag parts
+          BigSmallFragment a = convert_op(source[layout(MatrixCoord{row,col})].real());
+          BigSmallFragment b = convert_op(source[layout(MatrixCoord{row,col})].imag());
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest_big_[i][pos] = a[kBigIndex];
+          dest_big_[i+MmaIterations::kColumn][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kBigIndex] : b[kBigIndex]);
+
+          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
+          dest_small_[i][pos] = a[kSmallIndex];
+          dest_small_[i+MmaIterations::kColumn][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kSmallIndex] : b[kSmallIndex]);
+
+          // next position
+          pos++;       
+        }
+      }
+    }
+  }
 };
+} // namespace detail 
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization for row-major output with unit diagonal (OperatorClass TensorOp)
 template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Side Mode for the kernel
-    SideMode kSideMode,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
-                  kSideMode, kFillMode, DiagType::kUnit, 
-                  ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, kSideMode, kFillMode, DiagType::kUnit, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, kSideMode, FillMode::kFull, DiagType::kInvalid, AccessTypeB>;
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Data type of A elements
+  typename RealElementA,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Data type of B elements
+  typename RealElementB,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Element type of C matrix
+  typename RealElementC,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Complex transform on A operand
+  ComplexTransform TransformA = ComplexTransform::kNone,
+  /// Complex transform on B operand
+  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Used for partial specialization
+  typename Enable = bool
+>
+class MmaComplexTensorOpFastF32;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Partial specialization for complex*complex+complex => complex:
+//  Operands data type: complex<float>
+//  Rounding: float -> tfloat32_t (round half_ulp_truncate nearest)
+//  Math instruction: MMA.1688.F32.TF32
+//  Output data type: complex<float>
+// 
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+  /// Size of the Gemm problem - concept: gemm::GemmShape<>
+  typename Shape_,
+  /// Layout of A matrix (concept: MatrixLayout)
+  typename LayoutA_,
+  /// Layout of B matrix (concept: MatrixLayout)
+  typename LayoutB_,
+  /// Layout of C matrix (concept: MatrixLayout)
+  typename LayoutC_,
+  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
+  typename Policy_,
+  /// Complex transform on A operand
+  ComplexTransform TransformA,
+  /// Complex transform on B operand
+  ComplexTransform TransformB,
+  /// Used for partial specialization
+  typename Enable
+>
+class MmaComplexTensorOpFastF32<
+  Shape_, 
+  complex<float>, 
+  LayoutA_, 
+  complex<float>,
+  LayoutB_,
+  complex<float>,
+  LayoutC_,
+  Policy_,
+  TransformA,
+  TransformB,
+  Enable>  {
+public:
+  /// Shape of warp-level matrix operation (concept: GemmShape)
+  using Shape = Shape_;
+
+  /// Data type of members of complex multiplicand A
+  using RealElementA = float;
+
+  /// Data type of multiplicand A
+  using ElementA = complex<RealElementA>;
+
+  /// Layout of multiplicand A
+  using LayoutA = LayoutA_;
+
+  /// Data type of members of complex multiplicand B
+  using RealElementB = float;
+
+  /// Data type of multiplicand B
+  using ElementB = complex<RealElementB>;
+
+  /// Layout of multiplicand B
+  using LayoutB = LayoutB_;
+
+  /// Data type of members of complex accumulator matrix C
+  using RealElementC = float;
+
+  /// Data type of accumulator matrix C
+  using ElementC = complex<RealElementC>;
+
+  /// Layout of accumulator matrix C
+  using LayoutC = LayoutC_;
+
+  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
+  using Policy = Policy_;
+
+  /// Underlying matrix multiply operator (concept: arch::Mma)
+  using ArchMmaOperator = typename Policy::Operator;
+
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
+  /// Underlying arch tag
+  using ArchTag = typename ArchMmaOperator::ArchTag;
+
+  /// Indicates class of matrix operator
+  using OperatorClass = arch::OpClassTensorOp;
+
+  /// Indicates math operator 
+  using MathOperator = arch::OpMultiplyAddComplexFastF32;
   
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
-};
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = TransformA;
 
-////////////////////////////////////////////////////////////////////////////////
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = TransformB;
 
-/// Specialization for row-major output, right side mode, unit diagonal (OperatorClass TensorOp)
-template <
-    /// Element type for A matrix operand
-    typename ElementA,
-    /// Layout type for A matrix operand
-    typename LayoutA,
-    /// Access granularity of A matrix in units of elements
-    int kAlignmentA,
-    /// Element type for B matrix operand
-    typename ElementB,
-    /// Layout type for B matrix operand
-    typename LayoutB,
-    /// Access granularity of B matrix in units of elements
-    int kAlignmentB,
-    /// Fill Mode for the triangular matrix
-    FillMode kFillMode,
-    /// Element type for internal accumulation
-    typename ElementAccumulator,
-    /// Tag indicating architecture to tune for
-    typename ArchTag,
-    /// Threadblock-level tile size (concept: GemmShape)
-    typename ThreadblockShape,
-    /// Warp-level tile size (concept: GemmShape)
-    typename WarpShape,
-    /// Instruction-level tile size (concept: GemmShape)
-    typename InstructionShape,
-    /// Number of stages used in the multistage mainloop
-    int Stages,
-    /// Operation perfomed by GEMM
-    typename Operator
-    >
-struct DefaultTrmm<ElementA, LayoutA, kAlignmentA, ElementB, LayoutB, kAlignmentB, 
-                  SideMode::kRight, kFillMode, DiagType::kUnit, 
-                  ElementAccumulator, layout::RowMajor,
-                  arch::OpClassTensorOp, ArchTag, ThreadblockShape, WarpShape,
-                  InstructionShape, Stages, Operator, false> {
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpA =
-      ((sizeof_bits<ElementA>::value * kAlignmentA) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  static cutlass::arch::CacheOperation::Kind const CacheOpB =
-      ((sizeof_bits<ElementB>::value * kAlignmentB) == 128)
-          ? cutlass::arch::CacheOperation::Global
-          : cutlass::arch::CacheOperation::Always;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementAccumulator, layout::RowMajor, arch::OpClassTensorOp,
-      Stages, Operator, false, CacheOpA, CacheOpB>;
-
-  // Define iterators over tiles from the A operand
-  using ThreadMapA = typename MmaCore::IteratorThreadMapA;
-  using AccessTypeA = cutlass::Array<ElementA, kAlignmentA>;
-
-  using IteratorA =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kM, ThreadblockShape::kK>,
-          ElementA, LayoutA, 1, ThreadMapA, SideMode::kRight, FillMode::kFull, DiagType::kInvalid, AccessTypeA>;
-
-  // Define iterators over tiles from the B operand
-  using ThreadMapB = typename MmaCore::IteratorThreadMapB;
-  using AccessTypeB = cutlass::Array<ElementB, kAlignmentB>;
-
-  using IteratorB =
-      cutlass::transform::threadblock::PredicatedTileAccessIteratorTriangularMatrix<
-          cutlass::MatrixShape<ThreadblockShape::kK, ThreadblockShape::kN>,
-          ElementB, LayoutB, 0, ThreadMapB, SideMode::kRight, kFillMode, DiagType::kUnit, AccessTypeB>;
-
-  // Define the threadblock-scoped multistage matrix multiply
-  using ThreadblockMma = cutlass::gemm::threadblock::MmaBlas3Multistage<
-      typename MmaCore::Shape, IteratorA, typename MmaCore::SmemIteratorA,
-      MmaCore::kCacheOpA, IteratorB, typename MmaCore::SmemIteratorB,
-      MmaCore::kCacheOpB, ElementAccumulator, layout::RowMajor,
-      typename MmaCore::MmaPolicy, Stages, SharedMemoryClearOption::kZfill>;
+  /// Number of threads participating in warp-level matrix product
+  static int const kThreadCount = 32;
+
+
+  /// Tune F32 to TF32 big small conversion for complex<float> operation
+  /// Different combination of big small conversin can cause different tradeoff
+  /// between speed and accuracy.  Generally, use round_half_ulp_truncate can
+  /// improve the performance but hur the accuracy.
+  using ComplexFastF32 = FastF32 <
+    FloatRoundStyle::round_toward_zero,        // kRoundBigA
+    FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallA
+    FloatRoundStyle::round_toward_zero,        // kRoundBigB
+    FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallB
+    TensorFloat32Op::k3xTF32                   // Number of TF32 operations 
+  >;
+
+  /// Index in fargments for the big and small part
+  static int const kBigIndex = 0;
+  static int const kSmallIndex = 1;
+
+public:
+
+  /// Iterates over the A operand in memory
+  using IteratorA = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kM, Shape::kK>,
+    Operand::kA,
+    ElementA,
+    LayoutA,
+    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+    Policy::OpDelta::kRow,
+    32,
+    1
+  >;
+
+  /// Storage for A tile
+  using FragmentA = typename IteratorA::Fragment;
+
+  /// Storage for transformed A tile
+  // (4 times the original FragmentA::kElements)
+  // (real_big), (imag_big), (real_small), (imag_small)
+  using TransformedFragmentA = Array<typename ArchMmaOperator::ElementA, 
+                                              FragmentA::kElements * 2 * 2>;
+
+  // Fragment bisecting big and small sections
+  // (real_big, imag_big), (real_small, imag_small)
+  using AccessTypeFragmentA = Array<typename ArchMmaOperator::ElementA, 
+                                                    FragmentA::kElements * 2>;
+
+  /// Iterates over the B operand in memory
+  using IteratorB = MmaTensorOpMultiplicandTileIterator<
+    MatrixShape<Shape::kK, Shape::kN>,
+    Operand::kB,
+    ElementB,
+    LayoutB,
+    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
+    Policy::OpDelta::kColumn,
+    32,
+    1
+  >;
+
+  /// Storage for B tile
+  using FragmentB = typename IteratorB::Fragment;
+
+  /// Storage for transformed B tile 
+  // (4 times the original FragmentB::kElements)
+  // (real_big), (imag_big), (real_small), (imag_small)
+  using TransformedFragmentB = Array<typename ArchMmaOperator::ElementB, 
+                                              FragmentB::kElements * 2 * 2>;
+
+  // Fragment bisecting big and small sections
+  // (real_big, imag_big), (real_small, imag_small)
+  using AccessTypeFragmentB = Array<typename ArchMmaOperator::ElementB, 
+                                                    FragmentB::kElements * 2>;
+
+  static_assert(
+    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
+    !(Shape::kN % ArchMmaOperator::Shape::kN),
+    "Shape of warp-level Mma must be divisible by operator shape.");
+
+  /// Number of complex products operations performed (one complex product needs four mma instructions)
+  using MmaIterations = MatrixShape<
+    Shape::kM / ArchMmaOperator::Shape::kM,
+    Shape::kN / ArchMmaOperator::Shape::kN
+  >;
+
+  /// Iterates over the C operand in memory
+  using IteratorC = MmaTensorOpAccumulatorTileIterator<
+     MatrixShape<Shape::kM, Shape::kN>, 
+     ElementC, 
+     LayoutC,
+     typename ArchMmaOperator::Shape, 
+     typename Policy::OpDelta>;
+
+  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
+  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
+  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
+  /// of Tensor Cores which are always real-valued matrix multiplies.
+  using FragmentC = typename IteratorC::Fragment;
+
+  //
+  // Alias types for underlying real-valued matrix multiply operator
+  //
+  using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
+  using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
+  using MmaOperandC = typename ArchMmaOperator::FragmentC;
+
+  static_assert(platform::is_same<cutlass::gemm::GemmShape<16, 8, 8>, typename ArchMmaOperator::Shape>::value, 
+    "This implementation only supports MMA.1688 math instructions.");
+
+  static_assert(InstMmaOperandA::kElements == 4, 
+    "This implementation only supports math instructions in which exactly four element is needed for the A operand."
+    "We can geneneralize later.");
+
+  static_assert(InstMmaOperandB::kElements == 2, 
+    "This implementation only supports math instructions in which exactly two element is needed for the B operand."
+    "We can geneneralize later.");
+
+private:
+
+  //
+  // Data members
+  //
+
+  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
+  ArchMmaOperator mma;
+
+public:
+
+  //
+  // Methods
+  //
+
+  /// Ctor
+  CUTLASS_DEVICE
+  MmaComplexTensorOpFastF32() {}
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void operator()(
+    FragmentC &D, 
+    TransformedFragmentA const &A, 
+    TransformedFragmentB const &B, 
+    FragmentC const &C
+  ) const {
+
+    AccessTypeFragmentA const *complex_A = reinterpret_cast<AccessTypeFragmentA const*>(&A);
+    AccessTypeFragmentB const *complex_B = reinterpret_cast<AccessTypeFragmentB const*>(&B);
+
+    //
+    // Accumulate in place
+    //
+    D = C;
+
+
+    complex_mma_operator(D, complex_A[kSmallIndex], complex_B[kBigIndex], D);
+
+    complex_mma_operator(D, complex_A[kBigIndex], complex_B[kSmallIndex], D);
+
+    complex_mma_operator(D, complex_A[kBigIndex], complex_B[kBigIndex], D);
+
+    if (ComplexFastF32::kPrecision == TensorFloat32Op::k4xTF32)
+      complex_mma_operator(D, complex_A[kSmallIndex], complex_B[kSmallIndex], D);
+  }
+
+  /// Performs a warp-level matrix multiply-accumulate operation
+  CUTLASS_DEVICE
+  void complex_mma_operator(
+    FragmentC &D, 
+    AccessTypeFragmentA const &complex_A, 
+    AccessTypeFragmentB const &complex_B, 
+    FragmentC const &C
+  ) const {
+
+    // Instruction Operands A & B holding real part followed by imaginary part for mma operations
+    InstMmaOperandA const *operand_A = reinterpret_cast<InstMmaOperandA const *>(&complex_A);
+    InstMmaOperandB const *operand_B = reinterpret_cast<InstMmaOperandB const *>(&complex_B);
+
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int m = 0; m < MmaIterations::kRow; ++m) {
+
+      // mma(accum.real(), a.real(), b.real(), accum.real());
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
+
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
+
+          mma(*accum, operand_A[m], operand_B[n], *accum);
+      }
+
+      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
+
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+
+        mma(*accum, operand_A[m], operand_B[n+MmaIterations::kColumn], *accum);
+      }
+
+      // mma(accum.real(), a.imag(), -b.imag(), accum.real())
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < MmaIterations::kColumn; ++n) {
+
+        // negate OperandB to accumulate  -(a.imag()*b.imag())
+        // negating OperandB emits less instrucitons than negating OperandA as OperandB has less elements
+        negate<InstMmaOperandB> negate_op;
+
+        // Real-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow);
+
+         mma(*accum, operand_A[m+MmaIterations::kRow], negate_op(operand_B[n+MmaIterations::kColumn]), *accum);
+      }
+
+      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
+
+        // Complex-valued accumulator part
+        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
+          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+
+        mma(*accum, operand_A[m+MmaIterations::kRow], operand_B[n], *accum);
+      }
+    }
+  }
+
+  /// Transform the mma operands to the required types
+  CUTLASS_DEVICE
+  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
+                 FragmentA const &A, FragmentB const &B) const {
+
+    detail::UnpackComplexConvertAndPackForMmaFastF32 <
+      RealElementA,
+      InstMmaOperandA,
+      FragmentA,
+      MmaIterations,
+      MatrixShape<2, 2>,
+      kTransformA,
+      Operand::kA,
+      ComplexFastF32::kRoundBigA,
+      ComplexFastF32::kRoundSmallA> convert_A;
+
+    detail::UnpackComplexConvertAndPackForMmaFastF32 <
+      RealElementB,
+      InstMmaOperandB,
+      FragmentB,
+      MmaIterations,
+      MatrixShape<2, 1>,
+      kTransformB,
+      Operand::kB,
+      ComplexFastF32::kRoundBigB,
+      ComplexFastF32::kRoundSmallB> convert_B;
+
+    // Convert Fragment[A|B] holding complex<RealElement[A|B]> to InstMmaOperand[A|B] holding InstMmaOperand[A|B]::Element
+    convert_A(reinterpret_cast<InstMmaOperandA *>(&dst_A), A); 
+    convert_B(reinterpret_cast<InstMmaOperandB *>(&dst_B), B); 
+  }
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
+} // namespace warp
 } // namespace gemm
-} // namespace cutlass 
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_multistage.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/ell_mma_pipelined.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/gemv.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/index_remat.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_base.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,15 +30,14 @@
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
-#include "cutlass/tensor_ref.h"
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -680,15 +680,15 @@
                           arch::OpMultiplyAddFastF32>::value
       || platform::is_same<typename Operator::MathOperator,
                            arch::OpMultiplyAddComplexFastF32>::value) {
       accum = plus_accum(accum, tmp_accum); 
     }
  
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
+      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -843,15 +843,15 @@
               warp_loaded_frag_A_gamma_beta[(warp_mma_k + 1) % 2]);
         }
       }
 
     }
     
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
+      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,42 +26,187 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
+
+    It loads two loop invariant vectors, norm and sum, in the prologue and
+    stores them in the register file.  We will call elementwise operation to
+    apply norm and sum between ldmatrix and warp mma.
 */
 
 #pragma once
 
-
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
-
+#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
+#include "cutlass/gemm/warp/softmax_scale_bias_transform.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
+    /// Policy describing tuning details (concept: MmaPolicy)
+    typename Policy_,
+    /// Number of stages,
+    int Stages,
+    /// Used for partial specialization
+    typename Enable = bool>
+class MmaMainloopFusionBase {
+ public:
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  using Shape = Shape_;
+
+  ///< Policy describing tuning details
+  using Policy = Policy_;
+
+  //
+  // Dependent types
+  //
+
+  /// Warp-level Mma
+  using Operator = typename Policy::Operator;
+
+  /// Shape describing the overall GEMM computed from shared memory
+  /// by each warp.
+  using WarpGemm = typename Policy::Operator::Shape;
+
+  /// Shape describing the number of warps filling the CTA
+  using WarpCount = cutlass::gemm::GemmShape<Shape::kM / WarpGemm::kM,
+                                             Shape::kN / WarpGemm::kN,
+                                             Shape::kK / WarpGemm::kK>;
+
+  /// Number of warp-level GEMM oeprations
+  static int const kWarpGemmIterations =
+      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
+
+  /// Number of stages
+  static int const kStages = Stages;
+
+  /// Tensor reference to the A operand
+  using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
+
+  /// Tensor reference to the B operand
+  using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
+
+  //
+  // Nested structs
+  //
+
+  /// Shared storage object needed by threadblock-scoped GEMM
+  class SharedStorage {
+   public:
+    //
+    // Type definitions
+    //
+
+    /// Shape of the A matrix operand in shared memory
+    using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
+                               Shape::kK * kStages +
+                                   Policy::SmemPaddingA::kColumn>;
+
+    /// Shape of the B matrix operand in shared memory
+    using ShapeB =
+        MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
+                    Shape::kN + Policy::SmemPaddingB::kColumn>;
+
+   public:
+    //
+    // Data members
+    //
+
+    /// Buffer for A operand
+    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount> operand_A;
+
+    /// Buffer for B operand
+    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B;
+
+   public:
+
+    //
+    // Methods
+    //
+
+    /// Returns a layout object for the A matrix
+    CUTLASS_DEVICE
+    static typename Operator::LayoutA LayoutA() {
+      return Operator::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
+    }
+
+    /// Returns a layout object for the B matrix
+    CUTLASS_HOST_DEVICE
+    static typename Operator::LayoutB LayoutB() {
+      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
+    }
+
+    /// Returns a TensorRef to the A operand
+    CUTLASS_HOST_DEVICE
+    TensorRefA operand_A_ref() {
+      return TensorRefA{operand_A.data(), LayoutA()};
+    }
+
+    /// Returns a TensorRef to the B operand
+    CUTLASS_HOST_DEVICE
+    TensorRefB operand_B_ref() {
+      return TensorRefB{operand_B.data(), LayoutB()};
+    }
+  };
+
+ protected:
+
+  //
+  // Data members
+  //
+
+  /// Iterator to load a warp-scoped tile of A operand from shared memory
+  typename Operator::IteratorA warp_tile_iterator_A_;
+
+  /// Iterator to load a warp-scoped tile of B operand from shared memory
+  typename Operator::IteratorB warp_tile_iterator_B_;
+
+public:
+
+  /// Construct from tensor references
+  CUTLASS_DEVICE
+  MmaMainloopFusionBase(
+      ///< Shared storage needed for internal use by threadblock-scoped GEMM
+      SharedStorage &shared_storage,
+      ///< ID within the threadblock
+      int thread_idx,
+      ///< ID of warp
+      int warp_idx,
+      ///< ID of each thread within a warp
+      int lane_idx)
+      : warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
+        warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {}
+};
+
+
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math
+/// instructions.
+template <
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename Shape_,
     /// Iterates over tiles of A operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorA_,
     /// Iterates over tiles of A operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorA_,
@@ -72,43 +217,54 @@
     //  MaskedTileIterator)
     typename IteratorB_,
     /// Iterates over tiles of B operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorB_,
     /// Cache operation for operand B
     cutlass::arch::CacheOperation::Kind CacheOpB,
+    /// Iterates over vectors of var and mean vector in global memory
+    //  (concept: ReadableTileIterator | ForwardTileIterator |
+    //  MaskedTileIterator)
+    typename IteratorNormSum_,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
+    /// Whether problem has been transformed. This determines to which operand
+    /// the softmax is applied.
+    bool InternalTranspose,
     /// Use zfill or predicate for out-of-bound cp.async
     SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaMultistage : 
-  public MmaBase<Shape_, Policy_, Stages> {
+class MmaSoftmaxMainloopFusionMultistage : 
+  public MmaMainloopFusionBase<Shape_, Policy_, Stages> {
 public:
-  ///< Base class
-  using Base = MmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
   ///< Iterates over tiles of B operand in global memory
   using IteratorB = IteratorB_;
+  ///< Iterates over tiles of the var and mean vectors in global memory
+  using IteratorNormSum = IteratorNormSum_;
+  ///< Policy describing tuning details
+  using Policy = Policy_;
+
+  ///< Base class
+  using Base = MmaMainloopFusionBase<Shape_, Policy, Stages>;
+
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
-  ///< Policy describing tuning details
-  using Policy = Policy_;
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
   static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
@@ -120,24 +276,28 @@
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
-
+  
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
+    static_assert(Base::kWarpGemmIterations > 1,
+                  "The pipelined structure requires at least two warp-level "
+                  "GEMM operations.");
+
     /// Number of cp.async instructions to load one stage of operand A
     static int const AsyncCopyIterationsPerStageA =
         IteratorA::ThreadMap::Iterations::kCount;
 
     /// Number of cp.async instructions to load one stage of operand B
     static int const AsyncCopyIterationsPerStageB =
         IteratorB::ThreadMap::Iterations::kCount;
@@ -148,151 +308,88 @@
     /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA =
         (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
     /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB =
         (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
-
-    // Optional staged-accumulation (e.g., tf32x3 kernels) for improved numerical
-    // accuracy, where each mainloop iteration first accumulates into a temporary
-    // set of freshly-cleared accumulators, which are subsequently added to the
-    // final accumulator set.
-    static bool const kStagedAccumulation =
-      platform::is_same<typename Operator::MathOperator, arch::OpMultiplyAddFastF32>::value ||
-      platform::is_same<typename Operator::MathOperator, arch::OpMultiplyAddComplexFastF32>::value;
-
   };
 
  private:
 
-
-  // Structure encapsulating pipeline state live from one iteration to the next
-  struct PipeState {
-
-    using WarpLoadedFragmentA = typename Operator::FragmentA;
-    using WarpLoadedFragmentB = typename Operator::FragmentB;
-    using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
-    using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
-
-    /// Temporary accumulator to facilitate staged-accumulation
-    FragmentC tmp_accum_;
-
-    /// Pair of A fragments used to overlap shared memory loads and math instructions
-    WarpLoadedFragmentA warp_loaded_frag_A_[2];
-    WarpTransformedFragmentA warp_transformed_frag_A_[2];
-
-    /// Pair of B fragments used to overlap shared memory loads and math instructions
-    WarpLoadedFragmentB warp_loaded_frag_B_[2];
-    WarpTransformedFragmentB warp_transformed_frag_B_[2];
-  };
+  using WarpLoadedFragmentA = typename Operator::FragmentA;
+  using WarpLoadedFragmentB = typename Operator::FragmentB;
+  using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
+  using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
+
+  using WarpLoadedFragmentNormSum = typename IteratorNormSum::Fragment;
+
+  static bool const kInternalTranspose = InternalTranspose;
+
+  using SoftmaxFragment = typename platform::conditional<kInternalTranspose,
+                                                         WarpTransformedFragmentB,
+                                                         WarpTransformedFragmentA>::type;
 
 
  private:
 
   //
   // Data members
   //
 
-  /// Warp-level MMA operator
-  Operator warp_mma_;
-
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
-  /// Shared memory write stage index
-  int smem_write_stage_idx_;
-
-  /// Shared memory read stage index
-  int smem_read_stage_idx_;
+  int warp_idx_m_;
 
+  int warp_idx_n_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaMultistage(
+  MmaSoftmaxMainloopFusionMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       typename Base::SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       Base(shared_storage, thread_idx, warp_idx, lane_idx),
       smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
-      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx),
-      smem_write_stage_idx_(0),
-      smem_read_stage_idx_(0)
+      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx)
   {
     // Compute warp location within threadblock tile by mapping the warp_id to
     // three coordinates:
     //   _m: the warp's position within the threadblock along the M dimension
     //   _n: the warp's position within the threadblock along the N dimension
     //   _k: the warp's position within the threadblock along the K dimension
 
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
     int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
 
-    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
-    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
+    warp_idx_m_ = warp_idx_mn % Base::WarpCount::kM;
+    warp_idx_n_ = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
     this->warp_tile_iterator_A_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+        {warp_idx_m_, Base::kWarpGemmIterations * warp_idx_k});
     this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n_});
   }
 
-  /// Advance shared memory read-iterators to the next stage
   CUTLASS_DEVICE
-  void advance_smem_read_stage()
-  {
-    ++smem_read_stage_idx_;
-
-    if (smem_read_stage_idx_ == Base::kStages) {
-      // Wrap back around to the 'start' of the circular buffer in shared memory
-      this->warp_tile_iterator_A_.add_tile_offset({0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
-      this->warp_tile_iterator_B_.add_tile_offset({-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
-      smem_read_stage_idx_ = 0;
-    }
-  }
-
-  /// Advance global memory read-iterators and shared memory write-iterators to the stage
-  CUTLASS_DEVICE
-  void advance_smem_write_stage(
-    IteratorA &iterator_A,
-    IteratorB &iterator_B)
-  {
-    // Advance global iterators
-    iterator_A.add_tile_offset({0, 1});
-    iterator_B.add_tile_offset({1, 0});
-
-    // Advance shared iterators
-    smem_iterator_A_.add_tile_offset({0, 1});
-    smem_iterator_B_.add_tile_offset({1, 0});
-
-    // Increment shared memory write stage index
-    ++smem_write_stage_idx_;
-
-    if (smem_write_stage_idx_ == Base::kStages) {
-      // Wrap back around to the 'start' of the circular buffer in shared memory
-      smem_iterator_A_.add_tile_offset({0, -Base::kStages});
-      smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-      smem_write_stage_idx_ = 0;
-    }
-  }
-
-  CUTLASS_DEVICE
-  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B,
+  void copy_tiles_and_advance(IteratorA &iterator_A,
+                              IteratorB &iterator_B,
                               int group_start_A = 0, int group_start_B = 0) {
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
     // Async Copy for operand A
     CUTLASS_PRAGMA_UNROLL
@@ -356,27 +453,43 @@
           ++iterator_B;
         }
         ++this->smem_iterator_B_;
       }
     }
   }
 
-  /// GEMM prologue.  Bootstrap the global->shared memory pipeline by fetching
-  /// the global fragments needed by the first kStages-1 threadblock mainloop iterations
+  /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
-  void prologue(
-    IteratorA &iterator_A,      ///< [in|out] iterator over A operand in global memory
-    IteratorB &iterator_B,      ///< [in|out] iterator over B operand in global memory
-    int &gemm_k_iterations)     ///< [in|out] number of threadblock mainloop iterations remaining
-  {
+  void operator()(
+      ///< problem size of GEMM
+      int gemm_k_iterations,
+      ///< destination accumulator tile
+      FragmentC &accum,
+      ///< iterator over A operand in global memory
+      IteratorA iterator_A,
+      ///< iterator over B operand in global memory
+      IteratorB iterator_B,
+      ///< iterator over B operand in global memory
+      IteratorNormSum iterator_norm_sum,
+      ///< initial value of accumulator
+      FragmentC const &src_accum) {
+
+    //
+    // Prologue
+    //
     // Issue several complete stages
+
+    WarpLoadedFragmentNormSum warp_loaded_frag_norm_sum;
+    iterator_norm_sum.add_tile_offset({0, warp_idx_m_});
+    iterator_norm_sum.load(warp_loaded_frag_norm_sum);
+
     CUTLASS_PRAGMA_UNROLL
-    for (int stage = 0; stage < Base::kStages - 1; ++stage, --gemm_k_iterations) {
+    for (int stage = 0; stage < Base::kStages - 1;
+         ++stage, --gemm_k_iterations) {
 
-      // Disable global fetching if done with global fetch iterations
       iterator_A.clear_mask(gemm_k_iterations == 0);
       iterator_B.clear_mask(gemm_k_iterations == 0);
 
       iterator_A.set_iteration_index(0);
       this->smem_iterator_A_.set_iteration_index(0);
 
       // Async Copy for operand A
@@ -426,321 +539,213 @@
 
           ++iterator_B;
         }
 
         ++this->smem_iterator_B_;
       }
 
-      // Move to the next write stage
-      advance_smem_write_stage(iterator_A, iterator_B);
+      // Move to the next stage
+      iterator_A.add_tile_offset({0, 1});
+      iterator_B.add_tile_offset({1, 0});
+
+      this->smem_iterator_A_.add_tile_offset({0, 1});
+      this->smem_iterator_B_.add_tile_offset({1, 0});
 
       // Defines the boundary of a stage of cp.async.
       cutlass::arch::cp_async_fence();
     }
 
-    // Optionally clear the remaining stages of SMEM. This is a functional requirement for
-    // some kernels so that all accumulator elements outside the GEMM footprint are zero.
-    if (SharedMemoryClear == SharedMemoryClearOption::kClearLastStage) {
-
-      /// Iterator to write threadblock-scoped tile of A operand to shared memory
-      SmemIteratorA last_smem_iterator_A(this->smem_iterator_A_);
-      typename IteratorA::AccessType zero_A;
-
-      zero_A.clear();
-      last_smem_iterator_A.set_iteration_index(0);
+    // Perform accumulation in the 'd' output operand
+    accum = src_accum;
 
-      // Async Copy for operand A
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
+    // Waits until kStages-2 stages have committed.
+    cutlass::arch::cp_async_wait<Base::kStages - 2>();
+    __syncthreads();
 
-        typename IteratorA::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorA::AccessType *>(
-                last_smem_iterator_A.get());
+    // Pair of fragments used to overlap shared memory loads and math
+    // instructions
+    WarpLoadedFragmentA warp_loaded_frag_A[2];
+    WarpLoadedFragmentB warp_loaded_frag_B[2];
+    WarpTransformedFragmentA warp_transformed_frag_A[2];
+    WarpTransformedFragmentB warp_transformed_frag_B[2];
+
+    Operator warp_mma;
+    cutlass::gemm::warp::SoftmaxScaleBiasTransform<
+        SoftmaxFragment, WarpLoadedFragmentNormSum> elementwise_transform;
 
-        *dst_ptr = zero_A;
+    this->warp_tile_iterator_A_.set_kgroup_index(0);
+    this->warp_tile_iterator_B_.set_kgroup_index(0);
 
-        ++last_smem_iterator_A;
-      }
+    this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
+    this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
 
-      /// Iterator to write threadblock-scoped tile of B operand to shared memory
-      SmemIteratorB last_smem_iterator_B(this->smem_iterator_B_);
-      typename IteratorB::AccessType zero_B;
+    ++this->warp_tile_iterator_A_;
+    ++this->warp_tile_iterator_B_;
 
-      zero_B.clear();
-      last_smem_iterator_B.set_iteration_index(0);
+    iterator_A.clear_mask(gemm_k_iterations == 0);
+    iterator_B.clear_mask(gemm_k_iterations == 0);
 
-      // Async Copy for operand B
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
+    // Start issuing the first group of the next stage outside of the mainloop
+    copy_tiles_and_advance(iterator_A, iterator_B);
 
-        typename IteratorB::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorB::AccessType *>(
-                last_smem_iterator_B.get());
+    int smem_write_stage_idx = Base::kStages - 1;
+    int smem_read_stage_idx = 0;
 
-        *dst_ptr = zero_B;
+    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
+                       warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
 
-        ++last_smem_iterator_B;
-      }
+    if (kInternalTranspose) {
+      elementwise_transform(warp_transformed_frag_B[0],
+                         warp_loaded_frag_norm_sum);
+    } else {
+      elementwise_transform(warp_transformed_frag_A[0],
+                         warp_loaded_frag_norm_sum);
     }
-  }
-
-
-  /// Wait until we have at least one completed global fetch stage
-  CUTLASS_DEVICE
-  void gmem_wait()
-  {
-    // Wait until we have at least one committed global fetch stage. (#uncommitted = Base::kStages - 1 - #committed)
-    cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
-  }
 
+    //
+    // Mainloop
+    //
 
-  /// Perform a threadblock mainloop iteration of matrix multiply-accumulate
-  CUTLASS_DEVICE
-  void mac_loop_iter(
-    PipeState &pipe_state,          ///< [in|out] loop-carried pipeline state
-    FragmentC &accum,               ///< [in|out] destination accumulator tile
-    IteratorA &iterator_A,          ///< [in|out] iterator over A operand in global memory
-    IteratorB &iterator_B,          ///< [in|out] iterator over B operand in global memory
-    int &gemm_k_iterations)         ///< [in|out] number of threadblock mainloop iterations remaining
-  {
-    // Unroll the warp-level MMA tiles of a threadblock's mainloop iteration
-    CUTLASS_PRAGMA_UNROLL
-    for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k) {
+    CUTLASS_GEMM_LOOP
+    for (; gemm_k_iterations > (-Base::kStages + 1);) {
+      //
+      // Loop over GEMM K dimension
+      //
 
-      // Load the next warp-tile's A fragment from shared memory
-      this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-      this->warp_tile_iterator_A_.load(pipe_state.warp_loaded_frag_A_[(warp_mma_k + 1) % 2]);
-      ++this->warp_tile_iterator_A_;
-
-      // Load the next warp-tile's B fragment from shared memory
-      this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-      this->warp_tile_iterator_B_.load(pipe_state.warp_loaded_frag_B_[(warp_mma_k + 1) % 2]);
-      ++this->warp_tile_iterator_B_;
-
-      // Except for the first warp-tile, all warp-tiles convert their incoming shared memory fragments as necessary
-      if (warp_mma_k > 0) {
-        warp_mma_.transform(
-          pipe_state.warp_transformed_frag_A_[warp_mma_k % 2],
-          pipe_state.warp_transformed_frag_B_[warp_mma_k % 2],
-          pipe_state.warp_loaded_frag_A_[warp_mma_k % 2],
-          pipe_state.warp_loaded_frag_B_[warp_mma_k % 2]);
-      }
+      // Computes a warp-level GEMM on data held in shared memory
+      // Each "warp_mma_k" refers to a warp-level matrix multiply-accumulate
+      CUTLASS_PRAGMA_UNROLL
+      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
+           ++warp_mma_k) {
 
-      // Execute the current warp-tile of MMA operations
-      if (Detail::kStagedAccumulation) {
-        warp_mma_(
-          pipe_state.tmp_accum_,
-          pipe_state.warp_transformed_frag_A_[warp_mma_k % 2],
-          pipe_state.warp_transformed_frag_B_[warp_mma_k % 2],
-          pipe_state.tmp_accum_
-        );
+        // Load warp-level tiles from shared memory, wrapping to k offset if
+        // this is the last group as the case may be.
 
-        if (warp_mma_k == 0) {
-          plus<FragmentC> plus_accum;
-          accum = plus_accum(accum, pipe_state.tmp_accum_);
-          pipe_state.tmp_accum_.clear();
+        this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        
+        this->warp_tile_iterator_A_.load(warp_loaded_frag_A[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_B_.load(warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+
+        ++this->warp_tile_iterator_A_;
+        ++this->warp_tile_iterator_B_;
+
+        if (warp_mma_k > 0) {
+          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                             warp_transformed_frag_B[warp_mma_k % 2],
+                             warp_loaded_frag_A[warp_mma_k % 2],
+                             warp_loaded_frag_B[warp_mma_k % 2]);
+
+              if (kInternalTranspose) {
+                elementwise_transform(warp_transformed_frag_B[warp_mma_k % 2],
+                                  warp_loaded_frag_norm_sum);
+              } else {
+                elementwise_transform(warp_transformed_frag_A[warp_mma_k % 2],
+                                  warp_loaded_frag_norm_sum);
+              }
         }
-      } else {
-        warp_mma_(
-          accum,
-          pipe_state.warp_transformed_frag_A_[warp_mma_k % 2],
-          pipe_state.warp_transformed_frag_B_[warp_mma_k % 2],
-          accum
-        );
-      }
-
-      // Except for the last warp-tile, all warp-tiles issue their share of
-      // global->shared fragment copies
-      if (warp_mma_k < Base::kWarpGemmIterations - 1) {
 
+        // Issue global->shared copies for the next stage
         int group_start_iteration_A, group_start_iteration_B;
-        group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
-        group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
-
-        copy_tiles_and_advance(
-            iterator_A,
-            iterator_B,
-            group_start_iteration_A,
-            group_start_iteration_B);
-      }
-
-      // The second-to-last warp-tile also:
-      //   - performs the last warp-tile's share of global->shared fragment copies
-      //   - moves to the next global fetch stage
-      if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
-
-        // Performs the last warp-tile's share of global->shared fragment copies
-        int group_start_iteration_A = (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
-        int group_start_iteration_B = (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
-
-        copy_tiles_and_advance(
-          iterator_A,
-          iterator_B,
-          group_start_iteration_A,
-          group_start_iteration_B);
-
-        // Inserts a memory fence between stages of cp.async instructions.
-        cutlass::arch::cp_async_fence();
-
-        // Wait until we have at least one completed global fetch stage
-        gmem_wait();
-
-        // Move to the next global fetch stage
-        advance_smem_write_stage(iterator_A, iterator_B);
-        advance_smem_read_stage();
-
-        // Disable global fetching when done with global fetch iterations
-        --gemm_k_iterations;
-        iterator_A.clear_mask(gemm_k_iterations == 0);
-        iterator_B.clear_mask(gemm_k_iterations == 0);
-      }
-
-      // The last warp-tile also converts the shared memory fragments used by
-      // the first warp-tile of the next iteration, if necessary (so we can
-      // immediately start issuing MMA instructions at the top of the loop )
-      if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
-
-        warp_mma_.transform(
-          pipe_state.warp_transformed_frag_A_[(warp_mma_k + 1) % 2],
-          pipe_state.warp_transformed_frag_B_[(warp_mma_k + 1) % 2],
-          pipe_state.warp_loaded_frag_A_[(warp_mma_k + 1) % 2],
-          pipe_state.warp_loaded_frag_B_[(warp_mma_k + 1) % 2]);
-      }
-
-    }
-  }
 
+        if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
+          group_start_iteration_A = 0;
+          group_start_iteration_B = 0;
+        } else {
+          group_start_iteration_A =
+              (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
+          group_start_iteration_B =
+              (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
+        }
 
-  /// Perform the specified number of threadblock mainloop iterations of matrix
-  /// multiply-accumulate.  Assumes prologue has been initiated.
-  CUTLASS_DEVICE
-  void gemm_iters(
-      int gemm_k_iterations,        ///< number of threadblock mainloop iterations
-      FragmentC &accum,             ///< [in|out] accumulator tile
-      IteratorA &iterator_A,        ///< [in|out] iterator over A operand in global memory
-      IteratorB &iterator_B)        ///< [in|out] iterator over B operand in global memory
-  {
-    PipeState pipe_state;
+        copy_tiles_and_advance(iterator_A, iterator_B,
+                               group_start_iteration_A,
+                               group_start_iteration_B);
+
+        warp_mma(
+          accum, 
+          warp_transformed_frag_A[warp_mma_k % 2],
+          warp_transformed_frag_B[warp_mma_k % 2], 
+          accum
+        );
 
-    // Disable global fetching if done with global fetch iterations
-    iterator_A.clear_mask(gemm_k_iterations == 0);
-    iterator_B.clear_mask(gemm_k_iterations == 0);
+        if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
 
-    // Load first warp-tile's A fragment from shared memory
-    this->warp_tile_iterator_A_.set_kgroup_index(0);
-    this->warp_tile_iterator_A_.load(pipe_state.warp_loaded_frag_A_[0]);
-    ++this->warp_tile_iterator_A_;
+          // Inserts a memory fence between stages of cp.async instructions.
+          cutlass::arch::cp_async_fence();
 
-    // Load first warp-tile's B fragment from shared memory
-    this->warp_tile_iterator_B_.set_kgroup_index(0);
-    this->warp_tile_iterator_B_.load(pipe_state.warp_loaded_frag_B_[0]);
-    ++this->warp_tile_iterator_B_;
+          // Waits until kStages-2 stages have committed.
+          arch::cp_async_wait<Base::kStages - 2>();
+          __syncthreads();
+
+          // Move to the next stage
+          iterator_A.add_tile_offset({0, 1});
+          iterator_B.add_tile_offset({1, 0});
+
+          this->smem_iterator_A_.add_tile_offset({0, 1});
+          this->smem_iterator_B_.add_tile_offset({1, 0});
+
+          // Add negative offsets to return iterators to the 'start' of the
+          // circular buffer in shared memory
+          if (smem_write_stage_idx == (Base::kStages - 1)) {
+            this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
+            this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+            smem_write_stage_idx = 0;
+          } else {
+            ++smem_write_stage_idx;
+          }
 
-    // Transform, if necessary, the first warp-tile's shared memory fragments
-    warp_mma_.transform(
-      pipe_state.warp_transformed_frag_A_[0],
-      pipe_state.warp_transformed_frag_B_[0],
-      pipe_state.warp_loaded_frag_A_[0],
-      pipe_state.warp_loaded_frag_B_[0]);
+          if (smem_read_stage_idx == (Base::kStages - 1)) {
+            this->warp_tile_iterator_A_.add_tile_offset(
+                {0, -Base::kStages * Policy::kPartitionsK *
+                        Base::kWarpGemmIterations});
+            this->warp_tile_iterator_B_.add_tile_offset(
+                {-Base::kStages * Policy::kPartitionsK *
+                     Base::kWarpGemmIterations,
+                 0});
+            smem_read_stage_idx = 0;
+          } else {
+            ++smem_read_stage_idx;
+          }
 
-    if (Detail::kStagedAccumulation) {
-      pipe_state.tmp_accum_.clear();
-    }
+          --gemm_k_iterations;
+          iterator_A.clear_mask(gemm_k_iterations == 0);
+          iterator_B.clear_mask(gemm_k_iterations == 0);
+        }
 
-    // Mainloop
-    CUTLASS_GEMM_LOOP
-    for (; gemm_k_iterations > (-Base::kStages + 1);) {
-      mac_loop_iter(
-        pipe_state,
-        accum,
-        iterator_A,
-        iterator_B,
-        gemm_k_iterations);
-    }
+        // Do any conversions feeding the first stage at the end of the loop so
+        // we can start right away on mma instructions
+        if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
+          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                             warp_transformed_frag_B[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+
+              if (kInternalTranspose) {
+                elementwise_transform(warp_transformed_frag_B[(warp_mma_k + 1) % 2],
+                                  warp_loaded_frag_norm_sum);
+              } else {
+                elementwise_transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                                  warp_loaded_frag_norm_sum);
+              }
+        }
+      }
 
-    if (Detail::kStagedAccumulation) {
-      plus<FragmentC> plus_accum;
-      accum = plus_accum(accum, pipe_state.tmp_accum_);
     }
-
-    // Optionally commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
+    
     if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
       __syncthreads();
     }
 
   }
-
-
-  /// Prepares the class for another prologue.
-  CUTLASS_DEVICE
-  void wind_down()
-  {
-    // Catch-up the smem-read iterator to the smem-write iterator (so this class can be reused for another tile's prologue)
-
-    // First, increment remaining warp tiles to get to the next full stage.  (Ideally we would
-    // just decrement one tile, but not all iterators implement --() decrement.)
-    #pragma unroll
-    for (int warp_mma_k = 1; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k)
-    {
-      this->warp_tile_iterator_A_.set_kgroup_index(warp_mma_k);
-      this->warp_tile_iterator_B_.set_kgroup_index(warp_mma_k);
-
-      ++this->warp_tile_iterator_A_;
-      ++this->warp_tile_iterator_B_;
-    }
-    smem_read_stage_idx_++;
-
-    // Then wrap back two full stages (one for the tile advancing we just did, and one to catch the write iterators)
-    static const int kStageIters = Policy::kPartitionsK * Base::kWarpGemmIterations;
-    if (smem_read_stage_idx_ > 1)
-    {
-      this->warp_tile_iterator_A_.add_tile_offset({0, (-2 * kStageIters)});
-      this->warp_tile_iterator_B_.add_tile_offset({(-2 * kStageIters), 0});
-    }
-    else
-    {
-      this->warp_tile_iterator_A_.add_tile_offset({0, ((Base::kStages - 2) * kStageIters)});
-      this->warp_tile_iterator_B_.add_tile_offset({((Base::kStages - 2) * kStageIters), 0});
-    }
-    smem_read_stage_idx_ = smem_write_stage_idx_;
-  }
-
-
-  /// Perform a threadblock-scoped matrix multiply-accumulate
-  CUTLASS_DEVICE
-  void operator()(
-      ///< problem size of GEMM
-      int gemm_k_iterations,
-      ///< destination accumulator tile
-      FragmentC &accum,
-      ///< iterator over A operand in global memory
-      IteratorA iterator_A,
-      ///< iterator over B operand in global memory
-      IteratorB iterator_B,
-      ///< initial value of accumulator
-      FragmentC const &src_accum) {
-
-    // Prologue (start fetching iterations of global fragments into shared memory)
-    prologue(iterator_A, iterator_B, gemm_k_iterations);
-
-    // Wait until we have at least one completed global fetch stage
-    gmem_wait();
-
-    // Initialize destination accumulators with source accumulators
-    accum = src_accum;
-
-    // Perform the MAC-iterations
-    gemm_iters(gemm_k_iterations, accum, iterator_A, iterator_B);
-  }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
 }  // namespace gemm
 }  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,157 +33,146 @@
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
 #include "cutlass/aligned_buffer.h"
-#include "cutlass/numeric_conversion.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/threadblock/mma_base.h"
+#include "cutlass/gemm/threadblock/mma_planar_complex_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math
+/// instructions.
 template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
-  typename Shape_,
-  /// Iterates over tiles of A operand in global memory 
-  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
-  typename IteratorA_,
-  /// Iterates over tiles of A operand in shared memory
-  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-  typename SmemIteratorA_,
-  /// Iterates over tiles of B operand in global memory
-  //  (concept: ReadableTileIterator | ForwardTileIterator | MaskedTileIterator)
-  typename IteratorB_,
-  /// Iterates over tiles of B operand in shared memory
-  /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-  typename SmemIteratorB_,
-  /// Data type of accumulator matrix
-  typename ElementC_,
-  /// Data type of accumulator matrix
-  typename LayoutC_,
-  /// Policy describing tuning details (concept: MmaPolicy)
-  typename Policy_,
-  /// Transformation applied to A operand
-  typename TransformA_ = NumericArrayConverter<
-    typename SmemIteratorA_::Element, 
-    typename IteratorA_::Element, 
-    IteratorA_::Fragment::kElements>,
-  ///
-  /// Transformation applied to B operand
-  typename TransformB_ = NumericArrayConverter<
-    typename SmemIteratorB_::Element, 
-    typename IteratorB_::Element, 
-    IteratorB_::Fragment::kElements>,
-  /// Used for partial specialization
-  typename Enable = bool
+    /// Size of the Gemm problem - concept: gemm::GemmShape<>
+    typename Shape_,
+    /// Iterates over tiles of A operand in global memory
+    //  (concept: ReadableTileIterator | ForwardTileIterator |
+    //  MaskedTileIterator)
+    typename IteratorA_,
+    /// Iterates over tiles of A operand in shared memory
+    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+    typename SmemIteratorA_,
+    /// Iterates over tiles of B operand in global memory
+    //  (concept: ReadableTileIterator | ForwardTileIterator |
+    //  MaskedTileIterator)
+    typename IteratorB_,
+    /// Iterates over tiles of B operand in shared memory
+    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+    typename SmemIteratorB_,
+    /// Data type of accumulator matrix
+    typename ElementC_,
+    /// Data type of accumulator matrix
+    typename LayoutC_,
+    /// Policy describing tuning details (concept: MmaPolicy)
+    typename Policy_,
+    /// Number of stages,
+    int Stages,
+    /// Transformation applied to A
+    ComplexTransform TransformA = ComplexTransform::kNone,
+    /// Transformation applied to B
+    ComplexTransform TransformB = ComplexTransform::kNone
 >
-class MmaPipelined : public MmaBase<Shape_, Policy_, 2> {
+class MmaPlanarComplexPipelined : 
+  public MmaPlanarComplexBase<Shape_, Policy_, Stages> {
 public:
-
   ///< Base class
-  using Base = MmaBase<Shape_, Policy_, 2>;
+  using Base = MmaPlanarComplexBase<Shape_, Policy_, Stages>;
+
+  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
+  using Shape = Shape_;
+
+  ///< Iterates over tiles of A operand in global memory
+  using IteratorA = IteratorA_;
+
+  ///< Iterates over tiles of B operand in global memory
+  using IteratorB = IteratorB_;
 
-  using Shape = Shape_;             ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  using IteratorA = IteratorA_;     ///< Iterates over tiles of A operand in global memory
-  using IteratorB = IteratorB_;     ///< Iterates over tiles of B operand in global memory
-  using ElementC = ElementC_;       ///< Data type of accumulator matrix
-  using LayoutC = LayoutC_;         ///< Layout of accumulator matrix
-  using Policy = Policy_;           ///< Policy describing tuning details
+  ///< Data type of accumulator matrix
+  using ElementC = ElementC_;
+
+  ///< Layout of accumulator matrix
+  using LayoutC = LayoutC_;
+
+  ///< Policy describing tuning details
+  using Policy = Policy_;
+
+  using ArchTag = typename Policy::Operator::ArchTag;
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
 
-  using TransformA = TransformA_;
-  using TransformB = TransformB_;
+  /// Transformation applied to A
+  static ComplexTransform const kTransformA = TransformA;
+
+  /// Transformation applied to B
+  static ComplexTransform const kTransformB = TransformB;
 
   //
   // Dependent types
   //
 
-  /// Fragment of operand A loaded from global memory
-  using FragmentA = typename IteratorA::Fragment;
-
-  /// Fragment of operand B loaded from global memory
-  using FragmentB = typename IteratorB::Fragment;
-
   /// Fragment of accumulator tile
-  using FragmentC = typename Policy::Operator::FragmentC;
+  using FragmentC = ArrayPlanarComplex<
+    typename Policy::Operator::FragmentC::Element,
+    Policy::Operator::FragmentC::kElements
+  >;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
-  /// Obtain the arch tag from the warp-level operator
-  using ArchTag = typename Policy::Operator::ArchTag;
-
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = Operator::kTransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = Operator::kTransformB;
+ private:
 
-  // staticaly assert kStages for MmaPipelined is two (Double-buffered pipeline)
-  static_assert((Base::kStages==2), "MmaPipelined requires kStages set to value 2");
+  using FragmentA = typename IteratorA::Fragment;
+  using FragmentB = typename IteratorB::Fragment;
+  using WarpFragmentA = typename Operator::FragmentA;
+  using WarpFragmentB = typename Operator::FragmentB;
 
-protected:
+ private:
 
   //
   // Data members
   //
 
-  /// Warp-level MMA operator
-  Operator warp_mma;
-
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
-  ///< transformation applied to A fragment
-  TransformA transform_A_;
-
-  ///< transformation applied to B fragment
-  TransformB transform_B_;
-
-  /// Shared memory write stage index
-  int smem_write_stage_idx;
-
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaPipelined(
-    typename Base::SharedStorage &shared_storage,       ///< Shared storage needed for internal use by threadblock-scoped GEMM
-    int thread_idx,                                     ///< ID within the threadblock
-    int warp_idx,                                       ///< ID of warp
-    int lane_idx,                                       ///< ID of each thread within a warp
-    TransformA transform_A = TransformA(),              ///< transformation applied to A fragment
-    TransformB transform_B = TransformB()               ///< transformation applied to B fragment
-  ):
-    Base(shared_storage, thread_idx, warp_idx, lane_idx),
-    smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
-    smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx),
-    transform_A_(transform_A),
-    transform_B_(transform_B),
-    smem_write_stage_idx(0)
+  MmaPlanarComplexPipelined(
+      ///< Shared storage needed for internal use by threadblock-scoped GEMM
+      typename Base::SharedStorage &shared_storage,
+      ///< ID within the threadblock
+      int thread_idx,
+      ///< ID of warp
+      int warp_idx,
+      ///< ID of each thread within a warp
+      int lane_idx
+    ):
+      Base(shared_storage, thread_idx, warp_idx, lane_idx),
+      smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
+      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx)
   {
-
     // Compute warp location within threadblock tile by mapping the warp_id to
     // three coordinates:
     //   _m: the warp's position within the threadblock along the M dimension
     //   _n: the warp's position within the threadblock along the N dimension
     //   _k: the warp's position within the threadblock along the K dimension
 
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
@@ -193,128 +182,150 @@
     int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
     this->warp_tile_iterator_A_.add_tile_offset({warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
     this->warp_tile_iterator_B_.add_tile_offset({Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
+private:
 
-  /// Advance shared memory write-iterators to the next stage
   CUTLASS_DEVICE
-  void advance_smem_write_stage()
-  {
-    ++this->smem_iterator_A_;
-    ++this->smem_iterator_B_;
+  void warp_mma_planar_complex(
+    Operator & warp_mma, 
+    FragmentC &accum,
+    WarpFragmentA const & real_A, 
+    WarpFragmentA const & imag_A, 
+    WarpFragmentB const & real_B, 
+    WarpFragmentB const & imag_B) {
+
+    cutlass::negate<Array<typename WarpFragmentB::Element, WarpFragmentB::kElements>> neg_op_B;
 
-    // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
-    if (smem_write_stage_idx == 1) {
-      this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
-      this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-    }
+    WarpFragmentB neg_real_B = neg_op_B(real_B);
+    WarpFragmentB neg_imag_B = neg_op_B(imag_B);
 
-    smem_write_stage_idx ^= 1;
-  }
+    warp_mma(accum.real, real_A, real_B, accum.real);  
 
-  /// Advance shared memory read- and write-iterators to the next stage
-  CUTLASS_DEVICE
-  void advance_smem_stages()
-  {
-    ++this->smem_iterator_A_;
-    ++this->smem_iterator_B_;
+    if (kTransformB == ComplexTransform::kNone) {
+      warp_mma(accum.imag, real_A, imag_B, accum.imag);
+    }
+    else {
+      warp_mma(accum.imag, real_A, neg_imag_B, accum.imag);
+    }
 
-    // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
-    if (smem_write_stage_idx == 1) {
-      // wrap write stage
-      this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
-      this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+    if (kTransformA == ComplexTransform::kNone) {
+      warp_mma(accum.imag, imag_A, real_B, accum.imag);
     }
-    else
-    {
-      // wrap read stage
-      this->warp_tile_iterator_A_.add_tile_offset(
-        {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
-      this->warp_tile_iterator_B_.add_tile_offset(
-        {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
+    else {
+      warp_mma(accum.imag, imag_A, neg_real_B, accum.imag);
     }
 
-    smem_write_stage_idx ^= 1;
+    if (kTransformA == ComplexTransform::kNone ^ kTransformB == ComplexTransform::kNone) {
+      warp_mma(accum.real, imag_A, imag_B, accum.real);
+    }
+    else {
+      warp_mma(accum.real, imag_A, neg_imag_B, accum.real);
+    }
   }
 
-
-  /// GEMM prologue.  Bootstrap the global->shared memory pipeline by fetching
-  /// the global fragments needed by the first kStages-1 threadblock mainloop iterations
+public:
+  
+  /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
-  void prologue(
-    IteratorA &iterator_A,      ///< [in|out] iterator over A operand in global memory
-    IteratorB &iterator_B,      ///< [in|out] iterator over B operand in global memory
-    int &gemm_k_iterations)     ///< [in|out] number of threadblock mainloop iterations remaining
-  {
+  void operator()(
+      ///< problem size of GEMM
+      int gemm_k_iterations,
+      ///< destination accumulator tile
+      FragmentC &accum,
+      ///< iterator over A operand in global memory
+      IteratorA iterator_A_real,
+      ///< iterator over A operand in global memory
+      IteratorA iterator_A_imag,
+      ///< iterator over B operand in global memory
+      IteratorB iterator_B_real,
+      ///< iterator over B operand in global memory
+      IteratorB iterator_B_imag,
+      ///< initial value of accumulator
+      FragmentC const &src_accum) {
+
+    //
+    // Prologue
+    //
+
+    // Perform accumulation in the 'd' output operand
+    accum = src_accum;
+
+    FragmentA tb_frag_A_real;
+    FragmentA tb_frag_A_imag;
+
+    FragmentB tb_frag_B_real;
+    FragmentB tb_frag_B_imag;
+
+    tb_frag_A_real.clear();
+    tb_frag_A_imag.clear();
+
+    tb_frag_B_real.clear();
+    tb_frag_B_imag.clear();
+
     // The last kblock is loaded in the prolog
+    iterator_A_real.load(tb_frag_A_real);
+    iterator_A_imag.load(tb_frag_A_imag);
 
-    // Load A fragment from global A
-    FragmentA tb_frag_A;
-    tb_frag_A.clear();
-    iterator_A.load(tb_frag_A);
-    ++iterator_A;
-
-    // Load B fragment from global B
-    FragmentB tb_frag_B;
-    tb_frag_B.clear();
-    iterator_B.load(tb_frag_B);
-    ++iterator_B;
-
-    // Store A and B fragments to shared
-    this->smem_iterator_A_.store(transform_A_(tb_frag_A));
-    this->smem_iterator_B_.store(transform_B_(tb_frag_B));
+    iterator_B_real.load(tb_frag_B_real);
+    iterator_B_imag.load(tb_frag_B_imag);
 
-    // Advance write stage
-    advance_smem_write_stage();
-  }
+    ++iterator_A_real;
+    ++iterator_A_imag;
 
-  /// Wait until we have at least one completed global fetch stage
-  CUTLASS_DEVICE
-  void gmem_wait()
-  {
-    __syncthreads();
-  }
+    ++iterator_B_real;
+    ++iterator_B_imag;
 
+    this->smem_iterator_A_.store(tb_frag_A_real);
+    this->smem_iterator_A_.store_with_pointer_offset(tb_frag_A_imag, Base::SharedStorage::kImaginaryStrideA);
 
-  /// Perform the specified number of threadblock mainloop iterations of matrix
-  /// multiply-accumulate.  Assumes prologue has been initiated.
-  CUTLASS_DEVICE
-  void gemm_iters(
-    int gemm_k_iterations,        ///< number of threadblock mainloop iterations
-    FragmentC &accum,             ///< [in|out] accumulator tile
-    IteratorA &iterator_A,        ///< [in|out] iterator over A operand in global memory
-    IteratorB &iterator_B)        ///< [in|out] iterator over B operand in global memory
-  {
-    using WarpFragmentA = typename Operator::FragmentA;
-    using WarpFragmentB = typename Operator::FragmentB;
+    this->smem_iterator_B_.store(tb_frag_B_real);
+    this->smem_iterator_B_.store_with_pointer_offset(tb_frag_B_imag, Base::SharedStorage::kImaginaryStrideB);
+
+    ++this->smem_iterator_A_;
+    ++this->smem_iterator_B_;
+
+    __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math instructions
-    WarpFragmentA warp_frag_A[2];
-    WarpFragmentB warp_frag_B[2];
+    WarpFragmentA warp_frag_real_A[2];
+    WarpFragmentA warp_frag_imag_A[2];
 
-    // Load A fragment from shared A
-    this->warp_tile_iterator_A_.set_kgroup_index(0);
-    this->warp_tile_iterator_A_.load(warp_frag_A[0]);
-    ++this->warp_tile_iterator_A_;
+    WarpFragmentB warp_frag_real_B[2];
+    WarpFragmentB warp_frag_imag_B[2];
 
-    // Load B fragment from shared B
+    this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.set_kgroup_index(0);
-    this->warp_tile_iterator_B_.load(warp_frag_B[0]);
+
+    this->warp_tile_iterator_A_.load(warp_frag_real_A[0]);
+    this->warp_tile_iterator_A_.load_with_pointer_offset(warp_frag_imag_A[0], Base::SharedStorage::kImaginaryStrideA);
+
+    this->warp_tile_iterator_B_.load(warp_frag_real_B[0]);
+    this->warp_tile_iterator_B_.load_with_pointer_offset(warp_frag_imag_B[0], Base::SharedStorage::kImaginaryStrideB);
+
+
+    ++this->warp_tile_iterator_A_;
     ++this->warp_tile_iterator_B_;
 
-    // Pair of fragments used to overlap global memory loads and math instructions;
-    FragmentA tb_frag_A;
-    FragmentB tb_frag_B;
+    Operator warp_mma;
+
+    int smem_write_stage_idx = 1;
 
     // Avoid reading out of bounds
-    iterator_A.clear_mask(gemm_k_iterations <= 1);
-    iterator_B.clear_mask(gemm_k_iterations <= 1);
+    iterator_A_real.clear_mask(gemm_k_iterations <= 1);
+    iterator_A_imag.clear_mask(gemm_k_iterations <= 1);
+    
+    iterator_B_real.clear_mask(gemm_k_iterations <= 1);
+    iterator_B_imag.clear_mask(gemm_k_iterations <= 1);
+
+    // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
+    // shared memory loads (which have the tighest latency requirement).
 
     //
     // Mainloop
     //
 
     // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
@@ -328,110 +339,84 @@
 
         // Load warp-level tiles from shared memory, wrapping to k offset if this is the last group
         // as the case may be.
 
         if (warp_mma_k == Base::kWarpGemmIterations - 1) {
 
           // Write fragments to shared memory
-          this->smem_iterator_A_.store(transform_A_(tb_frag_A));
+          this->smem_iterator_A_.store(tb_frag_A_real);
+          this->smem_iterator_A_.store_with_pointer_offset(tb_frag_A_imag, Base::SharedStorage::kImaginaryStrideA);
 
-          this->smem_iterator_B_.store(transform_B_(tb_frag_B));
+          this->smem_iterator_B_.store(tb_frag_B_real);
+          this->smem_iterator_B_.store_with_pointer_offset(tb_frag_B_imag, Base::SharedStorage::kImaginaryStrideB);
 
-          // Wait until we have at least one completed global fetch stage
-          gmem_wait();
+          __syncthreads();
+          
+          ++this->smem_iterator_B_;
+          ++this->smem_iterator_A_;
+
+          // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
+          if (smem_write_stage_idx == 1) {
+            this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
+            this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+          }
+          else {
+            this->warp_tile_iterator_A_.add_tile_offset(
+                {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
+            this->warp_tile_iterator_B_.add_tile_offset(
+                {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations,
+                 0});
+          }
 
-          // Advance smem read and write stages
-          advance_smem_stages();
+          smem_write_stage_idx ^= 1;
         }
 
         this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
         this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-
-        this->warp_tile_iterator_A_.load(warp_frag_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_B_.load(warp_frag_B[(warp_mma_k + 1) % 2]);
+        
+        this->warp_tile_iterator_A_.load(warp_frag_real_A[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_A_.load_with_pointer_offset(warp_frag_imag_A[(warp_mma_k + 1) % 2], Base::SharedStorage::kImaginaryStrideA);
+        
+        this->warp_tile_iterator_B_.load(warp_frag_real_B[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_B_.load_with_pointer_offset(warp_frag_imag_B[(warp_mma_k + 1) % 2], Base::SharedStorage::kImaginaryStrideB);
 
         ++this->warp_tile_iterator_A_;
         ++this->warp_tile_iterator_B_;
 
         if (warp_mma_k == 0) {
 
-          // Load fragment from global A
-          tb_frag_A.clear();
-          iterator_A.load(tb_frag_A);
-          ++iterator_A;
-
-          // Load fragment from global B
-          tb_frag_B.clear();
-          iterator_B.load(tb_frag_B);
-          ++iterator_B;
+          iterator_A_real.load(tb_frag_A_real);
+          iterator_A_imag.load(tb_frag_A_imag);
+
+          iterator_B_real.load(tb_frag_B_real);
+          iterator_B_imag.load(tb_frag_B_imag);
+
+          ++iterator_A_real;
+          ++iterator_A_imag;
+          ++iterator_B_real;
+          ++iterator_B_imag;
 
           // Avoid reading out of bounds if this was the last loop iteration
-          iterator_A.clear_mask(gemm_k_iterations <= 2);
-          iterator_B.clear_mask(gemm_k_iterations <= 2);
+          iterator_A_real.clear_mask(gemm_k_iterations <= 2);
+          iterator_A_imag.clear_mask(gemm_k_iterations <= 2);
+          iterator_B_real.clear_mask(gemm_k_iterations <= 2);
+          iterator_B_imag.clear_mask(gemm_k_iterations <= 2);
         }
 
-        warp_mma(
-          accum,
-          warp_frag_A[warp_mma_k % 2],
-          warp_frag_B[warp_mma_k % 2],
-          accum);
+        warp_mma_planar_complex(
+          warp_mma, 
+          accum, 
+          warp_frag_real_A[warp_mma_k % 2], 
+          warp_frag_imag_A[warp_mma_k % 2],
+          warp_frag_real_B[warp_mma_k % 2], 
+          warp_frag_imag_B[warp_mma_k % 2]);
       }
     }
 
   }
-
-
-  /// Prepares the class for another prologue.
-  CUTLASS_DEVICE
-  void wind_down()
-  {
-    // First, increment remaining warp tiles to catch it up with the write stage.
-    #pragma unroll
-    for (int warp_mma_k = 1; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k)
-    {
-      this->warp_tile_iterator_A_.set_kgroup_index(warp_mma_k);
-      this->warp_tile_iterator_B_.set_kgroup_index(warp_mma_k);
-
-      ++this->warp_tile_iterator_A_;
-      ++this->warp_tile_iterator_B_;
-    }
-
-    // If we bumped the read iterators to the end of the circular buffer, wrap them around to
-    // align them with the write iterators
-    if (smem_write_stage_idx == 0)
-    {
-      this->warp_tile_iterator_A_.add_tile_offset(
-        {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
-      this->warp_tile_iterator_B_.add_tile_offset(
-        {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
-    }
-  }
-
-  /// Perform a threadblock-scoped matrix multiply-accumulate
-  CUTLASS_DEVICE
-  void operator()(
-    int gemm_k_iterations,                            ///< number of iterations of the mainloop
-    FragmentC &accum,                                 ///< destination accumulator tile
-    IteratorA iterator_A,                             ///< iterator over A operand in global memory
-    IteratorB iterator_B,                             ///< iterator over B operand in global memory
-    FragmentC const &src_accum)                       ///< source accumulator tile
-  {
-    // Prologue
-    prologue(iterator_A, iterator_B, gemm_k_iterations);
-
-    // Wait until we have at least one completed global fetch stage
-    gmem_wait();
-
-    // Perform accumulation in the 'd' output operand
-    accum = src_accum;
-
-    // Perform the MAC-iterations
-    gemm_iters(gemm_k_iterations, accum, iterator_A, iterator_B);
-  }
-
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace threadblock
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_base.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h`

 * *Files 6% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -146,30 +146,32 @@
   /// Internal structure exposed for introspection.
   struct Detail {
 
     static_assert(Base::kWarpGemmIterations > 1,
                   "The pipelined structure requires at least two warp-level "
                   "GEMM operations.");
 
-    /// Number of cp.async instructions to load one stage of operand A
-    static int const TBLoadIterationsA =
+    /// Number of LDGSTS instructions to load one stage of operand A
+    static int const TBLDGSTSIterationsA =
         IteratorA::ThreadMap::Iterations::kCount;
 
-    /// Number of cp.async instructions to load one stage of operand B
-    static int const TBLoadIterationsB =
+    /// Number of LDGSTS instructions to load one stage of operand B
+    static int const TBLDGSTSIterationsB =
         IteratorB::ThreadMap::Iterations::kCount;
 
     /// Number of stages
     static int const kStages = Stages;
 
+    /// Number of LDGSTS instructions to load on group of operand A
     static int const kAccessesPerGroupA =
-        (TBLoadIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLDGSTSIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
+    /// Number of LDGSTS instructions to load on group of operand B
     static int const kAccessesPerGroupB =
-        (TBLoadIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLDGSTSIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
   };
 
  private:
 
   using WarpFragmentA = typename Operator::FragmentA;
   using WarpFragmentB = typename Operator::FragmentB;
 
@@ -233,15 +235,15 @@
     int group_start_A = 0, 
     int group_start_B = 0) {
 
     iterator_A_real.set_iteration_index(group_start_A * IteratorA::kAccessesPerVector);
     iterator_A_imag.set_iteration_index(group_start_A * IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
-    // Load for operand A
+    // LDGSTS for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
         
       typename IteratorA::AccessType *dst_ptr = 
         reinterpret_cast<typename IteratorA::AccessType *>(this->smem_iterator_A_.get());
           
       int const kSrcBytes = 
@@ -271,15 +273,15 @@
       ++this->smem_iterator_A_;
     }
 
     iterator_B_real.set_iteration_index(group_start_B * IteratorB::kAccessesPerVector);
     iterator_B_imag.set_iteration_index(group_start_B * IteratorB::kAccessesPerVector);
     this->smem_iterator_B_.set_iteration_index(group_start_B);
 
-    // Load for operand B
+    // LDGSTS for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
       typename IteratorB::AccessType *dst_ptr = 
         reinterpret_cast<typename IteratorB::AccessType *>(this->smem_iterator_B_.get());
       
       int const kSrcBytes = 
         sizeof_bits<typename IteratorB::Element>::value * 
@@ -380,17 +382,17 @@
       iterator_B_imag.clear_mask(gemm_k_iterations == 0);
 
       iterator_A_real.set_iteration_index(0);
       iterator_A_imag.set_iteration_index(0);
 
       this->smem_iterator_A_.set_iteration_index(0);
 
-      // Load for operand A
+      // LDGSTS for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsA; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsA; ++j) {
 
         typename IteratorA::AccessType *dst_ptr = 
           reinterpret_cast<typename IteratorA::AccessType *>(this->smem_iterator_A_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
 
@@ -421,17 +423,17 @@
       }
 
       iterator_B_real.set_iteration_index(0);
       iterator_B_imag.set_iteration_index(0);
 
       this->smem_iterator_B_.set_iteration_index(0);
 
-      // Load for operand B
+      // LDGSTS for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB; ++j) {
 
         typename IteratorB::AccessType *dst_ptr = 
           reinterpret_cast<typename IteratorB::AccessType *>(this->smem_iterator_B_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,23 +30,23 @@
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
 #include "cutlass/aligned_buffer.h"
-
-#include "cutlass/numeric_types.h"
+#include "cutlass/arch/memory.h"
+#include "cutlass/array.h"
+#include "cutlass/cutlass.h"
+#include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
+#include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/threadblock/mma_planar_complex_base.h"
+#include "cutlass/gemm/threadblock/mma_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
@@ -60,88 +60,113 @@
     /// Iterates over tiles of A operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorA_,
     /// Iterates over tiles of A operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorA_,
+    /// Cache operation for operand A
+    cutlass::arch::CacheOperation::Kind CacheOpA,
     /// Iterates over tiles of B operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorB_,
     /// Iterates over tiles of B operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorB_,
+    /// Cache operation for operand B
+    cutlass::arch::CacheOperation::Kind CacheOpB,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
-    /// Transformation applied to A
-    ComplexTransform TransformA = ComplexTransform::kNone,
-    /// Transformation applied to B
-    ComplexTransform TransformB = ComplexTransform::kNone
->
-class MmaPlanarComplexPipelined : 
-  public MmaPlanarComplexBase<Shape_, Policy_, Stages> {
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
+    /// Used for partial specialization
+    typename Enable = bool>
+class MmaWithReductionMultistage : 
+  public MmaBase<Shape_, Policy_, Stages> {
 public:
   ///< Base class
-  using Base = MmaPlanarComplexBase<Shape_, Policy_, Stages>;
-
+  using Base = MmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
-
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
-
   ///< Iterates over tiles of B operand in global memory
   using IteratorB = IteratorB_;
-
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
-
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
-
   ///< Policy describing tuning details
   using Policy = Policy_;
 
-  using ArchTag = typename Policy::Operator::ArchTag;
-
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
 
-  /// Transformation applied to A
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Transformation applied to B
-  static ComplexTransform const kTransformB = TransformB;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
 
   //
   // Dependent types
   //
 
   /// Fragment of accumulator tile
-  using FragmentC = ArrayPlanarComplex<
-    typename Policy::Operator::FragmentC::Element,
-    Policy::Operator::FragmentC::kElements
-  >;
+  using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
+  using FragmentReduction = typename Operator::FragmentReduction;
+
+  /// Minimum architecture is Sm80 to support cp.async
+  using ArchTag = arch::Sm80;
+  
+  /// Complex transform on A operand
+  static ComplexTransform const kTransformA = Operator::kTransformA;
+
+  /// Complex transform on B operand
+  static ComplexTransform const kTransformB = Operator::kTransformB;
+
+  static int const kReduceKForA = Operator::kReduceKForA;
+
+  /// Internal structure exposed for introspection.
+  struct Detail {
+
+    /// Number of cp.async instructions to load one stage of operand A
+    static int const AsyncCopyIterationsPerStageA =
+        IteratorA::ThreadMap::Iterations::kCount;
+
+    /// Number of cp.async instructions to load one stage of operand B
+    static int const AsyncCopyIterationsPerStageB =
+        IteratorB::ThreadMap::Iterations::kCount;
+
+    /// Number of stages
+    static int const kStages = Stages;
+
+    /// Number of cp.async instructions to load on group of operand A
+    static int const kAccessesPerGroupA =
+        (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+
+    /// Number of cp.async instructions to load on group of operand B
+    static int const kAccessesPerGroupB =
+        (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+  };
+
  private:
 
-  using FragmentA = typename IteratorA::Fragment;
-  using FragmentB = typename IteratorB::Fragment;
-  using WarpFragmentA = typename Operator::FragmentA;
-  using WarpFragmentB = typename Operator::FragmentB;
+  using WarpLoadedFragmentA = typename Operator::FragmentA;
+  using WarpLoadedFragmentB = typename Operator::FragmentB;
+  using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
+  using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
 
  private:
 
   //
   // Data members
   //
 
@@ -151,15 +176,15 @@
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaPlanarComplexPipelined(
+  MmaWithReductionMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       typename Base::SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
@@ -178,247 +203,345 @@
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
     int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
 
     int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
     int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
-    this->warp_tile_iterator_A_.add_tile_offset({warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
-    this->warp_tile_iterator_B_.add_tile_offset({Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+    this->warp_tile_iterator_A_.add_tile_offset(
+        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
+    this->warp_tile_iterator_B_.add_tile_offset(
+        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
   }
 
-private:
-
   CUTLASS_DEVICE
-  void warp_mma_planar_complex(
-    Operator & warp_mma, 
-    FragmentC &accum,
-    WarpFragmentA const & real_A, 
-    WarpFragmentA const & imag_A, 
-    WarpFragmentB const & real_B, 
-    WarpFragmentB const & imag_B) {
-
-    cutlass::negate<Array<typename WarpFragmentB::Element, WarpFragmentB::kElements>> neg_op_B;
-
-    WarpFragmentB neg_real_B = neg_op_B(real_B);
-    WarpFragmentB neg_imag_B = neg_op_B(imag_B);
+  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B,
+                              int group_start_A = 0, int group_start_B = 0) {
+    iterator_A.set_iteration_index(group_start_A *
+                                   IteratorA::kAccessesPerVector);
+    this->smem_iterator_A_.set_iteration_index(group_start_A);
+
+    // Async Copy for operand A
+    CUTLASS_PRAGMA_UNROLL
+    for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
+      if (group_start_A + j < Detail::AsyncCopyIterationsPerStageA) {
+        typename IteratorA::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorA::AccessType *>(
+                this->smem_iterator_A_.get());
+
+        int const kSrcBytes = sizeof_bits<typename IteratorA::Element>::value *
+                              IteratorA::ThreadMap::kElementsPerAccess /
+                              IteratorA::kAccessesPerVector / 8;
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
+          auto gmem_ptr = iterator_A.get();
+
+          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
+                dst_ptr + v, gmem_ptr, iterator_A.valid());
+          } else {
+            cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
+                dst_ptr + v, gmem_ptr, iterator_A.valid());
+          }
 
-    warp_mma(accum.real, real_A, real_B, accum.real);  
+          ++iterator_A;
+        }
 
-    if (kTransformB == ComplexTransform::kNone) {
-      warp_mma(accum.imag, real_A, imag_B, accum.imag);
-    }
-    else {
-      warp_mma(accum.imag, real_A, neg_imag_B, accum.imag);
+        ++this->smem_iterator_A_;
+      }
     }
 
-    if (kTransformA == ComplexTransform::kNone) {
-      warp_mma(accum.imag, imag_A, real_B, accum.imag);
-    }
-    else {
-      warp_mma(accum.imag, imag_A, neg_real_B, accum.imag);
-    }
+    iterator_B.set_iteration_index(group_start_B *
+                                   IteratorB::kAccessesPerVector);
+    this->smem_iterator_B_.set_iteration_index(group_start_B);
+
+    // Async Copy for operand B
+    CUTLASS_PRAGMA_UNROLL
+    for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
+      if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
+                this->smem_iterator_B_.get());
+
+        int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
+                              IteratorB::ThreadMap::kElementsPerAccess /
+                              IteratorB::kAccessesPerVector / 8;
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+          auto gmem_ptr = iterator_B.get();
+
+          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
+                dst_ptr + v, gmem_ptr, iterator_B.valid());
+          } else {
+            cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
+                dst_ptr + v, gmem_ptr, iterator_B.valid());
+          }
 
-    if (kTransformA == ComplexTransform::kNone ^ kTransformB == ComplexTransform::kNone) {
-      warp_mma(accum.real, imag_A, imag_B, accum.real);
-    }
-    else {
-      warp_mma(accum.real, imag_A, neg_imag_B, accum.real);
+          ++iterator_B;
+        }
+        ++this->smem_iterator_B_;
+      }
     }
   }
 
-public:
-  
   /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
   void operator()(
       ///< problem size of GEMM
       int gemm_k_iterations,
       ///< destination accumulator tile
       FragmentC &accum,
       ///< iterator over A operand in global memory
-      IteratorA iterator_A_real,
-      ///< iterator over A operand in global memory
-      IteratorA iterator_A_imag,
+      IteratorA iterator_A,
       ///< iterator over B operand in global memory
-      IteratorB iterator_B_real,
-      ///< iterator over B operand in global memory
-      IteratorB iterator_B_imag,
+      IteratorB iterator_B,
       ///< initial value of accumulator
-      FragmentC const &src_accum) {
+      FragmentC const &src_accum,
+      FragmentReduction &gemm_k_reduction_accum) {
 
     //
     // Prologue
     //
+    // Issue several complete stages
 
-    // Perform accumulation in the 'd' output operand
-    accum = src_accum;
+    CUTLASS_PRAGMA_UNROLL
+    for (int stage = 0; stage < Base::kStages - 1;
+         ++stage, --gemm_k_iterations) {
 
-    FragmentA tb_frag_A_real;
-    FragmentA tb_frag_A_imag;
+      iterator_A.clear_mask(gemm_k_iterations == 0);
+      iterator_B.clear_mask(gemm_k_iterations == 0);
 
-    FragmentB tb_frag_B_real;
-    FragmentB tb_frag_B_imag;
+      iterator_A.set_iteration_index(0);
+      this->smem_iterator_A_.set_iteration_index(0);
 
-    tb_frag_A_real.clear();
-    tb_frag_A_imag.clear();
+      // Async Copy for operand A
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
+        typename IteratorA::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorA::AccessType *>(
+                this->smem_iterator_A_.get());
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
+          int const kSrcBytes =
+              sizeof_bits<typename IteratorA::Element>::value *
+              IteratorA::ThreadMap::kElementsPerAccess /
+              IteratorA::kAccessesPerVector / 8;
 
-    tb_frag_B_real.clear();
-    tb_frag_B_imag.clear();
+          int src_bytes = (iterator_A.valid() ? kSrcBytes : 0);
 
-    // The last kblock is loaded in the prolog
-    iterator_A_real.load(tb_frag_A_real);
-    iterator_A_imag.load(tb_frag_A_imag);
+          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
+              dst_ptr + v, iterator_A.get(), iterator_A.valid());
 
-    iterator_B_real.load(tb_frag_B_real);
-    iterator_B_imag.load(tb_frag_B_imag);
+          ++iterator_A;
+        }
 
-    ++iterator_A_real;
-    ++iterator_A_imag;
+        ++this->smem_iterator_A_;
+      }
 
-    ++iterator_B_real;
-    ++iterator_B_imag;
+      iterator_B.set_iteration_index(0);
+      this->smem_iterator_B_.set_iteration_index(0);
 
-    this->smem_iterator_A_.store(tb_frag_A_real);
-    this->smem_iterator_A_.store_with_pointer_offset(tb_frag_A_imag, Base::SharedStorage::kImaginaryStrideA);
+      // Async Copy for operand B
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
+                this->smem_iterator_B_.get());
+
+        CUTLASS_PRAGMA_UNROLL
+        for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
+          int const kSrcBytes =
+              sizeof_bits<typename IteratorB::Element>::value *
+              IteratorB::ThreadMap::kElementsPerAccess /
+              IteratorB::kAccessesPerVector / 8;
+
+          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
+              dst_ptr + v, iterator_B.get(), iterator_B.valid());
+
+          ++iterator_B;
+        }
+
+        ++this->smem_iterator_B_;
+      }
 
-    this->smem_iterator_B_.store(tb_frag_B_real);
-    this->smem_iterator_B_.store_with_pointer_offset(tb_frag_B_imag, Base::SharedStorage::kImaginaryStrideB);
+      // Move to the next stage
+      iterator_A.add_tile_offset({0, 1});
+      iterator_B.add_tile_offset({1, 0});
 
-    ++this->smem_iterator_A_;
-    ++this->smem_iterator_B_;
+      this->smem_iterator_A_.add_tile_offset({0, 1});
+      this->smem_iterator_B_.add_tile_offset({1, 0});
 
+      // Defines the boundary of a stage of cp.async.
+      cutlass::arch::cp_async_fence();
+    }
+
+    // Perform accumulation in the 'd' output operand
+    accum = src_accum;
+
+    // Waits until kStages-2 stages have committed.
+    cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
-    // Pair of fragments used to overlap shared memory loads and math instructions
-    WarpFragmentA warp_frag_real_A[2];
-    WarpFragmentA warp_frag_imag_A[2];
+    // Pair of fragments used to overlap shared memory loads and math
+    // instructions
+    WarpLoadedFragmentA warp_loaded_frag_A[2];
+    WarpLoadedFragmentB warp_loaded_frag_B[2];
+    WarpTransformedFragmentA warp_transformed_frag_A[2];
+    WarpTransformedFragmentB warp_transformed_frag_B[2];
 
-    WarpFragmentB warp_frag_real_B[2];
-    WarpFragmentB warp_frag_imag_B[2];
+    Operator warp_mma;
 
     this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.set_kgroup_index(0);
 
-    this->warp_tile_iterator_A_.load(warp_frag_real_A[0]);
-    this->warp_tile_iterator_A_.load_with_pointer_offset(warp_frag_imag_A[0], Base::SharedStorage::kImaginaryStrideA);
-
-    this->warp_tile_iterator_B_.load(warp_frag_real_B[0]);
-    this->warp_tile_iterator_B_.load_with_pointer_offset(warp_frag_imag_B[0], Base::SharedStorage::kImaginaryStrideB);
-
+    this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
+    this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
 
     ++this->warp_tile_iterator_A_;
     ++this->warp_tile_iterator_B_;
 
-    Operator warp_mma;
-
-    int smem_write_stage_idx = 1;
+    iterator_A.clear_mask(gemm_k_iterations == 0);
+    iterator_B.clear_mask(gemm_k_iterations == 0);
 
-    // Avoid reading out of bounds
-    iterator_A_real.clear_mask(gemm_k_iterations <= 1);
-    iterator_A_imag.clear_mask(gemm_k_iterations <= 1);
-    
-    iterator_B_real.clear_mask(gemm_k_iterations <= 1);
-    iterator_B_imag.clear_mask(gemm_k_iterations <= 1);
+    int smem_write_stage_idx = Base::kStages - 1;
+    int smem_read_stage_idx = 0;
 
-    // Issue loads during the first warp-level matrix multiply-add *AFTER* issuing 
-    // shared memory loads (which have the tightest latency requirement).
+    warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
+                       warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
 
     //
     // Mainloop
     //
 
-    // Note: The main loop does not support Base::kWarpGemmIterations == 2.
     CUTLASS_GEMM_LOOP
-    for (; gemm_k_iterations > 0; --gemm_k_iterations) {
+    for (; gemm_k_iterations > (-Base::kStages + 1);) {
       //
       // Loop over GEMM K dimension
       //
 
+      // Computes a warp-level GEMM on data held in shared memory
+      // Each "warp_mma_k" refers to a warp-level matrix multiply-accumulate
       CUTLASS_PRAGMA_UNROLL
-      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations; ++warp_mma_k) {
+      for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
+           ++warp_mma_k) {
+
+        // Load warp-level tiles from shared memory, wrapping to k offset if
+        // this is the last group as the case may be.
+
+        this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        
+        this->warp_tile_iterator_A_.load(warp_loaded_frag_A[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_B_.load(warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+
+        ++this->warp_tile_iterator_A_;
+        ++this->warp_tile_iterator_B_;
+
+        if (warp_mma_k > 0)
+          warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
+                             warp_transformed_frag_B[warp_mma_k % 2],
+                             warp_loaded_frag_A[warp_mma_k % 2],
+                             warp_loaded_frag_B[warp_mma_k % 2]);
+
+        warp_mma(
+          accum, 
+          warp_transformed_frag_A[warp_mma_k % 2],
+          warp_transformed_frag_B[warp_mma_k % 2], 
+          accum,
+          gemm_k_reduction_accum
+        );
+
+        // Issue global->shared copies for the this stage
+        if (warp_mma_k < Base::kWarpGemmIterations - 1) {
+          int group_start_iteration_A, group_start_iteration_B;
+
+          group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
+          group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
 
-        // Load warp-level tiles from shared memory, wrapping to k offset if this is the last group
-        // as the case may be.
+          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
+                               group_start_iteration_B);
+        }
 
-        if (warp_mma_k == Base::kWarpGemmIterations - 1) {
+        if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
+          int group_start_iteration_A, group_start_iteration_B;
+          group_start_iteration_A =
+              (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
+          group_start_iteration_B =
+              (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
 
-          // Write fragments to shared memory
-          this->smem_iterator_A_.store(tb_frag_A_real);
-          this->smem_iterator_A_.store_with_pointer_offset(tb_frag_A_imag, Base::SharedStorage::kImaginaryStrideA);
+          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
+                               group_start_iteration_B);
 
-          this->smem_iterator_B_.store(tb_frag_B_real);
-          this->smem_iterator_B_.store_with_pointer_offset(tb_frag_B_imag, Base::SharedStorage::kImaginaryStrideB);
+          // Inserts a memory fence between stages of cp.async instructions.
+          cutlass::arch::cp_async_fence();
 
+          // Waits until kStages-2 stages have committed.
+          arch::cp_async_wait<Base::kStages - 2>();
           __syncthreads();
-          
-          ++this->smem_iterator_B_;
-          ++this->smem_iterator_A_;
 
-          // Add negative offsets to return iterators to the 'start' of the circular buffer in shared memory
-          if (smem_write_stage_idx == 1) {
+          // Move to the next stage
+          iterator_A.add_tile_offset({0, 1});
+          iterator_B.add_tile_offset({1, 0});
+
+          this->smem_iterator_A_.add_tile_offset({0, 1});
+          this->smem_iterator_B_.add_tile_offset({1, 0});
+
+          // Add negative offsets to return iterators to the 'start' of the
+          // circular buffer in shared memory
+          if (smem_write_stage_idx == (Base::kStages - 1)) {
             this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
             this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+            smem_write_stage_idx = 0;
+          } else {
+            ++smem_write_stage_idx;
           }
-          else {
+
+          if (smem_read_stage_idx == (Base::kStages - 1)) {
             this->warp_tile_iterator_A_.add_tile_offset(
-                {0, -Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations});
+                {0, -Base::kStages * Policy::kPartitionsK *
+                        Base::kWarpGemmIterations});
             this->warp_tile_iterator_B_.add_tile_offset(
-                {-Base::kStages * Policy::kPartitionsK * Base::kWarpGemmIterations,
+                {-Base::kStages * Policy::kPartitionsK *
+                     Base::kWarpGemmIterations,
                  0});
+            smem_read_stage_idx = 0;
+          } else {
+            ++smem_read_stage_idx;
           }
 
-          smem_write_stage_idx ^= 1;
-        }
-
-        this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        
-        this->warp_tile_iterator_A_.load(warp_frag_real_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_A_.load_with_pointer_offset(warp_frag_imag_A[(warp_mma_k + 1) % 2], Base::SharedStorage::kImaginaryStrideA);
-        
-        this->warp_tile_iterator_B_.load(warp_frag_real_B[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_B_.load_with_pointer_offset(warp_frag_imag_B[(warp_mma_k + 1) % 2], Base::SharedStorage::kImaginaryStrideB);
-
-        ++this->warp_tile_iterator_A_;
-        ++this->warp_tile_iterator_B_;
-
-        if (warp_mma_k == 0) {
-
-          iterator_A_real.load(tb_frag_A_real);
-          iterator_A_imag.load(tb_frag_A_imag);
-
-          iterator_B_real.load(tb_frag_B_real);
-          iterator_B_imag.load(tb_frag_B_imag);
-
-          ++iterator_A_real;
-          ++iterator_A_imag;
-          ++iterator_B_real;
-          ++iterator_B_imag;
-
-          // Avoid reading out of bounds if this was the last loop iteration
-          iterator_A_real.clear_mask(gemm_k_iterations <= 2);
-          iterator_A_imag.clear_mask(gemm_k_iterations <= 2);
-          iterator_B_real.clear_mask(gemm_k_iterations <= 2);
-          iterator_B_imag.clear_mask(gemm_k_iterations <= 2);
+          --gemm_k_iterations;
+          iterator_A.clear_mask(gemm_k_iterations == 0);
+          iterator_B.clear_mask(gemm_k_iterations == 0);
         }
 
-        warp_mma_planar_complex(
-          warp_mma, 
-          accum, 
-          warp_frag_real_A[warp_mma_k % 2], 
-          warp_frag_imag_A[warp_mma_k % 2],
-          warp_frag_real_B[warp_mma_k % 2], 
-          warp_frag_imag_B[warp_mma_k % 2]);
+        // Do any conversions feeding the first stage at the end of the loop so
+        // we can start right away on mma instructions
+        if (warp_mma_k + 1 == Base::kWarpGemmIterations)
+          warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
+                             warp_transformed_frag_B[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_A[(warp_mma_k + 1) % 2],
+                             warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
       }
+
+    }
+    
+    if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      cutlass::arch::cp_async_fence();
+      cutlass::arch::cp_async_wait<0>();
+      __syncthreads();
     }
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace gemm
-} // namespace cutlass
+}  // namespace threadblock
+}  // namespace gemm
+}  // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,187 +26,41 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Template for a double-buffered threadblock-scoped GEMM kernel.
-
-    It loads two loop invariant vectors, norm and sum, in the prologue and
-    stores them in the register file.  We will call elementwise operation to
-    apply norm and sum between ldmatrix and warp mma.
 */
 
 #pragma once
 
 #include "cutlass/aligned_buffer.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
-#include "cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h"
-#include "cutlass/gemm/threadblock/mma_base.h"
-#include "cutlass/gemm/warp/softmax_scale_bias_transform.h"
+
+#include "cutlass/gemm/threadblock/mma_sparse_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Structure to compute the matrix product targeting CUDA cores and SIMT math
 /// instructions.
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename Shape_,
-    /// Policy describing tuning details (concept: MmaPolicy)
-    typename Policy_,
-    /// Number of stages,
-    int Stages,
-    /// Used for partial specialization
-    typename Enable = bool>
-class MmaMainloopFusionBase {
- public:
-  ///< Size of the Gemm problem - concept: gemm::GemmShape<>
-  using Shape = Shape_;
-
-  ///< Policy describing tuning details
-  using Policy = Policy_;
-
-  //
-  // Dependent types
-  //
-
-  /// Warp-level Mma
-  using Operator = typename Policy::Operator;
-
-  /// Shape describing the overall GEMM computed from shared memory
-  /// by each warp.
-  using WarpGemm = typename Policy::Operator::Shape;
-
-  /// Shape describing the number of warps filling the CTA
-  using WarpCount = cutlass::gemm::GemmShape<Shape::kM / WarpGemm::kM,
-                                             Shape::kN / WarpGemm::kN,
-                                             Shape::kK / WarpGemm::kK>;
-
-  /// Number of warp-level GEMM oeprations
-  static int const kWarpGemmIterations =
-      (WarpGemm::kK / Operator::Policy::MmaShape::kK);
-
-  /// Number of stages
-  static int const kStages = Stages;
-
-  /// Tensor reference to the A operand
-  using TensorRefA = TensorRef<typename Operator::ElementA, typename Operator::LayoutA>;
-
-  /// Tensor reference to the B operand
-  using TensorRefB = TensorRef<typename Operator::ElementB, typename Operator::LayoutB>;
-
-  //
-  // Nested structs
-  //
-
-  /// Shared storage object needed by threadblock-scoped GEMM
-  class SharedStorage {
-   public:
-    //
-    // Type definitions
-    //
-
-    /// Shape of the A matrix operand in shared memory
-    using ShapeA = MatrixShape<Shape::kM + Policy::SmemPaddingA::kRow,
-                               Shape::kK * kStages +
-                                   Policy::SmemPaddingA::kColumn>;
-
-    /// Shape of the B matrix operand in shared memory
-    using ShapeB =
-        MatrixShape<Shape::kK * kStages + Policy::SmemPaddingB::kRow,
-                    Shape::kN + Policy::SmemPaddingB::kColumn>;
-
-   public:
-    //
-    // Data members
-    //
-
-    /// Buffer for A operand
-    AlignedBuffer<typename Operator::ElementA, ShapeA::kCount> operand_A;
-
-    /// Buffer for B operand
-    AlignedBuffer<typename Operator::ElementB, ShapeB::kCount> operand_B;
-
-   public:
-
-    //
-    // Methods
-    //
-
-    /// Returns a layout object for the A matrix
-    CUTLASS_DEVICE
-    static typename Operator::LayoutA LayoutA() {
-      return Operator::LayoutA::packed({ShapeA::kRow, ShapeA::kColumn});
-    }
-
-    /// Returns a layout object for the B matrix
-    CUTLASS_HOST_DEVICE
-    static typename Operator::LayoutB LayoutB() {
-      return Operator::LayoutB::packed({ShapeB::kRow, ShapeB::kColumn});
-    }
-
-    /// Returns a TensorRef to the A operand
-    CUTLASS_HOST_DEVICE
-    TensorRefA operand_A_ref() {
-      return TensorRefA{operand_A.data(), LayoutA()};
-    }
-
-    /// Returns a TensorRef to the B operand
-    CUTLASS_HOST_DEVICE
-    TensorRefB operand_B_ref() {
-      return TensorRefB{operand_B.data(), LayoutB()};
-    }
-  };
-
- protected:
-
-  //
-  // Data members
-  //
-
-  /// Iterator to load a warp-scoped tile of A operand from shared memory
-  typename Operator::IteratorA warp_tile_iterator_A_;
-
-  /// Iterator to load a warp-scoped tile of B operand from shared memory
-  typename Operator::IteratorB warp_tile_iterator_B_;
-
-public:
-
-  /// Construct from tensor references
-  CUTLASS_DEVICE
-  MmaMainloopFusionBase(
-      ///< Shared storage needed for internal use by threadblock-scoped GEMM
-      SharedStorage &shared_storage,
-      ///< ID within the threadblock
-      int thread_idx,
-      ///< ID of warp
-      int warp_idx,
-      ///< ID of each thread within a warp
-      int lane_idx)
-      : warp_tile_iterator_A_(shared_storage.operand_A_ref(), lane_idx),
-        warp_tile_iterator_B_(shared_storage.operand_B_ref(), lane_idx) {}
-};
-
-
-/// Structure to compute the matrix product targeting CUDA cores and SIMT math
-/// instructions.
-template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename Shape_,
     /// Iterates over tiles of A operand in global memory
     //  (concept: ReadableTileIterator | ForwardTileIterator |
     //  MaskedTileIterator)
     typename IteratorA_,
     /// Iterates over tiles of A operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorA_,
@@ -217,316 +71,356 @@
     //  MaskedTileIterator)
     typename IteratorB_,
     /// Iterates over tiles of B operand in shared memory
     /// (concept: WriteableTileIterator | RandomAccessTileIterator)
     typename SmemIteratorB_,
     /// Cache operation for operand B
     cutlass::arch::CacheOperation::Kind CacheOpB,
-    /// Iterates over vectors of var and mean vector in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorNormSum_,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
+    /// Iterates over tiles of E operand in global memory
+    //  (concept: ReadableTileIterator | ForwardTileIterator |
+    //  MaskedTileIterator)
+    typename IteratorE_,
+    /// Iterates over tiles of E operand in shared memory
+    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
+    typename SmemIteratorE_,
+    /// Cache operation for operand E
+    cutlass::arch::CacheOperation::Kind CacheOpE,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
-    /// Whether problem has been transformed. This determines to which operand
-    /// the softmax is applied.
-    bool InternalTranspose,
-    /// Use zfill or predicate for out-of-bound cp.async
-    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
-class MmaSoftmaxMainloopFusionMultistage : 
-  public MmaMainloopFusionBase<Shape_, Policy_, Stages> {
+class SparseMmaMultistage : 
+  public SparseMmaBase<Shape_, Policy_, Stages> {
 public:
+  ///< Base class
+  using Base = SparseMmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
   ///< Iterates over tiles of B operand in global memory
   using IteratorB = IteratorB_;
-  ///< Iterates over tiles of the var and mean vectors in global memory
-  using IteratorNormSum = IteratorNormSum_;
-  ///< Policy describing tuning details
-  using Policy = Policy_;
-
-  ///< Base class
-  using Base = MmaMainloopFusionBase<Shape_, Policy, Stages>;
-
+  ///< Iterates over tiles of E operand in global memory
+  using IteratorE = IteratorE_;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
+  ///< Policy describing tuning details
+  using Policy = Policy_;
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
+  using SmemIteratorE = SmemIteratorE_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
   static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
+  static cutlass::arch::CacheOperation::Kind const kCacheOpE = CacheOpE;
+
+  static int const kSparse = Policy::Operator::kSparse;
+  static int const kMetaSizeInBits = Policy::Operator::kMetaSizeInBits;
+  static int const kMaxID2 = Policy::Operator::kMaxID2;
+  static int const kElementsPerElementE =
+      Policy::Operator::kElementsPerElementE;
 
   //
   // Dependent types
   //
 
   /// Fragment of accumulator tile
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
+  /// ElementE
+  using ElementE = typename IteratorE::Element;
+
+  /// LayoutE
+  using LayoutE = typename IteratorE::Layout; 
+
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
   
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
-    static_assert(Base::kWarpGemmIterations > 1,
-                  "The pipelined structure requires at least two warp-level "
-                  "GEMM operations.");
-
-    /// Number of cp.async instructions to load one stage of operand A
-    static int const AsyncCopyIterationsPerStageA =
+    /// Number of async copies to load one stage of operand A
+    static int const TBLDGSTSIterationsA =
         IteratorA::ThreadMap::Iterations::kCount;
 
-    /// Number of cp.async instructions to load one stage of operand B
-    static int const AsyncCopyIterationsPerStageB =
+    /// Number of async copies to load one stage of operand B
+    static int const TBLDGSTSIterationsB =
         IteratorB::ThreadMap::Iterations::kCount;
 
+    /// Number of async copies to load one stage of operand E
+    static int const TBLDGSTSIterationsE =
+        IteratorE::ThreadMap::Iterations::kCount;
+
     /// Number of stages
     static int const kStages = Stages;
 
-    /// Number of cp.async instructions to load on group of operand A
+    /// Number of async copies to load one group of operand A
     static int const kAccessesPerGroupA =
-        (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLDGSTSIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
-    /// Number of cp.async instructions to load on group of operand B
+    /// Number of async copies to load one group of operand B
     static int const kAccessesPerGroupB =
-        (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (TBLDGSTSIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+
+    /// Number of async copies to load one group of operand E
+    static int const kAccessesPerGroupE =
+        (TBLDGSTSIterationsE + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+
+    /// E operand is tiny.  For the most of time, not all the warps are needed
+    /// to load it from the global memory.
+    static int const kValidWarps = IteratorE::ThreadMap::kThreads / 32;
+
+    /// B operand is twice as big as A which brings very high register pressure.
+    /// We have to sacrifice the double buffer when the warp tile size is big.
+    static int const kBBufferSize =
+        ((sizeof(typename Operator::ElementC) == 4) &&
+         ((platform::is_same<typename Operator::Policy::Operator::ElementA,
+                             typename Operator::ElementA>::value &&
+           platform::is_same<typename Operator::Policy::Operator::ElementB,
+                             typename Operator::ElementB>::value)) &&
+         (Operator::Shape::kM >= 64 && Operator::Shape::kN >= 64))
+            ? 1
+            : 2;
   };
 
  private:
 
   using WarpLoadedFragmentA = typename Operator::FragmentA;
   using WarpLoadedFragmentB = typename Operator::FragmentB;
   using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
   using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
-
-  using WarpLoadedFragmentNormSum = typename IteratorNormSum::Fragment;
-
-  static bool const kInternalTranspose = InternalTranspose;
-
-  using SoftmaxFragment = typename platform::conditional<kInternalTranspose,
-                                                         WarpTransformedFragmentB,
-                                                         WarpTransformedFragmentA>::type;
-
+  using WarpFragmentE = typename Operator::FragmentE;
 
  private:
 
   //
   // Data members
   //
 
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
-  int warp_idx_m_;
+  /// Iterator to write threadblock-scoped tile of E operand to shared memory
+  SmemIteratorE smem_iterator_E_;
 
-  int warp_idx_n_;
+  /// Warp id
+  bool is_warp_valid_;
 
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  MmaSoftmaxMainloopFusionMultistage(
+  SparseMmaMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       typename Base::SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       Base(shared_storage, thread_idx, warp_idx, lane_idx),
       smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
-      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx)
+      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx),
+      smem_iterator_E_(shared_storage.operand_E_ref(), thread_idx)
   {
+    is_warp_valid_ = warp_idx < Detail::kValidWarps;
+
     // Compute warp location within threadblock tile by mapping the warp_id to
     // three coordinates:
     //   _m: the warp's position within the threadblock along the M dimension
     //   _n: the warp's position within the threadblock along the N dimension
     //   _k: the warp's position within the threadblock along the K dimension
 
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
     int warp_idx_k = warp_idx / (Base::WarpCount::kM * Base::WarpCount::kN);
 
-    warp_idx_m_ = warp_idx_mn % Base::WarpCount::kM;
-    warp_idx_n_ = warp_idx_mn / Base::WarpCount::kM;
+    int warp_idx_m = warp_idx_mn % Base::WarpCount::kM;
+    int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
     this->warp_tile_iterator_A_.add_tile_offset(
-        {warp_idx_m_, Base::kWarpGemmIterations * warp_idx_k});
+        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
     this->warp_tile_iterator_B_.add_tile_offset(
-        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n_});
+        {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
+    this->warp_tile_iterator_E_.add_tile_offset(
+        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
   }
 
   CUTLASS_DEVICE
-  void copy_tiles_and_advance(IteratorA &iterator_A,
-                              IteratorB &iterator_B,
-                              int group_start_A = 0, int group_start_B = 0) {
+  void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B,
+                              IteratorE &iterator_E, int group_start_A = 0,
+                              int group_start_B = 0, int group_start_E = 0) {
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
-    // Async Copy for operand A
+    // async copy for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
-      if (group_start_A + j < Detail::AsyncCopyIterationsPerStageA) {
+      if (group_start_A + j < Detail::TBLDGSTSIterationsA) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA::Element>::value *
                               IteratorA::ThreadMap::kElementsPerAccess /
                               IteratorA::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_A.get();
 
-          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
-                dst_ptr + v, gmem_ptr, iterator_A.valid());
-          } else {
-            cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
-                dst_ptr + v, gmem_ptr, iterator_A.valid());
-          }
+          cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
+              dst_ptr + v, gmem_ptr, iterator_A.valid());
 
           ++iterator_A;
         }
 
         ++this->smem_iterator_A_;
       }
     }
 
     iterator_B.set_iteration_index(group_start_B *
                                    IteratorB::kAccessesPerVector);
     this->smem_iterator_B_.set_iteration_index(group_start_B);
 
-    // Async Copy for operand B
+    // async copy for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
-      if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
+      if (group_start_B + j < Detail::TBLDGSTSIterationsB) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
                               IteratorB::ThreadMap::kElementsPerAccess /
                               IteratorB::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B.get();
 
-          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
-                dst_ptr + v, gmem_ptr, iterator_B.valid());
-          } else {
-            cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
-                dst_ptr + v, gmem_ptr, iterator_B.valid());
-          }
+          cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
+              dst_ptr + v, gmem_ptr, iterator_B.valid());
 
           ++iterator_B;
         }
         ++this->smem_iterator_B_;
       }
     }
+
+    iterator_E.set_iteration_index(group_start_E);
+    this->smem_iterator_E_.set_iteration_index(group_start_E);
+
+    // async copy for operand E
+    CUTLASS_PRAGMA_UNROLL
+    for (int j = 0; j < Detail::kAccessesPerGroupE; ++j) {
+      if (group_start_E + j < Detail::TBLDGSTSIterationsE) {
+        typename IteratorE::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorE::AccessType *>(
+                this->smem_iterator_E_.get());
+
+        int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
+                              IteratorE::ThreadMap::kElementsPerAccess / 8;
+
+        auto gmem_ptr = iterator_E.get();
+
+        cutlass::arch::cp_async<kSrcBytes, kCacheOpE>(
+            dst_ptr, gmem_ptr, iterator_E.valid() && is_warp_valid_);
+
+        ++iterator_E;
+        ++this->smem_iterator_E_;
+      }
+    }
   }
 
   /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
   void operator()(
       ///< problem size of GEMM
       int gemm_k_iterations,
       ///< destination accumulator tile
       FragmentC &accum,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
       ///< iterator over B operand in global memory
       IteratorB iterator_B,
-      ///< iterator over B operand in global memory
-      IteratorNormSum iterator_norm_sum,
+      ///< iterator over E operand in global memory
+      IteratorE iterator_E,
       ///< initial value of accumulator
       FragmentC const &src_accum) {
 
     //
     // Prologue
     //
-    // Issue several complete stages
-
-    WarpLoadedFragmentNormSum warp_loaded_frag_norm_sum;
-    iterator_norm_sum.add_tile_offset({0, warp_idx_m_});
-    iterator_norm_sum.load(warp_loaded_frag_norm_sum);
 
+    // Issue several complete stages
     CUTLASS_PRAGMA_UNROLL
     for (int stage = 0; stage < Base::kStages - 1;
          ++stage, --gemm_k_iterations) {
 
       iterator_A.clear_mask(gemm_k_iterations == 0);
       iterator_B.clear_mask(gemm_k_iterations == 0);
+      iterator_E.clear_mask(gemm_k_iterations == 0);
 
       iterator_A.set_iteration_index(0);
       this->smem_iterator_A_.set_iteration_index(0);
 
-      // Async Copy for operand A
+      // async copy for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsA; ++j) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
           int const kSrcBytes =
               sizeof_bits<typename IteratorA::Element>::value *
               IteratorA::ThreadMap::kElementsPerAccess /
               IteratorA::kAccessesPerVector / 8;
 
-          int src_bytes = (iterator_A.valid() ? kSrcBytes : 0);
-
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
               dst_ptr + v, iterator_A.get(), iterator_A.valid());
 
           ++iterator_A;
         }
 
         ++this->smem_iterator_A_;
       }
 
       iterator_B.set_iteration_index(0);
       this->smem_iterator_B_.set_iteration_index(0);
 
-      // Async Copy for operand B
+      // async copy for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
+      for (int j = 0; j < Detail::TBLDGSTSIterationsB; ++j) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -539,72 +433,87 @@
 
           ++iterator_B;
         }
 
         ++this->smem_iterator_B_;
       }
 
+      iterator_E.set_iteration_index(0);
+      this->smem_iterator_E_.set_iteration_index(0);
+
+      // async copy for operand E
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < Detail::TBLDGSTSIterationsE; ++j) {
+        typename IteratorE::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorE::AccessType *>(
+                this->smem_iterator_E_.get());
+
+        int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
+                              IteratorE::ThreadMap::kElementsPerAccess / 8;
+        if (is_warp_valid_)
+          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpE>(
+              dst_ptr, iterator_E.get(), iterator_E.valid());
+
+        ++iterator_E;
+
+        ++this->smem_iterator_E_;
+      }
+
       // Move to the next stage
       iterator_A.add_tile_offset({0, 1});
       iterator_B.add_tile_offset({1, 0});
+      iterator_E.add_tile_offset({0, 1});
 
       this->smem_iterator_A_.add_tile_offset({0, 1});
       this->smem_iterator_B_.add_tile_offset({1, 0});
+      this->smem_iterator_E_.add_tile_offset({0, 1});
 
-      // Defines the boundary of a stage of cp.async.
+      // LDGDEPBAR - completes a stage
       cutlass::arch::cp_async_fence();
     }
 
     // Perform accumulation in the 'd' output operand
     accum = src_accum;
 
-    // Waits until kStages-2 stages have committed.
+    // DEPBAR+SYNC
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
-    WarpLoadedFragmentB warp_loaded_frag_B[2];
+    WarpLoadedFragmentB warp_loaded_frag_B[Detail::kBBufferSize];
     WarpTransformedFragmentA warp_transformed_frag_A[2];
-    WarpTransformedFragmentB warp_transformed_frag_B[2];
+    WarpTransformedFragmentB warp_transformed_frag_B[Detail::kBBufferSize];
+    WarpFragmentE warp_frag_E[2];
 
     Operator warp_mma;
-    cutlass::gemm::warp::SoftmaxScaleBiasTransform<
-        SoftmaxFragment, WarpLoadedFragmentNormSum> elementwise_transform;
 
     this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.set_kgroup_index(0);
+    this->warp_tile_iterator_E_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
     this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
+    this->warp_tile_iterator_E_.load(warp_frag_E[0]);
 
     ++this->warp_tile_iterator_A_;
     ++this->warp_tile_iterator_B_;
+    ++this->warp_tile_iterator_E_;
 
     iterator_A.clear_mask(gemm_k_iterations == 0);
     iterator_B.clear_mask(gemm_k_iterations == 0);
-
-    // Start issuing the first group of the next stage outside of the mainloop
-    copy_tiles_and_advance(iterator_A, iterator_B);
+    iterator_E.clear_mask(gemm_k_iterations == 0);
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
     warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
                        warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
 
-    if (kInternalTranspose) {
-      elementwise_transform(warp_transformed_frag_B[0],
-                         warp_loaded_frag_norm_sum);
-    } else {
-      elementwise_transform(warp_transformed_frag_A[0],
-                         warp_loaded_frag_norm_sum);
-    }
-
     //
     // Mainloop
     //
 
     CUTLASS_GEMM_LOOP
     for (; gemm_k_iterations > (-Base::kStages + 1);) {
       //
@@ -617,131 +526,134 @@
       for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
            ++warp_mma_k) {
 
         // Load warp-level tiles from shared memory, wrapping to k offset if
         // this is the last group as the case may be.
 
         this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_E_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
         
         this->warp_tile_iterator_A_.load(warp_loaded_frag_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_B_.load(warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_E_.load(warp_frag_E[(warp_mma_k + 1) % 2]);
 
         ++this->warp_tile_iterator_A_;
-        ++this->warp_tile_iterator_B_;
+        ++this->warp_tile_iterator_E_;
 
-        if (warp_mma_k > 0) {
+       if (Detail::kBBufferSize == 2) {
+          this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+          this->warp_tile_iterator_B_.load(
+              warp_loaded_frag_B[(warp_mma_k + 1) % Detail::kBBufferSize]);
+          ++this->warp_tile_iterator_B_;
+        }
+
+        if (warp_mma_k > 0)
           warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                             warp_transformed_frag_B[warp_mma_k % 2],
+                             warp_transformed_frag_B[warp_mma_k % Detail::kBBufferSize],
                              warp_loaded_frag_A[warp_mma_k % 2],
-                             warp_loaded_frag_B[warp_mma_k % 2]);
+                             warp_loaded_frag_B[warp_mma_k % Detail::kBBufferSize]);
+
+        warp_mma(
+          accum,
+          warp_transformed_frag_A[warp_mma_k % 2],
+          warp_transformed_frag_B[warp_mma_k % Detail::kBBufferSize], accum,
+          warp_frag_E[warp_mma_k % 2]
+        );
 
-              if (kInternalTranspose) {
-                elementwise_transform(warp_transformed_frag_B[warp_mma_k % 2],
-                                  warp_loaded_frag_norm_sum);
-              } else {
-                elementwise_transform(warp_transformed_frag_A[warp_mma_k % 2],
-                                  warp_loaded_frag_norm_sum);
-              }
+        if (Detail::kBBufferSize == 1) {
+          this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+          this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
+          ++this->warp_tile_iterator_B_;
+  
         }
 
-        // Issue global->shared copies for the next stage
-        int group_start_iteration_A, group_start_iteration_B;
+        // Issue global->shared copies for the this stage
+        if (warp_mma_k < Base::kWarpGemmIterations - 1) {
+          int group_start_iteration_A, group_start_iteration_B, group_start_iteration_E;
+
+          group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
+          group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
+          group_start_iteration_E = warp_mma_k * Detail::kAccessesPerGroupE;
+
+          copy_tiles_and_advance(
+              iterator_A, iterator_B, iterator_E, group_start_iteration_A,
+              group_start_iteration_B, group_start_iteration_E);
+        }
 
-        if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
-          group_start_iteration_A = 0;
-          group_start_iteration_B = 0;
-        } else {
+        if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
+          int group_start_iteration_A, group_start_iteration_B, group_start_iteration_E;
           group_start_iteration_A =
               (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
           group_start_iteration_B =
               (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
-        }
-
-        copy_tiles_and_advance(iterator_A, iterator_B,
-                               group_start_iteration_A,
-                               group_start_iteration_B);
-
-        warp_mma(
-          accum, 
-          warp_transformed_frag_A[warp_mma_k % 2],
-          warp_transformed_frag_B[warp_mma_k % 2], 
-          accum
-        );
+          group_start_iteration_E =
+              (warp_mma_k + 1) * Detail::kAccessesPerGroupE;
 
-        if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
+          copy_tiles_and_advance(
+              iterator_A, iterator_B, iterator_E, group_start_iteration_A,
+              group_start_iteration_B, group_start_iteration_E);
 
           // Inserts a memory fence between stages of cp.async instructions.
           cutlass::arch::cp_async_fence();
 
-          // Waits until kStages-2 stages have committed.
+          // Waits until kStages-2 stages have committed. 
           arch::cp_async_wait<Base::kStages - 2>();
           __syncthreads();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
           iterator_B.add_tile_offset({1, 0});
+          iterator_E.add_tile_offset({0, 1});
 
           this->smem_iterator_A_.add_tile_offset({0, 1});
           this->smem_iterator_B_.add_tile_offset({1, 0});
+          this->smem_iterator_E_.add_tile_offset({0, 1});
 
           // Add negative offsets to return iterators to the 'start' of the
           // circular buffer in shared memory
           if (smem_write_stage_idx == (Base::kStages - 1)) {
             this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
             this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
+            this->smem_iterator_E_.add_tile_offset({0, -Base::kStages});
             smem_write_stage_idx = 0;
           } else {
             ++smem_write_stage_idx;
           }
 
           if (smem_read_stage_idx == (Base::kStages - 1)) {
             this->warp_tile_iterator_A_.add_tile_offset(
                 {0, -Base::kStages * Policy::kPartitionsK *
                         Base::kWarpGemmIterations});
             this->warp_tile_iterator_B_.add_tile_offset(
                 {-Base::kStages * Policy::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
+            this->warp_tile_iterator_E_.add_tile_offset(
+                {0, -Base::kStages * Policy::kPartitionsK *
+                        Base::kWarpGemmIterations});
             smem_read_stage_idx = 0;
           } else {
             ++smem_read_stage_idx;
           }
 
           --gemm_k_iterations;
           iterator_A.clear_mask(gemm_k_iterations == 0);
           iterator_B.clear_mask(gemm_k_iterations == 0);
+          iterator_E.clear_mask(gemm_k_iterations == 0);
         }
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
-        if (warp_mma_k + 1 == Base::kWarpGemmIterations) {
+        if (warp_mma_k + 1 == Base::kWarpGemmIterations)
           warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
                              warp_transformed_frag_B[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
-
-              if (kInternalTranspose) {
-                elementwise_transform(warp_transformed_frag_B[(warp_mma_k + 1) % 2],
-                                  warp_loaded_frag_norm_sum);
-              } else {
-                elementwise_transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
-                                  warp_loaded_frag_norm_sum);
-              }
-        }
       }
 
     }
-    
-    if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
-      // commit and drain all pending and predicated cp.async pnz from the GEMM mainloop
-      cutlass::arch::cp_async_fence();
-      cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
-    }
 
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_base.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h`

 * *Files 13% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,15 +38,15 @@
 #include "cutlass/arch/memory.h"
 #include "cutlass/array.h"
 #include "cutlass/cutlass.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/numeric_types.h"
 
-#include "cutlass/gemm/threadblock/mma_sparse_base.h"
+#include "cutlass/gemm/threadblock/mma_base.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
@@ -75,182 +75,125 @@
     typename SmemIteratorB_,
     /// Cache operation for operand B
     cutlass::arch::CacheOperation::Kind CacheOpB,
     /// Data type of accumulator matrix
     typename ElementC_,
     /// Data type of accumulator matrix
     typename LayoutC_,
-    /// Iterates over tiles of E operand in global memory
-    //  (concept: ReadableTileIterator | ForwardTileIterator |
-    //  MaskedTileIterator)
-    typename IteratorE_,
-    /// Iterates over tiles of E operand in shared memory
-    /// (concept: WriteableTileIterator | RandomAccessTileIterator)
-    typename SmemIteratorE_,
-    /// Cache operation for operand E
-    cutlass::arch::CacheOperation::Kind CacheOpE,
     /// Policy describing tuning details (concept: MmaPolicy)
     typename Policy_,
     /// Number of stages,
     int Stages,
+    /// Use zfill or predicate for out-of-bound cp.async
+    SharedMemoryClearOption SharedMemoryClear = SharedMemoryClearOption::kNone,
     /// Used for partial specialization
     typename Enable = bool>
-class SparseMmaMultistage : 
-  public SparseMmaBase<Shape_, Policy_, Stages> {
+class MmaMultistage : 
+  public MmaBase<Shape_, Policy_, Stages> {
 public:
   ///< Base class
-  using Base = SparseMmaBase<Shape_, Policy_, Stages>;
+  using Base = MmaBase<Shape_, Policy_, Stages>;
   ///< Size of the Gemm problem - concept: gemm::GemmShape<>
   using Shape = Shape_;
   ///< Iterates over tiles of A operand in global memory
   using IteratorA = IteratorA_;
   ///< Iterates over tiles of B operand in global memory
   using IteratorB = IteratorB_;
-  ///< Iterates over tiles of E operand in global memory
-  using IteratorE = IteratorE_;
   ///< Data type of accumulator matrix
   using ElementC = ElementC_;
   ///< Layout of accumulator matrix
   using LayoutC = LayoutC_;
   ///< Policy describing tuning details
   using Policy = Policy_;
 
   using SmemIteratorA = SmemIteratorA_;
   using SmemIteratorB = SmemIteratorB_;
-  using SmemIteratorE = SmemIteratorE_;
 
   static cutlass::arch::CacheOperation::Kind const kCacheOpA = CacheOpA;
   static cutlass::arch::CacheOperation::Kind const kCacheOpB = CacheOpB;
-  static cutlass::arch::CacheOperation::Kind const kCacheOpE = CacheOpE;
-
-  static int const kSparse = Policy::Operator::kSparse;
-  static int const kMetaSizeInBits = Policy::Operator::kMetaSizeInBits;
-  static int const kMaxID2 = Policy::Operator::kMaxID2;
-  static int const kElementsPerElementE =
-      Policy::Operator::kElementsPerElementE;
 
   //
   // Dependent types
   //
 
   /// Fragment of accumulator tile
   using FragmentC = typename Policy::Operator::FragmentC;
 
   /// Warp-level Mma
   using Operator = typename Policy::Operator;
 
-  /// ElementE
-  using ElementE = typename IteratorE::Element;
-
-  /// LayoutE
-  using LayoutE = typename IteratorE::Layout; 
-
   /// Minimum architecture is Sm80 to support cp.async
   using ArchTag = arch::Sm80;
   
   /// Complex transform on A operand
   static ComplexTransform const kTransformA = Operator::kTransformA;
 
   /// Complex transform on B operand
   static ComplexTransform const kTransformB = Operator::kTransformB;
 
   /// Internal structure exposed for introspection.
   struct Detail {
 
-    /// Number of async copies to load one stage of operand A
-    static int const TBLoadIterationsA =
+    /// Number of cp.async instructions to load one stage of operand A
+    static int const AsyncCopyIterationsPerStageA =
         IteratorA::ThreadMap::Iterations::kCount;
 
-    /// Number of async copies to load one stage of operand B
-    static int const TBLoadIterationsB =
+    /// Number of cp.async instructions to load one stage of operand B
+    static int const AsyncCopyIterationsPerStageB =
         IteratorB::ThreadMap::Iterations::kCount;
 
-    /// Number of async copies to load one stage of operand E
-    static int const TBLoadIterationsE =
-        IteratorE::ThreadMap::Iterations::kCount;
-
     /// Number of stages
     static int const kStages = Stages;
 
-    /// Number of async copies to load one group of operand A
+    /// Number of cp.async instructions to load on group of operand A
     static int const kAccessesPerGroupA =
-        (TBLoadIterationsA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
+        (AsyncCopyIterationsPerStageA + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
 
-    /// Number of async copies to load one group of operand B
+    /// Number of cp.async instructions to load on group of operand B
     static int const kAccessesPerGroupB =
-        (TBLoadIterationsB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
-
-    /// Number of async copies to load one group of operand E
-    static int const kAccessesPerGroupE =
-        (TBLoadIterationsE + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
-
-    /// E operand is tiny.  For the most of time, not all the warps are needed
-    /// to load it from the global memory.
-    static int const kValidWarps = IteratorE::ThreadMap::kThreads / 32;
-
-    /// B operand is twice as big as A which brings very high register pressure.
-    /// We have to sacrifice the double buffer when the warp tile size is big.
-    static int const kBBufferSize =
-        ((sizeof(typename Operator::ElementC) == 4) &&
-         ((platform::is_same<typename Operator::Policy::Operator::ElementA,
-                             typename Operator::ElementA>::value &&
-           platform::is_same<typename Operator::Policy::Operator::ElementB,
-                             typename Operator::ElementB>::value)) &&
-         (Operator::Shape::kM >= 64 && Operator::Shape::kN >= 64))
-            ? 1
-            : 2;
+        (AsyncCopyIterationsPerStageB + Base::kWarpGemmIterations - 1) / Base::kWarpGemmIterations;
   };
 
  private:
 
   using WarpLoadedFragmentA = typename Operator::FragmentA;
   using WarpLoadedFragmentB = typename Operator::FragmentB;
   using WarpTransformedFragmentA = typename Operator::TransformedFragmentA;
   using WarpTransformedFragmentB = typename Operator::TransformedFragmentB;
-  using WarpFragmentE = typename Operator::FragmentE;
 
  private:
 
   //
   // Data members
   //
 
   /// Iterator to write threadblock-scoped tile of A operand to shared memory
   SmemIteratorA smem_iterator_A_;
 
   /// Iterator to write threadblock-scoped tile of B operand to shared memory
   SmemIteratorB smem_iterator_B_;
 
-  /// Iterator to write threadblock-scoped tile of E operand to shared memory
-  SmemIteratorE smem_iterator_E_;
-
-  /// Warp id
-  bool is_warp_valid_;
-
 public:
 
   /// Construct from tensor references
   CUTLASS_DEVICE
-  SparseMmaMultistage(
+  MmaMultistage(
       ///< Shared storage needed for internal use by threadblock-scoped GEMM
       typename Base::SharedStorage &shared_storage,
       ///< ID within the threadblock
       int thread_idx,
       ///< ID of warp
       int warp_idx,
       ///< ID of each thread within a warp
       int lane_idx
     ):
       Base(shared_storage, thread_idx, warp_idx, lane_idx),
       smem_iterator_A_(shared_storage.operand_A_ref(), thread_idx),
-      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx),
-      smem_iterator_E_(shared_storage.operand_E_ref(), thread_idx)
+      smem_iterator_B_(shared_storage.operand_B_ref(), thread_idx)
   {
-    is_warp_valid_ = warp_idx < Detail::kValidWarps;
-
     // Compute warp location within threadblock tile by mapping the warp_id to
     // three coordinates:
     //   _m: the warp's position within the threadblock along the M dimension
     //   _n: the warp's position within the threadblock along the N dimension
     //   _k: the warp's position within the threadblock along the K dimension
 
     int warp_idx_mn = warp_idx % (Base::WarpCount::kM * Base::WarpCount::kN);
@@ -260,167 +203,149 @@
     int warp_idx_n = warp_idx_mn / Base::WarpCount::kM;
 
     // Add per-warp offsets in units of warp-level tiles
     this->warp_tile_iterator_A_.add_tile_offset(
         {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
     this->warp_tile_iterator_B_.add_tile_offset(
         {Base::kWarpGemmIterations * warp_idx_k, warp_idx_n});
-    this->warp_tile_iterator_E_.add_tile_offset(
-        {warp_idx_m, Base::kWarpGemmIterations * warp_idx_k});
   }
 
   CUTLASS_DEVICE
   void copy_tiles_and_advance(IteratorA &iterator_A, IteratorB &iterator_B,
-                              IteratorE &iterator_E, int group_start_A = 0,
-                              int group_start_B = 0, int group_start_E = 0) {
+                              int group_start_A = 0, int group_start_B = 0) {
     iterator_A.set_iteration_index(group_start_A *
                                    IteratorA::kAccessesPerVector);
     this->smem_iterator_A_.set_iteration_index(group_start_A);
 
-    // async copy for operand A
+    // Async Copy for operand A
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupA; ++j) {
-      if (group_start_A + j < Detail::TBLoadIterationsA) {
+      if (group_start_A + j < Detail::AsyncCopyIterationsPerStageA) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorA::Element>::value *
                               IteratorA::ThreadMap::kElementsPerAccess /
                               IteratorA::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_A.get();
 
-          cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
-              dst_ptr + v, gmem_ptr, iterator_A.valid());
+          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
+                dst_ptr + v, gmem_ptr, iterator_A.valid());
+          } else {
+            cutlass::arch::cp_async<kSrcBytes, kCacheOpA>(
+                dst_ptr + v, gmem_ptr, iterator_A.valid());
+          }
 
           ++iterator_A;
         }
 
         ++this->smem_iterator_A_;
       }
     }
 
     iterator_B.set_iteration_index(group_start_B *
                                    IteratorB::kAccessesPerVector);
     this->smem_iterator_B_.set_iteration_index(group_start_B);
 
-    // async copy for operand B
+    // Async Copy for operand B
     CUTLASS_PRAGMA_UNROLL
     for (int j = 0; j < Detail::kAccessesPerGroupB; ++j) {
-      if (group_start_B + j < Detail::TBLoadIterationsB) {
+      if (group_start_B + j < Detail::AsyncCopyIterationsPerStageB) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         int const kSrcBytes = sizeof_bits<typename IteratorB::Element>::value *
                               IteratorB::ThreadMap::kElementsPerAccess /
                               IteratorB::kAccessesPerVector / 8;
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           auto gmem_ptr = iterator_B.get();
 
-          cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
-              dst_ptr + v, gmem_ptr, iterator_B.valid());
+          if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+            cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpB>(
+                dst_ptr + v, gmem_ptr, iterator_B.valid());
+          } else {
+            cutlass::arch::cp_async<kSrcBytes, kCacheOpB>(
+                dst_ptr + v, gmem_ptr, iterator_B.valid());
+          }
 
           ++iterator_B;
         }
         ++this->smem_iterator_B_;
       }
     }
-
-    iterator_E.set_iteration_index(group_start_E);
-    this->smem_iterator_E_.set_iteration_index(group_start_E);
-
-    // async copy for operand E
-    CUTLASS_PRAGMA_UNROLL
-    for (int j = 0; j < Detail::kAccessesPerGroupE; ++j) {
-      if (group_start_E + j < Detail::TBLoadIterationsE) {
-        typename IteratorE::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorE::AccessType *>(
-                this->smem_iterator_E_.get());
-
-        int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
-                              IteratorE::ThreadMap::kElementsPerAccess / 8;
-
-        auto gmem_ptr = iterator_E.get();
-
-        cutlass::arch::cp_async<kSrcBytes, kCacheOpE>(
-            dst_ptr, gmem_ptr, iterator_E.valid() && is_warp_valid_);
-
-        ++iterator_E;
-        ++this->smem_iterator_E_;
-      }
-    }
   }
 
   /// Perform a threadblock-scoped matrix multiply-accumulate
   CUTLASS_DEVICE
   void operator()(
       ///< problem size of GEMM
       int gemm_k_iterations,
       ///< destination accumulator tile
       FragmentC &accum,
       ///< iterator over A operand in global memory
       IteratorA iterator_A,
       ///< iterator over B operand in global memory
       IteratorB iterator_B,
-      ///< iterator over E operand in global memory
-      IteratorE iterator_E,
       ///< initial value of accumulator
       FragmentC const &src_accum) {
 
     //
     // Prologue
     //
 
     // Issue several complete stages
     CUTLASS_PRAGMA_UNROLL
     for (int stage = 0; stage < Base::kStages - 1;
          ++stage, --gemm_k_iterations) {
 
       iterator_A.clear_mask(gemm_k_iterations == 0);
       iterator_B.clear_mask(gemm_k_iterations == 0);
-      iterator_E.clear_mask(gemm_k_iterations == 0);
 
       iterator_A.set_iteration_index(0);
       this->smem_iterator_A_.set_iteration_index(0);
 
-      // async copy for operand A
+      // Async Copy for operand A
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsA; ++j) {
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
         typename IteratorA::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorA::AccessType *>(
                 this->smem_iterator_A_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorA::kAccessesPerVector; ++v) {
           int const kSrcBytes =
               sizeof_bits<typename IteratorA::Element>::value *
               IteratorA::ThreadMap::kElementsPerAccess /
               IteratorA::kAccessesPerVector / 8;
 
+          int src_bytes = (iterator_A.valid() ? kSrcBytes : 0);
+
           cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpA>(
               dst_ptr + v, iterator_A.get(), iterator_A.valid());
 
           ++iterator_A;
         }
 
         ++this->smem_iterator_A_;
       }
 
       iterator_B.set_iteration_index(0);
       this->smem_iterator_B_.set_iteration_index(0);
 
-      // async copy for operand B
+      // Async Copy for operand B
       CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsB; ++j) {
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
         typename IteratorB::AccessType *dst_ptr =
             reinterpret_cast<typename IteratorB::AccessType *>(
                 this->smem_iterator_B_.get());
 
         CUTLASS_PRAGMA_UNROLL
         for (int v = 0; v < IteratorB::kAccessesPerVector; ++v) {
           int const kSrcBytes =
@@ -433,86 +358,123 @@
 
           ++iterator_B;
         }
 
         ++this->smem_iterator_B_;
       }
 
-      iterator_E.set_iteration_index(0);
-      this->smem_iterator_E_.set_iteration_index(0);
-
-      // async copy for operand E
-      CUTLASS_PRAGMA_UNROLL
-      for (int j = 0; j < Detail::TBLoadIterationsE; ++j) {
-        typename IteratorE::AccessType *dst_ptr =
-            reinterpret_cast<typename IteratorE::AccessType *>(
-                this->smem_iterator_E_.get());
-
-        int const kSrcBytes = sizeof_bits<typename IteratorE::Element>::value *
-                              IteratorE::ThreadMap::kElementsPerAccess / 8;
-        if (is_warp_valid_)
-          cutlass::arch::cp_async_zfill<kSrcBytes, kCacheOpE>(
-              dst_ptr, iterator_E.get(), iterator_E.valid());
-
-        ++iterator_E;
-
-        ++this->smem_iterator_E_;
-      }
-
       // Move to the next stage
       iterator_A.add_tile_offset({0, 1});
       iterator_B.add_tile_offset({1, 0});
-      iterator_E.add_tile_offset({0, 1});
 
       this->smem_iterator_A_.add_tile_offset({0, 1});
       this->smem_iterator_B_.add_tile_offset({1, 0});
-      this->smem_iterator_E_.add_tile_offset({0, 1});
 
-      // cp.async.commit_group - completes a stage
+      // Defines the boundary of a stage of cp.async.
       cutlass::arch::cp_async_fence();
     }
 
     // Perform accumulation in the 'd' output operand
     accum = src_accum;
 
+    //
+    // Clear the remaining tiles of SMEM. This is a functional requirement for some kernels
+    // so that all accumulator elements outside the GEMM footprint are zero.
+    //
+
+    if (SharedMemoryClear == SharedMemoryClearOption::kClearLastStage) {
+
+      /// Iterator to write threadblock-scoped tile of A operand to shared memory
+      SmemIteratorA last_smem_iterator_A(this->smem_iterator_A_);
+
+      typename IteratorA::AccessType zero_A;
+      zero_A.clear();
+
+      last_smem_iterator_A.set_iteration_index(0);
+
+      // Async Copy for operand A
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageA; ++j) {
+
+        typename IteratorA::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorA::AccessType *>(
+                last_smem_iterator_A.get());
+
+        *dst_ptr = zero_A;
+
+        ++last_smem_iterator_A;
+      }
+
+      /// Iterator to write threadblock-scoped tile of B operand to shared memory
+      SmemIteratorB last_smem_iterator_B(this->smem_iterator_B_);
+      typename IteratorB::AccessType zero_B;
+
+      zero_B.clear();
+      last_smem_iterator_B.set_iteration_index(0);
+
+      // Async Copy for operand B
+      CUTLASS_PRAGMA_UNROLL
+      for (int j = 0; j < Detail::AsyncCopyIterationsPerStageB; ++j) {
+
+        typename IteratorB::AccessType *dst_ptr =
+            reinterpret_cast<typename IteratorB::AccessType *>(
+                last_smem_iterator_B.get());
+
+        *dst_ptr = zero_B;
+
+        ++last_smem_iterator_B;
+      }
+    }
+
+    // Waits until stages up to the previous (kStages-2)th stage have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
     __syncthreads();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
     WarpLoadedFragmentA warp_loaded_frag_A[2];
-    WarpLoadedFragmentB warp_loaded_frag_B[Detail::kBBufferSize];
+    WarpLoadedFragmentB warp_loaded_frag_B[2];
     WarpTransformedFragmentA warp_transformed_frag_A[2];
-    WarpTransformedFragmentB warp_transformed_frag_B[Detail::kBBufferSize];
-    WarpFragmentE warp_frag_E[2];
+    WarpTransformedFragmentB warp_transformed_frag_B[2];
 
     Operator warp_mma;
 
     this->warp_tile_iterator_A_.set_kgroup_index(0);
     this->warp_tile_iterator_B_.set_kgroup_index(0);
-    this->warp_tile_iterator_E_.set_kgroup_index(0);
 
     this->warp_tile_iterator_A_.load(warp_loaded_frag_A[0]);
     this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
-    this->warp_tile_iterator_E_.load(warp_frag_E[0]);
 
     ++this->warp_tile_iterator_A_;
     ++this->warp_tile_iterator_B_;
-    ++this->warp_tile_iterator_E_;
 
     iterator_A.clear_mask(gemm_k_iterations == 0);
     iterator_B.clear_mask(gemm_k_iterations == 0);
-    iterator_E.clear_mask(gemm_k_iterations == 0);
 
     int smem_write_stage_idx = Base::kStages - 1;
     int smem_read_stage_idx = 0;
 
     warp_mma.transform(warp_transformed_frag_A[0], warp_transformed_frag_B[0],
                        warp_loaded_frag_A[0], warp_loaded_frag_B[0]);
 
+    // tf32x3 kernels use staging accumulation. warp_mma uses a temporary
+    // accumulator and this temporary accumulator is added to the final
+    // accumulator once in every mainloop iteration.
+    plus<FragmentC> plus_accum;
+
+    FragmentC tmp_accum;
+
+    if (platform::is_same<typename Operator::MathOperator,
+                          arch::OpMultiplyAddFastF32>::value
+      || platform::is_same<typename Operator::MathOperator,
+                           arch::OpMultiplyAddComplexFastF32>::value) {
+
+      tmp_accum.clear();
+    }
+
     //
     // Mainloop
     //
 
     CUTLASS_GEMM_LOOP
     for (; gemm_k_iterations > (-Base::kStages + 1);) {
       //
@@ -525,135 +487,141 @@
       for (int warp_mma_k = 0; warp_mma_k < Base::kWarpGemmIterations;
            ++warp_mma_k) {
 
         // Load warp-level tiles from shared memory, wrapping to k offset if
         // this is the last group as the case may be.
 
         this->warp_tile_iterator_A_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-        this->warp_tile_iterator_E_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
+        this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
         
         this->warp_tile_iterator_A_.load(warp_loaded_frag_A[(warp_mma_k + 1) % 2]);
-        this->warp_tile_iterator_E_.load(warp_frag_E[(warp_mma_k + 1) % 2]);
+        this->warp_tile_iterator_B_.load(warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
 
         ++this->warp_tile_iterator_A_;
-        ++this->warp_tile_iterator_E_;
-
-       if (Detail::kBBufferSize == 2) {
-          this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-          this->warp_tile_iterator_B_.load(
-              warp_loaded_frag_B[(warp_mma_k + 1) % Detail::kBBufferSize]);
-          ++this->warp_tile_iterator_B_;
-        }
+        ++this->warp_tile_iterator_B_;
 
         if (warp_mma_k > 0)
           warp_mma.transform(warp_transformed_frag_A[warp_mma_k % 2],
-                             warp_transformed_frag_B[warp_mma_k % Detail::kBBufferSize],
+                             warp_transformed_frag_B[warp_mma_k % 2],
                              warp_loaded_frag_A[warp_mma_k % 2],
-                             warp_loaded_frag_B[warp_mma_k % Detail::kBBufferSize]);
+                             warp_loaded_frag_B[warp_mma_k % 2]);
 
-        warp_mma(
-          accum,
-          warp_transformed_frag_A[warp_mma_k % 2],
-          warp_transformed_frag_B[warp_mma_k % Detail::kBBufferSize], accum,
-          warp_frag_E[warp_mma_k % 2]
-        );
-
-        if (Detail::kBBufferSize == 1) {
-          this->warp_tile_iterator_B_.set_kgroup_index((warp_mma_k + 1) % Base::kWarpGemmIterations);
-          this->warp_tile_iterator_B_.load(warp_loaded_frag_B[0]);
-          ++this->warp_tile_iterator_B_;
-  
+        if (platform::is_same<typename Operator::MathOperator,
+                              arch::OpMultiplyAddFastF32>::value
+          || platform::is_same<typename Operator::MathOperator,
+                               arch::OpMultiplyAddComplexFastF32>::value) {
+
+          warp_mma(
+            tmp_accum, 
+            warp_transformed_frag_A[warp_mma_k % 2],
+            warp_transformed_frag_B[warp_mma_k % 2], 
+            tmp_accum
+          );
+
+          if (warp_mma_k == 0) {
+            accum = plus_accum(accum, tmp_accum);
+            tmp_accum.clear();
+          }
+        } else {
+          warp_mma(
+            accum, 
+            warp_transformed_frag_A[warp_mma_k % 2],
+            warp_transformed_frag_B[warp_mma_k % 2], 
+            accum
+          );
         }
 
         // Issue global->shared copies for the this stage
         if (warp_mma_k < Base::kWarpGemmIterations - 1) {
-          int group_start_iteration_A, group_start_iteration_B, group_start_iteration_E;
+          int group_start_iteration_A, group_start_iteration_B;
 
           group_start_iteration_A = warp_mma_k * Detail::kAccessesPerGroupA;
           group_start_iteration_B = warp_mma_k * Detail::kAccessesPerGroupB;
-          group_start_iteration_E = warp_mma_k * Detail::kAccessesPerGroupE;
 
-          copy_tiles_and_advance(
-              iterator_A, iterator_B, iterator_E, group_start_iteration_A,
-              group_start_iteration_B, group_start_iteration_E);
+          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
+                               group_start_iteration_B);
         }
 
         if (warp_mma_k + 2 == Base::kWarpGemmIterations) {
-          int group_start_iteration_A, group_start_iteration_B, group_start_iteration_E;
+          int group_start_iteration_A, group_start_iteration_B;
           group_start_iteration_A =
               (warp_mma_k + 1) * Detail::kAccessesPerGroupA;
           group_start_iteration_B =
               (warp_mma_k + 1) * Detail::kAccessesPerGroupB;
-          group_start_iteration_E =
-              (warp_mma_k + 1) * Detail::kAccessesPerGroupE;
 
-          copy_tiles_and_advance(
-              iterator_A, iterator_B, iterator_E, group_start_iteration_A,
-              group_start_iteration_B, group_start_iteration_E);
+          copy_tiles_and_advance(iterator_A, iterator_B, group_start_iteration_A, 
+                               group_start_iteration_B);
 
           // Inserts a memory fence between stages of cp.async instructions.
           cutlass::arch::cp_async_fence();
 
-          // Waits until kStages-2 stages have committed. 
+          // Waits until stages up to the previous (kStages-2)th stage have committed.
           arch::cp_async_wait<Base::kStages - 2>();
           __syncthreads();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
           iterator_B.add_tile_offset({1, 0});
-          iterator_E.add_tile_offset({0, 1});
 
           this->smem_iterator_A_.add_tile_offset({0, 1});
           this->smem_iterator_B_.add_tile_offset({1, 0});
-          this->smem_iterator_E_.add_tile_offset({0, 1});
 
           // Add negative offsets to return iterators to the 'start' of the
           // circular buffer in shared memory
           if (smem_write_stage_idx == (Base::kStages - 1)) {
             this->smem_iterator_A_.add_tile_offset({0, -Base::kStages});
             this->smem_iterator_B_.add_tile_offset({-Base::kStages, 0});
-            this->smem_iterator_E_.add_tile_offset({0, -Base::kStages});
             smem_write_stage_idx = 0;
           } else {
             ++smem_write_stage_idx;
           }
 
           if (smem_read_stage_idx == (Base::kStages - 1)) {
             this->warp_tile_iterator_A_.add_tile_offset(
                 {0, -Base::kStages * Policy::kPartitionsK *
                         Base::kWarpGemmIterations});
             this->warp_tile_iterator_B_.add_tile_offset(
                 {-Base::kStages * Policy::kPartitionsK *
                      Base::kWarpGemmIterations,
                  0});
-            this->warp_tile_iterator_E_.add_tile_offset(
-                {0, -Base::kStages * Policy::kPartitionsK *
-                        Base::kWarpGemmIterations});
             smem_read_stage_idx = 0;
           } else {
             ++smem_read_stage_idx;
           }
 
           --gemm_k_iterations;
           iterator_A.clear_mask(gemm_k_iterations == 0);
           iterator_B.clear_mask(gemm_k_iterations == 0);
-          iterator_E.clear_mask(gemm_k_iterations == 0);
         }
 
         // Do any conversions feeding the first stage at the end of the loop so
         // we can start right away on mma instructions
         if (warp_mma_k + 1 == Base::kWarpGemmIterations)
           warp_mma.transform(warp_transformed_frag_A[(warp_mma_k + 1) % 2],
                              warp_transformed_frag_B[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_A[(warp_mma_k + 1) % 2],
                              warp_loaded_frag_B[(warp_mma_k + 1) % 2]);
       }
 
     }
 
+    if (platform::is_same<typename Operator::MathOperator,
+                          arch::OpMultiplyAddFastF32>::value
+      || platform::is_same<typename Operator::MathOperator,
+                           arch::OpMultiplyAddComplexFastF32>::value) {
+      accum = plus_accum(accum, tmp_accum); 
+    }
+ 
+    if (SharedMemoryClear == SharedMemoryClearOption::kZfill) {
+      // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
+      cutlass::arch::cp_async_fence();
+      cutlass::arch::cp_async_wait<0>();
+      __syncthreads();
+    }
+
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
 }  // namespace gemm
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle.h`

 * *Files 17% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -37,25 +37,79 @@
 
 #include "cutlass/cutlass.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/platform/platform.h"
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/conv/conv2d_problem_size.h"
 #include "cutlass/conv/conv3d_problem_size.h"
-#include "cutlass/gemm/threadblock/index_remat.h"
-#include "cutlass/gemm/threadblock/threadblock_swizzle_streamk.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace threadblock {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeThreadIdxX() {
+  return threadIdx.x;
+}
+
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeThreadIdxY() {
+  return threadIdx.y;
+}
+
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeThreadIdxZ() {
+  return threadIdx.z;
+}
+
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockIdxX() {
+  return blockIdx.x;
+}
+
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockIdxY() {
+  return blockIdx.y;
+}
+
+/// Helper to rematerialize block Idx. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockIdxZ() {
+  return blockIdx.z;
+}
+
+/// Helper to rematerialize block Dim. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockDimX() {
+  return blockDim.x;
+}
+
+/// Helper to rematerialize block Dim. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockDimY() {
+  return blockDim.y;
+}
+
+/// Helper to rematerialize block Dim. Reduces register liveness.
+CUTLASS_DEVICE
+int RematerializeBlockDimZ() {
+  return blockDim.z;
+}
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 /// Threadblock swizzling function for GEMMs
 template <int N = 1>
 struct GemmIdentityThreadblockSwizzle {
 
   CUTLASS_HOST_DEVICE
   GemmIdentityThreadblockSwizzle() { }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/threadblock/threadblock_swizzle_streamk.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_complex_tensor_op.h`

 * *Files 11% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -210,15 +210,15 @@
     TransformB>;
 };
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use TF32 tensor operation internally
-//  4 real-valued mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 operations on TF32 
+//  4 real-valued MMA.1688.F32.TF32 operations on TF32 
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -242,15 +242,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddComplex> {
 
-  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 mma instruction
+  // Complex floating point tensor operation use MMA.1688.F32.TF32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         tfloat32_t,
         cutlass::layout::RowMajor,
         tfloat32_t,
@@ -274,15 +274,15 @@
     TransformA,
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use BF16 tensor operation internally
-//  4 real-valued mma.sync.aligned.m16n8k8.f32.bf16.bf16.f32 operations on BF16
+//  4 real-valued MMA.1688.F32.BF16 operations on BF16
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -306,15 +306,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddFastBF16> {
 
-  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.bf16.bf16.f32 mma instruction
+  // Complex floating point tensor operation use MMA.1688.F32.BF16 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         bfloat16_t,
         cutlass::layout::RowMajor,
         bfloat16_t,
@@ -338,15 +338,15 @@
     TransformA,
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Partial specialization - input and output types are complex<float>*complex<float> 
 //  Use F16 tensor operation internally
-//  4 real-valued mma.sync.aligned.m16n8k8.f32.f16.f16.f32 operations on F16
+//  4 real-valued MMA.1688.F32.F16 operations on F16
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
     typename WarpShape_,
     /// Shape of one matrix production operation (concept: GemmShape)
@@ -370,15 +370,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddFastF16> {
 
-  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.f16.f16.f32 mma instruction
+  // Complex floating point tensor operation use MMA.1688.F32.F16 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         half_t,
         cutlass::layout::RowMajor,
         half_t,
@@ -403,15 +403,15 @@
     TransformB>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// 3xTF32 or 4xTF32 (fast and accurate complex<float> operation)
 /// Partial specialization - input and output types are complex<float> * complex<float> 
 //  Use 3xTF32 or 4xTF32 tensor operation internally
-//  4 real-valued mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 operations on TF32 
+//  4 real-valued MMA.1688.F32.TF32 operations on TF32 
 //  A = (ar + j ai), B (br +j bi), D = AB
 //  D = dr + j di = 3x[(ar*br - ai*bi) + j (ar*bi + ai*br)]
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
     /// Size of the Gemm problem - concept: gemm::GemmShape<>
@@ -437,15 +437,15 @@
     LayoutB,
     complex<float>,
     LayoutC,
     TransformA,
     TransformB,
     arch::OpMultiplyAddComplexFastF32> {
 
-  // Complex floating point tensor operation use mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 mma instruction
+  // Complex floating point tensor operation use MMA.1688.F32.TF32 mma instruction
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
       cutlass::arch::Mma<
         InstructionShape_, 
         32, 
         tfloat32_t,
         cutlass::layout::RowMajor,
         tfloat32_t,
@@ -466,147 +466,10 @@
     complex<float>,
     LayoutC, 
     Policy,
     TransformA,
     TransformB>;
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for complex<double>*complex<double> case
-//  4 real-valued mma.sync.aligned.m16n8k4.f64.f64.f64.f64 operations
-//  A = (ar + j ai), B (br +j bi), D = AB
-//  D = dr + j di = (ar*br - ai*bi) + j (ar*bi + ai*br) 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename WarpShape_,
-    /// Real-valued underlying type of complex-valued A operand
-    typename RealElementA,
-    /// Layout of A matrix (concept: MatrixLayout)
-    typename LayoutA,
-    /// Real-valued underlying type of complex-valued B operand
-    typename RealElementB,
-    /// Layout of B matrix (concept: MatrixLayout)
-    typename LayoutB,
-    /// Real-valued underlying type of complex-valued C operand
-    typename RealElementC,
-    /// Layout of C matrix (concept: MatrixLayout)
-    typename LayoutC,
-    /// Complex transform on A operand
-    ComplexTransform TransformA,
-    /// Complex transform on B operand
-    ComplexTransform TransformB>
-struct DefaultMmaComplexTensorOp<
-    WarpShape_,
-    GemmShape<16, 8, 4>,
-    complex<RealElementA>,
-    LayoutA,
-    complex<RealElementB>,
-    LayoutB,
-    complex<RealElementC>,
-    LayoutC,
-    TransformA,
-    TransformB,
-    arch::OpMultiplyAddComplex> {
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-      cutlass::arch::Mma<
-        GemmShape<16, 8, 4>,
-        32, 
-        RealElementA,
-        cutlass::layout::RowMajor,
-        RealElementB,
-        cutlass::layout::ColumnMajor,
-        RealElementC,
-        cutlass::layout::RowMajor, 
-        arch::OpMultiplyAdd>,
-      cutlass::MatrixShape<1, 1>
-    >;
-
-  // Define the warp-level tensor op
-  using Type = cutlass::gemm::warp::MmaComplexTensorOp<
-    WarpShape_,
-    complex<RealElementA>,
-    LayoutA,
-    complex<RealElementB>,
-    LayoutB,
-    complex<RealElementC>,
-    LayoutC, 
-    Policy,
-    TransformA,
-    TransformB,
-    true>;
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// Partial specialization for complex<T>*complex<T> case using GaussianComplex operation
-//  3 real-valued mma.sync.aligned.m16n8k4.f64.f64.f64.f64 operations 
-//  A  = (ar + j ai), B = (br +j bi), D = AB
-//  P1 = (ar + ai) * br, P2 = - ar * (br - bi), P3 = ai * (br + bi) 
-//  D  = dr + j di = (P1 - P3) + j (P1 + P2)
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <
-    /// Size of the Gemm problem - concept: gemm::GemmShape<>
-    typename WarpShape_,
-    /// Real-valued underlying type of complex-valued A operand
-    typename RealElementA,
-    /// Layout of A matrix (concept: MatrixLayout)
-    typename LayoutA,
-    /// Real-valued underlying type of complex-valued B operand
-    typename RealElementB,
-    /// Layout of B matrix (concept: MatrixLayout)
-    typename LayoutB,
-    /// Real-valued underlying type of complex-valued C operand
-    typename RealElementC,
-    /// Layout of C matrix (concept: MatrixLayout)
-    typename LayoutC,
-    /// Complex transform on A operand
-    ComplexTransform TransformA,
-    /// Complex transform on B operand
-    ComplexTransform TransformB>
-struct DefaultMmaComplexTensorOp<
-    WarpShape_,
-    GemmShape<16, 8, 4>,
-    complex<RealElementA>,
-    LayoutA,
-    complex<RealElementB>,
-    LayoutB,
-    complex<RealElementC>,
-    LayoutC,
-    TransformA,
-    TransformB,
-    arch::OpMultiplyAddGaussianComplex> {
-
-  using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
-      cutlass::arch::Mma<
-        GemmShape<16, 8, 4>,
-        32, 
-        RealElementA,
-        cutlass::layout::RowMajor,
-        RealElementB,
-        cutlass::layout::ColumnMajor,
-        RealElementC,
-        cutlass::layout::RowMajor, 
-        arch::OpMultiplyAdd>,
-      cutlass::MatrixShape<1, 1>
-    >;
-
-  // Define the warp-level tensor op
-  using Type = cutlass::gemm::warp::MmaGaussianComplexTensorOp<
-    WarpShape_,
-    complex<RealElementA>,
-    LayoutA,
-    complex<RealElementB>,
-    LayoutB,
-    complex<RealElementC>,
-    LayoutC, 
-    Policy,
-    TransformA,
-    TransformB,
-    true>;
-};
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_sparse_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_tensor_op_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_with_reduction_tensor_op.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,17 +57,15 @@
     /// Layout of B matrix (concept: MatrixLayout)
     typename LayoutB,
     /// Element type of C matrix
     typename ElementC,
     /// Layout of C matrix (concept: MatrixLayout)
     typename LayoutC,
     /// Operator describing the tensor operation
-    typename Operator_,
-    /// Reduce operand A or B along K dimension
-    bool ReduceKForA_,
+    typename Operator_ = arch::OpMultiplyAdd,
     /// Number of partitions along K dimension
     int PartitionsK = 1,
     /// Store the accumulators in row major or column major.  Row major is used
     /// when output layout is interleaved.
     bool AccumulatorsInRowMajor = false>
 struct DefaultMmaWithReductionTensorOp {
   using Policy = cutlass::gemm::warp::MmaTensorOpPolicy<
@@ -76,15 +74,15 @@
                          cutlass::layout::ColumnMajor, ElementC,
                          cutlass::layout::RowMajor, Operator_>,
       cutlass::MatrixShape<1, 1> >;
 
   // Define the warp-level tensor op
   using Type = cutlass::gemm::warp::MmaWithReductionTensorOp<
       WarpShape_, ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC,
-      Policy, ReduceKForA_, PartitionsK, AccumulatorsInRowMajor>;
+      Policy, PartitionsK, AccumulatorsInRowMajor>;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/default_mma_wmma_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/layernorm_scale_bias_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_fast_f32.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fast_f32.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -33,628 +33,436 @@
     \brief Templates implementing warp-level matrix multiply-accumulate operations targeting
       Tensor Cores.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
 #include "cutlass/array.h"
-#include "cutlass/complex.h"
+#include "cutlass/platform/platform.h"
+
+#include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
-#include "cutlass/functional.h"
 
-#include "cutlass/arch/memory_sm75.h"
-#include "cutlass/arch/mma_sm75.h"
 #include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
 #include "cutlass/gemm/warp/mma_tensor_op.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator.h"
 #include "cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h"
-#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
-namespace detail {
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <
-  /// Data type of real & imag members of complex numbers in the SourceFragment
-  typename RealElement,
-  /// Destination fragment required by the mma operation 
-  typename DestinationFragment,
-  /// Source fragment holding complex<RealElement> elements
-  typename SourceFragment,
-  /// Number of mma operations performed
-  typename MmaIterations,
-  /// Shape of operand elements
-  typename MmaOperandShape,
-  /// Complex transform on A operand
-  ComplexTransform Transform_,
-  /// Operand A or Operand B
-  Operand Operand_,
-  /// Floating-point rounding style for big part
-  FloatRoundStyle RoundBig_,
-  /// Floating-point rounding style for small part
-  FloatRoundStyle RoundSmall_>
-struct UnpackComplexConvertAndPackForMmaFastF32;
+enum class TensorFloat32Op {
+  k3xTF32, 
+  k4xTF32 
+}; 
 
-// Partial specialization for OperandA and Congruous smem layout
 template <
-  typename RealElement,
-  typename DestinationFragment, 
-  typename SourceFragment,
-  typename MmaIterations,
-  typename MmaOperandShape,
-  ComplexTransform Transform_,
-  FloatRoundStyle RoundBig_,
-  FloatRoundStyle RoundSmall_>
-struct UnpackComplexConvertAndPackForMmaFastF32 <
-  RealElement,
-  DestinationFragment,
-  SourceFragment,
-  MmaIterations,
-  MmaOperandShape,
-  Transform_,
-  Operand::kA,
-  RoundBig_,
-  RoundSmall_> {
-  
-  //
-  // Type definitions
-  //
-  static Operand const kOperand = Operand::kA;
-  static ComplexTransform const kTransform = Transform_;
-  static FloatRoundStyle const kRoundBig = RoundBig_;
-  static FloatRoundStyle const kRoundSmall = RoundSmall_;
-
-  // Data type of elements in the destination fragment
-  using MmaElement = typename DestinationFragment::Element;
-
-  // Numeric convertor MmaElementBig, MmaElementSmall <= RealElement
-  using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
-
-  // Operand layout parameters
-  using SourceFragmentLayout = layout::ColumnMajor;
-  static int const kLdm = MmaIterations::kRow * MmaOperandShape::kRow;
-
-  // BigSmall Fragment holding two TF32 elements (big, small) for every float
-  using BigSmallFragment = Array<MmaElement, 2>;
-
-  /// Index in fargments for the big and small part
-  static int const kBigIndex = 0;
-  static int const kSmallIndex = 1;
-
-  /// Ctor
-  CUTLASS_DEVICE
-  UnpackComplexConvertAndPackForMmaFastF32() {}
-
-  CUTLASS_DEVICE
-  void operator()(DestinationFragment *dest, SourceFragment const &source) {
-    
-    Converter convert_op;
-    SourceFragmentLayout layout(kLdm);
-
-    DestinationFragment *dest_big_ = reinterpret_cast<DestinationFragment*>(dest);
-    DestinationFragment *dest_small_ = reinterpret_cast<DestinationFragment*>(&dest[MmaIterations::kRow * 2]);
-
-    CUTLASS_PRAGMA_UNROLL
-    for(int i=0; i<MmaIterations::kRow; i++) {
-      int pos = 0;
-      CUTLASS_PRAGMA_UNROLL
-      for(int c=0; c<MmaOperandShape::kColumn; c++) {
-        CUTLASS_PRAGMA_UNROLL
-        for(int r=0; r<MmaOperandShape::kRow; r++) {
-          // Logical position of element in source fragment
-          int row = r + i * MmaOperandShape::kRow;
-          int col = c;
-
-          // Access complex<RealElement> and apply rounding on real and imag parts
-          BigSmallFragment a = convert_op(source[layout(MatrixCoord{row,col})].real());
-          BigSmallFragment b = convert_op(source[layout(MatrixCoord{row,col})].imag());
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest_big_[i][pos] = a[kBigIndex];
-          dest_big_[i+MmaIterations::kRow][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kBigIndex] : b[kBigIndex]);
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest_small_[i][pos] = a[kSmallIndex];
-          dest_small_[i+MmaIterations::kRow][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kSmallIndex] : b[kSmallIndex]);
-
-          // Next position
-          pos++;
-        }
-      }
-    }
-  }
+  /// Floating-point rounding style
+  FloatRoundStyle RoundBigA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundSmallA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundBigB_ = RoundBigA_,
+  /// Floating-point rounding style
+  FloatRoundStyle RoundSmallB_ = RoundSmallA_,
+  /// Precision for TensorFloat32Op 
+  // (k3xTF32: BigxBig, BigxSmall, SmallxBig)
+  // (k4xTF32: BigxBig, BigxSmall, SmallxBig, SmallxSmall)
+  TensorFloat32Op Precision_ = TensorFloat32Op::k3xTF32
+  >
+struct FastF32 {
+
+  static FloatRoundStyle const kRoundBigA = RoundBigA_;
+  static FloatRoundStyle const kRoundSmallA = RoundSmallA_;
+  static FloatRoundStyle const kRoundBigB = RoundBigB_;
+  static FloatRoundStyle const kRoundSmallB = RoundSmallB_;
+  static TensorFloat32Op const kPrecision = Precision_;
 };
 
-// Partial specialization for OperandB and Congruous smem layout
-template <
-  typename RealElement,
-  typename DestinationFragment, 
-  typename SourceFragment,
-  typename MmaIterations,
-  typename MmaOperandShape,
-  ComplexTransform Transform_,
-  FloatRoundStyle RoundBig_,
-  FloatRoundStyle RoundSmall_>
-struct UnpackComplexConvertAndPackForMmaFastF32 <
-  RealElement,
-  DestinationFragment,
-  SourceFragment,
-  MmaIterations,
-  MmaOperandShape,
-  Transform_,
-  Operand::kB,
-  RoundBig_,
-  RoundSmall_> {
-  
-  //
-  // Type definitions
-  //
-  static Operand const kOperand = Operand::kB;
-  static ComplexTransform const kTransform = Transform_;
-  static FloatRoundStyle const kRoundBig = RoundBig_;
-  static FloatRoundStyle const kRoundSmall = RoundSmall_;
-
-  // Data type of elements in the destination fragment
-  using MmaElement = typename DestinationFragment::Element;
-
-  // Numeric convertor MmaElementBig, MmaElementSmall <= RealElement
-  using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
-
-  // Operand layout parameters
-  using SourceFragmentLayout = layout::RowMajor;
-  static int const kLdm = MmaIterations::kColumn * MmaOperandShape::kColumn;
-
-  // BigSmall Fragment holding two TF32 elements (big, small) for every float
-  using BigSmallFragment = Array<MmaElement, 2>;
-
-  /// Index in fargments for the big and small part
-  static int const kBigIndex = 0;
-  static int const kSmallIndex = 1;
-
-  /// Ctor
-  CUTLASS_DEVICE
-  UnpackComplexConvertAndPackForMmaFastF32() {}
 
-  CUTLASS_HOST_DEVICE
-  void operator()(DestinationFragment *dest, SourceFragment const &source) {
-    
-    Converter convert_op;
-    SourceFragmentLayout layout(kLdm);
+namespace detail {
 
-    DestinationFragment *dest_big_ = reinterpret_cast<DestinationFragment*>(dest);
-    DestinationFragment *dest_small_ = reinterpret_cast<DestinationFragment*>(&dest[MmaIterations::kColumn * 2]);
+  template<
+    int N,
+    FloatRoundStyle RoundBig = FloatRoundStyle::round_toward_zero,
+    FloatRoundStyle RoundSmall = FloatRoundStyle::round_half_ulp_truncate
+  >
+  struct ConvertAndPackAccurateF32 {
+  
+    /// Rounding styles for big and small part
+    static FloatRoundStyle const kRoundBig = RoundBig;
+    static FloatRoundStyle const kRoundSmall = RoundSmall;
+
+    /// Converter type
+    using Converter = NumericConverterFastF32<kRoundBig, kRoundSmall>;
+
+    /// Source fragement
+    using SourceFragment = Array<float, N>;
+
+    /// Destination fragment
+    using DestinationFragment = Array<tfloat32_t, N>;
+
+    /// Converter Fragment holding two tfloat32_t elements for every float
+    using ConverterFragment = Array<tfloat32_t, 2>;
+
+    /// Index in fargments for the big and small part
+    static int const kBigIndex = 0;
+    static int const kSmallIndex = 1;
+
+    CUTLASS_HOST_DEVICE
+    void operator()(SourceFragment const &source,
+                    DestinationFragment &dst_big,
+                    DestinationFragment &dst_small) {
+      
+      Converter convert_;
+      ConverterFragment result_;
 
-    CUTLASS_PRAGMA_UNROLL
-    for(int i=0; i<MmaIterations::kColumn; i++) {
-      int pos = 0;
       CUTLASS_PRAGMA_UNROLL
-      for(int c=0; c<MmaOperandShape::kColumn; c++) {
-        CUTLASS_PRAGMA_UNROLL
-        for(int r=0; r<MmaOperandShape::kRow; r++) {
-          // Logical position of element in source fragment
-          int row = r;
-          int col = c + i * MmaOperandShape::kColumn;
-
-          // Access complex<RealElement> apply rounding on real and imag parts
-          BigSmallFragment a = convert_op(source[layout(MatrixCoord{row,col})].real());
-          BigSmallFragment b = convert_op(source[layout(MatrixCoord{row,col})].imag());
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest_big_[i][pos] = a[kBigIndex];
-          dest_big_[i+MmaIterations::kColumn][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kBigIndex] : b[kBigIndex]);
-
-          // Unpack rounded complex<MmaElement> and pack into DestinationFragment for mma operation
-          dest_small_[i][pos] = a[kSmallIndex];
-          dest_small_[i+MmaIterations::kColumn][pos] = (kTransform == ComplexTransform::kConjugate ? -b[kSmallIndex] : b[kSmallIndex]);
-
-          // next position
-          pos++;       
-        }
+      for (int i = 0; i < N; ++i) {
+        // convert source to result fragment
+        result_ = convert_(source[i]);
+
+        // store converted result fragments to destination fragment
+        dst_big[i] = result_[kBigIndex];
+        dst_small[i] = result_[kSmallIndex];
       }
     }
-  }
-};
-} // namespace detail 
+  };
+} // namespace detail
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Structure to compute the matrix product targeting CUDA cores and SIMT math instructions.
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Data type of A elements
-  typename RealElementA,
+  typename ElementA_,
   /// Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
   /// Data type of B elements
-  typename RealElementB,
+  typename ElementB_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
   /// Element type of C matrix
-  typename RealElementC,
+  typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Complex transform on A operand
-  ComplexTransform TransformA = ComplexTransform::kNone,
-  /// Complex transform on B operand
-  ComplexTransform TransformB = ComplexTransform::kNone,
+  /// Number of partitions along K dimension
+  int PartitionsK_ = 1,
+  /// Store the accumulators in row major or column major.  Row major is used
+  /// when output layout is interleaved.
+  bool AccumulatorsInRowMajor = false,
   /// Used for partial specialization
   typename Enable = bool
 >
-class MmaComplexTensorOpFastF32;
+class MmaTensorOpFastF32;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex*complex+complex => complex:
-//  Operands data type: complex<float>
-//  Rounding: float -> tfloat32_t (round half_ulp_truncate nearest)
-//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
-//  Output data type: complex<float>
-// 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+/// Partial specialization for float*float+float => float using TF32 TensorOps
 template <
   /// Size of the Gemm problem - concept: gemm::GemmShape<>
   typename Shape_,
   /// Layout of A matrix (concept: MatrixLayout)
   typename LayoutA_,
   /// Layout of B matrix (concept: MatrixLayout)
   typename LayoutB_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Complex transform on A operand
-  ComplexTransform TransformA,
-  /// Complex transform on B operand
-  ComplexTransform TransformB,
+  /// Number of partitions along K dimension
+  int PartitionsK_,
+  /// Store the accumulators in row major or column major.  Row major is used
+  /// when output layout is interleaved.
+  bool AccumulatorsInRowMajor,
   /// Used for partial specialization
   typename Enable
 >
-class MmaComplexTensorOpFastF32<
-  Shape_, 
-  complex<float>, 
-  LayoutA_, 
-  complex<float>,
-  LayoutB_,
-  complex<float>,
-  LayoutC_,
-  Policy_,
-  TransformA,
-  TransformB,
-  Enable>  {
+class MmaTensorOpFastF32<
+  Shape_,
+  float, LayoutA_,
+  float, LayoutB_,
+  float, LayoutC_,
+  Policy_, PartitionsK_,
+  AccumulatorsInRowMajor, Enable> {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
-  /// Data type of members of complex multiplicand A
-  using RealElementA = float;
-
   /// Data type of multiplicand A
-  using ElementA = complex<RealElementA>;
+  using ElementA = float;
 
   /// Layout of multiplicand A
   using LayoutA = LayoutA_;
 
-  /// Data type of members of complex multiplicand B
-  using RealElementB = float;
-
   /// Data type of multiplicand B
-  using ElementB = complex<RealElementB>;
+  using ElementB = float;
 
   /// Layout of multiplicand B
   using LayoutB = LayoutB_;
 
-  /// Data type of members of complex accumulator matrix C
-  using RealElementC = float;
-
   /// Data type of accumulator matrix C
-  using ElementC = complex<RealElementC>;
+  using ElementC = float;
 
   /// Layout of accumulator matrix C
   using LayoutC = LayoutC_;
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
-  /// Shape of underlying instruction
-  using InstructionShape = typename ArchMmaOperator::Shape;
+  /// Indicates math operator 
+  using MathOperator = arch::OpMultiplyAddFastF32;
 
-  /// Underlying arch tag
+  /// Architecture tag from underlying instruction
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
-  /// Indicates math operator 
-  using MathOperator = arch::OpMultiplyAddComplexFastF32;
-  
+  /// Shape of underlying instruction
+  using InstructionShape = typename ArchMmaOperator::Shape;
+
   /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
+  static ComplexTransform const kTransformA = ComplexTransform::kNone;
 
   /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
+  static ComplexTransform const kTransformB = ComplexTransform::kNone;
 
   /// Number of threads participating in warp-level matrix product
   static int const kThreadCount = 32;
 
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
 
-  /// Tune F32 to TF32 big small conversion for complex<float> operation
+  /// Tune F32 to TF32 big small conversion for float operation
   /// Different combination of big small conversin can cause different tradeoff
   /// between speed and accuracy.  Generally, use round_half_ulp_truncate can
   /// improve the performance but hur the accuracy.
-  using ComplexFastF32 = FastF32 <
+  using MmaFastF32 = FastF32 <
     FloatRoundStyle::round_toward_zero,        // kRoundBigA
     FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallA
     FloatRoundStyle::round_toward_zero,        // kRoundBigB
     FloatRoundStyle::round_half_ulp_truncate,  // kRoundSmallB
     TensorFloat32Op::k3xTF32                   // Number of TF32 operations 
   >;
 
-  /// Index in fargments for the big and small part
-  static int const kBigIndex = 0;
-  static int const kSmallIndex = 1;
-
 public:
 
   /// Iterates over the A operand in memory
   using IteratorA = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
-    Policy::OpDelta::kRow,
-    32,
-    1
+      MatrixShape<Shape::kM, Shape::kK>, 
+      Operand::kA, 
+      ElementA, 
+      LayoutA,
+      MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
+      Policy::OpDelta::kRow, 
+      kThreadCount, 
+      kPartitionsK
   >;
 
   /// Storage for A tile
   using FragmentA = typename IteratorA::Fragment;
 
   /// Storage for transformed A tile
-  // (4 times the original FragmentA::kElements)
-  // (real_big), (imag_big), (real_small), (imag_small)
-  using TransformedFragmentA = Array<typename ArchMmaOperator::ElementA, 
-                                              FragmentA::kElements * 2 * 2>;
-
-  // Fragment bisecting big and small sections
-  // (real_big, imag_big), (real_small, imag_small)
-  using AccessTypeFragmentA = Array<typename ArchMmaOperator::ElementA, 
-                                                    FragmentA::kElements * 2>;
+  using TransformedFragmentA =
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements * 2>;
+
+  /// Fragment bisecting big and small sections
+  using AccessTypeFragmentA = 
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements>;
 
   /// Iterates over the B operand in memory
   using IteratorB = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
-    Policy::OpDelta::kColumn,
-    32,
-    1
+      MatrixShape<Shape::kK, Shape::kN>, 
+      Operand::kB, 
+      ElementB, 
+      LayoutB,
+      MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
+      Policy::OpDelta::kRow, 
+      kThreadCount, 
+      kPartitionsK
   >;
 
   /// Storage for B tile
   using FragmentB = typename IteratorB::Fragment;
 
-  /// Storage for transformed B tile 
-  // (4 times the original FragmentB::kElements)
-  // (real_big), (imag_big), (real_small), (imag_small)
-  using TransformedFragmentB = Array<typename ArchMmaOperator::ElementB, 
-                                              FragmentB::kElements * 2 * 2>;
-
-  // Fragment bisecting big and small sections
-  // (real_big, imag_big), (real_small, imag_small)
-  using AccessTypeFragmentB = Array<typename ArchMmaOperator::ElementB, 
-                                                    FragmentB::kElements * 2>;
-
-  static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
+  /// Storage for transformed B tile
+  using TransformedFragmentB =
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements * 2>;
+
+  /// Fragment bisecting big and small sections
+  using AccessTypeFragmentB = 
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements>;
 
-  /// Number of complex products operations performed (one complex product needs four mma instructions)
-  using MmaIterations = MatrixShape<
-    Shape::kM / ArchMmaOperator::Shape::kM,
-    Shape::kN / ArchMmaOperator::Shape::kN
-  >;
+  /// Index in fargments for the big and small part
+  static int const kBigIndex = 0;
+  static int const kSmallIndex = 1;
 
   /// Iterates over the C operand in memory
   using IteratorC = MmaTensorOpAccumulatorTileIterator<
-     MatrixShape<Shape::kM, Shape::kN>, 
-     ElementC, 
-     LayoutC,
-     typename ArchMmaOperator::Shape, 
-     typename Policy::OpDelta>;
-
-  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
-  /// storage arrangement is to be considered 'planar complex' in the sense that all real-valued
-  /// parts are stored consecutively followed by all imaginary parts. This matches the structure
-  /// of Tensor Cores which are always real-valued matrix multiplies.
-  using FragmentC = typename IteratorC::Fragment;
+     MatrixShape<Shape::kM, Shape::kN>, ElementC, LayoutC,
+     typename ArchMmaOperator::Shape, typename Policy::OpDelta>;
 
-  //
-  // Alias types for underlying real-valued matrix multiply operator
-  //
-  using InstMmaOperandA = typename ArchMmaOperator::FragmentA;
-  using InstMmaOperandB = typename ArchMmaOperator::FragmentB;
-  using MmaOperandC = typename ArchMmaOperator::FragmentC;
-
-  static_assert(platform::is_same<cutlass::gemm::GemmShape<16, 8, 8>, typename ArchMmaOperator::Shape>::value, 
-    "This implementation only supports mma.m16n8k8 math instructions.");
-
-  static_assert(InstMmaOperandA::kElements == 4, 
-    "This implementation only supports math instructions in which exactly four element is needed for the A operand."
-    "We can geneneralize later.");
-
-  static_assert(InstMmaOperandB::kElements == 2, 
-    "This implementation only supports math instructions in which exactly two element is needed for the B operand."
-    "We can geneneralize later.");
+  /// Storage for C tile
+  using FragmentC = typename IteratorC::Fragment;
 
-private:
+  /// Number of mma operations performed
+  using MmaIterations = MatrixShape<
+    (Shape::kM + ArchMmaOperator::Shape::kM - 1) / ArchMmaOperator::Shape::kM,
+    (Shape::kN + ArchMmaOperator::Shape::kN - 1) / ArchMmaOperator::Shape::kN
+  >;
 
-  //
-  // Data members
-  //
+public:
 
-  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
+  /// Underlying matrix multiply operator (concept: arch::Mma)
   ArchMmaOperator mma;
 
 public:
 
   //
   // Methods
   //
 
   /// Ctor
   CUTLASS_DEVICE
-  MmaComplexTensorOpFastF32() {}
+  MmaTensorOpFastF32() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
     FragmentC &D, 
     TransformedFragmentA const &A, 
     TransformedFragmentB const &B, 
     FragmentC const &C
   ) const {
 
-    AccessTypeFragmentA const *complex_A = reinterpret_cast<AccessTypeFragmentA const*>(&A);
-    AccessTypeFragmentB const *complex_B = reinterpret_cast<AccessTypeFragmentB const*>(&B);
+    AccessTypeFragmentA const *ptr_A = reinterpret_cast<AccessTypeFragmentA const*>(&A);
+    AccessTypeFragmentB const *ptr_B = reinterpret_cast<AccessTypeFragmentB const*>(&B);
 
     //
     // Accumulate in place
     //
     D = C;
+    
+    mma_operator(D, ptr_A[kSmallIndex], ptr_B[kBigIndex], D);
 
+    mma_operator(D, ptr_A[kBigIndex], ptr_B[kSmallIndex], D);
 
-    complex_mma_operator(D, complex_A[kSmallIndex], complex_B[kBigIndex], D);
-
-    complex_mma_operator(D, complex_A[kBigIndex], complex_B[kSmallIndex], D);
-
-    complex_mma_operator(D, complex_A[kBigIndex], complex_B[kBigIndex], D);
+    mma_operator(D, ptr_A[kBigIndex], ptr_B[kBigIndex], D);
 
-    if (ComplexFastF32::kPrecision == TensorFloat32Op::k4xTF32)
-      complex_mma_operator(D, complex_A[kSmallIndex], complex_B[kSmallIndex], D);
+    if (MmaFastF32::kPrecision == TensorFloat32Op::k4xTF32)
+      mma_operator(D, ptr_A[kSmallIndex], ptr_B[kSmallIndex], D);
   }
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
-  void complex_mma_operator(
+  void mma_operator(
     FragmentC &D, 
-    AccessTypeFragmentA const &complex_A, 
-    AccessTypeFragmentB const &complex_B, 
+    AccessTypeFragmentA const &A, 
+    AccessTypeFragmentB const &B, 
     FragmentC const &C
   ) const {
 
-    // Instruction Operands A & B holding real part followed by imaginary part for mma operations
-    InstMmaOperandA const *operand_A = reinterpret_cast<InstMmaOperandA const *>(&complex_A);
-    InstMmaOperandB const *operand_B = reinterpret_cast<InstMmaOperandB const *>(&complex_B);
-
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int m = 0; m < MmaIterations::kRow; ++m) {
-
-      // mma(accum.real(), a.real(), b.real(), accum.real());
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
-
-        // Real-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
-
-          mma(*accum, operand_A[m], operand_B[n], *accum);
-      }
-
-      // mma(accum.imag(), a.real(), b.imag(), accum.imag()); 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
 
-        // Complex-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+      using MmaOperandA = typename ArchMmaOperator::FragmentA;
+      using MmaOperandB = typename ArchMmaOperator::FragmentB;
+      using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
-        mma(*accum, operand_A[m], operand_B[n+MmaIterations::kColumn], *accum);
-      }
+      MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+      MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+      MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
 
-      // mma(accum.real(), a.imag(), -b.imag(), accum.real())
+      // Serpentine visitation order maximizing reuse of Ra
       CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
-
-        // negate OperandB to accumulate  -(a.imag()*b.imag())
-        // negating OperandB emits less instrucitons than negating OperandA as OperandB has less elements
-        negate<InstMmaOperandB> negate_op;
-
-        // Real-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
-
-         mma(*accum, operand_A[m+MmaIterations::kRow], negate_op(operand_B[n+MmaIterations::kColumn]), *accum);
-      }
+      for (int m = 0; m < MmaIterations::kRow; ++m) {
 
-      // mma(accum.imag(), a.imag(), b.real(), accum.imag())
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int n = 0; n < MmaIterations::kColumn; ++n) {
 
-        // Complex-valued accumulator part
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
+          // This allows to reuse of Rb when at serpentine turns
+          int n_serpentine = ((m % 2) ? (MmaIterations::kColumn - 1 - n) : n);
 
-        mma(*accum, operand_A[m+MmaIterations::kRow], operand_B[n], *accum);
-      }
-    }
+          if (AccumulatorsInRowMajor) {  // matrix B is reordered
+            mma(
+              ptr_D[n_serpentine + m * MmaIterations::kColumn],
+              ptr_A[m],
+              ptr_B[n_serpentine],
+              ptr_D[n_serpentine + m * MmaIterations::kColumn]);
+          } else {
+            mma(
+              ptr_D[m + n_serpentine * MmaIterations::kRow],
+              ptr_A[m],
+              ptr_B[n_serpentine],
+              ptr_D[m + n_serpentine * MmaIterations::kRow]);
+          }
+        } // end n loop
+      } // end m loop
+    #else
+      assert(0);
+    #endif
   }
 
   /// Transform the mma operands to the required types
   CUTLASS_DEVICE
   void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
                  FragmentA const &A, FragmentB const &B) const {
 
-    detail::UnpackComplexConvertAndPackForMmaFastF32 <
-      RealElementA,
-      InstMmaOperandA,
-      FragmentA,
-      MmaIterations,
-      MatrixShape<2, 2>,
-      kTransformA,
-      Operand::kA,
-      ComplexFastF32::kRoundBigA,
-      ComplexFastF32::kRoundSmallA> convert_A;
-
-    detail::UnpackComplexConvertAndPackForMmaFastF32 <
-      RealElementB,
-      InstMmaOperandB,
-      FragmentB,
-      MmaIterations,
-      MatrixShape<2, 1>,
-      kTransformB,
-      Operand::kB,
-      ComplexFastF32::kRoundBigB,
-      ComplexFastF32::kRoundSmallB> convert_B;
-
-    // Convert Fragment[A|B] holding complex<RealElement[A|B]> to InstMmaOperand[A|B] holding InstMmaOperand[A|B]::Element
-    convert_A(reinterpret_cast<InstMmaOperandA *>(&dst_A), A); 
-    convert_B(reinterpret_cast<InstMmaOperandB *>(&dst_B), B); 
+    //
+    // Define conversions from source type to instruction type
+    //
+    #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
+      
+      detail::ConvertAndPackAccurateF32<
+        FragmentA::kElements / 2,
+        MmaFastF32::kRoundBigA,
+        MmaFastF32::kRoundSmallA> convert_A;
+      
+      detail::ConvertAndPackAccurateF32<
+        FragmentB::kElements,
+        MmaFastF32::kRoundBigB,
+        MmaFastF32::kRoundSmallB> convert_B;
+      
+      Array<typename ArchMmaOperator::ElementB, FragmentB::kElements> *ptr_dst_B = 
+        reinterpret_cast<Array<typename ArchMmaOperator::ElementB, FragmentB::kElements> *>(&dst_B);
+      
+      convert_B(B, ptr_dst_B[0], ptr_dst_B[1]);
+
+      Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *ptr_dst_A =
+        reinterpret_cast<Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *>(&dst_A);
+      
+      Array<ElementA, FragmentA::kElements / 2> const *ptr_A = 
+        reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
+      
+      convert_A(ptr_A[0], ptr_dst_A[0], ptr_dst_A[2]);
+      
+      convert_A(ptr_A[1], ptr_dst_A[1], ptr_dst_A[3]);
+    #else
+      assert(0);
+    #endif
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -80,16 +80,14 @@
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
   /// Complex transform on A operand
   ComplexTransform TransformA = ComplexTransform::kNone,
   /// Complex transform on B operand
   ComplexTransform TransformB = ComplexTransform::kNone,
-  /// Do source operands need more than one elements
-  bool GeneralizedOperatorElements = false,
   /// Used for partial specialization
   typename Enable = bool
 >
 class MmaGaussianComplexTensorOp;
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -110,27 +108,30 @@
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
   /// Complex transform on A operand
   ComplexTransform TransformA,
   /// Complex transform on B operand
-  ComplexTransform TransformB
+  ComplexTransform TransformB,
+  /// Used for partial specialization
+  typename Enable
 >
 class MmaGaussianComplexTensorOp<
   Shape_, 
   complex<RealElementA>, 
   LayoutA_, 
   complex<RealElementB>,
   LayoutB_,
   complex<RealElementC>,
   LayoutC_,
   Policy_,
   TransformA,
-  TransformB>  {
+  TransformB,
+  Enable>  {
 public:
   /// Shape of warp-level matrix operation (concept: GemmShape)
   using Shape = Shape_;
 
   /// Data type of multiplicand A
   using ElementA = complex<RealElementA>;
 
@@ -354,290 +355,14 @@
     dst_A = A;
     dst_B = B;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for complex*complex+complex => complex using real-valued TensorOps
-template <
-  /// Size of the Gemm problem - concept: gemm::GemmShape<>
-  typename Shape_,
-  /// Data type of A elements
-  typename RealElementA,
-  /// Layout of A matrix (concept: MatrixLayout)
-  typename LayoutA_,
-  /// Data type of B elements
-  typename RealElementB,
-  /// Layout of B matrix (concept: MatrixLayout)
-  typename LayoutB_,
-  /// Element type of C matrix
-  typename RealElementC,
-  /// Layout of C matrix (concept: MatrixLayout)
-  typename LayoutC_,
-  /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
-  typename Policy_,
-  /// Complex transform on A operand
-  ComplexTransform TransformA,
-  /// Complex transform on B operand
-  ComplexTransform TransformB
->
-class MmaGaussianComplexTensorOp<
-  Shape_, 
-  complex<RealElementA>, 
-  LayoutA_, 
-  complex<RealElementB>,
-  LayoutB_,
-  complex<RealElementC>,
-  LayoutC_,
-  Policy_,
-  TransformA,
-  TransformB,
-  true>  {
-public:
-  /// Shape of warp-level matrix operation (concept: GemmShape)
-  using Shape = Shape_;
-
-  /// Data type of multiplicand A
-  using ElementA = complex<RealElementA>;
-
-  /// Layout of multiplicand A
-  using LayoutA = LayoutA_;
-
-  /// Data type of multiplicand B
-  using ElementB = complex<RealElementB>;
-
-  /// Layout of multiplicand B
-  using LayoutB = LayoutB_;
-
-  /// Data type of accumulator matrix C
-  using ElementC = complex<RealElementC>;
-
-  /// Layout of accumulator matrix C
-  using LayoutC = LayoutC_;
-
-  /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
-  using Policy = Policy_;
-
-  /// Underlying matrix multiply operator (concept: arch::Mma)
-  using ArchMmaOperator = typename Policy::Operator;
-
-  /// Shape of underlying instruction
-  using InstructionShape = typename ArchMmaOperator::Shape;
-
-  /// Underlying arch tag
-  using ArchTag = typename ArchMmaOperator::ArchTag;
-
-  /// Indicates class of matrix operator
-  using OperatorClass = arch::OpClassTensorOp;
-
-  /// Indicates math operator 
-  using MathOperator = arch::OpMultiplyAddGaussianComplex;
-  
-  /// Complex transform on A operand
-  static ComplexTransform const kTransformA = TransformA;
-
-  /// Complex transform on B operand
-  static ComplexTransform const kTransformB = TransformB;
-
-
-  /// Number of threads participating in warp-level matrix product
-  static int const kThreadCount = 32;
-
-public:
-
-  /// Iterates over the A operand in memory
-  using IteratorA = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kM, Shape::kK>,
-    Operand::kA,
-    ElementA,
-    LayoutA,
-    MatrixShape<ArchMmaOperator::Shape::kM, ArchMmaOperator::Shape::kK>,
-    Policy::OpDelta::kRow,
-    32,
-    1
-  >;
-
-  /// Storage for A tile
-  using FragmentA = typename IteratorA::Fragment;
-
-  /// Storage for transformed A tile
-  using TransformedFragmentA = FragmentA;
-
-  /// Iterates over the B operand in memory
-  using IteratorB = MmaTensorOpMultiplicandTileIterator<
-    MatrixShape<Shape::kK, Shape::kN>,
-    Operand::kB,
-    ElementB,
-    LayoutB,
-    MatrixShape<ArchMmaOperator::Shape::kK, ArchMmaOperator::Shape::kN>,
-    Policy::OpDelta::kColumn,
-    32,
-    1
-  >;
-
-  /// Storage for B tile
-  using FragmentB = typename IteratorB::Fragment;
-
-  /// Storage for transformed B tile
-  using TransformedFragmentB = FragmentB;
-
-  static_assert(
-    !(Shape::kM % ArchMmaOperator::Shape::kM) && 
-    !(Shape::kN % ArchMmaOperator::Shape::kN),
-    "Shape of warp-level Mma must be divisible by operator shape.");
-
-  /// Number of mma operations performed
-  using MmaIterations = MatrixShape<
-    Shape::kM / ArchMmaOperator::Shape::kM,
-    Shape::kN / ArchMmaOperator::Shape::kN
-  >;
-
-  /// Iterates over the C operand in memory
-  using IteratorC = MmaTensorOpGaussianComplexAccumulatorTileIterator<
-     MatrixShape<Shape::kM, Shape::kN>, 
-     ElementC, 
-     LayoutC,
-     typename ArchMmaOperator::Shape, 
-     typename Policy::OpDelta>;
-
-  /// Storage for C tile, the accumulator. Note, regardless of multiplicand type, this
-  /// storage arrangement is to be considered 'gaussian complex' in the sense that the accumulation is
-  /// done in three parts namely part1, part2, and part3. The parts 1, 2, and 3 are stored consecutively 
-  /// in InteratorC::Frament. This matches the structure of Tensor Cores which are always real-valued matrix multiplies.
-  using FragmentC = typename IteratorC::Fragment;
-
-  static_assert(
-    FragmentC::kElements == 3 * MmaIterations::kCount * ArchMmaOperator::FragmentC::kElements,
-    "Unexpected gaussian complex fragment length.");
-
-private:
-
-  //
-  // Data members
-  //
-
-  /// Underlying real-valued matrix multiply operator (concept: arch::Mma)
-  ArchMmaOperator mma;
-
-public:
-
-  //
-  // Methods
-  //
-
-  /// Ctor
-  CUTLASS_DEVICE
-  MmaGaussianComplexTensorOp() {}
-
-  /// Performs a warp-level matrix multiply-accumulate operation
-  CUTLASS_DEVICE
-  void operator()(
-    FragmentC &D, 
-    FragmentA const &A, 
-    FragmentB const &B, 
-    FragmentC const &C
-  ) const {
-
-    // Alias types for underlying real-valued matrix multiply operator
-    using MmaOperandA = typename ArchMmaOperator::FragmentA;
-    using MmaOperandB = typename ArchMmaOperator::FragmentB;
-    using MmaOperandC = typename ArchMmaOperator::FragmentC;
-
-    D = C;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int m = 0; m < MmaIterations::kRow; ++m) {
-
-      // mma(accum.part1(), (a.real() + a.imag()), b.real(), accum.part1());
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
-
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Asum;
-        MmaOperandB operand_Br;
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
-          operand_Asum[mk] = A[m*MmaOperandA::kElements + mk].real() + ((kTransformA == ComplexTransform::kConjugate) ?
-                            -A[m*MmaOperandA::kElements + mk].imag() : +A[m*MmaOperandA::kElements + mk].imag());
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
-          operand_Br[nk] = B[n*MmaOperandB::kElements + nk].real();
-
-        // accumulator part1
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow);
-
-        mma(*accum, operand_Asum, operand_Br, *accum);
-      }
-
-      // mma(accum.part2(), -a.real(), (b.real() - b.imag()), accum.part2()); 
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = MmaIterations::kColumn - 1; n >= 0; --n) {
-
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Ar;
-        MmaOperandB operand_Bdiff;
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
-          operand_Ar[mk] = -A[m*MmaOperandA::kElements + mk].real();
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
-          operand_Bdiff[nk] = B[n*MmaOperandB::kElements + nk].real() - ((kTransformB == ComplexTransform::kConjugate) ?
-                              -B[n*MmaOperandB::kElements + nk].imag() : +B[n*MmaOperandB::kElements + nk].imag());
-
-        // accumulator part2
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + MmaIterations::kCount;
-
-        mma(*accum, operand_Ar, operand_Bdiff, *accum);
-      }
-
-      // mma(accum.part3(), a.imag(), (b.real() + b.imag()), accum.part3())
-      CUTLASS_PRAGMA_UNROLL
-      for (int n = 0; n < MmaIterations::kColumn; ++n) {
-
-        // Pack operands together. This may result in actual MOVs 
-        MmaOperandA operand_Ai;
-        MmaOperandB operand_Bsum;
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int mk = 0; mk < MmaOperandA::kElements; ++mk)
-          operand_Ai[mk] = (kTransformA == ComplexTransform::kConjugate) ?
-                           -A[m*MmaOperandA::kElements + mk].imag() : +A[m*MmaOperandA::kElements + mk].imag();
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int nk = 0; nk < MmaOperandB::kElements; ++nk)
-          operand_Bsum[nk] = B[n*MmaOperandB::kElements + nk].real() + ((kTransformB == ComplexTransform::kConjugate) ?
-                             -B[n*MmaOperandB::kElements + nk].imag() : +B[n*MmaOperandB::kElements + nk].imag());
-
-        // accumulator part3
-        MmaOperandC *accum = reinterpret_cast<MmaOperandC *>(&D) + 
-          (m + n * MmaIterations::kRow) + 2 * MmaIterations::kCount;
-
-        mma(*accum, operand_Ai, operand_Bsum, *accum);
-      }
-    }
-  }
-
-  /// Transform the mma operands to the required types
-  CUTLASS_DEVICE
-  void transform(TransformedFragmentA &dst_A, TransformedFragmentB &dst_B,
-                 FragmentA const &A, FragmentB const &B) const {
-    dst_A = A;
-    dst_B = B;
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_gaussian_complex_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,16 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Defines iterators used by warp-level matrix multiply operations targeting Tensor Cores.
+    \brief Defines iterators to load sparse meta data used by warp-level matrix multiply operations
+   targeting Sparse Tensor Cores.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
 #include "cutlass/array.h"
@@ -43,82 +44,69 @@
 
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/gemm.h"
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/tensor.h"
 #include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor_op_multiplicand_sm80.h"
-#include "cutlass/gemm/warp/mma_complex_tensor_op_tile_iterator_sm80.h"
+#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
 
 #include "cutlass/platform/platform.h"
 #include "cutlass/fast_math.h"
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace warp {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Element type
-    typename Element_,
-    /// Layout of operand in memory
-    typename Layout_,
-    /// Shape of one matrix product operation (concept: MatrixShape)
-    typename InstructionShape_,
-    /// Interval between adjacent *MMA instructions (in units of MMA
-    /// instructions, concept: MatrixShape)
-    typename OpDelta_>
-class MmaTensorOpGaussianComplexAccumulatorTileIterator;
-
 ////////////////////////////////////////////////////////////////////////////////
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/// 
-/// Partial specialization for complex<T>
-///
 template <
     /// Size of the matrix to load (concept: MatrixShape)
     typename Shape_,
-    /// Data type of underlying field of reals.
-    typename RealElement,
-    /// Shape of one matrix product operation (concept: MatrixShape)
+    /// Data type of A elements
+    typename Element_,
+    /// Layout of operand
+    typename Layout_,
+    /// Shape of one matrix production operation (concept: GemmShape)
     typename InstructionShape_,
-    /// Interval between adjacent *MMA instructions (in units of MMA
-    /// instructions, concept: MatrixShape)
-    typename OpDelta_>
-class MmaTensorOpGaussianComplexAccumulatorTileIterator<
-    Shape_, complex<RealElement>, cutlass::layout::RowMajor, InstructionShape_, OpDelta_> {
+    /// Delta between *MMA operations (in units of *MMA operations, concept:
+    /// MatrixShape)
+    int OpDelta_,
+    /// Number of threads participating in one matrix operation
+    int Threads,
+    /// Number of partitions along K dimension
+    int PartitionsK_ = 1>
+class SparseMmaTensorOpMetaTileIterator {
  public:
-
-  /// Shape of tile to load (concept: MatrixShape)
+  /// Shape of tile to load (concept: PitchLinearShape)
   using Shape = Shape_;
 
-  /// Operand tag
-  static Operand const kOperand = Operand::kC;
-
   /// Element type
-  using Element = complex<RealElement>;
+  using Element = Element_;
 
   /// Layout of source tile
-  using Layout = cutlass::layout::RowMajor;
+  using Layout = Layout_;
 
-  /// Shape of one matrix product operation (concept: MatrixShape)
+  /// Shape of one matrix product operation (concept: GemmShape)
   using InstructionShape = InstructionShape_;
 
-  /// Delta between *MMA operations (in units of *MMA operations, concept: MatrixShape)
-  using OpDelta = OpDelta_;
+  /// Delta between *MMA operations (in units of *MMA operations, concept:
+  /// MatrixShape)
+  static int const kOpDelta = OpDelta_;
 
   /// Number of participating threads
   static int const kThreads = 32;
 
+  /// Number of partitions along K dimension
+  static int const kPartitionsK = PartitionsK_;
+
+  static int const kSparse = 2;
+
   /// TensorRef type for loading element from a tensor
   using TensorRef = TensorRef<Element, Layout>;
 
   /// Index type
   using Index = typename TensorRef::Index;
 
   /// Long Index type
@@ -126,265 +114,267 @@
 
   /// Coordinate for an element in the tensor
   using TensorCoord = typename TensorRef::TensorCoord;
 
   /// Internal structure of iterator - made public to enable introspection
   struct Policy {
     static_assert(
-        !(Shape::kRow % InstructionShape::kM) &&
-            !(Shape::kColumn % InstructionShape::kN),
+        !(Shape::kColumn % InstructionShape::kColumn),
         "Shape of warp-level Mma must be divisible by operator shape.");
+    
+    static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
 
-    static_assert(platform::is_same<TensorCoord, MatrixCoord>::value,
-      "Layouts must be defined for logical MatrixCoord coordinate space.");
-
-    /// Number of mma operations performed
-    using MmaIterations = MatrixShape<Shape::kRow / InstructionShape::kM,
-                                      Shape::kColumn / InstructionShape::kN>;
+    // Determine number of elements along outer dimension per individual LDSM op
+    static int const kLdsmOpOuter = InstructionShape::kColumn;
+    static int const kLdsmOpInner = 8 * kElementsPerAccess / kLdsmOpOuter;
+
+    static_assert(!(Shape::kColumn % kLdsmOpOuter),
+                  "Shape of warp-level mma must be divisible by LDSM's "
+                  "fundamental tile size.");
+
+    static_assert(!(Shape::kRow % kLdsmOpInner),
+                  "Shape of warp-level mma must be divisible by LDSM's "
+                  "fundamental tile size.");
+
+    /// Shape of one individual LDSM instruction
+    static int const LdsmShapeColumn =
+        InstructionShape::kColumn / kLdsmOpOuter;
+    static int const LdsmShapeRow =
+        ((4 / LdsmShapeColumn * kLdsmOpInner) > Shape::kRow)
+            ? (Shape::kRow / kLdsmOpInner)
+            : (4 / LdsmShapeColumn);
+    using LdsmShape =
+        layout::PitchLinearShape<LdsmShapeRow, LdsmShapeColumn>;
+
+    /// Number and arrangement of LDSM instructions
+    using LdsmIterations = layout::PitchLinearShape<
+        Shape::kRow / kLdsmOpInner / LdsmShapeRow,
+        1>;
+
+    /// Number of groups for each tile
+    static int const kGroupsPerTile =
+        Shape::kColumn / InstructionShape::kColumn;
   };
 
-private:
+ private:
+  /// Not working on this feature at the moment.
+  static_assert(kOpDelta == 1,
+                "Alternative arrangements not supported at present.");
 
-  // Assume accumulator tile is an arrangement of 8-by-8 tiles replicated over the entire
-  // shape, with each quad mapped to one row and each thread mapped to 1/4 of the elements
-  // of that row. The accumulators within one row are assumed to be consecutive.
- static int const kElementsPerAccess = InstructionShape::kN / 4;
- static int const kRowsPerTile = 8;
- static int const kAccumulatorRows = InstructionShape::kM / kRowsPerTile;
-
-public:
+  /// Pointer type used for accesses
+  using AccessType = Array<Element, Policy::kElementsPerAccess>;
 
+ public:
   //
   // Derived quantities
   //
 
-  /// Fragment object holding a thread's part of a tile. It is assumed that the accumulators
-  /// are stored in a gaussian complex arrangement with parts 1, 2, and 3 as entirely contiguous
-  /// arranged as [part1, part2, part3]
-  using Fragment = Array<RealElement, (Shape::kCount / kThreads) * 3>;
-
-  static int const kPart1Index = (Shape::kCount / kThreads) * 0;
-  static int const kPart2Index = (Shape::kCount / kThreads) * 1;
-  static int const kPart3Index = (Shape::kCount / kThreads) * 2;
+  /// Fragment object holding a thread's part of a tile
+  using Fragment =
+      Array<Element, Shape::kRow * InstructionShape::kColumn / kThreads>;
+
+ private:
+
+  /// Layout object storing stride values
+  Index stride_;
+
+  /// Shared memory base pointers - not advanced
+  AccessType const *pointer_;
 
-private:
+  /// Byte offset incremented as iterator advances
+  Index byte_offset_;
 
-  /// Reference to output tensor
-  TensorRef ref_;
+  /// Internal counter used to determine when to increment byte offset and when
+  /// to XOR it
+  int k_group_idx_;
 
-public:
-  
+ public:
   /// Default ctor constructs null iterator
   CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator() { }
+  SparseMmaTensorOpMetaTileIterator()
+      : pointer_(nullptr),
+        stride_(0),
+        byte_offset_(0),
+        k_group_idx_(0) {}
 
   /// Constructor from TensorRef
-  CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator(
-    TensorRef const &ref, 
-    int lane_id
-  ):
-    ref_(ref) {
-
-    int quad = (lane_id >> 2);
-    int lane_in_quad = (lane_id & 3);
+  CUTLASS_DEVICE
+  SparseMmaTensorOpMetaTileIterator(TensorRef const &ref, int lane_id)
+      : pointer_(reinterpret_cast<AccessType const *>(ref.data())),
+        stride_(ref.stride(0) / Policy::kElementsPerAccess),
+        byte_offset_(0),
+        k_group_idx_(0) {
 
-    MatrixCoord lane_offset(quad, lane_in_quad * kElementsPerAccess);
+    int access_contiguous = (lane_id % (Shape::kRow / Policy::kElementsPerAccess));
+    int access_strided = (lane_id / (Shape::kRow / Policy::kElementsPerAccess));
 
-    ref_.add_coord_offset(lane_offset);
+    byte_offset_ = (access_contiguous + access_strided * stride_) *
+                   sizeof_bits<Element>::value * Policy::kElementsPerAccess / 8;
   }
 
   /// Adds a pointer offset to internal pointer(s) to advance through memory
-  CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator &add_pointer_offset(LongIndex offset) {
-    ref_.add_pointer_offset(offset);
+  CUTLASS_DEVICE
+  SparseMmaTensorOpMetaTileIterator &add_pointer_offset(LongIndex offset) {
+    byte_offset_ += offset * sizeof_bits<Element>::value / 8;
+
     return *this;
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator &add_tile_offset(TensorCoord const &tile_offset) {
-
-    ref_.add_coord_offset(tile_offset * make_Coord(Shape::kRow, Shape::kColumn));
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
+  CUTLASS_DEVICE
+  SparseMmaTensorOpMetaTileIterator &add_tile_offset(
+      TensorCoord const &tile_offset) {
+    int offset = tile_offset.row() * Shape::kRow +
+                 tile_offset.column() * InstructionShape::kColumn * stride_ *
+                     Policy::kElementsPerAccess;
 
+    add_pointer_offset(offset);
     return *this;
   }
 
   /// Advances the iterator along the advance dimension
-  CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator++() {
-    // deliberate no-op
+  CUTLASS_DEVICE
+  SparseMmaTensorOpMetaTileIterator &operator++() {
+    add_tile_offset({0, 1});
+
+    if (kPartitionsK > 1) {
+      ++k_group_idx_;
+      // Jump to next stage
+      if (k_group_idx_ == Policy::kGroupsPerTile) {
+        k_group_idx_ = 0;
+        add_tile_offset(
+            {0, ((kPartitionsK - 1) * Policy::kGroupsPerTile)});
+      }
+    }
+
     return *this;
   }
 
   /// Advances the iterator along the advance dimension
   CUTLASS_HOST_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator--() {
-    // deliberate no-op
-    return *this;
+  SparseMmaTensorOpMetaTileIterator &operator--(){
+    byte_offset_ -= stride_ * InstructionShape::kColumn *
+                    sizeof_bits<Element>::value * Policy::kElementsPerAccess /
+                    8;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
-  CUTLASS_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator+=(TensorCoord const &tile_offset) {
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
+  CUTLASS_DEVICE SparseMmaTensorOpMetaTileIterator &
+  operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  ///< advances in units of whole tiles along the logical coordinate space of
+  ///< the tensor
   CUTLASS_DEVICE
-  MmaTensorOpGaussianComplexAccumulatorTileIterator & operator-=(TensorCoord const &tile_offset) {
+  SparseMmaTensorOpMetaTileIterator &operator-=(
+      TensorCoord const &tile_offset) {
     add_tile_offset(-tile_offset);
     return *this;
   }
 
   /// Loads a fragment from memory at the location pointed to by the iterator.
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const {
-    load_with_pointer_offset(frag, 0);
-  }
+  void load(Fragment &frag) const { load_with_byte_offset(frag, 0); }
 
   /// Loads a fragment from memory with additional logical offset
   CUTLASS_DEVICE
-  void load_with_pointer_offset(
-    Fragment &frag,                             ///< fragment to load from the tensor
-    Index pointer_offset) const {               ///< loads a tile with a linear offset
-  
-    TensorRef offset_ref(ref_);
-    offset_ref.add_pointer_offset(pointer_offset);
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset in units of bytes
+      Index byte_offset) const {
+    Array<unsigned, Policy::LdsmShape::kCount> *fetch_ptr =
+        reinterpret_cast<Array<unsigned, Policy::LdsmShape::kCount> *>(&frag);
 
     CUTLASS_PRAGMA_UNROLL
-    for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
+    for (int s = 0; s < Policy::LdsmIterations::kStrided; ++s) {
       CUTLASS_PRAGMA_UNROLL
-      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-        
-        int mma_accum_start = kAccumulatorRows * kElementsPerAccess * 
-          (mma_n * Policy::MmaIterations::kRow + mma_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int row = 0; row < kAccumulatorRows; ++row) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int col = 0; col < kElementsPerAccess; ++col) {
-            int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
-                          row * kRowsPerTile;
-            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn + col;
-
-            Element z = offset_ref.at({accum_m, accum_n});
-
-            frag[mma_accum_start + row * kElementsPerAccess + col + kPart1Index] = z.real() + z.imag();
-            frag[mma_accum_start + row * kElementsPerAccess + col + kPart2Index] = -z.real();
-            frag[mma_accum_start + row * kElementsPerAccess + col + kPart3Index] = z.imag();
-          }
-        }
+      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
+
+        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
+
+        AccessType const *source_ptr =
+            pointer_ +
+            Policy::LdsmShape::kContiguous * Policy::kLdsmOpInner * c +
+            Policy::LdsmShape::kStrided * s * stride_;
+
+        char const *source_byte_ptr = reinterpret_cast<char const *>(source_ptr) +
+                                      byte_offset + byte_offset_;
+
+        cutlass::arch::ldsm<layout::RowMajor, Policy::LdsmShape::kCount>(
+            fetch_ptr[access_idx], source_byte_ptr);
       }
     }
   }
 
   /// Loads a fragment from memory with additional logical offset
   CUTLASS_DEVICE
-  void load_with_byte_offset(
-    Fragment &frag,                             ///< fragment to load from the tensor
-    Index byte_offset) const {                  ///< loads a tile with a linear offset
-
-    load_with_pointer_offset(byte_offset / sizeof(Element));
+  void load_with_pointer_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a linear offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
   }
 
   /// Loads a fragment from memory with logical offset in units of whole tiles.
   CUTLASS_DEVICE
   void load(
-    Fragment &frag,                             ///< fragment to load from the tensor
-    TensorCoord const &tile_offset) const {     ///< loads a tile with a logical offset in units of whole tiles
-
-    load(frag, tile_offset, 0);
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset) const {
+    load_with_byte_offset(frag, tile_offset, 0);
   }
 
   /// Loads a fragment from memory with logical offset in units of whole tiles.
   CUTLASS_DEVICE
   void load(
-    Fragment &frag,                             ///< fragment to load from the tensor
-    TensorCoord const &tile_offset,             ///< loads a tile with a logical offset in units of whole tiles
-    Index pointer_offset) const {               ///< loads a tile with a logical offset AND a pointer offset
-
-    load_with_pointer_offset(frag, ref_.offset(tile_offset) + pointer_offset);
-  }
-
-  /// Stores a fragment to memory
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) const {
-    store_with_pointer_offset(frag, 0);
-  }
-
-  /// Stores a fragment to memory with additional pointer offset
-  CUTLASS_DEVICE
-  void store_with_pointer_offset(
-    Fragment const &frag,                       ///< fragment to store from the tensor
-    Index pointer_offset) const {               ///< store a tile with a linear offset
-  
-    TensorRef offset_ref(ref_);
-    offset_ref.add_pointer_offset(pointer_offset);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int mma_n = 0; mma_n < Policy::MmaIterations::kColumn; ++mma_n) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int mma_m = 0; mma_m < Policy::MmaIterations::kRow; ++mma_m) {
-        
-        int mma_accum_start = kAccumulatorRows * kElementsPerAccess * 
-          (mma_n * Policy::MmaIterations::kRow + mma_m);
-
-        CUTLASS_PRAGMA_UNROLL
-        for (int row = 0; row < kAccumulatorRows; ++row) {
-          CUTLASS_PRAGMA_UNROLL
-          for (int col = 0; col < kElementsPerAccess; ++col) {
-            int accum_m = mma_m * InstructionShape::kM * OpDelta::kRow +
-                          row * kRowsPerTile;
-            int accum_n = mma_n * InstructionShape::kN * OpDelta::kColumn + col;
-            int idx = mma_accum_start + row * kElementsPerAccess + col;
-
-            Element z(frag[kPart1Index + idx] - frag[kPart3Index + idx], 
-                      frag[kPart1Index + idx] + frag[kPart2Index + idx]);
-
-            offset_ref.at({accum_m, accum_n}) = z;
-          }
-        }
-      }
-    }
-  }
-
-  /// Stores a fragment to memory with additional pointer offset
-  CUTLASS_DEVICE
-  void store_with_byte_offset(
-    Fragment const &frag,                       ///< fragment to store from the tensor
-    Index byte_offset) const {                  ///< store a tile with a linear offset
-
-    store_with_pointer_offset(byte_offset / sizeof(Element));
-  }
-
-  /// Stores a fragment to memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void store(
-    Fragment &frag,                             ///< fragment to store to the tensor
-    TensorCoord const &tile_offset) const {     ///< stores a tile with a logical offset in units of whole tiles
-
-    store(frag, tile_offset, 0);
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
+      TensorCoord const &tile_offset,
+      /// loads a tile with a logical offset AND a pointer offset
+      Index pointer_offset) const {
+    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
   }
 
-  /// Stores a fragment from memory with logical offset in units of whole tiles.
+  /// Loads a fragment from memory with logical offset in units of whole tiles.
   CUTLASS_DEVICE
-  void store(
-      /// fragment to store to the tensor
-      Fragment const &frag,
-      /// stores a tile with a logical offset in units of whole tiles
+  void load_with_byte_offset(
+      /// fragment to load from the tensor
+      Fragment &frag,
+      /// loads a tile with a logical offset in units of whole tiles
       TensorCoord const &tile_offset,
-      /// stores a tile with a logical offset AND a pointer offset
-      Index pointer_offset) const {
-    store_with_pointer_offset(frag, ref_.offset(tile_offset) + pointer_offset);
+      /// loads a tile with a logical offset AND a pointer offset
+      Index byte_offset) const {
+    Index pointer_offset = 
+      tile_offset.contiguous() * Shape::kRow / Layout::kElementsPerAccess + 
+      tile_offset.strided() * InstructionShape::kColumn * stride_;
+
+    byte_offset += sizeof(AccessType) * pointer_offset;
+
+    load_with_byte_offset(frag, byte_offset);
+  }
+
+  /// Notify the iterator which k-group it is currently pointing to.
+  ///
+  /// This does not advance the iterator. Rather, it overrides its internal
+  /// tracking with constant-valued k-group index to enable the compiler to
+  /// fold constants and achieve more efficient code.
+  ///
+  /// This is used by some nontrivial permuted layouts.
+  CUTLASS_DEVICE
+  void set_kgroup_index(int k_group) {
+    // no op
   }
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace warp
 } // namespace gemm
 } // namespace cutlass
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_policy.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_simt_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_sparse_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_fragment_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_policy.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_access_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sparse.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h`

 * *Files 20% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,357 +24,394 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Defines iterators to load sparse meta data used by warp-level matrix multiply operations
-   targeting Sparse Tensor Cores.
+    \brief Templates implementing computing the addresses of loading small
+    vectors from the global memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
 #include "cutlass/array.h"
-#include "cutlass/numeric_types.h"
-#include "cutlass/tensor_ref.h"
-#include "cutlass/matrix_shape.h"
-
-#include "cutlass/arch/memory_sm75.h"
-#include "cutlass/gemm/gemm.h"
-
+#include "cutlass/coord.h"
+#include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/tensor.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
-
-#include "cutlass/platform/platform.h"
-#include "cutlass/fast_math.h"
+#include "cutlass/matrix_coord.h"
+#include "cutlass/matrix_shape.h"
+#include "cutlass/tensor_ref.h"
 
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
-namespace warp {
+namespace transform {
+namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
+/// PredicatedVectorAccessIterator
+///
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Layout of operand
-    typename Layout_,
-    /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape_,
-    /// Delta between *MMA operations (in units of *MMA operations, concept:
-    /// MatrixShape)
-    int OpDelta_,
-    /// Number of threads participating in one matrix operation
-    int Threads,
-    /// Number of partitions along K dimension
-    int PartitionsK_ = 1>
-class SparseMmaTensorOpMetaTileIterator {
- public:
-  /// Shape of tile to load (concept: PitchLinearShape)
-  using Shape = Shape_;
+    /// Shape of the vector accessed by the entire threadblock
+    typename Shape,
+    /// Shape of the vector accessed by the warp
+    typename WarpShape,
+    /// Type of Element
+    typename Element,
+    /// Layout of the vector
+    typename Layout,
+    /// Number of elements for each access
+    int ElementsPerAccess,
+    /// Support residual tile
+    bool EnableResidualAccess = false
+>
+class PredicatedVectorAccessIterator;
 
-  /// Element type
-  using Element = Element_;
-
-  /// Layout of source tile
-  using Layout = Layout_;
-
-  /// Shape of one matrix product operation (concept: GemmShape)
-  using InstructionShape = InstructionShape_;
-
-  /// Delta between *MMA operations (in units of *MMA operations, concept:
-  /// MatrixShape)
-  static int const kOpDelta = OpDelta_;
+////////////////////////////////////////////////////////////////////////////////
 
-  /// Number of participating threads
-  static int const kThreads = 32;
+/// Vector access iterator specialized for vectors, e.g. scale and bias
+/// Thread arrangements are for TensorOps
+///
+template <
+  typename Shape_, 
+  typename WarpShape_, 
+  typename Element_, 
+  int ElementsPerAccess, 
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator <
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::PitchLinear,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+  public:
 
-  /// Number of partitions along K dimension
-  static int const kPartitionsK = PartitionsK_;
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::PitchLinear;
 
-  static int const kSparse = 2;
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-  /// TensorRef type for loading element from a tensor
   using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
-
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
+  static int const kElementsPerAccess = ElementsPerAccess;
+  static int const kThreads = 32;
+  static int const kRowsPerIteration = 8;
+  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
+  static int const kThreadsPerRowMask = 0x3;
+  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
+  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
 
-  /// Internal structure of iterator - made public to enable introspection
-  struct Policy {
-    static_assert(
-        !(Shape::kColumn % InstructionShape::kColumn),
-        "Shape of warp-level Mma must be divisible by operator shape.");
-    
-    static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
-
-    // Determine number of elements along outer dimension per individual LDSM op
-    static int const kLdsmOpOuter = InstructionShape::kColumn;
-    static int const kLdsmOpInner = 8 * kElementsPerAccess / kLdsmOpOuter;
-
-    static_assert(!(Shape::kColumn % kLdsmOpOuter),
-                  "Shape of warp-level mma must be divisible by LDSM's "
-                  "fundamental tile size.");
-
-    static_assert(!(Shape::kRow % kLdsmOpInner),
-                  "Shape of warp-level mma must be divisible by LDSM's "
-                  "fundamental tile size.");
-
-    /// Shape of one individual LDSM instruction
-    static int const LdsmShapeColumn =
-        InstructionShape::kColumn / kLdsmOpOuter;
-    static int const LdsmShapeRow =
-        ((4 / LdsmShapeColumn * kLdsmOpInner) > Shape::kRow)
-            ? (Shape::kRow / kLdsmOpInner)
-            : (4 / LdsmShapeColumn);
-    using LdsmShape =
-        layout::PitchLinearShape<LdsmShapeRow, LdsmShapeColumn>;
-
-    /// Number and arrangement of LDSM instructions
-    using LdsmIterations = layout::PitchLinearShape<
-        Shape::kRow / kLdsmOpInner / LdsmShapeRow,
-        1>;
-
-    /// Number of groups for each tile
-    static int const kGroupsPerTile =
-        Shape::kColumn / InstructionShape::kColumn;
-  };
+  using AccessType = AlignedArray<Element, kElementsPerAccess>;
 
  private:
-  /// Not working on this feature at the moment.
-  static_assert(kOpDelta == 1,
-                "Alternative arrangements not supported at present.");
-
-  /// Pointer type used for accesses
-  using AccessType = Array<Element, Policy::kElementsPerAccess>;
+  /// Internal pointer type permits fast address arithmetic
+  using BytePointer = char *;
 
- public:
+ private:
   //
-  // Derived quantities
+  // Data members
   //
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment =
-      Array<Element, Shape::kRow * InstructionShape::kColumn / kThreads>;
+  /// Internal pointer to first access of tile
+  BytePointer pointer_;
 
- private:
+  /// Extent of tensor
+  TensorCoord extent_;
 
-  /// Layout object storing stride values
-  Index stride_;
+  /// pointer offset of each thread
+  TensorCoord thread_offset_;
 
-  /// Shared memory base pointers - not advanced
-  AccessType const *pointer_;
+  /// iteration index
+  LongIndex iteration_;
 
-  /// Byte offset incremented as iterator advances
-  Index byte_offset_;
+  /// residual access
+  bool is_residual_;
 
-  /// Internal counter used to determine when to increment byte offset and when
-  /// to XOR it
-  int k_group_idx_;
+  /// residual offset of each thread
+  TensorCoord residual_offset_;
 
  public:
-  /// Default ctor constructs null iterator
+  /// Constructs a vector access iterator
   CUTLASS_HOST_DEVICE
-  SparseMmaTensorOpMetaTileIterator()
-      : pointer_(nullptr),
-        stride_(0),
-        byte_offset_(0),
-        k_group_idx_(0) {}
+  PredicatedVectorAccessIterator(
+    /// Pointer to the start of the vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    /// ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id,
+    /// Initial offset of threadblock
+    TensorCoord const &threadblock_offset)
+    : pointer_(reinterpret_cast<BytePointer>(
+                       const_cast<NonConstPointer>(pointer))),
+      extent_(extent),
+      is_residual_(false) {
+
+
+    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
+
+    // Per-thread offset in logical coordinates of tensor
+
+    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
+        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
+
+    set_iteration_index(0);
+
+    if(EnableResidualAccess) {
+      // compute residual offset
+      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
+      if (residual_size) {
+        is_residual_ = true;
+        residual_offset_ = make_Coord(residual_size, 0);
+      }
+    }
+  }
 
-  /// Constructor from TensorRef
-  CUTLASS_DEVICE
-  SparseMmaTensorOpMetaTileIterator(TensorRef const &ref, int lane_id)
-      : pointer_(reinterpret_cast<AccessType const *>(ref.data())),
-        stride_(ref.stride(0) / Policy::kElementsPerAccess),
-        byte_offset_(0),
-        k_group_idx_(0) {
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+    /// Pointer to start of vector
+    ConstPointer pointer,
+    /// Extent of vector
+    TensorCoord extent,
+    ///< ID of each participating thread
+    int thread_id,
+    /// ID of each participating warp
+    int warp_id)
+    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
+                                     make_Coord(0, 0)) {}
 
-    int access_contiguous = (lane_id % (Shape::kRow / Policy::kElementsPerAccess));
-    int access_strided = (lane_id / (Shape::kRow / Policy::kElementsPerAccess));
 
-    byte_offset_ = (access_contiguous + access_strided * stride_) *
-                   sizeof_bits<Element>::value * Policy::kElementsPerAccess / 8;
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
+    iteration_ = index;
   }
 
-  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
   CUTLASS_DEVICE
-  SparseMmaTensorOpMetaTileIterator &add_pointer_offset(LongIndex offset) {
-    byte_offset_ += offset * sizeof_bits<Element>::value / 8;
+  void add_tile_offset(
+      TensorCoord const &tile_offset) {
 
-    return *this;
+    thread_offset_ =
+        thread_offset_ +
+        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
-  CUTLASS_DEVICE
-  SparseMmaTensorOpMetaTileIterator &add_tile_offset(
-      TensorCoord const &tile_offset) {
-    int offset = tile_offset.row() * Shape::kRow +
-                 tile_offset.column() * InstructionShape::kColumn * stride_ *
-                     Policy::kElementsPerAccess;
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
 
-    add_pointer_offset(offset);
-    return *this;
+    return reinterpret_cast<AccessType *>(
+        pointer_ +
+        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
+        * sizeof_bits<Element>::value / 8));
   }
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_DEVICE
-  SparseMmaTensorOpMetaTileIterator &operator++() {
-    add_tile_offset({0, 1});
-
-    if (kPartitionsK > 1) {
-      ++k_group_idx_;
-      // Jump to next stage
-      if (k_group_idx_ == Policy::kGroupsPerTile) {
-        k_group_idx_ = 0;
-        add_tile_offset(
-            {0, ((kPartitionsK - 1) * Policy::kGroupsPerTile)});
-      }
-    }
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator &operator++() {
+    ++iteration_;
+    if(iteration_ >= kIterations)
+      iteration_ = 0; 
 
     return *this;
   }
 
-  /// Advances the iterator along the advance dimension
+  /// Increment and return an instance to self.
   CUTLASS_HOST_DEVICE
-  SparseMmaTensorOpMetaTileIterator &operator--(){
-    byte_offset_ -= stride_ * InstructionShape::kColumn *
-                    sizeof_bits<Element>::value * Policy::kElementsPerAccess /
-                    8;
+  void advance() {
+    if(EnableResidualAccess && is_residual_) {
+      is_residual_ = false;
+      thread_offset_ += residual_offset_; 
+    }
+    else
+      add_tile_offset(TensorCoord(1, 0));
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
-  CUTLASS_DEVICE SparseMmaTensorOpMetaTileIterator &
-  operator+=(TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
-    return *this;
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
+    operator++();
+    return self;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
-  CUTLASS_DEVICE
-  SparseMmaTensorOpMetaTileIterator &operator-=(
-      TensorCoord const &tile_offset) {
-    add_tile_offset(-tile_offset);
-    return *this;
+  /// Returns whether access is valid or not
+  CUTLASS_HOST_DEVICE
+  bool valid() {
+    return ((thread_offset_.contiguous() + 
+              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
   }
+};
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Specialization of PredicatedVectorAccessIterator for row-major data.
+///
+template <
+  typename Shape_,
+  typename WarpShape_,
+  typename Element_,
+  int ElementsPerAccess,
+  bool EnableResidualAccess
+>
+class PredicatedVectorAccessIterator<
+  Shape_,
+  WarpShape_,
+  Element_,
+  layout::RowMajor,
+  ElementsPerAccess,
+  EnableResidualAccess
+> {
+ public:
+
+  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorView = TensorView<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ConstPointer = const Element *;
+  using NonConstPointer = typename platform::remove_const<Element>::type *;
+
+  using UnderlyingIterator = PredicatedVectorAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
+      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
+      Element,
+      layout::PitchLinear,
+      ElementsPerAccess,
+      EnableResidualAccess>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
+  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
+  static int const kThreads = UnderlyingIterator::kThreads;
+  static int const kIterations = UnderlyingIterator::kIterations;
+
+ private:
+  //
+  // Data members
+  //
+
+  /// Underlying pitch-linear tile iterator
+  UnderlyingIterator iterator_;
+
+ public:
+  /// Constructs a TileIterator from its precomputed state, threadblock offset,
+  /// and thread ID
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const { load_with_byte_offset(frag, 0); }
+  PredicatedVectorAccessIterator(
+      ///< Pointer to the start of the vector
+      ConstPointer pointer,
+      ///< Extent of tensor
+      TensorCoord extent,
+      ///< ID of each participating thread
+      int thread_id,
+      ///< ID of each participating warp
+      int warp_id,
+      ///< Initial offset of threadblock
+      TensorCoord const &threadblock_offset)
+      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
+                  thread_id, warp_id,
+                  layout::PitchLinearCoord(threadblock_offset.column(),
+                                           threadblock_offset.row())) {}
 
-  /// Loads a fragment from memory with additional logical offset
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset in units of bytes
-      Index byte_offset) const {
-    Array<unsigned, Policy::LdsmShape::kCount> *fetch_ptr =
-        reinterpret_cast<Array<unsigned, Policy::LdsmShape::kCount> *>(&frag);
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < Policy::LdsmIterations::kStrided; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
-
-        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
-
-        AccessType const *source_ptr =
-            pointer_ +
-            Policy::LdsmShape::kContiguous * Policy::kLdsmOpInner * c +
-            Policy::LdsmShape::kStrided * s * stride_;
+  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator(
+      ConstPointer pointer,   ///< Pointer to the start of the vector
+      TensorCoord extent,     ///< Extent of tensor
+      int thread_id,          ///< ID of each participating thread
+      int warp_id             ///< ID of each participating warp
+      )
+      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
+                                        make_Coord(0, 0)) {}
 
-        char const *source_byte_ptr = reinterpret_cast<char const *>(source_ptr) +
-                                      byte_offset + byte_offset_;
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-        cutlass::arch::ldsm<layout::RowMajor, Policy::LdsmShape::kCount>(
-            fetch_ptr[access_idx], source_byte_ptr);
-      }
-    }
+  /// Advances an iterator along logical dimensions of matrix in units of whole
+  /// tiles
+  CUTLASS_HOST_DEVICE
+  void add_tile_offset(TensorCoord const &tile_offset) {
+    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
   }
 
-  /// Loads a fragment from memory with additional logical offset
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset
-      Index pointer_offset) const {
-    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset) const {
-    load_with_byte_offset(frag, tile_offset, 0);
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index pointer_offset) const {
-    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
+  /// Advances to the next tile in memory.
+  ///
+  /// The first time this method is called, predicates are updated, and the
+  /// iterator's internal pointer is reverted to the first "steady state" tile.
+  /// Subsequent calls are lightweight and must only update the internal
+  /// pointer.
+  CUTLASS_HOST_DEVICE
+  PredicatedVectorAccessIterator operator++(int) {
+    PredicatedVectorAccessIterator self(*this);
+    operator++();
+    return self;
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index byte_offset) const {
-    Index pointer_offset = 
-      tile_offset.contiguous() * Shape::kRow / Layout::kElementsPerAccess + 
-      tile_offset.strided() * InstructionShape::kColumn * stride_;
-
-    byte_offset += sizeof(AccessType) * pointer_offset;
-
-    load_with_byte_offset(frag, byte_offset);
+  /// Increment and return an instance to self.
+  CUTLASS_HOST_DEVICE
+  void advance() {
+    iterator_.advance();
   }
 
-  /// Notify the iterator which k-group it is currently pointing to.
-  ///
-  /// This does not advance the iterator. Rather, it overrides its internal
-  /// tracking with constant-valued k-group index to enable the compiler to
-  /// fold constants and achieve more efficient code.
-  ///
-  /// This is used by some nontrivial permuted layouts.
-  CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    // no op
+  /// Returns whether access is valid or not
+  CUTLASS_HOST_DEVICE
+  bool valid() {
+    return iterator_.valid();
   }
 };
 
-} // namespace warp
-} // namespace gemm
-} // namespace cutlass
 
 ////////////////////////////////////////////////////////////////////////////////
+
+}  // namespace threadblock
+}  // namespace transform 
+}  // namespace cutlass
+
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_wmma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -489,15 +489,15 @@
 
   /// Advances the iterator along the opposite of the advance dimension
   CUTLASS_HOST_DEVICE
   MmaTensorOpWmmaMultiplicandTileIterator & operator--() {
 
     Index elements_offset = layout_({WmmaShape::kRow, 0});
 
-    byte_offset_ -= (elements_offset * sizeof_bits<Element>::value) / 8;
+    byte_offset_ -= (elements_offset + sizeof_bits<Element>::value) / 8;
     return *this;
   }
 
   ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_DEVICE
   MmaTensorOpWmmaMultiplicandTileIterator & operator+=(TensorCoord const &tile_offset) {
     add_tile_offset(tile_offset);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_tensor_op_wmma.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/mma_with_reduction_tensor_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -40,15 +40,15 @@
 #include "cutlass/platform/platform.h"
 
 #include "cutlass/numeric_conversion.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/arch/memory_sm75.h"
-#include "cutlass/arch/mma_sm75.h"
+#include "cutlass/arch/mma_sm75.h" 
 #include "cutlass/arch/mma_sm80.h"
 
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/warp/mma.h"
 
 #include "cutlass/gemm/warp/mma_tensor_op_policy.h"
 #include "cutlass/gemm/warp/mma_tensor_op.h"
@@ -77,15 +77,15 @@
   typename LayoutB_,
   /// Element type of C matrix
   typename ElementC_,
   /// Layout of C matrix (concept: MatrixLayout)
   typename LayoutC_,
   /// Policy describing warp-level MmaTensorOp (concept: MmaTensorOp policy)
   typename Policy_,
-  /// Reduce operand A or B along K dimension
+  ///
   bool ReduceKForA_,
   /// Number of partitions along K dimension
   int PartitionsK_ = 1,
   /// Store the accumulators in row major or column major.  Row major is used
   /// when output layout is interleaved.
   bool AccumulatorsInRowMajor = false,
   /// Used for partial specialization
@@ -116,17 +116,17 @@
 
   /// Shape of the warp in units of thread (concept: MmaLanePolicySimt)
   using Policy = Policy_;
 
   /// Underlying matrix multiply operator (concept: arch::Mma)
   using ArchMmaOperator = typename Policy::Operator;
 
-  /// Indicates math operator
+  /// Indicates math operator 
   using MathOperator = typename ArchMmaOperator::Operator;
-
+  
   /// Architecture tag from underlying instruction
   using ArchTag = typename ArchMmaOperator::ArchTag;
 
   /// Indicates class of matrix operator
   using OperatorClass = arch::OpClassTensorOp;
 
   /// Shape of underlying instruction
@@ -219,30 +219,30 @@
   /// Ctor
   CUTLASS_DEVICE
   MmaWithReductionTensorOp() {}
 
   /// Performs a warp-level matrix multiply-accumulate operation
   CUTLASS_DEVICE
   void operator()(
-    FragmentC &D,
-    TransformedFragmentA const &A,
-    TransformedFragmentB const &B,
+    FragmentC &D, 
+    TransformedFragmentA const &A, 
+    TransformedFragmentB const &B, 
     FragmentC const &C,
     FragmentReduction &gemm_k_reduction
   ) const {
 
     using MmaOperandA = typename ArchMmaOperator::FragmentA;
     using MmaOperandB = typename ArchMmaOperator::FragmentB;
     using MmaOperandC = typename ArchMmaOperator::FragmentC;
 
     D = C;
 
-    [[maybe_unused]] MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
-    [[maybe_unused]] MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
-    [[maybe_unused]] MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
+    MmaOperandA const *ptr_A = reinterpret_cast<MmaOperandA const *>(&A);
+    MmaOperandB const *ptr_B = reinterpret_cast<MmaOperandB const *>(&B);
+    MmaOperandC *ptr_D = reinterpret_cast<MmaOperandC *>(&D);
 
     #if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ < 800)
       assert(0);
     #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
       // Serpentine visitation order maximizing reuse of Ra
       CUTLASS_PRAGMA_UNROLL
       for (int m = 0; m < MmaIterations::kRow; ++m) {
@@ -254,15 +254,15 @@
 
           mma(ptr_D[m + n_serpentine * MmaIterations::kRow],
               ptr_A[m],
               ptr_B[n_serpentine],
               ptr_D[m + n_serpentine * MmaIterations::kRow]);
 
           if (!kReduceKForA && m == 0) {
-            #if 0
+            #if 0 
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 1]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 2]);
             gemm_k_reduction[n_serpentine] += float(B[n_serpentine * 4 + 3]);
             #else
             uint32_t const *tmp = reinterpret_cast<uint32_t const *>(&B);
 
@@ -302,20 +302,20 @@
             } else {
                 assert(0);
             }
             #endif
           }
 
           if (kReduceKForA && (n == 0)) {
-            #if 0
+            #if 0 
             gemm_k_reduction[m * 2] += float(A[m * 8]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 1]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 4]);
             gemm_k_reduction[m * 2] += float(A[m * 8 + 5]);
-
+  
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 2]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 3]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 6]);
             gemm_k_reduction[m * 2 + 1] += float(A[m * 8 + 7]);
             #else
             uint32_t const *tmp = reinterpret_cast<uint32_t const *>(&A);
 
@@ -407,17 +407,17 @@
                             FragmentB::kElements / 2, kRoundB>
           convert_B;
       Array<ElementB, FragmentB::kElements / 2> const *ptr_B =
           reinterpret_cast<Array<ElementB, FragmentB::kElements / 2> const *>(&B);
       Array<typename ArchMmaOperator::ElementB, FragmentB::kElements / 2> *
           ptr_dst_B = reinterpret_cast<Array<typename ArchMmaOperator::ElementB,
                                              FragmentB::kElements / 2> *>(&dst_B);
-
+  
       dst_A = convert_A(A);
-
+  
       ptr_dst_B[0] = convert_B(ptr_B[0]);
       ptr_dst_B[1] = convert_B(ptr_B[1]);
 
     #elif defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 800)
       detail::ConvertAndPack<typename ArchMmaOperator::ElementA, ElementA,
                             FragmentA::kElements / 2, kRoundA>
           convert_A;
@@ -425,17 +425,17 @@
                             FragmentB::kElements, kRoundB>
           convert_B;
       Array<ElementA, FragmentA::kElements / 2> const *ptr_A =
           reinterpret_cast<Array<ElementA, FragmentA::kElements / 2> const *>(&A);
       Array<typename ArchMmaOperator::ElementA, FragmentA::kElements / 2> *
           ptr_dst_A = reinterpret_cast<Array<typename ArchMmaOperator::ElementA,
                                              FragmentA::kElements / 2> *>(&dst_A);
-
+  
       dst_B = convert_B(B);
-
+  
       ptr_dst_A[0] = convert_A(ptr_A[0]);
       ptr_dst_A[1] = convert_A(ptr_A[1]);
     #else
       assert(0);
     #endif
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/scale_bias_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,551 +24,529 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Defines iterators used by warp-level loading scale and bias vectors.
-   Every scale/bias data only needs to be loaded once for every channel.
+    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
+
+    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
+    first, with the objective of minimizing predicate mask updates during steady-state operation.
+
+    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
+    and integer addition is used to advance the pointer through memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-
-#include "cutlass/array.h"
-#include "cutlass/numeric_types.h"
 #include "cutlass/tensor_ref.h"
-#include "cutlass/matrix_shape.h"
-
-#include "cutlass/arch/memory_sm75.h"
-#include "cutlass/gemm/gemm.h"
-
 #include "cutlass/layout/matrix.h"
-#include "cutlass/layout/tensor.h"
 #include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/tensor_op_multiplicand_sm75.h"
 
-#include "cutlass/platform/platform.h"
-#include "cutlass/fast_math.h"
+#include "regular_tile_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace gemm {
-namespace warp {
+namespace transform {
+namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
+/// Regular tile iterator specialized for pitch-linear.  This one is used by 2-stage SIMT kernels
+/// and sparse tensor core meta data.
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Data type of A elements
-    typename Element_,
-    /// Layout of operand
-    typename Layout_,
-    /// Shape of one matrix production operation (concept: GemmShape)
-    typename InstructionShape_,
-    /// Policy of the details of LDSM shape and iterations
-    typename Policy_,
-    /// Number of threads participating in one matrix operation
-    int Threads,
-    /// Number of partitions along K dimension
-    int PartitionsK_ = 1>
-class ScaleBiasTileIterator;
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// This tile iterator is specialized for 32-thread TensorOps. It uses LDSM to
-/// load from shared memory and therefore must be initialized with a TensorRef
-/// to shared memory.
-///
-/// Satisfies:
-///   ReadableRandomAccessContiguousTileIteratorConcept
-///
-template <
-    /// Size of the matrix to load (concept: PitchLinearShape)
-    typename Shape_,
-    /// Data type of elements
-    typename Element_,
-    /// Shape of one matrix product operation (concept: PitchLinearShape)
-    typename InstructionShape_,
-    /// Policy of the details of LDSM shape and iterations
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK_>
-class ScaleBiasTileIterator<Shape_, Element_, cutlass::layout::PitchLinear,
-                             InstructionShape_, Policy_, 32, PartitionsK_> {
- public:
-  /// Shape of tile to load (concept: PitchLinearShape)
-  using Shape = Shape_;
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
-  /// Element type
+  using Shape = Shape_;
   using Element = Element_;
+  using Layout = layout::PitchLinear;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
 
-  /// Layout of source tile
-  using Layout = cutlass::layout::PitchLinear;
-
-  /// Shape of one matrix product operation (concept: GemmShape)
-  using InstructionShape = InstructionShape_;
-
-  /// Number of participating threads
-  static int const kThreads = 32;
-
-  /// Number of partitions along K dimension
-  static int const kPartitionsK = PartitionsK_;
-
-  /// Number of partitions along K dimension
-  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
-
-  /// TensorRef type for loading element from a tensor
   using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
-
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+  
+  using AccessType = AlignedArray<Element, ThreadMap::kElementsPerAccess, kAlignment>;
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the contiguous or strided dimensions.");
 
-  /// Internal structure of iterator - made public to enable introspection
-  using Policy = Policy_;
+private:
 
- private:
-
-  /// Pointer type used for accesses
-  using AccessType = Array<Element, kElementsPerAccess>;
+  //
+  // Types
+  //
 
- public:
   //
-  // Derived quantities
+  // Data members
   //
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = Array<Element, 2 * Policy::kLdsmOpInner *
-                                      InstructionShape::kContiguous / kThreads>;
+  /// Pointer to memory
+  uint8_t *pointer_;
 
- private:
+  /// Stride quantity
+  StrideIndex stride_;
 
-  /// Shared memory base pointers - not advanced
-  AccessType const *pointer_;
+  /// Amount to increment pointer along strided dimension
+  Index increment_strided_;
 
-  /// Byte offset incremented as iterator advances
-  Index byte_offset_;
+  /// Amount to advance pointer between tiles
+  Index increment_advance_;
 
-  /// Internal counter used to determine when to increment byte offset and when
-  /// to XOR it
-  int k_group_idx_;
+public:
 
- public:
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator()
-      : pointer_(nullptr),
-        byte_offset_(0),
-        k_group_idx_(0) {}
+  CUTLASS_DEVICE
+  RegularTileIterator(): pointer_(nullptr), increment_strided_(0), increment_advance_(0) { }
 
-  /// Constructor from TensorRef
   CUTLASS_DEVICE
-  ScaleBiasTileIterator(TensorRef const &ref_scale_bias,
-                         int lane_id)
-      : byte_offset_(0), k_group_idx_(0) {
-    /// 16816 only
-    pointer_ = reinterpret_cast<AccessType const *>(ref_scale_bias.data()) +
-               ((lane_id >> 3) & 1) * Shape::kContiguous / kElementsPerAccess +
-               (lane_id >> 4);
+  RegularTileIterator(
+    TensorRef const &ref, 
+    int thread_idx
+  ): 
+    pointer_(reinterpret_cast<uint8_t *>(ref.data()) + (ref.offset(ThreadMap::initial_offset(thread_idx)) * sizeof_bits<Element>::value / 8)) {
+    
+    stride_ = ref.stride()[0];
+    increment_strided_ = (ref.stride()[0] * sizeof_bits<Element>::value) * ThreadMap::Delta::kStrided / 8;
+    
+    increment_advance_ = 
+      (kAdvanceRank == 0 ? 
+        Shape::kContiguous * sizeof_bits<Element>::value / 8 : 
+        Shape::kStrided * (ref.stride()[0] * sizeof_bits<Element>::value / 8));
   }
 
-  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  /// Loads a fragment
   CUTLASS_DEVICE
-  ScaleBiasTileIterator &add_pointer_offset(LongIndex offset) {
-    byte_offset_ += offset * sizeof_bits<Element>::value / 8;
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
 
-    return *this;
-  }
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    uint8_t const *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &add_tile_offset(
-      TensorCoord const &tile_offset) {
-    int whole_tiles = tile_offset.contiguous() / Policy::kGroupsPerTile;
-    int k_groups_delta = tile_offset.contiguous() % Policy::kGroupsPerTile;
-
-    byte_offset_ += k_groups_delta * sizeof_bits<Element>::value *
-                    kElementsPerAccess * Policy::LdsmShape::kContiguous / 8;
-
-    // Multiply by 2 because scale and bias belonging to the same stage are next
-    // to each other in the shared memory.
-    pointer_ += (2 * whole_tiles * Shape::kContiguous / kElementsPerAccess);
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
 
-    return *this;
-  }
+      AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_pointer);
 
-  /// Advances the iterator along the advance dimension
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &operator++() {
-    byte_offset_ += Policy::LdsmShape::kContiguous *
-                    sizeof_bits<Element>::value * kElementsPerAccess / 8;
-
-    k_group_idx_++;
-
-    if (k_group_idx_ == (Policy::kGroupsPerTile / kPartitionsK)) {
-      k_group_idx_ = 0;
-      byte_offset_ -= (Policy::kGroupsPerTile / kPartitionsK) *
-                      Policy::LdsmShape::kContiguous *
-                      sizeof_bits<Element>::value * kElementsPerAccess / 8;
-      add_tile_offset({Policy::kGroupsPerTile, 0});
-    }
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-    return *this;
+        int idx = c + s * ThreadMap::Iterations::kContiguous;
+        frag_ptr[idx] = access_ptr[c * ThreadMap::Delta::kContiguous /
+                                   ThreadMap::kElementsPerAccess];
+      }
+
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
   }
 
-  /// Advances the iterator along the advance dimension
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator &operator--() { assert(0); }
-
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &operator+=(
-      TensorCoord const &tile_offset) {
-    add_tile_offset(tile_offset);
-    return *this;
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    load_with_pointer_offset(
+      frag, 
+      tile_offset.contiguous() * Shape::kContiguous / ThreadMap::kElementsPerAccess + 
+        tile_offset.strided() * Shape::kStrided * stride_
+    );
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &operator-=(
-      TensorCoord const &tile_offset) {
-    add_tile_offset(-tile_offset);
-    return *this;
+  /// Loads a fragment
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const { load_with_byte_offset(frag, 0); }
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-  /// Loads a fragment from memory with additional logical offset
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset in units of bytes
-      Index byte_offset) const {
-    Array<unsigned, 4> *fetch_ptr =
-        reinterpret_cast<Array<unsigned, 4> *>(&frag);
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const*>(&frag);
+    uint8_t *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
 
     CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < 1; ++s) {
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < Policy::LdsmIterations::kContiguous; ++c) {
-        int access_idx = c + s * Policy::LdsmIterations::kContiguous;
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
 
-        AccessType const *source_ptr =
-            pointer_ + Policy::LdsmShape::kContiguous * c;
+      AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_pointer);
 
-        char const *source_byte_ptr =
-            reinterpret_cast<char const *>(source_ptr) + byte_offset +
-            byte_offset_;
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
+
+        int idx = c + s * ThreadMap::Iterations::kContiguous;
+        access_ptr[c * ThreadMap::Delta::kContiguous /
+                   ThreadMap::kElementsPerAccess] = frag_ptr[idx];
+      }
 
-        cutlass::arch::ldsm<layout::RowMajor, 4>(
-            fetch_ptr[access_idx], source_byte_ptr);
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
       }
     }
   }
 
-  /// Loads a fragment from memory with additional logical offset
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset
-      Index pointer_offset) const {
-    load_with_byte_offset(frag, pointer_offset * sizeof(Element));
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    store_with_pointer_offset(
+      frag,
+      tile_offset.contiguous() * Shape::kContiguous + tile_offset.strided() * Shape::kStrided * stride_
+    );
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset) const {
-    load_with_byte_offset(frag, tile_offset, 0);
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index pointer_offset) const {
-    load_with_byte_offset(frag, tile_offset, pointer_offset * sizeof(Element));
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator &operator++() {
+    pointer_ += increment_advance_;
+    return *this;
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index byte_offset) const {
-    Index pointer_offset = tile_offset.contiguous() *
-                               InstructionShape::kContiguous /
-                               kElementsPerAccess;
-
-    byte_offset += sizeof_bits<AccessType>::value * pointer_offset / 8;
-
-    load_with_byte_offset(frag, byte_offset);
-  }
-
-  /// Notify the iterator which k-group it is currently pointing to.
-  ///
-  /// This does not advance the iterator. Rather, it overrides its internal
-  /// tracking with constant-valued k-group index to enable the compiler to
-  /// fold constants and achieve more efficient code.
-  ///
-  /// This is used by some nontrivial permuted layouts.
-  CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    k_group_idx_ = k_group % (Policy::kGroupsPerTile / kPartitionsK);
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator &operator--() {
+    pointer_ -= increment_advance_;
+    return *this;
   }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    pointer_ += pointer_offset;
+  }
+
+  /// Adds a tile offset in the unit of tile.
+  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
+  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
+  ///   For row major A operand, k dimension is contiguous dimension;
+  ///   For col major A operand, k dimension is strided dimension;
+  ///   For row major B operand, k dimension is strided dimension;
+  ///   For col major B operand, k dimension is contiguous dimension.
+  /// Below two classes map col/row major to the pitch linear coordinates used
+  /// in this base class.
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    int offset = sizeof_bits<Element>::value *
+        (coord.contiguous() * Shape::kContiguous + coord.strided() * Shape::kStrided * stride_) / 8;
+    add_pointer_offset(offset);
+  }
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
+  }
+
+    /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+#if 0
+    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
+    int stride_idx = (iteration_strided_ & ~1);
+
+    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ +
+                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
+                            ThreadMap::kElementsPerAccess;
+
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
+#endif
+    return reinterpret_cast<AccessType *>(pointer_);
+  }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// This tile iterator is specialized for 32-thread TensorOps. It uses LDSM to
-/// load from shared memory and therefore must be initialized with a TensorRef
-/// to shared memory.
-///
-/// Satisfies:
-///   ReadableRandomAccessContiguousTileIteratorConcept
-///
+/// Regular tile iterator specialized for pitch-linear
 template <
-    /// Size of the matrix to load (concept: MatrixShape)
-    typename Shape_,
-    /// Data type of elements
-    typename Element_,
-    /// Shape of one matrix product operation (concept: MatrixShape)
-    typename InstructionShape_,
-    /// Policy of the details of LDSM shape and iterations
-    typename Policy_,
-    /// Number of partitions along K dimension
-    int PartitionsK_>
-class ScaleBiasTileIterator<Shape_, Element_, cutlass::layout::RowMajor,
-                             InstructionShape_, Policy_, 32, PartitionsK_> {
- public:
-  /// Shape of tile to load (concept: PitchLinearShape)
-  using Shape = Shape_;
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator<Shape_, Element_, layout::RowMajor, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
-  /// Element type
+  using Shape = Shape_;
   using Element = Element_;
+  using Layout = layout::RowMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
 
-  /// Layout of source tile
-  using Layout = cutlass::layout::RowMajor;
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
 
-  /// Shape of one matrix product operation (concept: MatrixShape)
-  using InstructionShape = InstructionShape_;
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
 
-  /// Number of participating threads
-  static int const kThreads = 32;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
 
-  /// TensorRef type for loading element from a tensor
-  using TensorRef = TensorRef<Element, Layout>;
+  using Underlying = RegularTileIterator<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 1 : 0),
+    ThreadMap,
+    kAlignment
+  >;
 
-  /// Index type
-  using Index = typename TensorRef::Index;
+  using AccessType = typename Underlying::AccessType;
 
-  /// Long Index type
-  using LongIndex = typename TensorRef::LongIndex;
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  /// Coordinate for an element in the tensor
-  using TensorCoord = typename TensorRef::TensorCoord;
+private:
 
-  /// Internal structure of iterator - made public to enable introspection
-  using Policy = Policy_;
-
-  /// Underlying tile iterator implementation
-  using Base = ScaleBiasTileIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::PitchLinear,
-      layout::PitchLinearShape<InstructionShape::kColumn,
-                               InstructionShape::kRow>,
-      Policy, kThreads, PartitionsK_>;
+  Underlying iterator_;
 
- public:
-  //
-  // Derived quantities
-  //
+public:
 
-  /// Fragment object holding a thread's part of a tile
-  using Fragment = typename Base::Fragment;
+  CUTLASS_DEVICE
+  RegularTileIterator() { }
 
- private:
-  /// Underlying tile iterator
-  Base iterator_;
+  CUTLASS_DEVICE
+  RegularTileIterator(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx) {
 
- public:
-  /// Default ctor constructs null iterator
-  CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator() {}
+  }
 
-  /// Constructor from TensorRef
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator(TensorRef const &ref_scale_bias, int lane_id)
-      : iterator_({ref_scale_bias.data(), ref_scale_bias.stride()}, lane_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Adds a pointer offset to internal pointer(s) to advance through memory
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator &add_pointer_offset(LongIndex offset) {
-    iterator_.add_pointer_offset(offset);
-
-    return *this;
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator &add_tile_offset(
-      TensorCoord const &tile_offset) {
-    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
+  }
 
-    return *this;
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &add_tile_offset_negative(
-      TensorCoord const &tile_offset) {
-    iterator_.add_tile_offset_negative({tile_offset.column(), tile_offset.row()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
 
-    return *this;
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances the iterator along the advance dimension
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator &operator++() {
+  RegularTileIterator &operator++() {
     ++iterator_;
-
     return *this;
   }
 
-  /// Advances the iterator along the advance dimension
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  ScaleBiasTileIterator &operator--() {
+  RegularTileIterator &operator--() {
     --iterator_;
-
     return *this;
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
-  CUTLASS_DEVICE
-  ScaleBiasTileIterator &operator+=(
-      TensorCoord const &tile_offset) {
-    add_tile_offset(PitchLinearCoord(tile_offset.column(), tile_offset.row()));
-    return *this;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
-  ///< advances in units of whole tiles along the logical coordinate space of
-  ///< the tensor
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  ScaleBiasTileIterator &operator-=(
-      TensorCoord const &tile_offset) {
-    add_tile_offset(-PitchLinearCoord(tile_offset.column(), tile_offset.row()));
-    return *this;
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
   }
 
-  /// Loads a fragment from memory at the location pointed to by the iterator.
+  /// Returns a pointer
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) const { iterator_.load(frag); }
+  AccessType *get() const {
+    return iterator_.get();
+  }
+
+};
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Regular tile iterator specialized for pitch-linear
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator<Shape_, Element_, layout::ColumnMajor, AdvanceRank, ThreadMap_, Alignment> {
+public:
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::ColumnMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
+
+  using Underlying = RegularTileIterator<
+    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 0 : 1),
+    ThreadMap
+  >;
+
+  using AccessType = typename Underlying::AccessType;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
+
+private:
+
+  Underlying iterator_;
+
+public:
 
-  /// Loads a fragment from memory with additional logical offset
   CUTLASS_DEVICE
-  void load_with_pointer_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset
-      Index pointer_offset) const {
+  RegularTileIterator() { }
+
+  CUTLASS_DEVICE
+  RegularTileIterator(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx) {
+
+  }
+
+  /// Loads a fragment
+  CUTLASS_HOST_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
     iterator_.load_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Loads a fragment from memory with additional logical offset
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a linear offset
-      Index byte_offset) const {
-    iterator_.load_with_byte_offset(frag, byte_offset);
+  /// Loads a fragment
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset) const {
-    // TODO
-    assert(0);
+  /// Loads a fragment
+  CUTLASS_HOST_DEVICE
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index pointer_offset) const {
-    // TODO
-    assert(0);
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Loads a fragment from memory with logical offset in units of whole tiles.
-  CUTLASS_DEVICE
-  void load_with_byte_offset(
-      /// fragment to load from the tensor
-      Fragment &frag,
-      /// loads a tile with a logical offset in units of whole tiles
-      TensorCoord const &tile_offset,
-      /// loads a tile with a logical offset AND a pointer offset
-      Index byte_offset) const {
-    iterator_.load_with_byte_offset(
-        frag, {tile_offset.strided(), tile_offset.contiguous()}, byte_offset);
-  }
-
-  /// Notify the iterator which k-group it is currently pointing to.
-  ///
-  /// This does not advance the iterator. Rather, it overrides its internal
-  /// tracking with constant-valued k-group index to enable the compiler to
-  /// fold constants and achieve more efficient code.
-  ///
-  /// This is used by some nontrivial permuted layouts.
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
+  }
+
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator &operator++() {
+    ++iterator_;
+    return *this;
+  }
+
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator &operator--() {
+    --iterator_;
+    return *this;
+  }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
+
+  /// Adds a tile offset
   CUTLASS_DEVICE
-  void set_kgroup_index(int k_group) {
-    iterator_.set_kgroup_index(k_group); 
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) {
   }
+
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return iterator_.get();
+  }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace warp
-} // namespace gemm 
+} // namespace threadblock
+} // namespace transform
 } // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/softmax_scale_bias_transform.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/gemm/warp/tile_iterator_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/half.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/half.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -36,16 +36,33 @@
 #pragma once
 
 #ifndef CUTLASS_ENABLE_F16C
 #define CUTLASS_ENABLE_F16C 0
 #endif
 
 #if defined(__CUDACC_RTC__)
-
-#include "cutlass/floating_point_nvrtc.h"
+/* All floating-point numbers can be put in one of these categories.  */
+enum
+  {
+    FP_NAN =
+# define FP_NAN 0
+      FP_NAN,
+    FP_INFINITE =
+# define FP_INFINITE 1
+      FP_INFINITE,
+    FP_ZERO =
+# define FP_ZERO 2
+      FP_ZERO,
+    FP_SUBNORMAL =
+# define FP_SUBNORMAL 3
+      FP_SUBNORMAL,
+    FP_NORMAL =
+# define FP_NORMAL 4
+      FP_NORMAL
+  };
 
 // F16C extensions are not meaningful when compiling for NVRTC which only accommodates device code.
 #undef CUTLASS_ENABLE_F16C
 #define CUTLASS_ENABLE_F16C 0
 
 #else
 //
@@ -58,15 +75,14 @@
 #endif
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include <cuda_fp16.h>
 
 #include "cutlass/cutlass.h"
-#include "cutlass/float8.h"
 #include "cutlass/platform/platform.h"
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 // Optionally target F16C extentions to accelerate half-precision conversion.
 #if !defined(__CUDA_ARCH__) && (CUTLASS_ENABLE_F16C)
 #if defined(_MSC_VER)
@@ -352,15 +368,16 @@
   }
 
   //
   // Methods
   //
 
   /// Default constructor
-  half_t() = default;
+  CUTLASS_HOST_DEVICE
+  half_t() : storage(0) { }
 
   /// Reinterpret cast from CUDA's half type
   CUTLASS_HOST_DEVICE
   explicit half_t(half const & x) {
     #if defined(__CUDA_ARCH__)
     storage = reinterpret_cast<uint16_t const &>(x);
     #else
@@ -377,26 +394,14 @@
 
   /// Floating point conversion
   CUTLASS_HOST_DEVICE
   explicit half_t(double x): half_t(float(x)) {
 
   }
 
-  /// float_e4m3_t conversion
-  CUTLASS_HOST_DEVICE
-  explicit half_t(float_e4m3_t x): half_t(float(x)) {
-
-  }
-
-  /// float_e5m2_t conversion
-  CUTLASS_HOST_DEVICE
-  explicit half_t(float_e5m2_t x): half_t(float(x)) {
-
-  }
-
   /// Integer conversion - round to nearest even
   CUTLASS_HOST_DEVICE
   explicit half_t(int x) {
     storage = convert(x).storage;
   }
 
   /// Integer conversion - round toward zero
@@ -609,27 +614,27 @@
 
   /// Maximum finite value
   static cutlass::half_t max() { return cutlass::half_t::bitcast(0x7bff); }
 
   /// Returns smallest finite value
   static cutlass::half_t epsilon() { return cutlass::half_t::bitcast(0x1800); }
 
-  /// Returns maximum rounding error
+  /// Returns smallest finite value
   static cutlass::half_t round_error() { return cutlass::half_t(0.5f); }
 
-  /// Returns positive infinity value
+  /// Returns smallest finite value
   static cutlass::half_t infinity() { return cutlass::half_t::bitcast(0x7c00); }
 
-  /// Returns quiet NaN value
+  /// Returns smallest finite value
   static cutlass::half_t quiet_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns signaling NaN value
+  /// Returns smallest finite value
   static cutlass::half_t signaling_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest positive subnormal value
+  /// Returns smallest finite value
   static cutlass::half_t denorm_min() { return cutlass::half_t::bitcast(0x0001); }
 };
 }  // namespace std
 #endif
 
 namespace platform {
 
@@ -671,31 +676,31 @@
   CUTLASS_HOST_DEVICE
   static cutlass::half_t max() { return cutlass::half_t::bitcast(0x7bff); }
 
   /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t epsilon() { return cutlass::half_t::bitcast(0x1800); }
 
-  /// Returns maximum rounding error
+  /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t round_error() { return cutlass::half_t(0.5f); }
 
-  /// Returns positive infinity value
+  /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t infinity() { return cutlass::half_t::bitcast(0x7c00); }
 
-  /// Returns quiet NaN value
+  /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t quiet_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns signaling NaN value
+  /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t signaling_NaN() { return cutlass::half_t::bitcast(0x7fff); }
 
-  /// Returns smallest positive subnormal value
+  /// Returns smallest finite value
   CUTLASS_HOST_DEVICE
   static cutlass::half_t denorm_min() { return cutlass::half_t::bitcast(0x0001); }
 };
 }  // namespace platform 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/integer_subbyte.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -73,15 +73,16 @@
   Storage storage;
 
   //
   // Methods
   //
 
   /// No operation
-  integer_subbyte() = default;
+  CUTLASS_HOST_DEVICE
+  integer_subbyte() { }
 
   /// Conversion from integer type
   CUTLASS_HOST_DEVICE
   integer_subbyte(int value)
       : storage(reinterpret_cast<Storage const &>(value) & kMask) {}
 
   CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/kernel_launch.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/matrix.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -35,16 +35,14 @@
     data to describe strides between elements.
 
     Layout functions must implement all members in the public interface of IdentityTensorLayout<>
     defined in cutlass/tensor_ref.h.
 */
 #pragma once
 
-#include "cute/layout.hpp"
-
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/pitch_linear_coord.h"
 
 namespace cutlass {
 namespace layout {
@@ -141,23 +139,14 @@
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(MatrixCoord const &extent) const {
     return LongIndex(extent.row()) * LongIndex(stride_[0]);
   }
-
-  CUTLASS_HOST_DEVICE
-  cute::Layout<cute::Shape<int, int>, cute::Stride<int64_t, cute::Int<1> > > 
-  to_cute_layout(MatrixCoord const &extent) const {
-    return cute::Layout<cute::Shape<int, int>, cute::Stride<int64_t, cute::Int<1> > >{
-      {extent[0], extent[1]},
-      {stride(0), cute::Int<1>{}}
-    };
-  }
 };
 
 /// Mapping function for column-major matrices.
 class ColumnMajor {
 public:
   /// Logical rank of tensor
   static int const kRank = 2;
@@ -243,23 +232,14 @@
   }
 
   /// Compute the number of contiguous elements needed to store a tensor with the given size
   CUTLASS_HOST_DEVICE
   LongIndex capacity(MatrixCoord const &extent) const {
     return LongIndex(extent.column()) * LongIndex(stride_[0]);
   }
-
-  CUTLASS_HOST_DEVICE
-  cute::Layout<cute::Shape<int, int>, cute::Stride< cute::Int<1>, int64_t> > 
-  to_cute_layout(MatrixCoord const &extent) const {
-    return cute::Layout<cute::Shape<int, int>, cute::Stride<cute::Int<1>, int64_t> >{
-      {extent[0], extent[1]},
-      {cute::Int<1>{}, stride(0)}
-    };
-  }
 };
 
 /// Mapping function for interleaved matrices. Matrix is structured
 /// as row-major arrangement of fixed-size columns.
 template <int Interleave>
 struct RowMajorInterleaved {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/permute.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -68,14 +68,17 @@
 
   MatrixCoord extent_;
 
   Index stride_unit_; //  sizeof(AccessType) / kElementsPerAccess in epilogue's predicated_tile_iterator
 
   Index stride_permute_;
 
+  Index col_permute_;
+  Index row_permute_;
+
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -112,14 +115,17 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
+  Index col_permute_;
+  Index row_permute_;
+
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -149,18 +155,18 @@
 
     int l = col_init % D3;
     int k = col_init / D3;
     int j = row_init % D1;
     int i = row_init / D1;
 
     // After the Permute Op
-    Index col_permute = l + j * D3;
-    Index row_permute = k + i * D2;
+    col_permute_ = l + j * D3;
+    row_permute_ = k + i * D2;
 
-    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
+    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
   }
 
   /// Return D1
   CUTLASS_HOST_DEVICE
   Index d1() const {
     return D1;
   }
@@ -188,14 +194,17 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
+  Index col_permute_;
+  Index row_permute_;
+
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -227,29 +236,29 @@
 
     int l = col_init;
     int k = row_init;
     int j = BMM_batch_idx % D1;
     int i = BMM_batch_idx / D1;
 
     // After the Permute Op
-    Index col_permute = l + j * D3;
-    Index row_permute = k + i * D2;
+    col_permute_ = l + j * D3;
+    row_permute_ = k + i * D2;
 
-    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
+    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
   }
 
   /// Return D1
   CUTLASS_HOST_DEVICE
   Index d1() const {
     return D1;
   }
 };
 
 /// Permute layout function for 5-D permuted tensors with output matrix (dimension as [M, N]) reshaped
-/// as [M/T1, T1, T2, T3, N/T2/T3]. Then perform permute([2, 0, 3, 1, 4]) on the corresponding output tensor.
+/// as [M/T1, T1, T2, T3, N/T3]. Then perform permute([2, 0, 3, 1, 4]) on the corresponding output tensor.
 template <int T1, int T2, int T3>
 class Tensor5DPermute20314 {
 public:
   /// Index type used for coordinates
   using Index = int32_t;
 
   /// Long index type used for offsets
@@ -260,14 +269,17 @@
   // Data members
   //
 
   MatrixCoord extent_;
 
   Index stride_permute_;
   
+  Index col_permute_;
+  Index row_permute_;
+
 public:
   //
   // Methods
   //
 
   /// Constructor
   CUTLASS_HOST_DEVICE
@@ -297,18 +309,18 @@
     int m = col_init % T4;
     int l = int(col_init / T4) % T3;
     int k = int(col_init / T4) / T3;
     int j = row_init % T1;
     int i = row_init / T1;
 
     // After the Permute Op
-    Index col_permute = m + j * T4 + l * T1 * T4;
-    Index row_permute = i + k * T0;
+    col_permute_ = m + j * T4 + l * T1 * T4;
+    row_permute_ = i + k * T0;
 
-    return LongIndex(row_permute) * LongIndex(stride_permute_) + LongIndex(col_permute);
+    return LongIndex(row_permute_) * LongIndex(stride_permute_) + LongIndex(col_permute_);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace layout
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/pitch_linear.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm75.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/tensor_op_multiplicand_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/layout/vector.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/matrix_shape.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_conversion.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -970,15 +970,15 @@
   result_type operator()(source_type const &s) {
     return convert(s);
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for Array<bfloat16_t> <= Array<float>
+/// Partial specialization for Array<half> <= Array<float>
 template <
   int N,
   FloatRoundStyle Round
 >
 struct NumericArrayConverter<bfloat16_t, float, N, Round> {
 
   using result_type = Array<bfloat16_t, N>;
@@ -1268,857 +1268,14 @@
     return convert(s);
   }
 };
 
 #endif
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<float, N> <=> Array<float_e4m3_t, N>
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Array<float, 4> <= Array<float_e4m3_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float, float_e4m3_t, 4, Round> {
-  using result_element = float;
-  using source_element = float_e4m3_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out_fp16[2];
-    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo, hi;\n" \
-        "mov.b32 {lo, hi}, %2;\n" \
-        "cvt.rn.f16x2.e4m3x2 %0, lo;\n" \
-        "cvt.rn.f16x2.e4m3x2 %1, hi;\n" \
-        "}\n" : "=r"(out_fp16[0]), "=r"(out_fp16[1]) : "r"(src_packed));
-
-    float2 res0 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[0]));
-    float2 res1 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[1]));
-
-    result_type out;
-    out[0] = res0.x;
-    out[1] = res0.y;
-    out[2] = res1.x;
-    out[3] = res1.y;
-    return out;
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, float, 4, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = float;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out;
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo;\n" \
-        ".reg .b16 hi;\n" \
-        "cvt.rn.satfinite.e4m3x2.f32   lo, %2, %1;\n" \
-        "cvt.rn.satfinite.e4m3x2.f32   hi, %4, %3;\n" \
-        "mov.b32 %0, {lo, hi};\n" \
-        "}" \
-        : "=r"(out) : "f"(source[0]), "f"(source[1]), "f"(source[2]), "f"(source[3]));
-
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float, 4> <= Array<float_e5m2_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float, float_e5m2_t, 4, Round> {
-  using result_element = float;
-  using source_element = float_e5m2_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out_fp16[2];
-    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo, hi;\n" \
-        "mov.b32 {lo, hi}, %2;\n" \
-        "cvt.rn.f16x2.e5m2x2 %0, lo;\n" \
-        "cvt.rn.f16x2.e5m2x2 %1, hi;\n" \
-        "}\n" : "=r"(out_fp16[0]), "=r"(out_fp16[1]) : "r"(src_packed));
-
-    float2 res0 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[0]));
-    float2 res1 = __half22float2(reinterpret_cast<__half2 &>(out_fp16[1]));
-
-    result_type out;
-    out[0] = res0.x;
-    out[1] = res0.y;
-    out[2] = res1.x;
-    out[3] = res1.y;
-    return out;
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, float, 4, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = float;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out;
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo;\n" \
-        ".reg .b16 hi;\n" \
-        "cvt.rn.satfinite.e5m2x2.f32   lo, %2, %1;\n" \
-        "cvt.rn.satfinite.e5m2x2.f32   hi, %4, %3;\n" \
-        "mov.b32 %0, {lo, hi};\n" \
-        "}" \
-        : "=r"(out) : "f"(source[0]), "f"(source[1]), "f"(source[2]), "f"(source[3]));
-
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<half_t, 4> <=> Array<float_e4m3_t, 4>
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Array<half_t, 4> <= Array<float_e4m3_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<half_t, float_e4m3_t, 4, Round> {
-  using result_element = half_t;
-  using source_element = float_e4m3_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out[2];
-    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo, hi;\n" \
-        "mov.b32 {lo, hi}, %2;\n" \
-        "cvt.rn.f16x2.e4m3x2 %0, lo;\n" \
-        "cvt.rn.f16x2.e4m3x2 %1, hi;\n" \
-        "}\n" : "=r"(out[0]), "=r"(out[1]) : "r"(src_packed));
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<half_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, half_t, 4, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = half_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out;
-    uint32_t const* src_packed = reinterpret_cast<uint32_t const*>(&source);
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo;\n" \
-        ".reg .b16 hi;\n" \
-        "cvt.rn.satfinite.e4m3x2.f16x2   lo, %1;\n" \
-        "cvt.rn.satfinite.e4m3x2.f16x2   hi, %2;\n" \
-        "mov.b32 %0, {lo, hi};\n" \
-        "}" \
-        : "=r"(out) : "r"(src_packed[0]), "r"(src_packed[1]));
-
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<half_t, 4> <= Array<float_e5m2_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<half_t, float_e5m2_t, 4, Round> {
-  using result_element = half_t;
-  using source_element = float_e5m2_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out[2];
-    uint32_t const& src_packed = reinterpret_cast<uint32_t const&>(source);
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo, hi;\n" \
-        "mov.b32 {lo, hi}, %2;\n" \
-        "cvt.rn.f16x2.e5m2x2 %0, lo;\n" \
-        "cvt.rn.f16x2.e5m2x2 %1, hi;\n" \
-        "}\n" : "=r"(out[0]), "=r"(out[1]) : "r"(src_packed));
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<half_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, half_t, 4, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = half_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    uint32_t out;
-    uint32_t const* src_packed = reinterpret_cast<uint32_t const*>(&source);
-
-    asm volatile( \
-        "{\n" \
-        ".reg .b16 lo;\n" \
-        ".reg .b16 hi;\n" \
-        "cvt.rn.satfinite.e5m2x2.f16x2   lo, %1;\n" \
-        "cvt.rn.satfinite.e5m2x2.f16x2   hi, %2;\n" \
-        "mov.b32 %0, {lo, hi};\n" \
-        "}" \
-        : "=r"(out) : "r"(src_packed[0]), "r"(src_packed[1]));
-
-    return reinterpret_cast<result_type const &>(out);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<bfloat16_t, 4> <=> Array<float_e4m3_t, 4>
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e4m3_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<bfloat16_t, float_e4m3_t, 4, Round> {
-  using result_element = bfloat16_t;
-  using source_element = float_e4m3_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    // Convert f8 to float
-    NumericArrayConverter<float, source_element, 4, Round> src2float;
-    Array<float, 4> tmp_floats = src2float(source);
-
-    // Convert float to bf16
-    result_type out;
-    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp_floats);
-    Array<result_element, 2>* packed_out = reinterpret_cast<Array<result_element, 2>*>(&out);
-    NumericArrayConverter<result_element, float, 2, Round> float2result;
-    packed_out[0] = float2result(packed_tmp[0]);
-    packed_out[1] = float2result(packed_tmp[1]);
-
-    return out;
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<bfloat16_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, bfloat16_t, 4, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = bfloat16_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    // Convert bf16 to float
-    Array<float, 4> tmp;
-    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp);
-    Array<source_element, 2> const* packed_source = reinterpret_cast<Array<source_element, 2> const*>(&source);
-    NumericArrayConverter<float, source_element, 2, Round> src2float;
-    packed_tmp[0] = src2float(packed_source[0]);
-    packed_tmp[1] = src2float(packed_source[1]);
-
-    // Convert float to f8
-    NumericArrayConverter<result_element, float, 4, Round> float2result;
-    return float2result(tmp);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<bfloat16_t, 4> <= Array<float_e5m2_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<bfloat16_t, float_e5m2_t, 4, Round> {
-  using result_element = bfloat16_t;
-  using source_element = float_e5m2_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    // Convert f8 to float
-    NumericArrayConverter<float, source_element, 4, Round> src2float;
-    Array<float, 4> tmp_floats = src2float(source);
-
-    // Convert float to bf16
-    result_type out;
-    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp_floats);
-    Array<result_element, 2>* packed_out = reinterpret_cast<Array<result_element, 2>*>(&out);
-    NumericArrayConverter<result_element, float, 2, Round> float2result;
-    packed_out[0] = float2result(packed_tmp[0]);
-    packed_out[1] = float2result(packed_tmp[1]);
-
-    return out;
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<bfloat16_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, bfloat16_t, 4, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = bfloat16_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-
-  #if defined(CUDA_PTX_FP8_CVT_ENABLED)
-    // Convert bf16 to float
-    Array<float, 4> tmp;
-    Array<float, 2>* packed_tmp = reinterpret_cast<Array<float, 2>*>(&tmp);
-    Array<source_element, 2> const* packed_source = reinterpret_cast<Array<source_element, 2> const*>(&source);
-    NumericArrayConverter<float, source_element, 2, Round> src2float;
-    packed_tmp[0] = src2float(packed_source[0]);
-    packed_tmp[1] = src2float(packed_source[1]);
-
-    // Convert float to f8
-    NumericArrayConverter<result_element, float, 4, Round> float2result;
-    return float2result(tmp);
-  #else
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  #endif
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for Array<float_e4m3_t, 4> <=> Array<float_e5m2_t, 4>
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float_e5m2_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, float_e5m2_t, 4, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = float_e5m2_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float_e4m3_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, float_e4m3_t, 4, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = float_e4m3_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-    result_type result;
-    NumericConverter<result_element, source_element, Round> converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < 4; ++i) {
-      result[i] = converter(source[i]);
-    }
-
-    return result;
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specializations for:
-//      Array<float_e4m3_t, 4> <=> Array<float_e4m3_t, 4>
-//      Array<float_e5m2_t, 4> <=> Array<float_e5m2_t, 4>
-//
-// These are needed to avoid multiple-matching-template compilation errors (e.g., when
-// compiling float_e4m3_t <=> float_e4m3_t, which among T <= float_e4m3_t and float_e4m3_t <= T
-// should be used?)
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Partial specialization for Array<float_e4m3_t, 4> <= Array<float_e4m3_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, float_e4m3_t, 4, Round> {
-  using result_element = float_e4m3_t;
-  using source_element = float_e4m3_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return s;
-  }
-};
-
-/// Partial specialization for Array<float_e5m2_t, 4> <= Array<float_e5m2_t, 4>
-template <
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, float_e5m2_t, 4, Round> {
-  using result_element = float_e5m2_t;
-  using source_element = float_e5m2_t;
-
-  using result_type = Array<result_element, 4>;
-  using source_type = Array<source_element, 4>;
-  static FloatRoundStyle const round_style = Round;
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return s;
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// Partial specialziations for:
-//       Array<T, N> <=> Array<float_e4m3_t, N>
-//       Array<T, N> <=> Array<float_e5m2_t, N>
-// using packed converter under the hood
-//
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <
-  typename T,
-  typename S,
-  int N,
-  FloatRoundStyle Round
->
-struct PackedNumericArrayConverter {
-  using result_element = T;
-  using source_element = S;
-
-  using result_type = Array<result_element, N>;
-  using source_type = Array<source_element, N>;
-
-  static FloatRoundStyle const round_style = Round;
-
-private:
-  using packed_result_type = Array<result_element, 4>;
-  using packed_source_type = Array<source_element, 4>;
-
-public:
-  CUTLASS_DEVICE
-  static result_type convert(source_type const & source) {
-    result_type result;
-    packed_result_type* packed_result = reinterpret_cast<packed_result_type*>(&result);
-    const packed_source_type* packed_source = reinterpret_cast<const packed_source_type*>(&source);
-
-    NumericArrayConverter<result_element, source_element, 4, Round> packed_converter;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N / 4; ++i) {
-      packed_result[i] = packed_converter(packed_source[i]);
-    }
-
-    // Handle leftovers
-    NumericConverter<result_element, source_element, Round> converter;
-    CUTLASS_PRAGMA_UNROLL
-    for (int i = 0; i < N % 4; ++i) {
-      int idx = ((N / 4) * 4) + i;
-      result[idx] = converter(source[idx]);
-    }
-
-    return result;
-  }
-
-  CUTLASS_HOST_DEVICE
-  result_type operator()(source_type const &s) {
-    return convert(s);
-  }
-};
-
-/// Partial specialization for Array<T, N> <= Array<float_e4m3_t, N>
-template <
-  typename T,
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<T, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<T, float_e4m3_t, N, Round> {};
-
-/// Partial specialization for Array<T, N> <= Array<float_e5m2_t, N>
-template <
-  typename T,
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<T, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<T, float_e5m2_t, N, Round> {};
-
-/// Partial specialization for Array<float_e4m3_t, N> <= Array<S, N>
-template <
-  typename S,
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, S, N, Round> :
-  public PackedNumericArrayConverter<float_e4m3_t, S, N, Round> {};
-
-/// Partial specialization for Array<float_e5m2_t, N> <= Array<S, N>
-template <
-  typename S,
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, S, N, Round> :
-  public PackedNumericArrayConverter<float_e5m2_t, S, N, Round> {};
-
-/// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e5m2_t, N>
-template <
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<float_e4m3_t, float_e5m2_t, N, Round> {};
-
-/// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e4m3_t, N>
-template <
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<float_e5m2_t, float_e4m3_t, N, Round> {};
-
-/// Partial specialization for Array<float_e4m3_t, N> <= Array<float_e4m3_t, N>
-template <
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> :
-  public PackedNumericArrayConverter<float_e4m3_t, float_e4m3_t, N, Round> {};
-
-/// Partial specialization for Array<float_e5m2_t, N> <= Array<float_e5m2_t, N>
-template <
-  int N,
-  FloatRoundStyle Round
->
-struct NumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> :
-  public PackedNumericArrayConverter<float_e5m2_t, float_e5m2_t, N, Round> {};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Partial specialization for Array<int8_t> <= Array<float>
 /// Conversion is performed with saturation regardless of setting of
 /// the `Round` template parameter.
 template <
   int N,
   FloatRoundStyle Round
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/numeric_types.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -84,11 +84,10 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/integer_subbyte.h"
 
 #include "cutlass/half.h"
 #include "cutlass/bfloat16.h"
 #include "cutlass/tfloat32.h"
-#include "cutlass/float8.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/pitch_linear_coord.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/platform/platform.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -571,28 +571,14 @@
 #else
 
 using std::is_trivially_copyable;
 
 #endif
 
 //-----------------------------------------------------------------------------
-// bit_cast <bit>
-//-----------------------------------------------------------------------------
-
-template< class To, class From >
-constexpr To CUTLASS_HOST_DEVICE bit_cast(const From& from ) noexcept;
-
-template <class To, class From>
-constexpr To CUTLASS_HOST_DEVICE bit_cast(const From& src) noexcept
-{
-  static_assert(sizeof(To) == sizeof(From), "sizes must match");
-  return reinterpret_cast<To const &>(src);
-}
-
-//-----------------------------------------------------------------------------
 // Alignment and layout utilities
 //-----------------------------------------------------------------------------
 
 #if defined(__CUDACC_RTC__) || (!defined(_MSC_VER) && (__cplusplus < 201103L)) || (defined(_MSC_VER) && (_MSC_VER < 1500))
 
 /// std::alignment_of
 template <typename value_t>
@@ -875,17 +861,9 @@
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t lowest() noexcept { return 0;}
   CUTLASS_HOST_DEVICE
   static constexpr uint8_t max() noexcept { return 255U;}
   static constexpr bool is_integer = true;
 };
 
-template <>
-struct numeric_limits<float> {
-  CUTLASS_HOST_DEVICE
-  static constexpr float infinity() noexcept { return bit_cast<float, int32_t>(0x7f800000);}
-  static constexpr bool is_integer = false;
-  static constexpr bool has_infinity = true;
-};
-
 }  // namespace platform
 }  // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/predicate_vector.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/quaternion.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/quaternion.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,15 +31,14 @@
 /*! \file
     \brief Defines a densely packed quaternion object intended for storing data in registers and
     executing quaternion operations within a CUDA or host thread.
 */
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/functional.h"
 #include "cutlass/array.h"
 #include "cutlass/real.h"
 #include "cutlass/coord.h"
 #include "cutlass/matrix.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/layout/vector.h"
 
@@ -648,18 +647,15 @@
 
 CUTLASS_HOST_DEVICE
   static Quaternion<T> from_real(double x) {
     return Quaternion<T>(static_cast<T>(x));
   }
 };
 
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-// Factories
-////////////////////////////////////////////////////////////////////////////////////////////////////
+//////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <>
 CUTLASS_HOST_DEVICE
 cutlass::Quaternion<half_t> from_real<cutlass::Quaternion<half_t> >(double r) {
   return cutlass::Quaternion<half_t>(half_t(r));
 }
 
@@ -673,81 +669,11 @@
 CUTLASS_HOST_DEVICE
 cutlass::Quaternion<double> from_real<cutlass::Quaternion<double> >(double r) {
   return cutlass::Quaternion<double>(r);
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-// functional.h numeric specializations
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename T>
-struct multiplies<Quaternion<T>> {
-  CUTLASS_HOST_DEVICE
-  Quaternion<T> operator()(Quaternion<T> lhs, Quaternion<T> const &rhs) const {
-    lhs = lhs * rhs;
-    return lhs;
-  }
-};
-
-/// Squares with optional conversion
-template <typename T, typename Output>
-struct magnitude_squared<Quaternion<T>, Output> {
-  CUTLASS_HOST_DEVICE
-  Output operator()(Quaternion<T> lhs) const {
-    multiplies<Output> mul_op;
-
-    Output y_w = Output(lhs.w());
-    Output y_x = Output(lhs.x());
-    Output y_y = Output(lhs.y());
-    Output y_z = Output(lhs.z());
-
-    return mul_op(y_w, y_w) + mul_op(y_x, y_x) + mul_op(y_y, y_y) + \
-           mul_op(y_z, y_z);
-  }
-};
-
-template <typename T>
-struct multiply_add<Quaternion<T>, Quaternion<T>, Quaternion<T>> {
-  CUTLASS_HOST_DEVICE
-  Quaternion<T> operator()(
-    Quaternion<T> const &a,
-    Quaternion<T> const &b,
-    Quaternion<T> const &c) const {
-
-    T x = c.x();
-    T y = c.y();
-    T z = c.z();
-    T w = c.w();
-
-    x += a.w() * b.x();
-    x += b.w() * a.x();
-    x += a.y() * b.z();
-    x += -a.z() * b.y(),
-
-    y += a.w() * b.y();
-    y += b.w() * a.y();
-    y += a.z() * b.x();
-    y += -a.x() * b.z();
-
-    z += a.w() * b.z();
-    z += b.w() * a.z();
-    z += a.x() * b.y();
-    z += -a.y() * b.x();
-
-    w += a.w() * b.w();
-    w += -a.x() * b.x();
-    w += -a.y() * b.y();
-    w += -a.z() * b.z();
-
-    return cutlass::make_Quaternion(x, y, z, w);
-  }
-};
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/real.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/real.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/reduce_split_k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_contiguous.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/device/tensor_reduce_affine_strided.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_softmax_final.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/reduce_split_k.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_contiguous.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -79,15 +79,15 @@
   int64_t workspace_stride;                     /// stride (units of bytes) between workspace
   int workspace_count;                          /// number of workspaces
   
   uint64_t inner_count;                          /// Number of elements in reduced index space
   uint64_t outer_count;                          /// Number of elements in outer index space
 
   ElementOutput * destination;                  /// Pointer to output tensor of rank kReducedRank
-  ElementSource const * source;                 /// Pointer to source pointer of rank kRank
+  ElementSource const * source;                 /// Poitner to source pointer of rank kRank
   ReductionOp reduction_op;                     /// Reduction operator
   ElementCompute reduction_identity;            /// Identity element used by reduction operator
   ElementCompute *device_workspace;             /// Pointer to device workspace for inter-CTA reductions
 
   //
   // Methods
   //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/kernel/tensor_reduce_affine_strided.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -81,15 +81,15 @@
   int64_t workspace_outer_stride;               /// stride (units of bytes) between 'rows' of the workspace
   int workspace_count;                          /// number of workspaces
   
   uint64_t inner_count;                          /// Number of elements in reduced index space
   uint64_t outer_count;                          /// Number of elements in outer index space
 
   ElementOutput * destination;                  /// Pointer to output tensor of rank kReducedRank
-  ElementSource const * source;                 /// Pointer to source pointer of rank kRank
+  ElementSource const * source;                 /// Poitner to source pointer of rank kRank
   ReductionOp reduction_op;                     /// Reduction operator
   ElementCompute reduction_identity;            /// Identity element for reduction operator
   ElementCompute *device_workspace;             /// Pointer to device workspace for inter-CTA reductions
 
   //
   // Methods
   //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduce.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/thread/reduction_operators.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/reduction/threadblock_swizzle.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/relatively_equal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/semaphore.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/semaphore.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,14 +32,15 @@
     \brief Implementation of a CTA-wide semaphore for inter-CTA synchronization.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 
+#include "cutlass/aligned_buffer.h"
 #include "cutlass/array.h"
 
 #include "cutlass/numeric_types.h"
 #include "cutlass/matrix_shape.h"
 
 #include "cutlass/gemm/gemm.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/subbyte_reference.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -290,26 +290,16 @@
   explicit operator double() const {
     return double(get());
   }
 };
 
 template <
   typename Element_,              /// CUTLASS numeric element type.
-  typename Storage_ =             /// Underlying storage type. Must be able to hold an integer
+  typename Storage_ = uint8_t     /// Underlying storage type. Must be able to hold an integer 
                                   ///   number of objects of type Element.
-
-#if defined(__CUDA_ARCH__)        /// Default size depends on width of atomicCas() overloads.
-  #if (__CUDA_ARCH__ >= 700)      ///
-  uint16_t
-  #else
-  uint32_t
-  #endif
-#else
-  uint8_t
-#endif
 >
 class SubbyteReference {
 public:
 
   using Element = Element_;
   using Storage = Storage_;
   using StoragePointer = Storage *;
@@ -393,49 +383,22 @@
     return reinterpret_cast<Element const &>(item);
   }
 
   /// Stores an element to memory
   CUTLASS_HOST_DEVICE
   SubbyteReference & set(Element const &x) {
 
-    Storage item        = (reinterpret_cast<Storage const &>(x) & kMask);
-    Storage kUpdateMask = Storage(~(kMask << (offset_ * cutlass::sizeof_bits<Element>::value)));
-    Storage new_bits    = Storage(item << (offset_ * cutlass::sizeof_bits<Element>::value));
-
-#if defined(__CUDA_ARCH__)
-
-    //
-    // Homebrew read-modify-write
-    //
-    Storage original;
-    Storage updated;
-
-    do {
-
-      original = (*ptr_);
+    Storage item = (reinterpret_cast<Storage const &>(x) & kMask);
 
-      updated  = Storage((original & kUpdateMask) | new_bits);
-
-      original = atomicCAS(ptr_, original, updated);
-
-    } while (updated != original);
-
-#else
-
-    Storage original = (*ptr_);
-    Storage updated  = Storage((original & kUpdateMask) | new_bits);
-    *ptr_ = updated;
-
-#endif
+    Storage kUpdateMask = Storage(~(kMask << (offset_ * sizeof_bits<Element>::value)));
+    *ptr_ = Storage((*ptr_ & kUpdateMask) | Storage(item << (offset_ * sizeof_bits<Element>::value)));
 
     return *this;
   }
 
-  ////
-
   /// Unpacks an element from memory
   CUTLASS_HOST_DEVICE
   operator Element() const {
     return get();
   }
 
   /// Stores an element to memory
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_coord.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_ref_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tensor_view_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/tfloat32.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,17 +30,15 @@
  **************************************************************************************************/
 /*!
     \file
     \brief Defines a proxy class for storing Tensor Float 32 data type.
 */
 #pragma once
 
-#if defined(__CUDACC_RTC__)
-#include "cutlass/floating_point_nvrtc.h"
-#else
+#if !defined(__CUDACC_RTC__)
 #include <cmath>
 #include <limits>
 #include <cstdint>
 #endif
 
 #include "cutlass/cutlass.h"
 
@@ -85,31 +83,30 @@
     }
     #endif
 
     return tfloat32_t::bitcast(x);
   }
 
   /// Default constructor
-  tfloat32_t() = default;
+  CUTLASS_HOST_DEVICE
+  tfloat32_t() : storage(0) { }
 
   /// Floating-point conversion - round toward nearest even
   CUTLASS_HOST_DEVICE
-//  explicit tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
-  tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
+  explicit tfloat32_t(float x): storage(round_half_ulp_truncate(x).storage) { }
 
   /// Floating-point conversion - round toward nearest even
   CUTLASS_HOST_DEVICE
-//  explicit tfloat32_t(double x): tfloat32_t(float(x)) {
-  tfloat32_t(double x): tfloat32_t(float(x)) {
+  explicit tfloat32_t(double x): tfloat32_t(float(x)) {
+
   }
 
   /// Integer conversion - round toward zero
   CUTLASS_HOST_DEVICE
-//  explicit tfloat32_t(int x) {
-  tfloat32_t(int x) {
+  explicit tfloat32_t(int x) {
     float flt = static_cast<float>(x);
     #if defined(__CUDA_ARCH__)
     storage = reinterpret_cast<uint32_t const &>(flt);
     #else
     std::memcpy(&storage, &flt, sizeof(storage));
     #endif
   }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/thread/matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/trace.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/trace.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/pitch_linear_thread_map.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,15 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing how threads are mapped to a given tile.
+    \brief Templates implementing how threads are mapped to a given tile. 
 
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
@@ -84,39 +84,43 @@
   using ThreadAccessShape = layout::PitchLinearShape<kElementsPerAccess, 1>;
 
   /// Internal implementation details
   struct Detail {
 
     static_assert(!(Shape::kContiguous % kElementsPerAccess), "");
 
+    static_assert(!((Shape::kContiguous * Shape::kStrided) % (kThreads * kElementsPerAccess)), 
+      "Shape must be divisible thread count.");
+
     /// Shape of the tile in units of vectors
     using ShapeVec = layout::PitchLinearShape<
       Shape::kContiguous / kElementsPerAccess,
       Shape::kStrided
     >;
 
-    static_assert((Threads < ShapeVec::kContiguous && !(ShapeVec::kContiguous % kThreads)) ||
-                      (!(kThreads % ShapeVec::kContiguous)),
-                  "Shape must be divisible by number of iterations of each thread.");
+    static_assert(
+      (Threads < ShapeVec::kContiguous && !(ShapeVec::kContiguous % kThreads)) ||
+      (!(kThreads % ShapeVec::kContiguous) && !(ShapeVec::kStrided % (kThreads / ShapeVec::kContiguous))),
+      "Shape must be divisible by number of iterations of each thread."
+    );
   };
 
   /// Number of iterations by each thread
   using Iterations = typename platform::conditional<
       Threads >= Detail::ShapeVec::kContiguous,
       layout::PitchLinearShape<
           1,
           // Redo the comparison here to work around divide by zero compiler
           // error.  The compiler evaluates both path of platform::conditional.
           (Threads >= Detail::ShapeVec::kContiguous
-               ? (Detail::ShapeVec::kStrided + (kThreads / Detail::ShapeVec::kContiguous - 1)) /
+               ? Detail::ShapeVec::kStrided /
                      (kThreads / Detail::ShapeVec::kContiguous)
                : 0)>,
       layout::PitchLinearShape<Detail::ShapeVec::kContiguous / kThreads,
                                Detail::ShapeVec::kStrided>>::type;
-  
 
   /// Interval between accesses along each dimension of the tensor's logical coordinate space
   /// (in units of Elements)
   using Delta = typename platform::conditional<
     Threads >= Detail::ShapeVec::kContiguous,
     layout::PitchLinearShape<
       1,
@@ -124,21 +128,14 @@
     >,
     layout::PitchLinearShape<
       kThreads * kElementsPerAccess,
       1
     >
   >::type;
 
-  /// Shape of the tile in units of vectors
-  using StorageShape = typename platform::conditional<
-      Threads >= Detail::ShapeVec::kContiguous,
-      layout::PitchLinearShape<Shape::kContiguous,
-                               Iterations::kStrided*(kThreads / Detail::ShapeVec::kContiguous)>,
-      layout::PitchLinearShape<Shape::kContiguous, Shape::kStrided>>::type;
-
   /// Maps thread ID to a coordinate offset within the tensor's logical coordinate space
   /// (in units of Elements)
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id) {
     return TensorCoord(
       (thread_id % Detail::ShapeVec::kContiguous) * kElementsPerAccess, 
       thread_id / Detail::ShapeVec::kContiguous);
@@ -159,17 +156,17 @@
   using TensorCoord = layout::PitchLinearCoord;
 
   static int const kThreads = Threads;
   static int const kElementsPerAccess = ElementsPerAccess;
 
   using Iterations = layout::PitchLinearShape<
                       Shape::kContiguous / (kThreads * kElementsPerAccess),
-                      Shape::kStrided>;
+                      Shape::kStrided>;                      
 
-  using Delta = layout::PitchLinearShape<1, 1>;
+  using Delta = layout::PitchLinearShape<1, 1>;  
 
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id)
   {
     return TensorCoord(thread_id * Iterations::kContiguous * kElementsPerAccess, 0);
   }
 };
@@ -179,32 +176,32 @@
   int Threads,
   int ElementsPerAccess = 1
 >
 struct PitchLinearTilePolicyStripminedThreadStrided
 {
   static_assert((Shape::kStrided % Threads == 0),
                 "Strided shape must divide number of threads");
-
+  
   using TensorCoord = layout::PitchLinearCoord;
 
   static int const kThreads = Threads;
   static int const kElementsPerAccess = ElementsPerAccess;
 
   using Iterations = layout::PitchLinearShape<
                       Shape::kContiguous / kElementsPerAccess,
-                      Shape::kStrided / kThreads>;
+                      Shape::kStrided / kThreads>;       
 
-  using Delta = layout::PitchLinearShape<1, 1>;
+  using Delta = layout::PitchLinearShape<1, 1>;  
 
   using ShapeVec = Shape;
 
   CUTLASS_HOST_DEVICE
   static TensorCoord initial_offset(int thread_id)
   {
-
+    
     return TensorCoord(0, thread_id * Iterations::kStrided);
   }
 };
 
 
 ////////////////////////////////////////////////////////////////////////////////
 
@@ -330,15 +327,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -456,15 +453,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -597,29 +594,29 @@
     static int const kThreads = ThreadMap::kThreads;
 
     /// Extract vector length from Layout
     static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
 
     static_assert(kElementsPerAccess == 1 , "Simt transpose requires elements per access to be 1");
     ///< Iterations along each dimension (concept: PitchLinearShape)
-    using Iterations =
+    using Iterations = 
         layout::PitchLinearShape<ThreadMap::Iterations::kStrided,
         ThreadMap::Iterations::kContiguous>;
 
     static_assert(Iterations::kCount, "Number of iterations must be non-zero");
 
     static_assert(Iterations::kStrided == 1,
       "Strided iteration has to be one to reuse the same shared store function with those that don't need transpose");
 
     /// Shape of access by each thread
     using ThreadAccessShape = typename ThreadMap::ThreadAccessShape;
 
     ///< Delta betweeen accesses (units of elements, concept: PitchLinearShape)
     using Delta =
-        layout::PitchLinearShape<ThreadMap::Delta::kStrided,
+        layout::PitchLinearShape<ThreadMap::Delta::kStrided, 
         ThreadMap::Delta::kContiguous>;
 
 
     /// Maps thread ID to a coordinate offset within the tensor's logical
     /// coordinate space Note this is slightly different from the one of
     /// PitchLinearWarpRakedThreadMap.
     CUTLASS_HOST_DEVICE
@@ -689,20 +686,20 @@
     using WarpAccessIterations = layout::PitchLinearShape<
       ShapeInAccesses::kContiguous / WarpThreadArrangement::kContiguous,
       ShapeInAccesses::kStrided / WarpThreadArrangement::kStrided
     >;
 
     // Divide it into the number of warps, first partitioning the strided dimension then the
     // contiguous.
-    static int const kWarpsStrided =
-      (WarpAccessIterations::kStrided >= kWarpCount
+    static int const kWarpsStrided = 
+      (WarpAccessIterations::kStrided >= kWarpCount 
         ? kWarpCount : (kWarpCount / WarpAccessIterations::kStrided));
 
-    static int const kWarpsContiguous =
-      (kWarpCount > WarpAccessIterations::kStrided ?
+    static int const kWarpsContiguous = 
+      (kWarpCount > WarpAccessIterations::kStrided ? 
         WarpAccessIterations::kContiguous / kWarpsStrided : 1);
 
     /// Arrangement of warps within a threadblock-scoped tile
     using WarpArrangement = layout::PitchLinearShape<
       kWarpsContiguous, kWarpsStrided
     >;
   };
@@ -748,15 +745,15 @@
     // This is the offset of a specific thread within a warp (units of vectors)
     layout::PitchLinearCoord thread_offset_in_warp{
       lane_id % Detail::WarpThreadArrangement::kContiguous,
       lane_id / Detail::WarpThreadArrangement::kContiguous
     };
 
     // This is the offset of a thread within a threadblock tile (units of vectors)
-    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec =
+    layout::PitchLinearCoord thread_offset_in_threadblock_tile_vec = 
       warp_footprint * warp_offset + thread_offset_in_warp;
 
     // This is the offset of a thread within a threadblock tile (units of elements)
     layout::PitchLinearCoord thread_offset_in_threadblock_tile_base{
       thread_offset_in_threadblock_tile_vec.contiguous() * kElementsPerAccess,
       thread_offset_in_threadblock_tile_vec.strided()
     };
@@ -772,15 +769,15 @@
 /// The tile must be divisible by the thread count such that all threads may execute the same
 /// number of iterations with the same delta to exhaustively cover the tile.
 ///
 /// This class satisfies the "RegularThreadMapping" concept.
 template <
   typename Shape_,
   int Threads,
-        typename ThreadTileShape
+	typename ThreadTileShape
 >
 struct PitchLinear2DThreadTileStripminedThreadMap;
 
 
 template <
   typename Shape_,
   int Threads
@@ -863,15 +860,15 @@
 
     return TensorCoord(
       (thread_id % Detail::ShapeVec::kContiguous) * ThreadAccessShape::kContiguous,
       (thread_id / Detail::ShapeVec::kContiguous) * ThreadAccessShape::kStrided);
   }
 };
 
-/// Thread Mapping a 2D threadtiled mapping as a transposed Pitchlinear2DThreadTile mapping
+/// Thread Mapping a 2D threadtiled mapping as a tranposed Pitchlinear2DThreadTile mapping
 template <typename ThreadMap_>
 struct TransposePitchLinearThreadMap2DThreadTile {
     /// Underlying ThreadMap
     using ThreadMap = ThreadMap_;
 
     /// Tensor coordinate
     using TensorCoord = typename ThreadMap::TensorCoord;
@@ -884,26 +881,26 @@
 
     /// Extract vector length from Layout
     static int const kElementsPerAccess = ThreadMap::kElementsPerAccess;
 
 
     static_assert(kElementsPerAccess > 1 , "Simt transpose requires elements per access to be 1");
     ///< Iterations along each dimension (concept: PitchLinearShape)
-    using Iterations =
+    using Iterations = 
         layout::PitchLinearShape<ThreadMap::Iterations::kStrided,
         ThreadMap::Iterations::kContiguous>;
 
     static_assert(Iterations::kCount, "Number of iterations must be non-zero");
 
     /// Shape of access by each thread
     using ThreadAccessShape = typename ThreadMap::ThreadAccessShape;
 
     ///< Delta betweeen accesses (units of elements, concept: PitchLinearShape)
     using Delta =
-        layout::PitchLinearShape<ThreadMap::Delta::kStrided,
+        layout::PitchLinearShape<ThreadMap::Delta::kStrided, 
         ThreadMap::Delta::kContiguous>;
 
 
     /// Maps thread ID to a coordinate offset within the tensor's logical
     /// coordinate space Note this is slightly different from the one of
     /// PitchLinearWarpRakedThreadMap.
     CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/transpose.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/thread/unary_op.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_access_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/ell_predicated_tile_iterator.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_access_iterator.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_scale_bias_vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -217,18 +217,15 @@
     thread_offset_ = threadblock_offset + ThreadMap::initial_offset(thread_id);
 
     compute_predicates_(residue_extent, false);
 
     set_iteration_index(0);
   }
 
-  /// Default constructor
-  PredicatedTileAccessIteratorPredicates() = default;
-
-  /// Constructs a TileIterator from its precomputed state, threadblock offset,
+    /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIteratorPredicates(
       /// Extent of tensor
       TensorCoord extent)
       : extent_(extent) {
 	}
@@ -359,16 +356,17 @@
   using Mask = typename UnderlyingPredicates::Mask;
 
   /// Uses a non-template class
   struct Params : PredicatedTileAccessIteratorParams {
     
     using Base = PredicatedTileAccessIteratorParams;
 
-    /// Default constructor
-    Params() = default;
+    // Default ctor
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout) : 
       Base(layout.stride(0),
             MakePredicatedTileAccessIteratorDesc<Shape, Element, Layout, kAdvanceRank, ThreadMap>()()
         ) { }
@@ -386,24 +384,24 @@
   //
   // Data members
   //
 
   UnderlyingPredicates the_predicates;
 
   /// Parameters object with precomputed internal state
-  Params params_;
+  Params const &params_;
 
   /// Internal pointer to first access of tile
   BytePointer pointer_;
 
   /// Used for out-of-order visitation
   bool is_residue_tile_;
 
   /// Below is used when Gather is turned on.  We need to record strided_offset
-  /// and contiguous_offset separated to compute the offset by using
+  /// and contiguous_offset seperated to compute the offset by using
   ///
   /// offset = contiguous_offset + indices[strided_offset]
   ///
 
   /// Gather indices
   int const *indices_;
 
@@ -417,18 +415,15 @@
       TensorCoord extent,
       /// optionally, simplify predicate calculation during 'steady state' phase
       bool is_steady_state = false) {
 	  the_predicates.compute_predicates_(extent, is_steady_state);
   }
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
+          
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -700,16 +695,17 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default constructor
-    Params() = default;
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))){};
 
     /// Construct the Params object given a pitch-linear tensor's layout
@@ -723,18 +719,14 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -887,16 +879,17 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default constructor
-    Params() = default;
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))){};
 
     /// Construct the Params object given a pitch-linear tensor's layout
@@ -910,18 +903,14 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1129,15 +1118,15 @@
   using BytePointer = char *;
 
   //
   // Data members
   //
 
   /// Parameters object with precomputed internal state
-  Params params_;
+  Params const &params_;
 
   /// Internal pointer to first access of tile
   BytePointer pointer_;
 
   UnderlyingPredicates the_predicates;
 
   /// Used for out-of-order visitation
@@ -1151,18 +1140,14 @@
       TensorCoord extent,
       /// optionally, simplify predicate calculation during 'steady state' phase
       bool is_steady_state = false) {
           the_predicates.compute_predicates_(extent, is_steady_state);
   }
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1382,16 +1367,17 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default constructor
-    Params() = default;
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1))){};
   };
 
@@ -1400,18 +1386,14 @@
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1564,16 +1546,17 @@
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
 
-    /// Default constructor
-    Params() = default;
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::AffineRankN<2>(layout.stride(1), layout.stride(0))){};
   };
 
@@ -1582,18 +1565,14 @@
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       ///< Precomputed parameters object
       Params const &params,
       ///< Pointer to start of tensor
@@ -1750,17 +1729,16 @@
    private:
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params() {}
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
@@ -1773,18 +1751,14 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1941,17 +1915,16 @@
    private:
     friend PredicatedTileAccessIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params() {}
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
@@ -1964,18 +1937,14 @@
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileAccessIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileAccessIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_2dthreadtile.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_params.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_access_iterator_triangular_matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -203,21 +203,21 @@
    private:
     /// Parameters object
     typename TileAccessIterator::Params params_;
 
    public:
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout) : params_(layout) {}
-
-    /// Default constructor
-    Params() = default;
+    Params(Layout const &layout) : params_(layout) { }
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     CUTLASS_HOST_DEVICE
-    Params(Base const &base)
+    Params(Base const &base) 
         : params_(base) {}
   };
 
  private:
   /// Internal pointer type permits fast address arithmetic
   using BytePointer = char *;
 
@@ -226,18 +226,14 @@
   // Data members
   //
 
   /// Data member to the tile access iterator
   TileAccessIterator address_iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -457,25 +453,26 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-
-    /// Default constructor
-    Params() = default;
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0)))
-    {}
+    Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {
+
+    }
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base)
+    Params(typename UnderlyingIterator::Params::Base const &base) 
         : params_(base) {}
   };
 
 
 private:
 
   //
@@ -483,17 +480,14 @@
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -672,24 +666,24 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-
-    /// Default constructor
-    Params() = default;
+    
+    CUTLASS_HOST_DEVICE
+    Params() { } 
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout): params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base)
+    Params(typename UnderlyingIterator::Params::Base const &base) 
         : params_(base) {}
 
   };
 
 private:
 
   //
@@ -697,17 +691,14 @@
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -883,18 +874,18 @@
    private:
     /// Parameters object
     typename TileAccessIterator::Params params_;
 
    public:
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout) : params_(layout) {}
-
-    /// Default constructor
-    Params() = default;
+    Params(Layout const &layout) : params_(layout) { }
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
   };
 
  private:
   /// Internal pointer type permits fast address arithmetic
   using BytePointer = char *;
 
  private:
@@ -902,18 +893,14 @@
   // Data members
   //
 
   /// Data member to the tile access iterator
   TileAccessIterator address_iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1132,38 +1119,36 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-
-    /// Default constructor
-    Params() = default;
+    
+    CUTLASS_HOST_DEVICE
+    Params() { }
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
-    Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1)))
-    {}
+    Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(0), layout.stride(1))) {
+
+    }
   };
 
 private:
 
   //
   // Data members
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -1340,17 +1325,17 @@
 
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
   public:
-
-    /// Default constructor
-    Params() = default;
+    
+    CUTLASS_HOST_DEVICE
+    Params() { } 
 
     /// Construct the Params object given an AffineRankN<2> tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout): params_(layout::AffineRankN<2>(layout.stride(1), layout.stride(0))) {}
   };
 
 
@@ -1361,17 +1346,14 @@
   //
 
   /// Underlying AffineRankN<2> tile iterator
   UnderlyingIterator iterator_;
 
 public:
 
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset, and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
     Params const &params,                         ///< Precomputed parameters object 
     Pointer pointer,                              ///< Pointer to start of tensor
     TensorCoord extent,                           ///< Extent of tensor
     int thread_id,                                ///< ID of each participating thread
@@ -1544,42 +1526,37 @@
    private:
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params() {}
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base)
+    Params(typename UnderlyingIterator::Params::Base const &base) 
         : params_(base) {}
 
   };
 
  private:
   //
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
@@ -1720,15 +1697,15 @@
   using UnderlyingIterator = PredicatedTileIterator<
       layout::PitchLinearShape<Shape::kColumn * kInterleavedK,
                                Shape::kRow / kInterleavedK>,
       Element, layout::PitchLinear, (kAdvanceRank == 0 ? 1 : 0), ThreadMap, AccessSize>;
 
 
   using AccessType = typename UnderlyingIterator::AccessType;
-
+  
   /// Fragment object to be loaded or stored
   using Fragment = cutlass::Array<Element, ThreadMap::Iterations::kCount *
                                                ThreadMap::kElementsPerAccess>;
 
   /// Predicate vector stores mask to guard accesses
   using Mask = typename UnderlyingIterator::Mask;
 
@@ -1737,41 +1714,36 @@
    private:
     friend PredicatedTileIterator;
 
     /// Parameters object
     typename UnderlyingIterator::Params params_;
 
    public:
-
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params() {}
 
     /// Construct the Params object given a pitch-linear tensor's layout
     CUTLASS_HOST_DEVICE
     Params(Layout const &layout)
         : params_(layout::PitchLinear(layout.stride(0))) {}
 
     CUTLASS_HOST_DEVICE
-    Params(typename UnderlyingIterator::Params::Base const &base)
+    Params(typename UnderlyingIterator::Params::Base const &base) 
         : params_(base) {}
   };
 
  private:
   //
   // Data members
   //
 
   /// Underlying pitch-linear tile iterator
   UnderlyingIterator iterator_;
 
  public:
-
-  /// Default constructor
-  PredicatedTileIterator() = default;
-
   /// Constructs a TileIterator from its precomputed state, threadblock offset,
   /// and thread ID
   CUTLASS_HOST_DEVICE
   PredicatedTileIterator(
       /// Precomputed parameters object
       Params const &params,
       /// Pointer to start of tensor
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_2dthreadtile.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_tile_iterator_triangular_matrix.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/predicated_vector_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,394 +24,385 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Templates implementing computing the addresses of loading small
-    vectors from the global memory.
+    \brief Templates implementing computing the addresses of storing of tiles
+   from pitch-linear rank=2 tensors.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/array.h"
-#include "cutlass/coord.h"
 #include "cutlass/layout/pitch_linear.h"
 #include "cutlass/layout/matrix.h"
-#include "cutlass/layout/tensor.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/matrix_shape.h"
 #include "cutlass/tensor_ref.h"
 
+#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// PredicatedVectorAccessIterator
+/// Tile iterator specialized for congruous arrangements for TensorOps
 ///
-template <
-    /// Shape of the vector accessed by the entire threadblock
-    typename Shape,
-    /// Shape of the vector accessed by the warp
-    typename WarpShape,
-    /// Type of Element
-    typename Element,
-    /// Layout of the vector
-    typename Layout,
-    /// Number of elements for each access
-    int ElementsPerAccess,
-    /// Support residual tile
-    bool EnableResidualAccess = false
->
-class PredicatedVectorAccessIterator;
-
-////////////////////////////////////////////////////////////////////////////////
-
-/// Vector access iterator specialized for vectors, e.g. scale and bias
-/// Thread arrangements are for TensorOps
 ///
-template <
-  typename Shape_, 
-  typename WarpShape_, 
-  typename Element_, 
-  int ElementsPerAccess, 
-  bool EnableResidualAccess
->
-class PredicatedVectorAccessIterator <
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::PitchLinear,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
-  public:
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::PitchLinear,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
-  using WarpShape = WarpShape_;
   using Element = Element_;
   using Layout = layout::PitchLinear;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
+  using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
+  using ThreadMap = ThreadMap_;
 
-//  static int const kElementsPerAccess = 128 / sizeof_bits<Element>::value;
-  static int const kElementsPerAccess = ElementsPerAccess;
-  static int const kThreads = 32;
-  static int const kRowsPerIteration = 8;
-  static int const kThreadsPerRow = kThreads / kRowsPerIteration;
-  static int const kThreadsPerRowMask = 0x3;
-  static int const kIterations = WarpShape::kContiguous / (kThreadsPerRow * kElementsPerAccess); 
-  static int const kWarpCountStrided = Shape::kStrided / WarpShape::kStrided;
-
-  using AccessType = AlignedArray<Element, kElementsPerAccess>;
-
- private:
-  /// Internal pointer type permits fast address arithmetic
-  using BytePointer = char *;
+  /// Element type per access
+  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
 
  private:
   //
   // Data members
   //
 
-  /// Internal pointer to first access of tile
-  BytePointer pointer_;
-
-  /// Extent of tensor
-  TensorCoord extent_;
+  /// Stride value
+  StrideIndex stride_;
 
-  /// pointer offset of each thread
-  TensorCoord thread_offset_;
+  /// Internal pointer to first access of tile
+  AccessType *pointer_;
 
-  /// iteration index
-  LongIndex iteration_;
+  /// Internal byte offset
+  Index byte_offset_;
 
-  /// residual access
-  bool is_residual_;
+  /// Iteration in the contiguous dimension
+  int iteration_contiguous_;
 
-  /// residual offset of each thread
-  TensorCoord residual_offset_;
+  /// Iteration in the strided dimension
+  int iteration_strided_;
 
  public:
-  /// Constructs a vector access iterator
+  /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to the start of the vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    /// ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id,
-    /// Initial offset of threadblock
-    TensorCoord const &threadblock_offset)
-    : pointer_(reinterpret_cast<BytePointer>(
-                       const_cast<NonConstPointer>(pointer))),
-      extent_(extent),
-      is_residual_(false) {
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
+        byte_offset_(0) {
 
+    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
 
-    int warp_offset = (warp_id / kWarpCountStrided) * WarpShape::kContiguous;
-
-    // Per-thread offset in logical coordinates of tensor
-
-    thread_offset_ = threadblock_offset + TensorCoord(warp_offset, 0) +
-        TensorCoord((thread_id & kThreadsPerRowMask) * kElementsPerAccess, 0);
+    // initialize pointer
+    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
 
     set_iteration_index(0);
-
-    if(EnableResidualAccess) {
-      // compute residual offset
-      typename TensorCoord::Index residual_size = extent_.contiguous() % WarpShape::kContiguous;
-      if (residual_size) {
-        is_residual_ = true;
-        residual_offset_ = make_Coord(residual_size, 0);
-      }
-    }
   }
 
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-    /// Pointer to start of vector
-    ConstPointer pointer,
-    /// Extent of vector
-    TensorCoord extent,
-    ///< ID of each participating thread
-    int thread_id,
-    /// ID of each participating warp
-    int warp_id)
-    : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id,
-                                     make_Coord(0, 0)) {}
-
-
   /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
   void set_iteration_index(int index) {
-    iteration_ = index;
+    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
+    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
   }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole tiles
-  CUTLASS_DEVICE
-  void add_tile_offset(
-      TensorCoord const &tile_offset) {
-
-    thread_offset_ =
-        thread_offset_ +
-        TensorCoord(WarpShape::kContiguous * tile_offset.contiguous(), 0);
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    byte_offset_ += pointer_offset * sizeof(Element);
   }
 
   /// Returns a pointer
-  CUTLASS_HOST_DEVICE
+  CUTLASS_DEVICE
   AccessType *get() const {
 
-    return reinterpret_cast<AccessType *>(
-        pointer_ +
-        ((thread_offset_.contiguous() + iteration_ * kThreadsPerRow * kElementsPerAccess) 
-        * sizeof_bits<Element>::value / 8));
-  }
+    AccessType *access_ptr = pointer_;
 
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
-    ++iteration_;
-    if(iteration_ >= kIterations)
-      iteration_ = 0; 
+    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
+                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
+                            ThreadMap::kElementsPerAccess;
 
-    return *this;
+    char *access_byte_ptr =
+        reinterpret_cast<char *>(access_ptr + access_offset);
+
+    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
   }
 
-  /// Increment and return an instance to self.
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void advance() {
-    if(EnableResidualAccess && is_residual_) {
-      is_residual_ = false;
-      thread_offset_ += residual_offset_; 
+  RegularTileAccessIterator &operator++() {
+    ++iteration_contiguous_;
+
+    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
+      return *this;
+
+    // Enter here only if (iteration_contiguous_ ==
+    // ThreadMap::Iteration::kContiguous)
+    iteration_contiguous_ = 0;
+    ++iteration_strided_;
+
+    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
+      return *this;
     }
-    else
-      add_tile_offset(TensorCoord(1, 0));
-  }
 
-  /// Increment and return an instance to self.
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
+    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
+    // which means we enter the next tile.
+    iteration_strided_ = 0;
+
+    return *this;
   }
 
-  /// Returns whether access is valid or not
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  bool valid() {
-    return ((thread_offset_.contiguous() + 
-              iteration_ * kThreadsPerRow * kElementsPerAccess) < extent_.contiguous());
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    this->operator++();
+
+    return prev;
+  }
+
+  /// Adds a tile offset in the unit of tile.
+  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
+  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
+  ///   For row major A operand, k dimension is contiguous dimension;
+  ///   For col major A operand, k dimension is strided dimension;
+  ///   For row major B operand, k dimension is strided dimension;
+  ///   For col major B operand, k dimension is contiguous dimension.
+  /// Below two classes map col/row major to the pitch linear coordinates used
+  /// in this base class.
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
+                       coord.strided() * Shape::kStrided * stride_ *
+                           ThreadMap::kElementsPerAccess);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////
 
-/// Specialization of PredicatedVectorAccessIterator for row-major data.
+/// Tile iterator specialized for column major layouts
+///
 ///
-template <
-  typename Shape_,
-  typename WarpShape_,
-  typename Element_,
-  int ElementsPerAccess,
-  bool EnableResidualAccess
->
-class PredicatedVectorAccessIterator<
-  Shape_,
-  WarpShape_,
-  Element_,
-  layout::RowMajor,
-  ElementsPerAccess,
-  EnableResidualAccess
-> {
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::ColumnMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
  public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
 
   using Shape = Shape_;
-  using WarpShape = WarpShape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
-  using TensorView = TensorView<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ConstPointer = const Element *;
-  using NonConstPointer = typename platform::remove_const<Element>::type *;
+  using ThreadMap = ThreadMap_;
 
-  using UnderlyingIterator = PredicatedVectorAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, 
-      layout::PitchLinearShape<WarpShape::kColumn, WarpShape::kRow>, 
-      Element,
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
       layout::PitchLinear,
-      ElementsPerAccess,
-      EnableResidualAccess>;
+      (kAdvanceRank == 0 ? 0 : 1), 
+      ThreadMap_>;
 
   using AccessType = typename UnderlyingIterator::AccessType;
-  static int const kElementsPerAccess = UnderlyingIterator::kElementsPerAccess;
-  static int const kRowsPerIteration = UnderlyingIterator::kRowsPerIteration;
-  static int const kThreads = UnderlyingIterator::kThreads;
-  static int const kIterations = UnderlyingIterator::kIterations;
 
  private:
-  //
-  // Data members
-  //
 
-  /// Underlying pitch-linear tile iterator
+  /// Underlying iterator
   UnderlyingIterator iterator_;
 
  public:
-  /// Constructs a TileIterator from its precomputed state, threadblock offset,
-  /// and thread ID
+  /// Construct a TileIterator with zero threadblock offset
   CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ///< Pointer to the start of the vector
-      ConstPointer pointer,
-      ///< Extent of tensor
-      TensorCoord extent,
-      ///< ID of each participating thread
-      int thread_id,
-      ///< ID of each participating warp
-      int warp_id,
-      ///< Initial offset of threadblock
-      TensorCoord const &threadblock_offset)
-      : iterator_(pointer, layout::PitchLinearCoord(extent.column(), extent.row()),
-                  thread_id, warp_id,
-                  layout::PitchLinearCoord(threadblock_offset.column(),
-                                           threadblock_offset.row())) {}
-
-  /// Construct a PredicatedVectorAccessIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator(
-      ConstPointer pointer,   ///< Pointer to the start of the vector
-      TensorCoord extent,     ///< Extent of tensor
-      int thread_id,          ///< ID of each participating thread
-      int warp_id             ///< ID of each participating warp
-      )
-      : PredicatedVectorAccessIterator(pointer, extent, thread_id, warp_id, 
-                                        make_Coord(0, 0)) {}
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
 
   /// Overrides the internal iteration index
   CUTLASS_HOST_DEVICE
   void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
 
-  /// Advances an iterator along logical dimensions of matrix in units of whole
-  /// tiles
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  void add_tile_offset(TensorCoord const &tile_offset) {
-    iterator_.add_tile_offset({tile_offset.column(), tile_offset.row()});
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
 
   /// Returns a pointer
   CUTLASS_HOST_DEVICE
   AccessType *get() const {
     return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator &operator++() {
+  RegularTileAccessIterator &operator++() {
     ++iterator_;
     return *this;
   }
 
   /// Advances to the next tile in memory.
-  ///
-  /// The first time this method is called, predicates are updated, and the
-  /// iterator's internal pointer is reverted to the first "steady state" tile.
-  /// Subsequent calls are lightweight and must only update the internal
-  /// pointer.
   CUTLASS_HOST_DEVICE
-  PredicatedVectorAccessIterator operator++(int) {
-    PredicatedVectorAccessIterator self(*this);
-    operator++();
-    return self;
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
+
+    return prev;
+  }
+};
+
+
+////////////////////////////////////////////////////////////////////////////////
+
+/// Tile iterator specialized for row major layouts
+///
+///
+/// Satisfies: ForwardTileIteratorConcept |
+///            ReadableContiguousTileIteratorConcept |
+///            WriteableContiguousTileIteratorConcept
+///
+template <typename Shape_, typename Element_, int AdvanceRank,
+          typename ThreadMap_, int Alignment>
+class RegularTileAccessIterator<
+    Shape_, Element_,
+    layout::RowMajor,
+    AdvanceRank, ThreadMap_, Alignment> {
+ public:
+  static_assert(
+      AdvanceRank == 0 || AdvanceRank == 1,
+      "Specialization for pitch-linear iterator may along advance along the "
+      "contiguous(rank=0) or strided(rank=1) dimension.");
+
+  using Shape = Shape_;
+  using Element = Element_;
+  using Layout = layout::RowMajor;
+  static int const kAdvanceRank = AdvanceRank;
+  static int const kAlignment = Alignment;
+
+  using Index = typename Layout::Index;
+  using LongIndex = typename Layout::LongIndex;
+
+  using TensorRef = TensorRef<Element, Layout>;
+  using TensorCoord = typename Layout::TensorCoord;
+
+  using ThreadMap = ThreadMap_;
+
+  /// Underlying iterator type
+  using UnderlyingIterator = RegularTileAccessIterator<
+      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
+      layout::PitchLinear,
+      (kAdvanceRank == 0 ? 1 : 0), 
+      ThreadMap_>;
+
+  using AccessType = typename UnderlyingIterator::AccessType;
+
+ private:
+
+  /// Underlying iterator
+  UnderlyingIterator iterator_;
+
+ public:
+  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_HOST_DEVICE
+  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
+                            int thread_id   ///< ID of each participating thread
+                            )
+      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+
+  /// Overrides the internal iteration index
+  CUTLASS_HOST_DEVICE
+  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
+  }
+
+  /// Returns a pointer
+  CUTLASS_HOST_DEVICE
+  AccessType *get() const {
+    return reinterpret_cast<AccessType *>(iterator_.get());
   }
 
-  /// Increment and return an instance to self.
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
+
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  void advance() {
-    iterator_.advance();
+  RegularTileAccessIterator &operator++() {
+    ++iterator_;
+    return *this;
   }
 
-  /// Returns whether access is valid or not
+  /// Advances to the next tile in memory.
   CUTLASS_HOST_DEVICE
-  bool valid() {
-    return iterator_.valid();
+  RegularTileAccessIterator operator++(int) {
+    RegularTileAccessIterator prev(*this);
+    ++iterator_;
+
+    return prev;
   }
 };
 
-
 ////////////////////////////////////////////////////////////////////////////////
 
 }  // namespace threadblock
-}  // namespace transform 
+}  // namespace transform
 }  // namespace cutlass
 
+////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_scale_bias_vector_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear_2dthreadtile.h`

 * *Files 18% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,384 +25,485 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing computing the addresses of storing of tiles
-   from pitch-linear rank=2 tensors.
+    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
+
+    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
+    first, with the objective of minimizing predicate mask updates during steady-state operation.
+
+    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
+    and integer addition is used to advance the pointer through memory.
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/layout/pitch_linear.h"
-#include "cutlass/layout/matrix.h"
-#include "cutlass/matrix_coord.h"
-#include "cutlass/matrix_shape.h"
 #include "cutlass/tensor_ref.h"
+#include "cutlass/layout/matrix.h"
+#include "cutlass/layout/pitch_linear.h"
 
-#include "cutlass/transform/threadblock/regular_tile_access_iterator.h"
+#include "regular_tile_iterator.h"
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace transform {
 namespace threadblock {
 
-////////////////////////////////////////////////////////////////////////////////
-
-/// Tile iterator specialized for congruous arrangements for TensorOps
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::PitchLinear,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+template <
+  typename Shape,
+  typename Element,
+  typename Layout,
+  int AdvanceRank,
+  typename ThreadMap,
+  int Alignment = sizeof_bits<Element>::value * ThreadMap::kElementsPerAccess / 8
+>
+class RegularTileIterator2dThreadTile;
+
+
+/// Regular tile iterator specialized for pitch-linear + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
   using Layout = layout::PitchLinear;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
   using StrideIndex = typename Layout::Stride::Index;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the contiguous or strided dimensions.");
+
+private:
 
-  /// Element type per access
-  using AccessType = Array<Element, ThreadMap::kElementsPerAccess>;
+  //
+  // Types
+  //
+  
+  using AccessType = AlignedArray<Element, ThreadMap::ThreadAccessShape::kCount, kAlignment>;
 
- private:
   //
   // Data members
   //
 
-  /// Stride value
+  /// Pointer to memory
+  uint8_t *pointer_;
+
+  /// Stride quantity
   StrideIndex stride_;
 
-  /// Internal pointer to first access of tile
-  AccessType *pointer_;
+  /// Amount to increment pointer along strided dimension
+  LongIndex increment_strided_;
 
-  /// Internal byte offset
-  Index byte_offset_;
+  /// Amount to advance pointer between tiles
+  LongIndex increment_advance_;
 
-  /// Iteration in the contiguous dimension
-  int iteration_contiguous_;
+public:
 
-  /// Iteration in the strided dimension
-  int iteration_strided_;
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(): pointer_(nullptr), increment_strided_(0), increment_advance_(0) { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : stride_(ref.stride(0) / ThreadMap::kElementsPerAccess),
-        byte_offset_(0) {
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx,
+    int interleave
+  ){ 
+    
+    TensorCoord t = ThreadMap::initial_offset(thread_idx);
+    long int offset = t[0] * interleave + t[1] * ref.stride()[0]/interleave;
+    pointer_ = reinterpret_cast<uint8_t *>(ref.data() + offset);
+
+    stride_ = ref.stride()[0] / interleave;
+    increment_strided_ = (ref.stride()[0] * sizeof_bits<Element>::value / 8) * ThreadMap::Delta::kStrided / interleave;
+
+    increment_advance_ = 
+      (kAdvanceRank == 0 ? 
+        Shape::kContiguous * sizeof_bits<Element>::value / 8 : 
+        Shape::kStrided * (ref.stride()[0] * sizeof_bits<Element>::value / 8) / interleave);
+  }
+
+  /// Loads a fragment
+  CUTLASS_DEVICE
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
+    uint8_t const *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
+
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+
+      AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_pointer);
 
-    layout::PitchLinearCoord thread_offset_base = ThreadMap::initial_offset(thread_id);
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-    // initialize pointer
-    pointer_ = reinterpret_cast<AccessType *>(ref.data() + ref.offset(thread_offset_base));
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+           frag_ptr[idx] = access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided];
+        }
 
-    set_iteration_index(0);
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
   }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
-    iteration_contiguous_ = index % ThreadMap::Iterations::kContiguous;
-    iteration_strided_ = index / ThreadMap::Iterations::kContiguous;
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    load_with_pointer_offset(
+      frag, 
+      tile_offset.contiguous() * Shape::kContiguous / ThreadMap::kElementsPerAccess + 
+        tile_offset.strided() * Shape::kStrided * stride_
+    );
   }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    byte_offset_ += pointer_offset * sizeof(Element);
+  void load(Fragment &frag) {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
-  CUTLASS_DEVICE
-  AccessType *get() const {
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
 
-    AccessType *access_ptr = pointer_;
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const*>(&frag);
+    uint8_t *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
 
-    int access_offset = iteration_strided_ * ThreadMap::Delta::kStrided * stride_ +
-                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
-                            ThreadMap::kElementsPerAccess;
+    CUTLASS_PRAGMA_UNROLL
+    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
 
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
+      AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_pointer);
 
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
-  }
+      CUTLASS_PRAGMA_UNROLL
+      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
 
-  /// Advances to the next tile in memory.
-  CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
-    ++iteration_contiguous_;
+          int idx = c + s * ThreadMap::Iterations::kContiguous;
+          access_ptr[c * ThreadMap::Delta::kContiguous / ThreadMap::ThreadAccessShape::kStrided] = frag_ptr[idx];
+      }
 
-    if (iteration_contiguous_ < ThreadMap::Iterations::kContiguous)
-      return *this;
+      if (s + 1 < ThreadMap::Iterations::kStrided) {
+        byte_pointer += increment_strided_;
+      }
+    }
+  }
 
-    // Enter here only if (iteration_contiguous_ ==
-    // ThreadMap::Iteration::kContiguous)
-    iteration_contiguous_ = 0;
-    ++iteration_strided_;
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    store_with_pointer_offset(
+      frag,
+      tile_offset.contiguous() * Shape::kContiguous + tile_offset.strided() * Shape::kStrided * stride_
+    );
+  }
 
-    if (iteration_strided_ < ThreadMap::Iterations::kStrided) {
-      return *this;
-    }
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
+  }
 
-    // Enter here only if (iteration_stride_ == ThreadMap::Iteration::kStrided)
-    // which means we enter the next tile.
-    iteration_strided_ = 0;
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator++() {
+    pointer_ += increment_advance_;
+    return *this;
+  }
 
+  /// Advances the pointer
+  CUTLASS_HOST_DEVICE
+  RegularTileIterator2dThreadTile &operator--() {
+    pointer_ -= increment_advance_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Adds a pointer offset in units of Element
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    this->operator++();
-
-    return prev;
-  }
-
-  /// Adds a tile offset in the unit of tile.
-  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
-  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
-  ///   For row major A operand, k dimension is contiguous dimension;
-  ///   For col major A operand, k dimension is strided dimension;
-  ///   For row major B operand, k dimension is strided dimension;
-  ///   For col major B operand, k dimension is contiguous dimension.
-  /// Below two classes map col/row major to the pitch linear coordinates used
-  /// in this base class.
+  void add_pointer_offset(LongIndex pointer_offset) {
+    pointer_ += pointer_offset;
+  }
+
+  /// Adds a tile offset
   CUTLASS_DEVICE
   void add_tile_offset(TensorCoord const &coord) {
-    add_pointer_offset(coord.contiguous() * Shape::kContiguous +
-                       coord.strided() * Shape::kStrided * stride_ *
-                           ThreadMap::kElementsPerAccess);
+    int offset = sizeof_bits<Element>::value *
+        (coord.contiguous() * Shape::kContiguous + coord.strided() * Shape::kStrided * stride_) / 8;
+    add_pointer_offset(offset);
   }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Tile iterator specialized for column major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::ColumnMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::RowMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::ColumnMajor;
+  using Layout = layout::RowMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 0 : 1), 
-      ThreadMap_>;
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 1 : 0),
+    ThreadMap,
+    kAlignment
+  >;
+
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
-};
 
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.column(), coord.row()});
+  }
 
-////////////////////////////////////////////////////////////////////////////////
+};
 
-/// Tile iterator specialized for row major layouts
-///
-///
-/// Satisfies: ForwardTileIteratorConcept |
-///            ReadableContiguousTileIteratorConcept |
-///            WriteableContiguousTileIteratorConcept
-///
-template <typename Shape_, typename Element_, int AdvanceRank,
-          typename ThreadMap_, int Alignment>
-class RegularTileAccessIterator<
-    Shape_, Element_,
-    layout::RowMajor,
-    AdvanceRank, ThreadMap_, Alignment> {
- public:
-  static_assert(
-      AdvanceRank == 0 || AdvanceRank == 1,
-      "Specialization for pitch-linear iterator may along advance along the "
-      "contiguous(rank=0) or strided(rank=1) dimension.");
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Regular tile iterator specialized for interleaved layout + 2d thread-tiled threadmapping
+template <
+  typename Shape_,
+  typename Element_,
+  int AdvanceRank,
+  typename ThreadMap_,
+  int Alignment
+>
+class RegularTileIterator2dThreadTile<Shape_, Element_, layout::ColumnMajorInterleaved<4>, AdvanceRank, ThreadMap_, Alignment> {
+public:
 
   using Shape = Shape_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
+  using Layout = layout::ColumnMajorInterleaved<4>;
   static int const kAdvanceRank = AdvanceRank;
+  using ThreadMap = ThreadMap_;
   static int const kAlignment = Alignment;
 
   using Index = typename Layout::Index;
   using LongIndex = typename Layout::LongIndex;
 
   using TensorRef = TensorRef<Element, Layout>;
   using TensorCoord = typename Layout::TensorCoord;
 
-  using ThreadMap = ThreadMap_;
+  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::ThreadAccessShape::kCount>;
+  using PitchLinearThreadMap = PitchLinearStripminedThreadMap< layout::PitchLinearShape<Shape::kRow, Shape::kColumn>, 
+                                  ThreadMap::kThreads, ThreadMap::ThreadAccessShape::kCount >;
+                        
+
+  using Underlying = RegularTileIterator2dThreadTile<
+    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
+    Element,
+    layout::PitchLinear,
+    (kAdvanceRank == 0 ? 0 : 1),
+    ThreadMap
+  >;
 
-  /// Underlying iterator type
-  using UnderlyingIterator = RegularTileAccessIterator<
-      layout::PitchLinearShape<Shape::kColumn, Shape::kRow>, Element,
-      layout::PitchLinear,
-      (kAdvanceRank == 0 ? 1 : 0), 
-      ThreadMap_>;
+  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
+    "Advance rank may only be along the row or column dimensions.");
 
-  using AccessType = typename UnderlyingIterator::AccessType;
+private:
 
- private:
+  Underlying iterator_;
 
-  /// Underlying iterator
-  UnderlyingIterator iterator_;
+public:
 
- public:
-  /// Construct a TileIterator with zero threadblock offset
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile() { }
+
+  CUTLASS_DEVICE
+  RegularTileIterator2dThreadTile(
+    TensorRef const &ref, 
+    int thread_idx
+  ):
+    iterator_({ref.data(), ref.stride()}, thread_idx, 4) {
+
+  }
+
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator(TensorRef ref,  ///< Pointer to start of tensor
-                            int thread_id   ///< ID of each participating thread
-                            )
-      : iterator_({ref.data(), ref.stride()}, thread_id) {}
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+    iterator_.load_with_pointer_offset(frag, pointer_offset);
+  }
 
-  /// Overrides the internal iteration index
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) { iterator_.set_iteration_index(index); }
+  void load(Fragment &frag, TensorCoord const & tile_offset) {
+    iterator_.load_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
 
-  /// Adds a pointer offset in units of Element
+  /// Loads a fragment
   CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
+  void load(Fragment &frag) {
+    iterator_.load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
+  /// Stores a fragment
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return reinterpret_cast<AccessType *>(iterator_.get());
+  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
+    iterator_.store_with_pointer_offset(frag, pointer_offset);
   }
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag, TensorCoord const & tile_offset) {
+    iterator_.store_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
+  }
+
+  /// Stores a fragment
+  CUTLASS_HOST_DEVICE
+  void store(Fragment const &frag) {
+    iterator_.store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator &operator++() {
+  RegularTileIterator2dThreadTile &operator++() {
     ++iterator_;
     return *this;
   }
 
-  /// Advances to the next tile in memory.
+  /// Advances the pointer
   CUTLASS_HOST_DEVICE
-  RegularTileAccessIterator operator++(int) {
-    RegularTileAccessIterator prev(*this);
-    ++iterator_;
+  RegularTileIterator2dThreadTile &operator--() {
+    --iterator_;
+    return *this;
+  }
 
-    return prev;
+  /// Adds a pointer offset in units of Element
+  CUTLASS_HOST_DEVICE
+  void add_pointer_offset(LongIndex pointer_offset) {
+    iterator_.add_pointer_offset(pointer_offset);
   }
+
+  /// Adds a tile offset
+  CUTLASS_DEVICE
+  void add_tile_offset(TensorCoord const &coord) {
+    iterator_.add_tile_offset({coord.row(), coord.column()});
+  }
+
 };
 
-////////////////////////////////////////////////////////////////////////////////
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-}  // namespace threadblock
-}  // namespace transform
-}  // namespace cutlass
+} // namespace threadblock
+} // namespace transform
+} // namespace cutlass
 
-////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_pitch_linear_direct_conv.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_access_iterator_tensor_op_sm80.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_pitch_linear.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/epilogue/warp/tile_iterator_simt.h`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,528 +25,468 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Templates implementing loading of tiles from pitch-linear rank=2 tensors. 
-
-    This iterator uses masks to guard out-of-bounds accesses and visits the last "residue" tile
-    first, with the objective of minimizing predicate mask updates during steady-state operation.
-
-    A precomputed "Params" object minimizes the amount of state that must be stored in registers,
-    and integer addition is used to advance the pointer through memory.
+    \brief 
 */
 
 #pragma once
 
-#include "cutlass/cutlass.h"
-#include "cutlass/tensor_ref.h"
+#include "cutlass/array.h"
 #include "cutlass/layout/matrix.h"
 #include "cutlass/layout/pitch_linear.h"
 
-#include "regular_tile_iterator.h"
+#include "cutlass/epilogue/warp/simt_policy.h"
+
+#define CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES 1
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
-namespace transform {
-namespace threadblock {
+namespace epilogue {
+namespace warp {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Regular tile iterator specialized for pitch-linear.  This one is used by 2-stage SIMT kernels
-/// and sparse tensor core meta data.
+/// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename Shape_,
-  typename Element_,
-  int AdvanceRank,
-  typename ThreadMap_,
-  int Alignment
+  typename WarpShape,     ///< shape of warp-level GEMM (concept: MatrixShape)
+  typename Operator,      ///< matrix multiply operation (concept: arch::Mma)
+  typename Element,       ///< data type of element to be written
+  typename Layout,        ///< target shared memory layout
+  typename MmaSimtPolicy          ///< policy defining lane arrangement (concept: MmaSimtPolicy)
 >
-class RegularTileIterator<Shape_, Element_, layout::PitchLinear, AdvanceRank, ThreadMap_, Alignment> {
+class TileIteratorSimt;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Template for reading and writing tiles of accumulators to shared memory
+template <
+  typename WarpShape_,     ///< shape of warp-level GEMM (concept: GemmShape)
+  typename Operator_,      ///< matrix multiply operation (concept: arch::Mma)
+  typename Element_,       ///< data type of element to be written
+  typename MmaSimtPolicy_         ///< policy defining lane arrangement (concept: MmaSimtPolicy)
+>
+class TileIteratorSimt<WarpShape_, Operator_, Element_, layout::RowMajor, MmaSimtPolicy_> {
 public:
 
-  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Operator = Operator_;
   using Element = Element_;
-  using Layout = layout::PitchLinear;
-  static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-  using StrideIndex = typename Layout::Stride::Index;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-  
-  using AccessType = AlignedArray<Element, ThreadMap::kElementsPerAccess, kAlignment>;
+  using Layout = layout::RowMajor;
 
-  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
-    "Advance rank may only be along the contiguous or strided dimensions.");
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
+
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    typename Operator::ElementC, 
+    Policy::kElementsPerIteration>;
+
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = Array<
+    typename Operator::ElementC, 
+    Policy::kAccumulatorElementCount>;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  /// Padding quantity
+  using Padding = MatrixShape<
+    0,
+    4 * Policy::kElementsPerAccess
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+    + 1
+#endif
+  >;
 
 private:
 
-  //
-  // Types
-  //
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    1
+  >;
+
+#else
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    Policy::kElementsPerAccess
+  >;
+#endif
 
   //
   // Data members
   //
 
-  /// Pointer to memory
-  uint8_t *pointer_;
-
-  /// Stride quantity
-  StrideIndex stride_;
-
-  /// Amount to increment pointer along strided dimension
-  Index increment_strided_;
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-  /// Amount to advance pointer between tiles
-  Index increment_advance_;
+  /// Internal layout object
+  Layout layout_;
 
 public:
 
-  CUTLASS_DEVICE
-  RegularTileIterator(): pointer_(nullptr), increment_strided_(0), increment_advance_(0) { }
-
-  CUTLASS_DEVICE
-  RegularTileIterator(
-    TensorRef const &ref, 
-    int thread_idx
-  ): 
-    pointer_(reinterpret_cast<uint8_t *>(ref.data()) + (ref.offset(ThreadMap::initial_offset(thread_idx)) * sizeof_bits<Element>::value / 8)) {
-    
-    stride_ = ref.stride()[0];
-    increment_strided_ = (ref.stride()[0] * sizeof_bits<Element>::value) * ThreadMap::Delta::kStrided / 8;
-    
-    increment_advance_ = 
-      (kAdvanceRank == 0 ? 
-        Shape::kContiguous * sizeof_bits<Element>::value / 8 : 
-        Shape::kStrided * (ref.stride()[0] * sizeof_bits<Element>::value / 8));
-  }
+  /// Default constructor
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimt(): pointer_(nullptr) { }
 
-  /// Loads a fragment
-  CUTLASS_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimt(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements) { 
 
-    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
-    uint8_t const *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
 
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
+    pointer_ += layout_({
+      lane_offset.row(),
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
+  }
 
-      AccessType const *access_ptr = reinterpret_cast<AccessType const *>(byte_pointer);
+  /// Adds a pointer offset
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimt & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
+    return *this;
+  }
 
-      CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimt & add_tile_offset(TensorCoord const &tile_offset) {
 
-        int idx = c + s * ThreadMap::Iterations::kContiguous;
-        frag_ptr[idx] = access_ptr[c * ThreadMap::Delta::kContiguous /
-                                   ThreadMap::kElementsPerAccess];
-      }
+    pointer_ += layout_({
+      tile_offset.row() * Shape::kRow, 
+      (tile_offset.column() * Shape::kColumn / int(AccessType::kElements))
+    });
 
-      if (s + 1 < ThreadMap::Iterations::kStrided) {
-        byte_pointer += increment_strided_;
-      }
-    }
+    return *this;
   }
 
-  /// Loads a fragment
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag, TensorCoord const & tile_offset) {
-    load_with_pointer_offset(
-      frag, 
-      tile_offset.contiguous() * Shape::kContiguous / ThreadMap::kElementsPerAccess + 
-        tile_offset.strided() * Shape::kStrided * stride_
-    );
-  }
+  TileIteratorSimt & operator+=(TensorCoord const &tile_offset) {
 
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) {
-    load_with_pointer_offset(frag, 0);
+    add_tile_offset(tile_offset);
+    
+    return *this;
   }
 
-  /// Stores a fragment
+  /// Store
   CUTLASS_HOST_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-
-    AccessType const *frag_ptr = reinterpret_cast<AccessType const*>(&frag);
-    uint8_t *byte_pointer = pointer_ + pointer_offset * sizeof_bits<Element>::value / 8;
-
-    CUTLASS_PRAGMA_UNROLL
-    for (int s = 0; s < ThreadMap::Iterations::kStrided; ++s) {
-
-      AccessType *access_ptr = reinterpret_cast<AccessType *>(byte_pointer);
+#if CUTLASS_SIMT_EPILOGUE_USE_SCALAR_STORES
+      // de-vectorized stores
+      using ScalarAccessType = AlignedArray<Element, 1>;
+      ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
+      ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
 
       CUTLASS_PRAGMA_UNROLL
-      for (int c = 0; c < ThreadMap::Iterations::kContiguous; ++c) {
-
-        int idx = c + s * ThreadMap::Iterations::kContiguous;
-        access_ptr[c * ThreadMap::Delta::kContiguous /
-                   ThreadMap::kElementsPerAccess] = frag_ptr[idx];
-      }
-
-      if (s + 1 < ThreadMap::Iterations::kStrided) {
-        byte_pointer += increment_strided_;
+      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+          scalarPointer[n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s] = scalarFragPtr[n * Policy::kElementsPerAccess + s];
+        }
       }
+#else
+    // original vector stores
+    AccessType const *frag_ptr = reinterpret_cast<AccessType const *>(&frag);
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)] = frag_ptr[n];
     }
+#endif
   }
 
-  /// Stores a fragment
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag, TensorCoord const & tile_offset) {
-    store_with_pointer_offset(
-      frag,
-      tile_offset.contiguous() * Shape::kContiguous + tile_offset.strided() * Shape::kStrided * stride_
-    );
-  }
-
-  /// Stores a fragment
+  /// Store
   CUTLASS_HOST_DEVICE
   void store(Fragment const &frag) {
     store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances the pointer
+  /// Load
   CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator++() {
-    pointer_ += increment_advance_;
-    return *this;
-  }
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
-  /// Advances the pointer
-  CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator--() {
-    pointer_ -= increment_advance_;
-    return *this;
-  }
-
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    pointer_ += pointer_offset;
-  }
-
-  /// Adds a tile offset in the unit of tile.
-  /// In GEMM/Conv implementation, this is used to move in the k dimension in the shared memory.
-  /// Below layouts are the shared memory layouts.  Current SM50 SIMT kernels only use col major A and row major B.
-  ///   For row major A operand, k dimension is contiguous dimension;
-  ///   For col major A operand, k dimension is strided dimension;
-  ///   For row major B operand, k dimension is strided dimension;
-  ///   For col major B operand, k dimension is contiguous dimension.
-  /// Below two classes map col/row major to the pitch linear coordinates used
-  /// in this base class.
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    int offset = sizeof_bits<Element>::value *
-        (coord.contiguous() * Shape::kContiguous + coord.strided() * Shape::kStrided * stride_) / 8;
-    add_pointer_offset(offset);
-  }
+    AccessType *frag_ptr = reinterpret_cast<AccessType *>(&frag);
 
-  /// Overrides the internal iteration index
-  CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      frag_ptr[n] = pointer_[n * Policy::MmaSimtPolicy::WarpShape::kColumn + pointer_offset / int(AccessType::kElements)];
+    }
   }
 
-    /// Returns a pointer
+  /// Load
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-#if 0
-    AccessType *access_ptr = pointer_[iteration_strided_ & 1];
-    int stride_idx = (iteration_strided_ & ~1);
-
-    int access_offset = stride_idx * ThreadMap::Delta::kStrided * stride_ +
-                        iteration_contiguous_ * ThreadMap::Delta::kContiguous /
-                            ThreadMap::kElementsPerAccess;
-
-    char *access_byte_ptr =
-        reinterpret_cast<char *>(access_ptr + access_offset);
-    return reinterpret_cast<AccessType *>(access_byte_ptr + byte_offset_);
-#endif
-    return reinterpret_cast<AccessType *>(pointer_);
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
   }
-
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Regular tile iterator specialized for pitch-linear
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+/// Template for reading and writing tiles of accumulators to shared memory
 template <
-  typename Shape_,
-  typename Element_,
-  int AdvanceRank,
-  typename ThreadMap_,
-  int Alignment
+  typename WarpShape_,        ///< shape of warp-level GEMM (concept: GemmShape)
+  typename Operator_,         ///< matrix multiply operation (concept: arch::Mma)
+  typename Element_,          ///< data type of element to be written
+  typename Layout_,            ///< target shared memory layout
+  typename MmaSimtPolicy_     ///< policy defining lane arrangement (concept: MmaSimtPolicy)
 >
-class RegularTileIterator<Shape_, Element_, layout::RowMajor, AdvanceRank, ThreadMap_, Alignment> {
+class TileIteratorSimtCanonical {
 public:
 
-  using Shape = Shape_;
+  using WarpShape = WarpShape_;
+  using Operator = Operator_;
   using Element = Element_;
-  using Layout = layout::RowMajor;
-  static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-
-  using Underlying = RegularTileIterator<
-    layout::PitchLinearShape<Shape::kColumn, Shape::kRow>,
-    Element,
-    layout::PitchLinear,
-    (kAdvanceRank == 0 ? 1 : 0),
-    ThreadMap,
-    kAlignment
-  >;
+  using Layout = Layout_;
 
-  using AccessType = typename Underlying::AccessType;
+  using TensorRef = TensorRef<Element, Layout>;         ///< Tensor Reference object
+  using TensorCoord = MatrixCoord;                      ///< Logical coordinate in referenced tensor
+  using Index = typename TensorRef::Index;
+  using LongIndex = typename TensorRef::LongIndex;
+
+  using Policy = SimtPolicy<WarpShape, Operator, Layout, MmaSimtPolicy_>;
+
+  /// Shape of the tile in memory
+  using Shape = MatrixShape<
+    Policy::kRowsPerIteration,
+    WarpShape::kN
+  >;
 
-  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
-    "Advance rank may only be along the row or column dimensions.");
+  /// This is the fragment size produced by one access of the iterator.
+  using Fragment = Array<
+    typename Operator::ElementC, 
+    Policy::kElementsPerIteration>;
+
+  /// This is the complete warp-level accumulator tile.
+  using AccumulatorTile = Array<
+    typename Operator::ElementC, 
+    Policy::kAccumulatorElementCount>;
+
+  /// Number of times this iterator can be incremented
+  static int const kIterations = Policy::kIterations;
+
+  /// Padding quantity
+  using Padding = MatrixShape<
+    0,
+    4 * Policy::kElementsPerAccess + 1
+  >;
 
 private:
 
-  Underlying iterator_;
+  /// Storage type for accessing memory
+  using AccessType = AlignedArray<
+    Element, 
+    1
+  >;
 
-public:
+  //
+  // Data members
+  //
 
-  CUTLASS_DEVICE
-  RegularTileIterator() { }
+  /// Internal pointer to memory
+  AccessType *pointer_;
 
-  CUTLASS_DEVICE
-  RegularTileIterator(
-    TensorRef const &ref, 
-    int thread_idx
-  ):
-    iterator_({ref.data(), ref.stride()}, thread_idx) {
+  /// Internal layout object
+  Layout layout_;
 
-  }
+  /// Guard to indicate whether the shape is divisible
+  bool divisible_;
 
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-    iterator_.load_with_pointer_offset(frag, pointer_offset);
-  }
+  /// Extent of the output tensor
+  MatrixCoord extent_;
 
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag, TensorCoord const & tile_offset) {
-    iterator_.load_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
-  }
-
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) {
-    iterator_.load_with_pointer_offset(frag, 0);
-  }
-
-  /// Stores a fragment
-  CUTLASS_HOST_DEVICE
-  void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    iterator_.store_with_pointer_offset(frag, pointer_offset);
-  }
+  /// Thread offset
+  MatrixCoord thread_offset_;
 
-  /// Stores a fragment
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag, TensorCoord const & tile_offset) {
-    iterator_.store_with_pointer_offset(frag, {tile_offset.column(), tile_offset.row()});
-  }
+public:
 
-  /// Stores a fragment
+  /// Default constructor
   CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    iterator_.store_with_pointer_offset(frag, 0);
-  }
+  TileIteratorSimtCanonical(): pointer_(nullptr) { }
 
-  /// Advances the pointer
+  /// Constructor from TensorRef
   CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator++() {
-    ++iterator_;
-    return *this;
-  }
+  TileIteratorSimtCanonical(
+    TensorRef const &ref,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements),
+    divisible_(true),
+    extent_(WarpShape::kM, WarpShape::kN) { 
+
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
+
+    thread_offset_ = {
+      lane_offset.row() * Shape::kRow, 
+      lane_offset.column() * Policy::kElementsPerAccess
+    };
+
+    pointer_ += layout_({
+      lane_offset.row() * Shape::kRow,
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
+  }
+
+  /// Constructor from TensorRef
+  CUTLASS_HOST_DEVICE
+  TileIteratorSimtCanonical(
+    TensorRef const &ref,
+    TensorCoord const &extent,
+    unsigned lane_id
+  ):
+    pointer_(reinterpret_cast<AccessType *>(ref.data())),
+    layout_(ref.stride()[0] / AccessType::kElements),
+    divisible_(false),
+    extent_(extent) { 
 
-  /// Advances the pointer
-  CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator--() {
-    --iterator_;
-    return *this;
-  }
+    auto lane_layout = Policy::MmaSimtPolicy::get_lane_layout();
+    MatrixCoord lane_offset = lane_layout.inverse(lane_id);
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
-  }
+    thread_offset_ = {
+      lane_offset.row() * Shape::kRow, 
+      lane_offset.column() * Policy::kElementsPerAccess
+    };
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.column(), coord.row()});
+    pointer_ += layout_({
+      lane_offset.row() * Shape::kRow,
+      lane_offset.column() * Policy::kElementsPerAccess / int(AccessType::kElements)
+    });
   }
 
-  /// Overrides the internal iteration index
+  /// Adds a pointer offset
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
+  TileIteratorSimtCanonical & add_pointer_offset(Index pointer_offset) {
+    pointer_ += pointer_offset / AccessType::kElements;
+    return *this;
   }
 
-  /// Returns a pointer
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return iterator_.get();
-  }
-
-};
+  TileIteratorSimtCanonical & add_tile_offset(TensorCoord const &tile_offset) {
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Regular tile iterator specialized for pitch-linear
-template <
-  typename Shape_,
-  typename Element_,
-  int AdvanceRank,
-  typename ThreadMap_,
-  int Alignment
->
-class RegularTileIterator<Shape_, Element_, layout::ColumnMajor, AdvanceRank, ThreadMap_, Alignment> {
-public:
-
-  using Shape = Shape_;
-  using Element = Element_;
-  using Layout = layout::ColumnMajor;
-  static int const kAdvanceRank = AdvanceRank;
-  using ThreadMap = ThreadMap_;
-  static int const kAlignment = Alignment;
-
-  using Index = typename Layout::Index;
-  using LongIndex = typename Layout::LongIndex;
-
-  using TensorRef = TensorRef<Element, Layout>;
-  using TensorCoord = typename Layout::TensorCoord;
-
-  using Fragment = Array<Element, ThreadMap::Iterations::kCount * ThreadMap::kElementsPerAccess>;
-
-  using Underlying = RegularTileIterator<
-    layout::PitchLinearShape<Shape::kRow, Shape::kColumn>,
-    Element,
-    layout::PitchLinear,
-    (kAdvanceRank == 0 ? 0 : 1),
-    ThreadMap
-  >;
-
-  using AccessType = typename Underlying::AccessType;
-
-  static_assert(kAdvanceRank == 0 || kAdvanceRank == 1, 
-    "Advance rank may only be along the row or column dimensions.");
-
-private:
-
-  Underlying iterator_;
-
-public:
-
-  CUTLASS_DEVICE
-  RegularTileIterator() { }
+    MatrixCoord coord_offset(
+      tile_offset.row(), 
+      tile_offset.column() * Shape::kColumn
+    );
 
-  CUTLASS_DEVICE
-  RegularTileIterator(
-    TensorRef const &ref, 
-    int thread_idx
-  ):
-    iterator_({ref.data(), ref.stride()}, thread_idx) {
+    thread_offset_ += coord_offset;
 
-  }
+    pointer_ += layout_({
+      coord_offset.row(), 
+      coord_offset.column()
+    });
 
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) {
-    iterator_.load_with_pointer_offset(frag, pointer_offset);
+    return *this;
   }
 
-  /// Loads a fragment
+  ///< advances in units of whole tiles along the logical coordinate space of the tensor
   CUTLASS_HOST_DEVICE
-  void load(Fragment &frag, TensorCoord const & tile_offset) {
-    iterator_.load_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
-  }
+  TileIteratorSimtCanonical & operator+=(TensorCoord const &tile_offset) {
 
-  /// Loads a fragment
-  CUTLASS_HOST_DEVICE
-  void load(Fragment &frag) {
-    iterator_.load_with_pointer_offset(frag, 0);
+    add_tile_offset(tile_offset);
+    
+    return *this;
   }
 
-  /// Stores a fragment
+  /// Store
   CUTLASS_HOST_DEVICE
   void store_with_pointer_offset(Fragment const &frag, Index pointer_offset) {
-    iterator_.store_with_pointer_offset(frag, pointer_offset);
-  }
 
-  /// Stores a fragment
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag, TensorCoord const & tile_offset) {
-    iterator_.store_with_pointer_offset(frag, {tile_offset.row(), tile_offset.column()});
-  }
+    // de-vectorized stores
+    using ScalarAccessType = AlignedArray<Element, 1>;
+    ScalarAccessType const *scalarFragPtr = reinterpret_cast<ScalarAccessType const *>(&frag);
+    ScalarAccessType *scalarPointer = reinterpret_cast<ScalarAccessType *>(pointer_) + pointer_offset;
 
-  /// Stores a fragment
-  CUTLASS_HOST_DEVICE
-  void store(Fragment const &frag) {
-    iterator_.store_with_pointer_offset(frag, 0);
+    CUTLASS_PRAGMA_UNROLL
+    for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+      CUTLASS_PRAGMA_UNROLL
+      for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+        
+        int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
+        int frag_idx = n * Policy::kElementsPerAccess + s;
+        
+        int col = thread_offset_.column() + ptr_idx;
+
+        if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
+          scalarPointer[ptr_idx] = scalarFragPtr[frag_idx];
+        }
+      }
+    }
   }
 
-  /// Advances the pointer
+  /// Store
   CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator++() {
-    ++iterator_;
-    return *this;
+  void store(Fragment const &frag) {
+    store_with_pointer_offset(frag, 0);
   }
 
-  /// Advances the pointer
+  /// Load
   CUTLASS_HOST_DEVICE
-  RegularTileIterator &operator--() {
-    --iterator_;
-    return *this;
-  }
+  void load_with_pointer_offset(Fragment &frag, Index pointer_offset) const {
 
-  /// Adds a pointer offset in units of Element
-  CUTLASS_HOST_DEVICE
-  void add_pointer_offset(LongIndex pointer_offset) {
-    iterator_.add_pointer_offset(pointer_offset);
-  }
+      // de-vectorized loads
+      using ScalarAccessType = AlignedArray<Element, 1>;
+      ScalarAccessType *scalarFragPtr = reinterpret_cast<ScalarAccessType *>(&frag);
+      ScalarAccessType const *scalarPointer = reinterpret_cast<ScalarAccessType const*>(pointer_) + pointer_offset;
 
-  /// Adds a tile offset
-  CUTLASS_DEVICE
-  void add_tile_offset(TensorCoord const &coord) {
-    iterator_.add_tile_offset({coord.row(), coord.column()});
+      CUTLASS_PRAGMA_UNROLL
+      for (int n = 0; n < Policy::kAccessesPerIteration; ++n) {
+        CUTLASS_PRAGMA_UNROLL
+        for (int s = 0; s < Policy::kElementsPerAccess; s++) {
+          
+          int ptr_idx = n * Policy::MmaSimtPolicy::WarpShape::kColumn * Policy::kElementsPerAccess + s;
+          int frag_idx = n * Policy::kElementsPerAccess + s;
+          
+          int col = thread_offset_.column() + ptr_idx;
+
+          if (divisible_ || (thread_offset_.row() < extent_.row() && col < extent_.column())) {
+            scalarFragPtr[frag_idx] = scalarPointer[ptr_idx];
+          }
+        }
+      }
   }
 
-  /// Overrides the internal iteration index
+  /// Load
   CUTLASS_HOST_DEVICE
-  void set_iteration_index(int index) {
+  void load(Fragment &frag) const {
+    load_with_pointer_offset(frag, 0);
   }
 
-  /// Returns a pointer
   CUTLASS_HOST_DEVICE
-  AccessType *get() const {
-    return iterator_.get();
+  TileIteratorSimtCanonical & operator++() {
+    return add_tile_offset({1, 0});
   }
 
 };
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
 
-} // namespace threadblock
-} // namespace transform
+} // namespace warp
+} // namespace epilogue
 } // namespace cutlass
 
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/regular_tile_iterator_tensor_op_sm70.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1075,15 +1075,15 @@
 
  private:
   //
   // Data members
   //
 
   /// The crosswised elements will be stored in a line.
-  /// line_size is size of crosswised dimension plus padding.
+  /// line_size is size of crosswised dimention plus padding.
   /// in units of AccessType
   Index line_size;
 
   /// Internal pointer to first access of tile
   AccessType *pointer_[Detail::kPointerCount];
 
   /// Internal byte offset
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/threadblock/vector_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/transform/warp/vector_fragment_iterator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/uint128.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/include/cutlass/uint128.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -50,15 +50,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Optionally enable GCC's built-in type
-#if (defined(__x86_64) || defined (__aarch64__)) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
+#if defined(__x86_64) && !defined(__CUDA_ARCH__) && defined(__GNUC__)
 #define CUTLASS_UINT128_NATIVE
 #elif defined(_MSC_VER) && defined(_M_AMD64) && !defined(__CUDA_ARCH__)
 #define CUTLASS_INT128_ARITHMETIC
 #include <intrin.h>
 #if _MSC_VER >= 1920
 #define CUTLASS_INT128_ARITHMETIC_DIV
 #include <immintrin.h>
@@ -67,22 +67,20 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 ///! Unsigned 128b integer type
 struct uint128_t {
 
   /// Size of one part of the uint's storage in bits
-  static constexpr int kPartSize = sizeof_bits<uint64_t>::value;
+  int const kPartSize = sizeof_bits<uint64_t>::value;
 
   struct hilo {
     uint64_t lo;
     uint64_t hi;
 
-    hilo() = default;
-
     CUTLASS_HOST_DEVICE hilo(uint64_t lo_, uint64_t hi_):lo(lo_), hi(hi_) {}
   };
 
   // Use a union to store either low and high parts or, if present, a built-in 128b integer type.
   union {
     struct hilo hilo_;
 
@@ -92,15 +90,16 @@
   };
 
   //
   // Methods
   //
 
   /// Default ctor
-  uint128_t() = default;
+  CUTLASS_HOST_DEVICE
+  uint128_t(): hilo_(0, 0) { }
 
   /// Constructor from uint64
   CUTLASS_HOST_DEVICE
   uint128_t(uint64_t lo_): hilo_(lo_, 0) { }
 
   /// Constructor from two 64b unsigned integers
   CUTLASS_HOST_DEVICE
@@ -154,15 +153,15 @@
 #endif
     return y;
   }
 
   /// Multiply by unsigned 64b integer yielding 128b integer
   CUTLASS_HOST_DEVICE
   uint128_t operator*(uint64_t const &rhs) const {
-    uint128_t y{};
+    uint128_t y;
 #if defined(CUTLASS_UINT128_NATIVE)
     y.native = native * rhs;
 #elif defined(CUTLASS_INT128_ARITHMETIC)
     // Multiply by the low part
     y.hilo_.lo = _umul128(hilo_.lo, rhs, &y.hilo_.hi);
 
     // Add the high part and ignore the overflow
@@ -219,15 +218,14 @@
     quotient = uint64_t(native / divisor);
     remainder = uint64_t(native % divisor);
 #elif defined(CUTLASS_INT128_ARITHMETIC_DIV)
     // implemented using MSVC's arithmetic intrinsics
     quotient = _udiv128(hilo_.hi, hilo_.lo, divisor, &remainder);
 #else
     // TODO - not implemented
-    CUTLASS_UNUSED(remainder);
     CUTLASS_UNUSED(divisor);
     exception();
 #endif
     return quotient;
   }
 
   /// Left-shifts a 128b unsigned integer
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/include/cutlass/wmma_array.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_reorder.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,110 +24,88 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
+
 /*! \file
-    \brief Statically sized array of elements that accommodates all CUTLASS-supported numeric types
-           and is safe to use in a union.
+    \brief reorder data from the host side 
 */
 
 #pragma once
 
-#include "cutlass/arch/wmma.h"
-
-#if defined(CUTLASS_ARCH_WMMA_ENABLED)
-
-#include "cutlass/cutlass.h"
-#include "cutlass/array.h"
-#include "cutlass/functional.h"
+#include "cutlass/coord.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/tensor_view.h"
+#include "cutlass/util/tensor_view_io.h"
+#include "cutlass/util/reference/host/gemm.h"
 
 namespace cutlass {
 
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Wmma array type (WmmaFragmentArray holds elements of of type nvcuda::wmma::fragment)
-template <
-  /// Element type
-  typename T,
-  /// Number of elements in the array
-  int N,
-  /// Whether the element type of T is half_t or __half
-  bool IsHalfType = (platform::is_same<typename T::element_type, cutlass::half_t>::value ||
-                     platform::is_same<typename T::element_type, __half>::value)
->
-class WmmaFragmentArray: public Array<T, N, true> {
-public:
-
-  /// Efficient clear method (override Array::clear())
-  CUTLASS_HOST_DEVICE
-  void clear()
-  {
-    for(int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      nvcuda::wmma::fill_fragment((*this)[i], (typename T::element_type)0);
+/// This is needed for the interleaved integer tensor core kernels.  The purpose
+/// is to use skip the shared memory part in the epilogue.
+template <int Interleaved, typename Element, typename Layout>
+void reorder_column(TensorRef<Element, Layout> dest,
+                    TensorRef<Element, Layout> src,
+                    cutlass::gemm::GemmCoord problem_size) {
+  const int InstructionShapeCol = 8;
+  // 4 threads per Quad
+  const int ElementsPerThread = InstructionShapeCol / 4;
+  // 4 threads per Quad
+  const int ReorderedElementsPerThread =
+      Interleaved / 4;
+
+  for (int n = 0; n < problem_size.n(); n++) {
+    for (int k = 0; k < problem_size.k(); k++) {
+      dest.at({k, (n / Interleaved) * Interleaved +
+                      ((n % ReorderedElementsPerThread) / ElementsPerThread) *
+                          InstructionShapeCol +
+                      ((n % Interleaved) / ReorderedElementsPerThread) *
+                          ElementsPerThread +
+                      (n % ElementsPerThread)}) = src.at({k, n});
     }
   }
+}
 
-  CUTLASS_HOST_DEVICE
-  WmmaFragmentArray<T, N>& operator+=(const WmmaFragmentArray<T, N>& rhs)
-  {
-    using element_type = typename T::element_type;
-    plus<T> add;
-
-    for (int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      (*this)[i] = add((*this)[i], rhs[i]);
-    }
-
-    return *this;
-  }
-};
-
-/// Partial specialization for the case in which T::element_type is
-/// half_t or __half. This is needed because the cast (typename T::element_type)0
-/// in the primary template flags as an error when __CUDA_NO_HALF_CONVERSIONS__
-/// is set.
-template <
-  /// Element type
-  typename T,
-  /// Number of elements in the array
-  int N
->
-class WmmaFragmentArray<T, N, true>: public Array<T, N, true> {
-public:
-
-  /// Efficient clear method (override Array::clear())
-  CUTLASS_HOST_DEVICE
-  void clear()
-  {
-    for(int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      nvcuda::wmma::fill_fragment((*this)[i], __float2half(0.f));
-    }
-  }
+template <int ColumnInterleaved, int LayoutInterleaved = ColumnInterleaved, typename Element, typename Layout>
+void reorder_convK(TensorRef<Element, Layout> dest,
+                    TensorRef<Element, Layout> src,
+                    cutlass::gemm::GemmCoord problem_size) {
+
+    TensorRef<Element, layout::RowMajorInterleaved<LayoutInterleaved>> mappedDest(dest.data(), dest.stride(0));
+    TensorRef<Element, layout::RowMajorInterleaved<LayoutInterleaved>> mappedSrc(src.data(), src.stride(0));
+    
+    reorder_column<ColumnInterleaved>(
+        mappedDest, mappedSrc, problem_size);
+}
+
+/// This is needed for the sparse tensor core kernels.  The purpose
+/// is to use ldmatrix to load from shared memory to the register file.
+template <typename Element, typename LayoutDest, typename LayoutSrc>
+void reorder_meta(TensorRef<Element, LayoutDest> dest,
+                  TensorRef<Element, LayoutSrc> src,
+                  cutlass::gemm::GemmCoord problem_size) {
+  for (int m = 0; m < problem_size.m(); m++) {
+    for (int k = 0; k < problem_size.k(); k++) {
+      // First reorder the rows.
+      int group = (sizeof(Element) == 2) ? 32 : 16;
+      int interweave = (sizeof(Element) == 2) ? 4 : 2;
+
+      int dest_row = m / group * group + (m % 8) * interweave + (m % group) / 8;
+      int dest_col = k;
+
+      // Next swizzle the 2x2 blocks from Z to N.
+      if (((dest_row % 2) == 0) && ((dest_col % 2) == 1)) {
+        ++dest_row;
+        --dest_col;
+      } else if (((dest_row % 2) == 1) && ((dest_col % 2) == 0)) {
+        --dest_row;
+        ++dest_col;
+      }
 
-  CUTLASS_HOST_DEVICE
-  WmmaFragmentArray<T, N>& operator+=(const WmmaFragmentArray<T, N>& rhs)
-  {
-    using element_type = typename T::element_type;
-    plus<T> add;
-
-    for (int i = 0; i < Array<T, N, true>::kElements; i++)
-    {
-      (*this)[i] = add((*this)[i], rhs[i]);
+      dest.at({dest_row, dest_col}) = src.at({m, k});
     }
-
-    return *this;
   }
-};
-
-////////////////////////////////////////////////////////////////////////////////////////////////////
-
+}
 } // namespace cutlass
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // if defined(CUTLASS_ARCH_WMMA_ENABLED)
-
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/cutlass_unit_test.h`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,47 +26,37 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 #pragma once
-#pragma warning (disable : 4068 ) /* disable unknown pragma warnings for visual studio */
+#pragma warning (disable : 4068 ) /* disable unknown pragma warnings for vistual studio */
 
 #pragma nv_diag_suppress boolean_controlling_expr_is_constant
 #include <gtest/gtest.h>
 #pragma nv_diag_warning boolean_controlling_expr_is_constant
 #pragma warning( disable : 4503)
 
 #include <cstdlib>
 #include <string>
-
-#include <cuda_runtime_api.h>
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Gets a CUDA device
-cudaDeviceProp GetCudaDevice();
-
-/// Prints device properties
-std::ostream &operator<<(std::ostream &out, cudaDeviceProp const &device);
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Sets flags for Unit test
 void FilterArchitecture();
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Reads environment variable `CUTLASS_UNIT_TEST_PROBLEM_COUNT` to control the number and order
 //  of problem sizes run by CUTLASS unit tests
 int CutlassUnitTestProblemCount();
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+
 // active test macro
 #define CUTLASS_TEST_LEVEL_ACTIVE(LEVEL,NAME_STATIC,NAME_DYNAMIC,...) \
     TEST(NAME_STATIC,L##LEVEL##_##NAME_DYNAMIC) __VA_ARGS__
 
 // disabled test macro
 #define CUTLASS_TEST_LEVEL_DISABLED(LEVEL,NAME_STATIC,NAME_DYNAMIC,...) \
     TEST(NAME_STATIC,DISABLED_L##LEVEL##_##NAME_DYNAMIC) {}
@@ -84,19 +74,7 @@
 #define CUTLASS_TEST_L1(NAME_STATIC,NAME_DYNAMIC,...)   CUTLASS_TEST_LEVEL_ACTIVE(1,NAME_STATIC,NAME_DYNAMIC,__VA_ARGS__)
 #define CUTLASS_TEST_L2(NAME_STATIC,NAME_DYNAMIC,...)   CUTLASS_TEST_LEVEL_ACTIVE(2,NAME_STATIC,NAME_DYNAMIC,__VA_ARGS__)
 #endif
 
 #if !defined(CUTLASS_TEST_UNIT_ENABLE_WARNINGS)
 #define CUTLASS_TEST_UNIT_ENABLE_WARNINGS false
 #endif
-
-#if (__CUDACC_VER_MAJOR__ >= 12)
-  #define CUDA_12_0_SM90_FEATURES_SUPPORTED true
-#else
-  #define CUDA_12_0_SM90_FEATURES_SUPPORTED false
-#endif
-
-#include <cutlass/cutlass.h>
-#include <cutlass/numeric_types.h>
-#include <cutlass/trace.h>
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/common/filter_architecture.cpp`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -31,57 +31,17 @@
 
 #include <cuda_runtime_api.h>
 
 #include "cutlass_unit_test.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Gets a CUDA device
-cudaDeviceProp GetCudaDevice() {
-
-  cudaError_t err;
-
-  int cudaDeviceId;
-  err = cudaGetDevice(&cudaDeviceId);
-  if (cudaSuccess != err) {
-    std::cerr << "*** Error: Could not detect active GPU device ID"
-              << " [" << cudaGetErrorString(err) << "]" << std::endl;
-    exit(1);
-  }
-
-  cudaDeviceProp deviceProperties;
-  err = cudaGetDeviceProperties(&deviceProperties, cudaDeviceId);
-
-  return deviceProperties;
-}
-
-/// Prints device properties
-std::ostream &operator<<(std::ostream &out, cudaDeviceProp const &deviceProperties) {
-
-  int deviceMajorMinor = deviceProperties.major * 10 + deviceProperties.minor;
-  if (deviceMajorMinor) {
-    int32_t clock_MHz = deviceProperties.clockRate / 1000;
-    out << "GPU(compute_"
-      << deviceMajorMinor << ", "
-      << deviceProperties.multiProcessorCount << " SMs @ " << clock_MHz << " MHz)";
-  }
-  else {
-    out << "No CUDA device.";
-  }
-
-  return out;
-}
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
 /// Sets flags for Unit test
 void FilterArchitecture() {
   // Default flags can be overwritten by --gtest_filter from commandline
-
-  int const kMaxDevice = 999;
-
   cudaError_t err;
 
   int cudaDeviceId;
   err = cudaGetDevice(&cudaDeviceId);
   if (cudaSuccess != err) {
     std::cerr << "*** Error: Could not detect active GPU device ID"
               << " [" << cudaGetErrorString(err) << "]" << std::endl;
@@ -93,14 +53,15 @@
   if (cudaSuccess != err) {
     std::cerr << "*** Error: Could not get device properties for GPU " << cudaDeviceId << " ["
               << cudaGetErrorString(err) << "]" << std::endl;
     exit(1);
   }
 
   int deviceMajorMinor = deviceProperties.major * 10 + deviceProperties.minor;
+  int const kMaxDevice = 999;
 
   // Defines text filters for each GEMM kernel based on minimum supported compute capability
   struct {
 
     /// Unit test filter string
     char const *filter;
 
@@ -113,15 +74,14 @@
   test_filters[] = {
     { "SM50*",                      50, kMaxDevice},
     { "SM60*",                      60, kMaxDevice},
     { "SM61*",                      61, kMaxDevice},
     { "SM70*",                      70, 75},
     { "SM75*",                      75, kMaxDevice},
     { "SM80*",                      80, kMaxDevice},
-    { "SM90*",                      90, 90        },
     { 0, 0, false }
   };
 
   // Set negative test filters
   std::stringstream ss;
   ss << "-";
   for (int i = 0, j = 0; test_filters[i].filter; ++i) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/cache_testbed_output.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm50.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4ncxhwx_s4cxrskx_s4ncxhwx_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s4nhwc_s4nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8ncxhwx_s8cxrskx_s8ncxhwx_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_s8nhwc_s8nhwc_s32nhwc_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_broadcast_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_fprop_with_reduction_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_problems.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -772,37 +772,14 @@
       {1, 1},                                         // stride (stride_h, stride_w)
       {1, 1},                                         // dilation (dilation_h, dilation_w)
       cutlass::conv::Mode::kCrossCorrelation,
       1,                                              // split_k_slices
       2                                               // groups
     ));
 
-    // Larger problem sizes
-    
-    default_single_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
-      {1, 56, 56, 696},                               // input size  (NHWC)
-      {768, 3, 3, 232},                               // filter size (KRSC)
-      {1, 1, 1, 1},                                   // padding (pad_h, _, pad_w, _)
-      {2, 2},                                         // stride (stride_h, stride_w)
-      {1, 1},                                         // dilation (dilation_h, dilation_w)
-      cutlass::conv::Mode::kCrossCorrelation,
-      1,                                              // split_k_slices
-      3                                               // groups
-    ));
-    default_single_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
-      {1, 14, 14, 1392},                              // input size  (NHWC)
-      {1536, 3, 3, 232},                              // filter size (KRSC)
-      {1, 1, 1, 1},                                   // padding (pad_h, _, pad_w, _)
-      {1, 1},                                         // stride (stride_h, stride_w)
-      {1, 1},                                         // dilation (dilation_h, dilation_w)
-      cutlass::conv::Mode::kCrossCorrelation,
-      1,                                              // split_k_slices
-      3                                               // groups
-    ));
-
     ////////////////////////////////////////////////////////////////////////////////////
     // One CTA calculate multiple groups: CTA::N % k_per_group = 0
     ////////////////////////////////////////////////////////////////////////////////////
 
     // 2 groups per CTA
     default_multiple_group_sizes.push_back(cutlass::conv::Conv2dProblemSize(
       {1, 8, 8, threadblock_k * 4},                   // input size  (NHWC)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed.h`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -188,15 +188,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -204,15 +204,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -301,23 +301,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta} 
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -569,15 +569,15 @@
 
   return true;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////////////
 // TestAllConv: Runs cutlass::conv::device::ImplicitGemmConvolution operator and compares it with reference
 // TestAllConv runs conv operator on default conv problem sizes from test::conv::device::TestbedConv2dProblemSizes
-// Additionally, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes
+// Additionaly, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes 
 // (conv_blacklist_sizes)
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename ImplicitGemm>
 bool TestAllConv2d(
   const Conv2dProblemVector & conv_test_sizes = Conv2dProblemVector(),
   const Conv2dProblemVector & conv_blacklist_sizes = Conv2dProblemVector()) {
 
@@ -633,43 +633,43 @@
     //
     // Procedurally disable certain cases
     //
   
     // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
     if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-        (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+        (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
           cutlass::conv::StrideSupport::kUnity)) {
       if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
         continue;
       }
     }
 
     // Fixed channels algorithm requires channel count to match access size
-    if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
+    if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFixedChannels) {
-      if (conv_problem.C != ImplicitGemm::UnderlyingKernel::Mma::IteratorA::AccessType::kElements) {
+      if (conv_problem.C != ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::AccessType::kElements) {
         continue;
       }
     }
 
     // Few channels algorithm requires channel count to match access size
-    if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
+    if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFewChannels) {
-      if (conv_problem.C % ImplicitGemm::UnderlyingKernel::Mma::IteratorA::AccessType::kElements) {
+      if (conv_problem.C % ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::AccessType::kElements) {
         continue;
       }
     }
 
     // CUTLASS DGRAD's *strided* stride specialization supports all stride {stride_h, stride_w} 
     // Although strided dgrad works for all stride combinations, we are only going 
     // to run strided dgrad for non-unity strides 
     if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-        (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+        (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
           cutlass::conv::StrideSupport::kStrided)) {
        if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
          continue;
        }
     }
     
     //
@@ -700,31 +700,31 @@
     if (CutlassUnitTestProblemCount() && 
         testbed.tested_problem_count > CutlassUnitTestProblemCount()) {
       return true;
     }
   }
 
   // Small-channels convolution can't run here.
-  if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
+  if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFixedChannels) {
 
     return true;
   }
 
   // Small-channels convolution can't run here.
-  if (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kIteratorAlgorithm ==
+  if (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kIteratorAlgorithm ==
         cutlass::conv::IteratorAlgorithm::kFewChannels) {
 
     return true;
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_testbed_interleaved.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -182,57 +182,21 @@
     tensor_B.sync_device();
     tensor_B_reordered.sync_device();
     tensor_C.sync_device();
     tensor_D_computed.sync_device();
     tensor_D_reference.sync_device();
   }
 
-  bool sufficient() const {
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.sharedMemPerMultiprocessor < smem_size) {
-      return false;
-    }
-
-    return true;
-  }
-
   /// Executes one test
   bool run(
     cutlass::conv::Conv2dProblemSize const &problem_size,
     cutlass::conv::SplitKMode const &split_k_mode = cutlass::conv::SplitKMode::kSerial,
     ElementCompute alpha = ElementCompute(1),
     ElementCompute beta = ElementCompute(0)) {
 
-    // Waive test if insufficient CUDA device
-    if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
-      }
-      return true;
-    }
-
 #if 0 //display conv2d problem size for debugging
     std::cout << problem_size << std::endl
               << "alpha, beta: (" << float(alpha) << ", " << float(beta) << ")" << std::endl
               << "split_k_mode: " << ((split_k_mode == cutlass::conv::SplitKMode::kSerial) ? "(serial)" : "(parallel)") << std::endl
               << std::endl;
 #endif
 
@@ -289,23 +253,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv2d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv2d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta}
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -513,15 +477,15 @@
   }
 
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////////////
 // TestAllConv: Runs cutlass::conv::device::ImplicitGemmConvolution operator and compares it with reference
 // TestAllConv runs conv operator on default conv problem sizes from test::conv::device::TestbedConv2dProblemSizes
-// Additionally, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes
+// Additionaly, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes 
 // (conv_blacklist_sizes)
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename ImplicitGemm, int InterleavedK>
 bool TestAllInterleavedConv2d(
   const Conv2dProblemVector & conv_test_sizes = Conv2dProblemVector(),
   const Conv2dProblemVector & conv_blacklist_sizes = Conv2dProblemVector()) {
 
@@ -568,15 +532,15 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's unity stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
       //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_cf32nhwc_cf32nhwc_cf32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_broadcast_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -116,15 +116,14 @@
   using ElementC = typename Conv2d::ElementC;
   using LayoutC = typename Conv2d::LayoutC;
   using ElementAccumulator = typename Conv2d::ElementAccumulator;
   using ElementCompute = typename Conv2d::ElementCompute;
   using EpilogueOutputOp = typename Conv2d::EpilogueOutputOp;
   using ElementZ = typename EpilogueOutputOp::ElementZ;
   using ElementT = typename EpilogueOutputOp::ElementT;
-  using ElementVector = typename EpilogueOutputOp::ElementVector;
 
   static cutlass::conv::Operator const kConvolutionalOperator = Conv2d::kConvolutionalOperator;
   static const bool kAddBroadcastFirst = AddBroadcastFirst;
   static const bool kStoreT = EpilogueOutputOp::kStoreT;
 
 public:
 
@@ -139,15 +138,15 @@
   cutlass::HostTensor<ElementC, LayoutC> tensor_C;
   cutlass::HostTensor<ElementAccumulator, LayoutC> tensor_C_reference;
   cutlass::HostTensor<ElementZ, LayoutC> tensor_Z_computed;
   cutlass::HostTensor<ElementZ, LayoutC> tensor_Z_reference;
   cutlass::HostTensor<ElementT, LayoutC> tensor_T_computed;
   cutlass::HostTensor<ElementT, LayoutC> tensor_T_reference;
   cutlass::HostTensor<ElementAccumulator, LayoutC> tensor_Y_reference;
-  cutlass::HostTensor<ElementVector, LayoutC> tensor_Broadcast;            // Input Broadcast
+  cutlass::HostTensor<ElementC, LayoutC> tensor_Broadcast;                 // Input Broadcast
 
 public:
 
   TestbedConv2dWithBroadcast(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
@@ -250,15 +249,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -266,15 +265,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -498,15 +497,15 @@
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////////////
 // TestAllConv: Runs cutlass::conv::device::ImplicitGemmConvolution operator and compares it with reference
 // TestAllConv runs conv operator on default conv problem sizes from test::conv::device::TestbedConv2dProblemSizes
-// Additionally, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes
+// Additionaly, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes 
 // (conv_blacklist_sizes)
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename ImplicitGemm,
           typename ReferenceOp = Conv2dWithBroadcastReferenceOp<ImplicitGemm>,
           bool AddBroadcastFirst = false,
           bool TestSplitK = true 
 >
@@ -554,26 +553,26 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
 #if 0 // relax restrictions on analytic strided dgrad
       // CUTLASS DGRAD's *strided* specialization only support stride >= {2, 2} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kStrided)) {
          if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
            continue;
          }
       }
 #endif
       
@@ -602,15 +601,15 @@
       }
     }
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv2d_with_reduction_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -178,15 +178,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv2d::UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv2d::ImplicitGemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -194,15 +194,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
@@ -460,15 +460,15 @@
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////////////
 // TestAllConv: Runs cutlass::conv::device::ImplicitGemmConvolution operator and compares it with reference
 // TestAllConv runs conv operator on default conv problem sizes from test::conv::device::TestbedConv2dProblemSizes
-// Additionally, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes
+// Additionaly, each conv2d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes 
 // (conv_blacklist_sizes)
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename ImplicitGemm>
 bool TestAllConv2dWithReduction(
   const Conv2dProblemVector & conv_test_sizes = Conv2dProblemVector(),
   const Conv2dProblemVector & conv_blacklist_sizes = Conv2dProblemVector()) {
 
@@ -512,26 +512,26 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's *unity* stride specialization only support stride {1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity)) {
         if (!((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
           continue;
         }
       }
 
 #if 0 // relax restrictions on analytic strided dgrad
       // CUTLASS DGRAD's *strided* specialization only support stride >= {2, 2} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kStrided)) {
          if (((conv_problem.stride_h == 1) && (conv_problem.stride_w == 1))) {
            continue;
          }
       }
 #endif
       
@@ -560,15 +560,15 @@
       }
     }
   }
 
   // CUTLASS DGRAD's *strided* specialization does not support split-k mode 
   if ((ImplicitGemm::kConvolutionalOperator == 
           cutlass::conv::Operator::kDgrad) && 
-      (ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+      (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
         cutlass::conv::StrideSupport::kStrided)) {
 
     passed = testbed.run(
       cutlass::conv::Conv2dProblemSize(
       {1, 56, 56, 8},   // input size (NHWC)
       {8, 1, 1, 8},     // filter size (KRSC)
       {0, 0, 0, 0},     // padding (pad_h, _, pad_w, _)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_dgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_fprop_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_problems.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_testbed.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -180,15 +180,15 @@
   }
 
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
-    int smem_size = int(sizeof(typename Conv3d::UnderlyingKernel::SharedStorage));
+    int smem_size = int(sizeof(typename Conv3d::ImplicitGemmKernel::SharedStorage));
 
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
@@ -196,15 +196,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
 
@@ -290,23 +290,23 @@
 
       typename ReductionDevice::Arguments reduction_args(
         cutlass::conv::implicit_gemm_problem_size(kConvolutionalOperator, problem_size).mn(),
         problem_size.split_k_slices,
         cutlass::conv::implicit_gemm_tensor_c_size(kConvolutionalOperator, problem_size),
         {
           reinterpret_cast<ElementAccumulator*> (workspace.get()),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_D_computed.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         {
           tensor_C.device_data(),
-          ReductionStrideIndex(tensor_C.stride()[Conv3d::UnderlyingKernel::kTensorCStrideIdx])
+          ReductionStrideIndex(tensor_C.stride()[Conv3d::ImplicitGemmKernel::kTensorCStrideIdx])
         },
         // apply alpha, beta to obtain the following equation alpha * ReduceAdd(A * B) + beta * C 
         {alpha, beta}
       );
 
       status = reduction_op.initialize(reduction_args, nullptr);
 
@@ -518,15 +518,15 @@
   }
 
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////////////
 // TestAllConv: Runs cutlass::conv::device::ImplicitGemmConvolution operator and compares it with reference
 // TestAllConv runs conv operator on default conv problem sizes from test::conv::device::TestbedConv2dProblemSizes
-// Additionally, each conv3d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes
+// Additionaly, each conv3d test can provide conv problem sizes (conv_test_sizes) and blacklist of sizes 
 // (conv_blacklist_sizes)
 /////////////////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename ImplicitGemm>
 bool TestAllConv3d(
   const Conv3dProblemVector & conv_test_sizes = Conv3dProblemVector(),
   const Conv3dProblemVector & conv_blacklist_sizes = Conv3dProblemVector()) {
@@ -569,17 +569,17 @@
       //
       // Procedurally disable certain cases
       //
   
       // CUTLASS DGRAD's unity stride specialization only support stride {1, 1, 1} 
       if ((ImplicitGemm::kConvolutionalOperator == 
             cutlass::conv::Operator::kDgrad) && 
-          ((ImplicitGemm::UnderlyingKernel::Mma::IteratorA::kStrideSupport == 
+          ((ImplicitGemm::ImplicitGemmKernel::Mma::IteratorA::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity) ||
-           (ImplicitGemm::UnderlyingKernel::Mma::IteratorB::kStrideSupport == 
+           (ImplicitGemm::ImplicitGemmKernel::Mma::IteratorB::kStrideSupport == 
             cutlass::conv::StrideSupport::kUnity))) {
         if (!((conv_problem.stride_d == 1) &&
               (conv_problem.stride_h == 1) && 
               (conv_problem.stride_w == 1))
           ) {
           continue;
         }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,371 +25,393 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide Implicit GEMM interface
+    \brief Unit tests for thread-level GEMM
 */
+#include "cutlass/arch/wmma.h"
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-
-
-#include "cutlass/conv/kernel/default_conv2d_group_fprop.h"
-#include "cutlass/conv/device/implicit_gemm_convolution.h"
-
-#include "conv2d_testbed.h"
-
-#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Group_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  SingleGroupPerCTA_128x128_64x3_64x64x64) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<128, 128, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<64, 64, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kSingleGroup,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_single_group_sizes));
+#ifdef CUTLASS_ARCH_WMMA_SM70_ENABLED
+#include "mma_pipelined_testbed.h"
+#include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
+
+/// All tests use single staged (kStages=1) mma pipeline for the gemm mainloop
+/// Test name format: SM[arch]_gemm_threadblock_singlestage_wmma_[alayout]_[blayout]_[clayout]_[dtype].[threadblock_shape]_[warp_shape]
+
+///////////////////////////////////////////////////////////////////////////////////////////////////////
+///       WMMA Floating point (f16 accumulation) - Single stage - Threadblock level tests          ////
+///////////////////////////////////////////////////////////////////////////////////////////////////////
+
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_16x16x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::half_t;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 32);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Group_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  SingleGroupPerCTA_64x64_64x3_32x32x64) {
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 128x128x32_64x64x32_16x16x16) {
 
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<64, 64, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<32, 32, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kSingleGroup,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_single_group_sizes));
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::half_t;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(128, 128, 64);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Conv2d_Group_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  MultipleGroupPerCTA_128x128_64x3_64x64x64) {
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, multicta_256x256x96_128x128x32_64x64x32_16x16x16) {
 
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<128, 128, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<64, 64, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kMultipleGroup,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_multiple_group_sizes));
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::half_t;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(256, 256, 96);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(2, 2);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Conv2d_Group_Fprop_Analytic_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  MutipleGroupPerCTA_64x64_64x3_32x32x64) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<64, 64, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<32, 32, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kMultipleGroup,
-    cutlass::conv::IteratorAlgorithm::kAnalytic
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_multiple_group_sizes));
+///////////////////////////////////////////////////////////////////////////////
+/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
+/// wmma.mma.sync.aligned.row.col.m32n8k16.f16.f16 (wmma native size 32x8x16)
+///////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_32x8x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::half_t;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<32, 8, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+//////////////////////////////////////////////////////////////////////////////
+/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
+/// wmma.mma.sync.aligned.row.col.m8n32k16.f16.f16  (wmma native size 8x32x16)
+//////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_8x32x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = cutlass::half_t;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1;   
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 32, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-TEST(SM80_Device_Conv2d_Group_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  SingleGroupPerCTA_128x128_64x3_64x64x64) {
 
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<128, 128, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<64, 64, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kSingleGroup,
-    cutlass::conv::IteratorAlgorithm::kOptimized
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_single_group_sizes));
+///////////////////////////////////////////////////////////////////////////////////////////////////////
+///       WMMA Floating point (f32 accumulation) - Single stage - Threadblock level tests          ////
+///////////////////////////////////////////////////////////////////////////////////////////////////////
+
+//////////////////////////////////////////////////////////////////////////////////
+/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
+/// wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32   (wmma native size 16x16x16)
+//////////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_16x16x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 128x128x32_64x64x32_16x16x16) {
 
-// Optimized multistage singleGroup kernel
-TEST(SM80_Device_Conv2d_Group_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  SingleGroupPerCTA_64x64_64x3_32x32x64) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = cutlass::half_t;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<64, 64, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<32, 32, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    3,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kSingleGroup,
-    cutlass::conv::IteratorAlgorithm::kOptimized
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_single_group_sizes));
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1;   
+
+  cutlass::gemm::GemmCoord problem_size(128, 128, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, multicta_256x256x96_128x128x32_64x64x32_16x16x16) {
 
-// Optimized 2 stage singleGroup kernel
-TEST(SM80_Device_Conv2d_Group_Fprop_Optimized_ImplicitGemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32,
-  SingleGroupPerCTA_64x64_64x2_32x32x64) {
-
-  /// Conv operation element types for the Gemm equivalent (ImplicitGemm)
-  using ElementA           = cutlass::half_t;
-  using ElementB           = cutlass::half_t;
-  using ElementC           = float;
-  using ElementAccumulator = float;
-  using ElementCompute     = float;
-  using ThreadblockShape   = cutlass::gemm::GemmShape<64, 64, 64>;
-  using WarpShape          = cutlass::gemm::GemmShape<32, 32, 64>;
-  using InstructionShape   = cutlass::gemm::GemmShape<16, 8, 16>;
-
-  /// Device-level Conv2d instance
-  using Conv2dGroupFpropKernel = typename cutlass::conv::kernel::DefaultConv2dGroupFprop<
-    ElementA, cutlass::layout::TensorNHWC,
-    ElementB, cutlass::layout::TensorNHWC,
-    ElementC, cutlass::layout::TensorNHWC,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    ThreadblockShape,
-    WarpShape,
-    InstructionShape,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      128 / cutlass::sizeof_bits<ElementC>::value,
-      ElementAccumulator,
-      ElementCompute
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    2,
-    cutlass::arch::OpMultiplyAdd,
-    cutlass::conv::GroupMode::kSingleGroup,
-    cutlass::conv::IteratorAlgorithm::kOptimized
-  >::Kernel;
-
-  using Conv2dGroupFprop = cutlass::conv::device::ImplicitGemmConvolution<Conv2dGroupFpropKernel>;
-
-  /// Run group conv unit test sizes with device-level Conv2d instance
-  test::conv::device::TestbedGroupConv2dProblemSizes problem_sizes(
-    ThreadblockShape::kN, ThreadblockShape::kK,
-    128/cutlass::sizeof_bits<ElementA>::value
-  );
-  EXPECT_TRUE(test::conv::device::TestSpecificConv2d<Conv2dGroupFprop>(problem_sizes.default_single_group_sizes));
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1;   
+
+  cutlass::gemm::GemmCoord problem_size(256, 256, 96);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(2, 2);
+  dim3 block(32, 4, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
 }
 
-////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////
+/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
+/// wmma.mma.sync.aligned.row.col.m32n8k16.f32.f32   (wmma native size 32x8x16)
+////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_32x8x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1;
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<32, 8, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-#endif  // CUTLASS_ARCH_MMA_SM80_SUPPORTED
+/////////////////////////////////////////////////////////////////////////////////
+/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
+/// wmma.mma.sync.aligned.row.col.m8n32k16.f32.f32   (wmma native size 8x32x16)
+/////////////////////////////////////////////////////////////////////////////////
+TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_8x32x16) {
+
+  using ElementA = cutlass::half_t;
+  using LayoutA = cutlass::layout::RowMajor;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using ElementC = float;
+  using LayoutC = cutlass::layout::RowMajor;
+  static const int kStages = 1; 
+
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 32, 16>;
+
+  float alpha = 1.f;
+  float beta = 0.0f;
+
+  // Define the MmaCore components
+  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
+      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
+      ElementB, LayoutB, ElementC, LayoutC,
+      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+
+  dim3 grid(1, 1);
+  dim3 block(32, 1, 1);
+
+  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
+                                            problem_size.k(), alpha, beta)
+      .run(grid, block);
+}
 
-////////////////////////////////////////////////////////////////////////////////
+#endif //CUTLASS_ARCH_WMMA_SM70_ENABLED
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/array.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/array.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/complex.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/complex.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/float8.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/float8.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/functional.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/functional.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/half.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/half.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/matrix.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/matrix_coord.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/predicate_vector.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/quaternion.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_ref.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tensor_view.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/test_unit_core.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/core/tfloat32.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/cp_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/ampere/ldsm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/bitfield.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/coalesce.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/compare.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/complement.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/composition.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_left.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/inverse_right.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_divide.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/logical_product.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/mixedbits.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/transform.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/core/tuple.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/stsm.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_load.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/cute/hopper/tma_store.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/cute/layout/layout_operator.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files 23% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2023 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,113 +24,70 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 /*! \file
-    \brief Unit tests Generic CuTe Layouts
+    \brief Tests for device-wide HERK interface
 */
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/cutlass.h"
-#include "cutlass/layout/layout.h"
-#include "cutlass/matrix_coord.h"
-
-// Cute includes
-#include <cute/layout.hpp>
-#include <cute/int_tuple.hpp>
-
-using namespace cutlass;
-using namespace cute;
-
-namespace test {
-namespace layout {
-
-template <typename GenericLayout, typename Layout> 
-  struct Testbed {
-
-
-    Testbed() {}
-
-    bool run() {
-      GenericLayout generic_layout;
-      Layout layout = Layout::packed({size<0>(generic_layout), size<1>(generic_layout)});
-
-      for (int m = 0; m < size<0>(generic_layout); m++) {
-        for (int n = 0; n < size<1>(generic_layout); n++) {
-          if (generic_layout(m, n) != layout({m, n})) return false;
-        }
-      }
-
-      return true;
-    }
-  };
-
-}
-}
-
-//////////////////////////////////////////////////////////////////////////
-//                      Test Generic CuTe Layouts
-//////////////////////////////////////////////////////////////////////////
-
-/// Canonical Layouts
-
-TEST(GenericLayout, ColumnMajor) {
-  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_1, _8>>;
-  using Layout = cutlass::layout::ColumnMajor;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
+#include <iostream>
 
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
-
-TEST(GenericLayout, RowMajor) {
-  using GenericLayout = cute::Layout<Shape<_8, _4>, Stride<_4, _1>>;
-  using Layout = cutlass::layout::RowMajor;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
-
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
-
-
-/// Swizzle Shared Memory layouts
-
-TEST(GenericLayout, RowMajorTensorOpMultiplicandCrosswise) {
-
-  using GenericLayout = decltype(
-        composition(
-          Swizzle<3,3,3>{},
-          Layout<Shape<_128, _64>, Stride<_64, _1>>{})
-  );
-
-  using Layout = cutlass::layout::RowMajorTensorOpMultiplicandCrosswise<
-      cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
+#include "../../common/cutlass_unit_test.h"
+#include "cutlass/blas3.h"
+#include "cutlass/gemm/device/rank_k.h"
+#include "cutlass/util/host_tensor.h"
+#include "cutlass/util/reference/host/rank_k_complex.h"
+#include "cutlass/util/reference/host/tensor_compare.h"
+#include "cutlass/util/reference/host/tensor_copy.h"
+#include "cutlass/util/reference/host/tensor_fill.h"
+#include "cutlass/util/tensor_view_io.h"
+
+#include "testbed_rank_k_universal.h"
+
+#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
+TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+
+  using ElementA = cutlass::complex<double>;
+  using LayoutA = cutlass::layout::RowMajor;
+
+  using ElementC = cutlass::complex<double>;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  using ElementAccumulator = cutlass::complex<double>;
+
+  using RankK = cutlass::gemm::device::RankK<
+    ElementA,
+    LayoutA,
+    ElementC,
+    LayoutC,
+    cutlass::FillMode::kLower,
+    ElementAccumulator,
+    cutlass::arch::OpClassTensorOp,
+    cutlass::arch::Sm90,
+    cutlass::gemm::GemmShape<32, 32, 16>,
+    cutlass::gemm::GemmShape<16, 16, 16>,
+    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::epilogue::thread::LinearCombination<
+      ElementC,
+      1,
+      ElementAccumulator,
+      ElementAccumulator
+    >,
+    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
+    4,     // kStages 
+    1,     // AlignmentA
+    false, // SplitKSerial
+    cutlass::arch::OpMultiplyAddComplex,
+    cutlass::ComplexTransform::kConjugate,
+    cutlass::BlasMode::kHermitian
+  >;
 
-  EXPECT_TRUE(testbed.run());
+  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
-//////////////////////////////////////////////////////////////////////////
-
-TEST(GenericLayout, ColumnMajorTensorOpMultiplicandCongruous) {
-
-  using GenericLayout = decltype(
-        composition(
-          Swizzle<3,3,4>{},
-          Layout<Shape<_128, _64>>{})
-  );
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-  using Layout = cutlass::layout::ColumnMajorTensorOpMultiplicandCongruous<
-    cutlass::sizeof_bits<cutlass::half_t>::value, 64>;
-
-
-  test::layout::Testbed<GenericLayout, Layout> testbed;
-
-  EXPECT_TRUE(testbed.run());
-}
-//////////////////////////////////////////////////////////////////////////
+#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/activation.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -30,15 +30,14 @@
  **************************************************************************************************/
 /*! \file
     \brief Unit tests for thread-level GEMM
 */
 
 #include "../../common/cutlass_unit_test.h"
 
-#include "cutlass/layout/layout.h"
 #include "cutlass/epilogue/thread/activation.h"
 
 #include "cutlass/util/host_tensor.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename T, int N, typename Func>
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/thread/linear_combination_planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_planar_complex.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_simt_sm61.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_volta_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_with_reduction_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/epilogue_wmma_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/output_tile_threadmap.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/predicated_tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/threadblock/testbed_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_volta_tensor_op.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/epilogue/warp/fragment_iterator_wmma_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -67,15 +67,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 256x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -89,15 +89,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 128x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -111,15 +111,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x256x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 256x64x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -155,15 +155,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x128x512_32x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -177,15 +177,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 128x64x512_64x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -199,15 +199,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_tensor_op_s32, 64x64x512_32x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -221,13 +221,13 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -66,15 +66,15 @@
       cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x128x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -86,15 +86,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x128x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -107,15 +107,15 @@
       cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x64x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -127,15 +127,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x256x1024_64x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -147,15 +147,15 @@
       cutlass::gemm::GemmShape<64, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x128x1024_32x64x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -167,15 +167,15 @@
       cutlass::gemm::GemmShape<32, 64, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x64x1024_64x32x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -187,15 +187,15 @@
       cutlass::gemm::GemmShape<64, 32, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x64x1024_32x32x1024) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -207,15 +207,15 @@
       cutlass::gemm::GemmShape<32, 32, 1024>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x256x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -227,15 +227,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x128x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -247,15 +247,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x128x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -267,15 +267,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 256x64x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -287,15 +287,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x256x512_64x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -307,15 +307,15 @@
       cutlass::gemm::GemmShape<64, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x128x512_32x64x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -327,15 +327,15 @@
       cutlass::gemm::GemmShape<32, 64, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 128x64x512_64x32x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -347,15 +347,15 @@
       cutlass::gemm::GemmShape<64, 32, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_XOR_b1t_b1n_s32n_tensor_op_s32, 64x64x512_32x32x512) {
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -367,13 +367,13 @@
       cutlass::gemm::GemmShape<32, 32, 512>, cutlass::gemm::GemmShape<16, 8, 256>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -79,15 +79,15 @@
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2, 128, 128, false, 
     cutlass::arch::OpXorPopc
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 256x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -110,15 +110,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 128x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -141,15 +141,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 64x128x512_32x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -172,15 +172,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 128x64x512_64x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -203,15 +203,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32n_wmma_tensor_op_s32, 64x64x512_32x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -234,10 +234,10 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 #endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -67,15 +67,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 256x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -89,15 +89,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 128x128x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -111,15 +111,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x256x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 256x64x512_64x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -155,15 +155,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x128x512_32x64x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
 
@@ -176,15 +176,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 128x64x512_64x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -198,15 +198,15 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_tensor_op_s32, 64x64x512_32x32x512) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -220,13 +220,13 @@
       cutlass::gemm::GemmShape<8, 8, 128>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 2, 128, 128,
       false, cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_b1t_b1n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -79,15 +79,15 @@
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2, 128, 128, false, 
     cutlass::arch::OpXorPopc
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 256x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -110,15 +110,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 128x128x512_64x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -141,15 +141,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 64x128x512_32x64x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -172,15 +172,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 128x64x512_64x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -203,15 +203,15 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_b1t_b1n_s32t_wmma_tensor_op_s32, 64x64x512_32x32x512_8x8x128) {
 
   using ElementOutput = int32_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = int32_t;
@@ -234,10 +234,10 @@
           128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, 
           ElementCompute>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 
       2, 128, 128, false, 
       cutlass::arch::OpXorPopc>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 #endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16n_bf16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_bf16t_bf16t_bf16t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -61,15 +61,15 @@
       cutlass::gemm::GemmShape<128, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x128x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -79,15 +79,15 @@
       cutlass::gemm::GemmShape<256, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x128x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -97,15 +97,15 @@
       cutlass::gemm::GemmShape<128, 128, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x64x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -115,15 +115,15 @@
       cutlass::gemm::GemmShape<256, 64, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x256x64_64x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -133,15 +133,15 @@
       cutlass::gemm::GemmShape<64, 256, 64>,
       cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x128x64_32x64x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -151,15 +151,15 @@
       cutlass::gemm::GemmShape<64, 128, 64>,
       cutlass::gemm::GemmShape<32, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x64x64_64x32x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -169,15 +169,15 @@
       cutlass::gemm::GemmShape<128, 64, 64>,
       cutlass::gemm::GemmShape<64, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x64x64_32x32x64) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -187,15 +187,15 @@
       cutlass::gemm::GemmShape<64, 64, 64>,
       cutlass::gemm::GemmShape<32, 32, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x256x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -205,15 +205,15 @@
       cutlass::gemm::GemmShape<128, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x128x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -223,15 +223,15 @@
       cutlass::gemm::GemmShape<256, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x128x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -241,15 +241,15 @@
       cutlass::gemm::GemmShape<128, 128, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 256x64x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -259,15 +259,15 @@
       cutlass::gemm::GemmShape<256, 64, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x256x32_64x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -277,15 +277,15 @@
       cutlass::gemm::GemmShape<64, 256, 32>,
       cutlass::gemm::GemmShape<64, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 4>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x128x32_32x64x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -295,15 +295,15 @@
       cutlass::gemm::GemmShape<64, 128, 32>,
       cutlass::gemm::GemmShape<32, 64, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 128x64x32_64x32x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -313,15 +313,15 @@
       cutlass::gemm::GemmShape<128, 64, 32>,
       cutlass::gemm::GemmShape<64, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 6>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM80_Device_Gemm_bf16t_bf16t_bf16t_tensor_op_f32, 64x64x32_32x32x32) {
   using ElementOutput = cutlass::bfloat16_t;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
@@ -331,13 +331,13 @@
       cutlass::gemm::GemmShape<64, 64, 32>,
       cutlass::gemm::GemmShape<32, 32, 32>, cutlass::gemm::GemmShape<16, 8, 16>,
       cutlass::epilogue::thread::LinearCombination<
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 10>;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 ////////////////////////////////////////////////////////////////////////////////
 
 #endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //  Operands data type: complex<float>
 //  Rounding: float -> tfloat32_t (half_ulp_truncate)
 //  Instruction operand data type: tfloat32_t (real part) and  tfloat32_t (imaginary part)
-//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+//  Math instruction: MMA.1688.F32.TF32
 //  Instruction output/accumulation data type: f32 (real part) and f32 (imaginary part)
 //  Output data type: complex<float>
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 
 TEST(SM80_Device_Gemm_cf32n_cf32t_cf32t_tensor_op_tf32_f32, 32x32x16_16x16x16) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //  Operands data type: complex<float>
 //  Rounding: float -> tfloat32_t (round to nearest)
 //  Instruction operand data type: tfloat32_t (real part) and  tfloat32_t (imaginary part)
-//  Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+//  Math instruction: MMA.1688.F32.TF32
 //  Instruction output/accumulation data type: f32 (real part) and f32 (imaginary part)
 //  Output data type: complex<float>
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM80_Device_Gemm_cf32t_cf32n_cf32t_tensor_op_tf32_f32, 32x32x16_16x16x16) {
 
   using Element = cutlass::complex<float>;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64n_cf64t_cf64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_gaussian_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_cf64t_cf64n_cf64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_direct_store_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -41,15 +41,14 @@
 #include "cutlass/util/reference/host/gemm.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed.h"
-#include "testbed_universal.h"
 
 #if (CUTLASS_ARCH_MMA_SM80_SUPPORTED)
  
 ////////////////////////////////////////////////////////////////////////////////
 
 CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x256x64_64x64x64, {
   using ElementOutput = float;
@@ -101,52 +100,14 @@
           ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
           ElementAccumulator, ElementAccumulator>,
       cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>, 3>;
 
   EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 128x128x64_64x64x64_sk, {
-  using ElementOutput = float;
-  using ElementAccumulator = float;
-
-  using Gemm = cutlass::gemm::device::GemmUniversal<
-      cutlass::half_t, cutlass::layout::ColumnMajor,
-      cutlass::half_t, cutlass::layout::RowMajor,
-      ElementOutput, cutlass::layout::RowMajor,
-      ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-      cutlass::gemm::GemmShape<128, 128, 64>,
-      cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
-      cutlass::epilogue::thread::LinearCombination<
-          ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
-          ElementAccumulator, ElementAccumulator>,
-      cutlass::gemm::threadblock::ThreadblockSwizzleStreamK, 3>;
-
-  EXPECT_TRUE(test::gemm::device::TestAllGemmUniversal<Gemm>());
-} )
-
-CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32n_tensor_op_f32, 128x128x64_64x64x64_sk, {
-  using ElementOutput = float;
-  using ElementAccumulator = float;
-
-  using Gemm = cutlass::gemm::device::GemmUniversal<
-      cutlass::half_t, cutlass::layout::ColumnMajor,
-      cutlass::half_t, cutlass::layout::RowMajor,
-      ElementOutput, cutlass::layout::ColumnMajor,
-      ElementAccumulator, cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-      cutlass::gemm::GemmShape<128, 128, 64>,
-      cutlass::gemm::GemmShape<64, 64, 64>, cutlass::gemm::GemmShape<16, 8, 16>,
-      cutlass::epilogue::thread::LinearCombination<
-          ElementOutput, 128 / cutlass::sizeof_bits<ElementOutput>::value,
-          ElementAccumulator, ElementAccumulator>,
-      cutlass::gemm::threadblock::ThreadblockSwizzleStreamK, 3>;
-
-  EXPECT_TRUE(test::gemm::device::TestAllGemmUniversal<Gemm>());
-} )
-
 CUTLASS_TEST_L1(SM80_Device_Gemm_f16n_f16t_f32t_tensor_op_f32, 256x64x64_64x64x64, {
   using ElementOutput = float;
   using ElementAccumulator = float;
 
   using Gemm = cutlass::gemm::device::Gemm<
       cutlass::half_t, cutlass::layout::ColumnMajor, cutlass::half_t,
       cutlass::layout::RowMajor, ElementOutput, cutlass::layout::RowMajor,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16n_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_singlestage_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_broadcast_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_slicedk_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f16_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_volta_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_singlestage_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16n_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16n_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f16_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f16t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32n_wmma_tensor_op_f32_sm70.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_volta_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f16t_f16t_f32t_wmma_tensor_op_f32_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_bf16_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f32t_f32t_f32t_tensor_op_f32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_f64t_f64n_f64t_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_scheduler_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_planar_complex_f16_f16_f32_tensor_op_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4n_s4t_s4n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_tensor_op_s32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32n_wmma_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s32t_wmma_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,15 +77,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -109,15 +109,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -141,15 +141,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -173,15 +173,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 256x64x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -205,15 +205,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x128x128_32x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -237,15 +237,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 128x64x128_64x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -269,15 +269,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4n_tensor_op_s32, 64x64x128_32x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -301,12 +301,12 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 #endif
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,15 +77,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -109,15 +109,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x128x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -141,15 +141,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x256x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -173,15 +173,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 256x64x128_64x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -205,15 +205,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x128x128_32x64x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
 
@@ -236,15 +236,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 128x64x128_64x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -268,15 +268,15 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 TEST(SM75_Device_Gemm_s4t_s4n_s4t_tensor_op_s32, 64x64x128_32x32x128) {
 
   using ElementOutput = cutlass::int4b_t;
   using ElementAccumulator = int32_t;
   using ElementCompute = float;
@@ -300,13 +300,13 @@
       ElementAccumulator,
       ElementCompute
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     2
   >;
 
-  EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+  EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #endif
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8n_s8t_s8n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32n_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8n_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /**************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_s8t_s8n_s8t_wmma_tensor_op_s32_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_serial_tensor_op_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_simt_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_splitk_tensor_op_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32n_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_tf32t_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_u8t_u8n_s32t_wmma_tensor_op_s32_sm72.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf32n_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_cf64n_cf64t_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32n_tensor_op_f32_sm75.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_universal_f16n_f16t_f32t_tensor_op_f32_sm75.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemm_with_reduction_f16t_f16n_f16n_tensorop_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/gemv.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf32h_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/hemm_cf64h_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64h_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/her2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf32h_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,15 +25,16 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide HERK interface
+    \brief Tests for device-wide SYRK interface
+  
 */
 
 #include <iostream>
 
 #include "../../common/cutlass_unit_test.h"
 #include "cutlass/blas3.h"
 #include "cutlass/gemm/device/rank_k.h"
@@ -42,52 +43,53 @@
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/tensor_view_io.h"
 
 #include "testbed_rank_k_universal.h"
 
-#if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-// HERK operator on CUBLAS_OP_C (row-major + conj) input layouts
-TEST(SM90_Device_Herk_cf64h_cf64n_l_tensor_op_f64, 64x64x16_32x32x16) {
+
+TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
 
   using ElementA = cutlass::complex<double>;
-  using LayoutA = cutlass::layout::RowMajor;
+  using LayoutA = cutlass::layout::ColumnMajor;
 
   using ElementC = cutlass::complex<double>;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::RowMajor;
   using ElementAccumulator = cutlass::complex<double>;
 
   using RankK = cutlass::gemm::device::RankK<
     ElementA,
     LayoutA,
     ElementC,
     LayoutC,
     cutlass::FillMode::kLower,
     ElementAccumulator,
     cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm90,
+    cutlass::arch::Sm80,
     cutlass::gemm::GemmShape<32, 32, 16>,
     cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<16, 8, 4>,
+    cutlass::gemm::GemmShape<8, 8, 4>,
     cutlass::epilogue::thread::LinearCombination<
       ElementC,
       1,
       ElementAccumulator,
       ElementAccumulator
     >,
     cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
     4,     // kStages 
     1,     // AlignmentA
     false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddComplex,
-    cutlass::ComplexTransform::kConjugate,
-    cutlass::BlasMode::kHermitian
+    cutlass::arch::OpMultiplyAddGaussianComplex,
+    cutlass::ComplexTransform::kNone,
+    cutlass::BlasMode::kSymmetric
   >;
 
   EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
 }
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-#endif // #if defined(CUTLASS_ARCH_MMA_SM90_F64_MMA_ENABLED)
+#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/herk_cf64h_cf64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -54,19 +54,14 @@
 namespace gemm {
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct MultistageTestbed {
-
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
-
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute =
       typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
@@ -126,15 +121,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,16 +28,14 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
 */
 
-#pragma once
-
 #include <iostream>
 #include <fstream>
 #include <sstream>
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/util/host_tensor.h"
@@ -53,33 +51,30 @@
 namespace test {
 namespace gemm {
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, int InterleavedK>
-struct MultistageInterleavedTestbed {
+struct InterleavedTestbed {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
   //
   // Methods
   //
 
-  MultistageInterleavedTestbed(
+  InterleavedTestbed(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
     uint64_t seed_ = 2080
   ):
     init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
@@ -109,15 +104,15 @@
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
+	/// Waives test if CUDA device is insufficient
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
     int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
 
@@ -143,23 +138,23 @@
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size, 
     ElementCompute alpha = ElementCompute(1), 
     ElementCompute beta = ElementCompute(0)) {
-    
+
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
     }
-
+    
     //
     // Allocate the GEMM workspace
     //
 
     cutlass::HostTensor<
       typename Gemm::ElementA, 
       typename Gemm::LayoutA> tensor_A(problem_size.mk());
@@ -292,31 +287,31 @@
   }
 
   /// Runs a set of problem sizes
   bool run_all() {
     bool passed = true;
 
     int problem_size_m[] = {
-      InterleavedK, 512 + InterleavedK
+      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
     };
 
     int problem_size_n[] = {
-      InterleavedK, 512 + InterleavedK
+      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
     };
 
     int problem_size_k[] = {
-      InterleavedK, Gemm::ThreadblockShape::kK * Gemm::kStages + InterleavedK
+      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
     };
 
     double problem_alpha[] = {
       1.0
     };
 
     double problem_beta[] = {
-      0.0
+      2.0
     };
 
     for (int m : problem_size_m) {
       for (int n : problem_size_n) {
         for (int k : problem_size_k) {
           for (double alpha : problem_alpha) {
             for (double beta : problem_beta) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/rank_2k_grouped_scheduler_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_nt_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tn_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_cgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_dgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_f8gemm_tn_sm50.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_hgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_igemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_perf.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_int8_igemm_sm61_sliced_k.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nn_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_nt_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tn_sm50.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_qgemm_tt_sm50.cu`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -72,15 +72,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   1
 //       Threadblock:  16 x  32 x  8
@@ -102,15 +102,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  32 x  8
@@ -132,15 +132,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:   8 x  64 x  8
@@ -162,15 +162,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  32 x  8
@@ -192,15 +192,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   1 x   2
 //       Threadblock:  16 x  64 x  8
@@ -222,15 +222,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   1 x   2
 //       Threadblock:  32 x  32 x  8
@@ -252,15 +252,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   1
 //       Threadblock:  32 x  32 x  8
@@ -282,15 +282,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  32 x  8
@@ -312,15 +312,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  16 x  64 x  8
@@ -342,15 +342,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  32 x  8
@@ -372,15 +372,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   2
 //       Threadblock:  32 x  64 x  8
@@ -402,15 +402,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   2
 //       Threadblock:  64 x  32 x  8
@@ -432,15 +432,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  16 x  64 x 16
@@ -462,15 +462,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  32 x  8
@@ -492,15 +492,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x  64 x  8
@@ -522,15 +522,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   2 x   4
 //       Threadblock:  32 x 128 x  8
@@ -552,15 +552,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   2 x   4
 //       Threadblock:  64 x  64 x  8
@@ -582,15 +582,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  32 x  32 x  8
@@ -612,15 +612,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  32 x  8
@@ -642,15 +642,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   2
 //       Threadblock:  64 x  64 x  8
@@ -672,15 +672,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   2
 //       Threadblock: 128 x  32 x  8
@@ -702,15 +702,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  32 x  64 x 16
@@ -732,15 +732,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   2 x   2
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  32 x 16
@@ -762,15 +762,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   2
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x  64 x  8
@@ -792,15 +792,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   4 x   8
 //     Warps / Block:   4 x   4
 //       Threadblock:  64 x 128 x  8
@@ -822,15 +822,15 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
 
 ////////////////////////////////////////////////////////////////////////////////
 // Elements / Thread:   4 x   4
 //    Threads / Warp:   8 x   4
 //     Warps / Block:   4 x   4
 //       Threadblock: 128 x  64 x  8
@@ -852,10 +852,10 @@
         cutlass::arch::OpClassSimt,
         cutlass::arch::Sm50,
         ThreadblockShape, WarpShape, InstructionShape,
         EpilogueOutputOp,
         cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
         2 // Stages
     >;
-    EXPECT_TRUE(test::gemm::device::TestAllGemmBasic<Gemm>());
+    EXPECT_TRUE(test::gemm::device::TestAllGemm<Gemm>());
 } )
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_nt_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tn_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_sgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_nt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tn_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/simt_zgemm_tt_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f32_f32_f32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm50_gemm_f64_f64_f64_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm61_gemm_s8_s8_s32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f16_f16_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f32_f32_f32_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_simt.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_f64_f64_f64_tensor_op_f64.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,28 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: BSD-3-Clause
+ * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
  *
- * Redistribution and use in source and binary forms, with or without
- * modification, are permitted provided that the following conditions are met:
- *
- * 1. Redistributions of source code must retain the above copyright notice, this
- * list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
- * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * Redistribution and use in source and binary forms, with or without modification, are permitted
+ * provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright notice, this list of
+ *       conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright notice, this list of
+ *       conditions and the following disclaimer in the documentation and/or other materials
+ *       provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used
+ *       to endorse or promote products derived from this software without specific prior written
+ *       permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
+ * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
+ * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
+ * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+ * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
 */
 
@@ -37,62 +31,56 @@
 #include "cutlass/cutlass.h"
 #include "cute/tensor.hpp"
 #include "cute/atom/mma_atom.hpp"
 
 #include "cutlass/numeric_types.h"
 
 #include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "default_gemm_configuration.hpp"
+#include "cutlass/gemm/kernel/gemm_universal.hpp"
+#include "cutlass/gemm/collective/collective_builder.hpp"
+#include "cutlass/epilogue/collective/default_epilogue.hpp"
+#include "cutlass/epilogue/collective/default_transposed_epilogue.hpp"
+#include "cutlass/epilogue/thread/linear_combination.h"
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "gemm_testbed_3x.hpp"
 
-using namespace cute;
-
-//#if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM80_Device_Gemm_f64n_f64t_f64n_tensor_op_f64, 128x128x64_64x64x64) {
-
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
+#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
 
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
-  >;
-
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+using namespace cute;
 
-TEST(SM80_Device_Gemm_f64t_f64n_f64n_tensor_op_f64, 128x128x64_64x64x64) {
+///////////////////////////////////////////////////////////////////////////////
 
-  using Config = cutlass::gemm::device::DefaultGemmConfigurationToCutlass3Types<
-    cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
-    double, cutlass::layout::RowMajor,
-    double, cutlass::layout::ColumnMajor,
-    double, cutlass::layout::ColumnMajor,
-    double>;
+TEST(SM90_Device_Gemm_f32t_f32n_f32n_tensor_op_gmma_f32, 64x128x32_1x2x1) {
+  using LayoutA = cutlass::layout::RowMajor;
+  using LayoutB = cutlass::layout::ColumnMajor;
+  using LayoutC = cutlass::layout::ColumnMajor;
+
+  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
+      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
+      float, LayoutA, 4,
+      float, LayoutB, 4,
+      float,
+      Shape<_64,_128,_128>, Shape<_1,_2,_1>,
+      cutlass::gemm::collective::StageCountAuto,
+      cutlass::gemm::collective::KernelScheduleAuto
+    >::CollectiveOp;
+
+  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
+      cutlass::gemm::TagToStrideC_t<LayoutC>,
+      cutlass::gemm::TagToStrideC_t<LayoutC>,
+      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
 
   using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
       Shape<int,int,int,int>,
-      Config::CollectiveMainloop,
-      Config::CollectiveEpilogue
+      CollectiveMainloop,
+      CollectiveEpilogue
   >;
 
   using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
   EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+///////////////////////////////////////////////////////////////////////////////
 
-// #endif
+#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_s8_s8_s32_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm80_gemm_tf32_tf32_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_alignx_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_unspecialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_persistent.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_f32_f32_f32_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h`

 * *Files 27% similar despite different names*

```diff
@@ -1,86 +1,96 @@
 /***************************************************************************************************
- * Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * SPDX-License-Identifier: BSD-3-Clause
  *
- * Redistribution and use in source and binary forms, with or without modification, are permitted
- * provided that the following conditions are met:
- *     * Redistributions of source code must retain the above copyright notice, this list of
- *       conditions and the following disclaimer.
- *     * Redistributions in binary form must reproduce the above copyright notice, this list of
- *       conditions and the following disclaimer in the documentation and/or other materials
- *       provided with the distribution.
- *     * Neither the name of the NVIDIA CORPORATION nor the names of its contributors may be used
- *       to endorse or promote products derived from this software without specific prior written
- *       permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR
- * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND
- * FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
- * BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
- * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
- * STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ * 1. Redistributions of source code must retain the above copyright notice, this
+ * list of conditions and the following disclaimer.
+ *
+ * 2. Redistributions in binary form must reproduce the above copyright notice,
+ * this list of conditions and the following disclaimer in the documentation
+ * and/or other materials provided with the distribution.
+ *
+ * 3. Neither the name of the copyright holder nor the names of its
+ * contributors may be used to endorse or promote products derived from
+ * this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
+ * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
+ * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+ * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
+ * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
+ * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/*! \file
-    \brief Tests for device-wide GEMM interface
+/* \file
+   \brief Execution environment
 */
 
-#include <iostream>
+#pragma once
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
+#include "cutlass/library/manifest.h"
+#include "cutlass/library/singleton.h"
+
+#include "options.h"
+#include "operation_profiler.h"
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
+namespace cutlass {
+namespace profiler {
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "cutlass/cutlass.h"
-#include "cute/tensor.hpp"
-#include "cute/atom/mma_atom.hpp"
+/// CUTLASS Profiler application 
+class CutlassProfiler {
+private:
 
-#include "cutlass/numeric_types.h"
+  //
+  // Data members
+  //
 
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
-#include "cutlass/gemm/kernel/gemm_universal.hpp"
-#include "cutlass/gemm/collective/collective_builder.hpp"
-#include "cutlass/epilogue/collective/default_epilogue.hpp"
-#include "cutlass/epilogue/collective/default_transposed_epilogue.hpp"
-#include "cutlass/epilogue/thread/linear_combination.h"
+  /// Performance testbench options
+  Options options_;
 
-#include "../../common/cutlass_unit_test.h"
+  /// Entry points for each operation
+  OperationProfilerVector operation_profilers_;
 
-#include "gemm_testbed_3x.hpp"
+private:
 
-#if defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+  /// Prints usage
+  void print_usage_(std::ostream &);
+  
+  /// Prints usage
+  void print_options_(std::ostream &);
 
-using namespace cute;
+  /// Initializes the device
+  void initialize_device_();
 
-///////////////////////////////////////////////////////////////////////////////
+  /// Enumerates all operations
+  void enumerate_();
 
-TEST(SM90_Device_Gemm_f32t_f32n_f32n_tensor_op_gmma_f32, 64x128x32_1x2x1) {
-  using LayoutA = cutlass::layout::RowMajor;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using LayoutC = cutlass::layout::ColumnMajor;
+  /// Profiles all operations
+  int profile_();
 
-  using CollectiveMainloop = typename cutlass::gemm::collective::CollectiveBuilder<
-      cutlass::arch::Sm90, cutlass::arch::OpClassTensorOp,
-      float, LayoutA, 4,
-      float, LayoutB, 4,
-      float,
-      Shape<_64,_128,_128>, Shape<_1,_2,_1>,
-      cutlass::gemm::collective::StageCountAuto,
-      cutlass::gemm::collective::KernelScheduleAuto
-    >::CollectiveOp;
+public:
 
-  using CollectiveEpilogue = cutlass::epilogue::collective::DefaultEpilogue<
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::gemm::TagToStrideC_t<LayoutC>,
-      cutlass::epilogue::thread::LinearCombination<float, 1, float, float>>;
+  CutlassProfiler(Options const &options);
+  ~CutlassProfiler();
 
-  using GemmKernel = cutlass::gemm::kernel::GemmUniversal<
-      Shape<int,int,int,int>,
-      CollectiveMainloop,
-      CollectiveEpilogue
-  >;
+  /// Invokes profiling operations
+  int operator()();
+};
 
-  using Gemm = cutlass::gemm::device::GemmUniversalAdapter<GemmKernel>;
-  EXPECT_TRUE(test::gemm::device::TestAll<Gemm>());
-}
+/////////////////////////////////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////
+} // namespace profiler
+} // namespace cutlass
 
-#endif // defined(CUTLASS_ARCH_MMA_SM90_SUPPORTED)
+/////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_alignx_tensor_op_s32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_s8_s8_s8_tensor_op_s32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_alignx_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf32n_cf32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_gaussian_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_ls_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_cf64n_cf64n_cf64n_tensor_op_rs_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64_f64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32n_f32n_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/symm_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_cf64t_cf64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32n_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f32t_f32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64_f64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_f64t_f64t_tensor_op_f64_grouped_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32n_f32n_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syr2k_tf32t_f32n_tensor_op_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32n_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_sm80.cu`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,71 +25,55 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
-    \brief Tests for device-wide SYRK interface
-  
+    \brief Unit tests for threadblock-level GEMM
 */
 
-#include <iostream>
+#include "cutlass/cutlass.h"
+#include "cutlass/gemm/threadblock/default_mma_planar_complex_multistage.h"
 
-#include "../../common/cutlass_unit_test.h"
-#include "cutlass/blas3.h"
-#include "cutlass/gemm/device/rank_k.h"
-#include "cutlass/util/host_tensor.h"
-#include "cutlass/util/reference/host/rank_k_complex.h"
-#include "cutlass/util/reference/host/tensor_compare.h"
-#include "cutlass/util/reference/host/tensor_copy.h"
-#include "cutlass/util/reference/host/tensor_fill.h"
-#include "cutlass/util/tensor_view_io.h"
-
-#include "testbed_rank_k_universal.h"
+#include "mma_planar_complex_testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+////////////////////////////////////////////////////////////////////////////////
 
-TEST(SM80_Device_Syrk_cf64n_cf64t_l_tensor_op_f64_gaussian, 32x32x16_16x16x16) {
+TEST(SM80_gemm_threadblock_planar_complex_congruous, tensor_op_64x64x32_64x64x32_16x8x16_3stage) {
 
-  using ElementA = cutlass::complex<double>;
+  using ElementA = cutlass::half_t;
   using LayoutA = cutlass::layout::ColumnMajor;
-
-  using ElementC = cutlass::complex<double>;
+  using ElementB = cutlass::half_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = float;
   using LayoutC = cutlass::layout::RowMajor;
-  using ElementAccumulator = cutlass::complex<double>;
 
-  using RankK = cutlass::gemm::device::RankK<
-    ElementA,
-    LayoutA,
-    ElementC,
-    LayoutC,
-    cutlass::FillMode::kLower,
-    ElementAccumulator,
-    cutlass::arch::OpClassTensorOp,
-    cutlass::arch::Sm80,
-    cutlass::gemm::GemmShape<32, 32, 16>,
-    cutlass::gemm::GemmShape<16, 16, 16>,
-    cutlass::gemm::GemmShape<8, 8, 4>,
-    cutlass::epilogue::thread::LinearCombination<
-      ElementC,
-      1,
-      ElementAccumulator,
-      ElementAccumulator
-    >,
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>,
-    4,     // kStages 
-    1,     // AlignmentA
-    false, // SplitKSerial
-    cutlass::arch::OpMultiplyAddGaussianComplex,
-    cutlass::ComplexTransform::kNone,
-    cutlass::BlasMode::kSymmetric
-  >;
+  cutlass::gemm::GemmCoord problem_size(64, 64, 8);
 
-  EXPECT_TRUE(test::gemm::device::TestAllRankKUniversal<RankK>());
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using WarpShape = cutlass::gemm::GemmShape<32, 32, 32>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
+  int const Stages = 3;
+
+  // Define the MmaCore components
+  using Mma = typename cutlass::gemm::threadblock::DefaultMmaPlanarComplexMultistage<
+      ElementA, LayoutA, 8,
+      ElementB, LayoutB, 8,
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassTensorOp,
+      cutlass::arch::Sm80,
+      ThreadblockShape, WarpShape, InstructionShape,
+      Stages>::ThreadblockMma;
+
+  dim3 grid(1, 1);
+  dim3 block(32, Mma::WarpCount::kCount, 1);
+
+  test::gemm::threadblock::TestbedPlanarComplex<Mma>(problem_size.m(), problem_size.n(),
+                                            problem_size.k())
+      .run(grid, block);
 }
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-#endif // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
+////////////////////////////////////////////////////////////////////////////////
+#endif  // #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f32t_f32t_tensor_op_fast_f32_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64_f64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64n_f64t_tensor_op_f64_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_f64t_f64n_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32n_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/syrk_tf32t_f32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -46,32 +46,27 @@
 #include "cutlass/util/reference/host/tensor_fill.h"
 #include "cutlass/util/reference/host/tensor_copy.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
 #include "cutlass/util/reference/host/tensor_norm.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 #include "testbed_utils.h"
-#include "testbed_universal.h"
 
 #include "cutlass/layout/matrix.h"
 #include "cutlass/matrix_coord.h"
-#include "cutlass/gemm/device/gemm_universal_adapter.h"
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, bool Relu = false>
 struct Testbed {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   typename Gemm::LayoutA::Stride stride_factor_A;
   typename Gemm::LayoutB::Stride stride_factor_B;
   typename Gemm::LayoutC::Stride stride_factor_C;
@@ -310,37 +305,28 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
 
   /// Executes one test
   bool run(
-    cutlass::gemm::GemmCoord problem_size,
+    cutlass::gemm::GemmCoord problem_size, 
     int split_k_slices = 1,
-    ElementCompute alpha = ElementCompute(1),
-    ElementCompute beta = ElementCompute(0))
-  {
-/*
-    std::cout << "\n-----------------------\n";
-    std::cout << "problem size: " << problem_size << "\n";
-    std::cout << "split_k_slices: " << split_k_slices << "\n";
-    std::cout << "alpha: " << alpha << "\n";
-    std::cout << "beta: " << beta << "\n";
-    std::cout << "-----------------------\n\n";
-*/
+    ElementCompute alpha = ElementCompute(1), 
+    ElementCompute beta = ElementCompute(0)) {
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -397,15 +383,15 @@
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, bool Relu=false>
-bool TestAllGemmBasic(
+bool TestAllGemm(
     const typename Gemm::LayoutA::Stride& stride_factor_A = typename Gemm::LayoutA::Stride(),
     const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
     const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride()) {
   bool passed = true;
 
   int const kMinimumOperandElementSize = 
     std::min(
@@ -488,60 +474,14 @@
     }
   }
 
   return passed;
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename Gemm, bool Relu=false>
-bool TestAllGemm(
-    const typename Gemm::LayoutA::Stride& stride_factor_A,
-    const typename Gemm::LayoutB::Stride& stride_factor_B = typename Gemm::LayoutB::Stride(),
-    const typename Gemm::LayoutC::Stride& stride_factor_C = typename Gemm::LayoutC::Stride())
-{
-  // Test basic GEMM with non-default stride factors
-  return TestAllGemmBasic<Gemm, Relu>(stride_factor_A, stride_factor_B, stride_factor_C);
-}
-
-template <typename Gemm, bool Relu=false>
-bool TestAllGemm()
-{
-#ifdef NDEBUG
-  // Non-debug builds also test basic GEMM with default stride factors
-  if (!TestAllGemmBasic<Gemm, Relu>()) {
-    return false;
-  }
-#endif // NDEBUG
-
-  // Test universal GEMM
-#if 0
-  // Define the universal kernel
-  using UniversalKernel = cutlass::gemm::kernel::GemmUniversal<
-    typename Gemm::GemmKernel::Mma,                                 // Mma
-    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
-    cutlass::gemm::threadblock::GemmIdentityThreadblockSwizzle<>    // ThreadblockSwizzle
-  >;
-#else
-  // Define the streamk universal kernel
-  using UniversalKernel = cutlass::gemm::kernel::GemmUniversalStreamk<
-    typename Gemm::GemmKernel::Mma,                                 // Mma
-    typename Gemm::GemmKernel::Epilogue,                            // Epilogue
-    cutlass::gemm::threadblock::ThreadblockSwizzleStreamK           // ThreadblockSwizzle
-  >;
-#endif
-
-  // Define the universal adaptor
-  using UniversalGemm = cutlass::gemm::device::GemmUniversalAdapter<UniversalKernel>;
-
-  // Test universal GEMM
-  return TestAllGemmUniversal<UniversalGemm, Relu>();
-}
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
 template <typename Gemm>
 bool TestGemmPerf(int iterations = 1) {
   bool passed = true;
 
   int problem_size_m[] = { 2048 };
 
   int problem_size_n[] = { 4352 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_complex.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -59,17 +59,14 @@
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct TestbedComplex : public Testbed<Gemm> {
 
   using Base = Testbed<Gemm>;
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
 
   //
   // Methods
   //
@@ -127,18 +124,18 @@
     
     result = cudaGetDeviceProperties(&properties, device_idx);
     
     if (result != cudaSuccess) {
     	throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
     
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
     	return false;
     }
-
+    
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size, 
     int split_k_slices = 1,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -96,34 +96,31 @@
 
 template <
   typename Gemm, 
   typename ReferenceOp = GemmWithBroadcastReferenceOp<Gemm>
 >
 struct TestbedGemmWithBroadcast {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
   using OutputOp = typename Gemm::GemmKernel::Epilogue::OutputOp;
   using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
-  using ElementCompute = typename OutputOp::ElementCompute;
-  using ElementVector = typename OutputOp::ElementVector;
+  using ElementCOmpute = typename OutputOp::ElementCompute;
   using ElementZ = typename OutputOp::ElementZ;
   using ElementT = typename OutputOp::ElementT;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
   cutlass::HostTensor<typename Gemm::ElementA, typename Gemm::LayoutA> tensor_A;          // Input A
   cutlass::HostTensor<typename Gemm::ElementB, typename Gemm::LayoutB> tensor_B;          // Input B
   cutlass::HostTensor<ElementC, typename Gemm::LayoutC> tensor_C;                         // Input C
-  cutlass::HostTensor<ElementVector, typename Gemm::LayoutC> tensor_Broadcast;            // Input Broadcast
+  cutlass::HostTensor<ElementC, typename Gemm::LayoutC> tensor_Broadcast;                 // Input Broadcast
 
   cutlass::HostTensor<ElementZ, typename Gemm::LayoutC> tensor_Z;
   cutlass::HostTensor<ElementT, typename Gemm::LayoutC> tensor_T;
 
   cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_C_ref;
   cutlass::HostTensor<ElementAccumulator, typename Gemm::LayoutC> tensor_Y_ref;
   cutlass::HostTensor<ElementZ, typename Gemm::LayoutC> tensor_Z_ref;
@@ -387,15 +384,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_gemm_with_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,15 +57,14 @@
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, typename BinaryOp>
 struct GemmWithReductionReference {
-
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::ElementCompute;
   using ElementC = typename Gemm::ElementC;
   using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
   //
   // Data members
   //
@@ -90,17 +89,14 @@
 
 template <
   typename Gemm,
   typename ReferenceOp
 >
 struct TestbedGemmWithReduction {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementT = typename Gemm::GemmKernel::Epilogue::ElementTensor;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -375,15 +371,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_rank_2k_scheduler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_grouped_scheduler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -308,16 +308,15 @@
           int PrefetchTileCount,
           int ThreadCount,
           bool Transpose,
           cutlass::gemm::kernel::GroupScheduleMode GroupScheduleMode0,
           cutlass::gemm::kernel::GroupScheduleMode... Args>
 struct TestbedGroupedGemmScheduler {
 
-  using PSHelper = cutlass::gemm::kernel::detail::GemmGroupedProblemSizeHelper<ThreadblockShape, Transpose>;
-  using BaselinePV = BaselineProblemVisitor<PSHelper,
+  using BaselinePV = BaselineProblemVisitor<cutlass::gemm::kernel::detail::GemmGroupedProblemSizeHelper<Transpose>,
                                             ThreadblockShape,
                                             PrefetchTileCount,
                                             ThreadCount>;
 
   //
   // Data members
   //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_interleaved.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/multistage_testbed_interleaved.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -28,14 +28,16 @@
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /*! \file
     \brief Tests for device-wide GEMM interface
 */
 
+#pragma once
+
 #include <iostream>
 #include <fstream>
 #include <sstream>
 
 #include "../../common/cutlass_unit_test.h"
 
 #include "cutlass/util/host_tensor.h"
@@ -51,33 +53,30 @@
 namespace test {
 namespace gemm {
 namespace device {
 
 ////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm, int InterleavedK>
-struct InterleavedTestbed {
+struct MultistageInterleavedTestbed {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
 
   //
   // Methods
   //
 
-  InterleavedTestbed(
+  MultistageInterleavedTestbed(
     cutlass::Distribution::Kind init_A_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_B_ = cutlass::Distribution::Uniform,
     cutlass::Distribution::Kind init_C_ = cutlass::Distribution::Uniform,
     uint64_t seed_ = 2080
   ):
     init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
@@ -107,56 +106,19 @@
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
   }
 
-	/// Waives test if CUDA device is insufficient
-  bool sufficient() const {
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    int smem_size = int(sizeof(typename Gemm::GemmKernel::SharedStorage));
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.sharedMemPerBlockOptin < smem_size) {
-      return false;
-    }
-
-    return true;
-  }
-
   /// Executes one test
   bool run(
     cutlass::gemm::GemmCoord problem_size, 
     ElementCompute alpha = ElementCompute(1), 
     ElementCompute beta = ElementCompute(0)) {
-
-    // Waive test if insufficient CUDA device
-    if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
-      }
-      return true;
-    }
     
     //
     // Allocate the GEMM workspace
     //
 
     cutlass::HostTensor<
       typename Gemm::ElementA, 
@@ -290,31 +252,31 @@
   }
 
   /// Runs a set of problem sizes
   bool run_all() {
     bool passed = true;
 
     int problem_size_m[] = {
-      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
+      InterleavedK, 512 + InterleavedK
     };
 
     int problem_size_n[] = {
-      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
+      InterleavedK, 512 + InterleavedK
     };
 
     int problem_size_k[] = {
-      InterleavedK, 256 + InterleavedK, 512 + InterleavedK
+      InterleavedK, Gemm::ThreadblockShape::kK * Gemm::kStages + InterleavedK
     };
 
     double problem_alpha[] = {
       1.0
     };
 
     double problem_beta[] = {
-      2.0
+      0.0
     };
 
     for (int m : problem_size_m) {
       for (int n : problem_size_n) {
         for (int k : problem_size_k) {
           for (double alpha : problem_alpha) {
             for (double beta : problem_beta) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_planar_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -136,15 +136,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
   
   bool run(
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank2k_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,17 +60,14 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Rank2K>
 struct TestbedRank2KUniversal {
 
-  using ElementA = typename Rank2K::ElementA;
-  using ElementB = typename Rank2K::ElementB;
-  using ElementC = typename Rank2K::ElementC;
   using ElementAccumulator = typename Rank2K::ElementAccumulator;
   using ElementCompute = typename Rank2K::Rank2Kkernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -297,17 +294,18 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
+
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmUniversalMode mode,
     cutlass::gemm::GemmCoord problem_size,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_rank_k_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -59,16 +59,14 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename RankK>
 struct TestbedRank2KUniversal {
 
-  using ElementA = typename RankK::ElementA;
-  using ElementC = typename RankK::ElementC;
   using ElementAccumulator = typename RankK::ElementAccumulator;
   using ElementCompute = typename RankK::RankKkernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_C;
   uint64_t seed;
@@ -284,15 +282,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sanity.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_sparse.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,17 +60,14 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Gemm>
 struct SparseTestbed {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   static int const kSparse = Gemm::GemmKernel::kSparse;
   static int const kMetaSizeInBits = Gemm::GemmKernel::kMetaSizeInBits;
   static int const kMaxID2 = Gemm::GemmKernel::kMaxID2;
   static int const kElementsPerElementE = Gemm::GemmKernel::kElementsPerElementE;
@@ -322,15 +319,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_splitk.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -84,15 +84,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
   
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_symm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -60,17 +60,14 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Symm>
 struct TestbedSymmUniversal {
 
-  using ElementA = typename Symm::ElementA;
-  using ElementB = typename Symm::ElementB;
-  using ElementC = typename Symm::ElementC;
   using ElementAccumulator = typename Symm::ElementAccumulator;
   using ElementCompute = typename Symm::SymmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -323,15 +320,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_trmm_universal.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -62,17 +62,14 @@
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Trmm>
 struct TestbedTrmmUniversal {
 
-  using ElementA = typename Trmm::ElementA;
-  using ElementB = typename Trmm::ElementB;
-  using ElementC = typename Trmm::ElementC;
   using ElementAccumulator = typename Trmm::ElementAccumulator;
   using ElementCompute = typename Trmm::TrmmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_D;
@@ -363,15 +360,15 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_universal.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -54,20 +54,17 @@
 
 namespace test {
 namespace gemm {
 namespace device {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-template <typename Gemm, bool Relu = false>
+template <typename Gemm>
 struct TestbedUniversal {
 
-  using ElementA = typename Gemm::ElementA;
-  using ElementB = typename Gemm::ElementB;
-  using ElementC = typename Gemm::ElementC;
   using ElementAccumulator = typename Gemm::ElementAccumulator;
   using ElementCompute = typename Gemm::GemmKernel::Epilogue::OutputOp::ElementCompute;
 
   /// Initialization
   cutlass::Distribution::Kind init_A;
   cutlass::Distribution::Kind init_B;
   cutlass::Distribution::Kind init_C;
@@ -90,15 +87,15 @@
     uint64_t seed_ = 2080
   ):
     init_A(init_A_), init_B(init_B_), init_C(init_C_), seed(seed_) { }
 
   /// Helper to initialize a tensor view
   template <typename Element, typename Layout>
   bool initialize_tensor(
-    cutlass::TensorView<Element, Layout> view,
+    cutlass::TensorView<Element, Layout> view, 
     cutlass::Distribution::Kind dist_kind,
     uint64_t seed) {
 
     if (dist_kind == cutlass::Distribution::Uniform) {
 
       double scope_max, scope_min;
       int bits_input = cutlass::sizeof_bits<Element>::value;
@@ -116,28 +113,28 @@
       } else {
         scope_max = 8;
         scope_min = -8;
       }
 
       cutlass::reference::host::TensorFillRandomUniform(
         view, seed, scope_max, scope_min, 0);
-    }
+    } 
     else if (dist_kind == cutlass::Distribution::Identity) {
 
       cutlass::reference::host::TensorFillIdentity(view);
-    }
+    } 
     else if (dist_kind == cutlass::Distribution::Gaussian) {
 
       cutlass::reference::host::TensorFillRandomGaussian(view, seed, 0, 0.5);
     }
     else if (dist_kind == cutlass::Distribution::Sequential) {
 
       cutlass::reference::host::BlockFillSequential(
         view.data(), view.capacity());
-    }
+    } 
     else {
       // TODO: Implement the rest
       EXPECT_TRUE(false) << "Not implemented";
       return false;
     }
 
     return true;
@@ -157,39 +154,38 @@
 
     EXPECT_TRUE(initialize_tensor(tensor_A.host_view(), init_A, seed + 2019));
     EXPECT_TRUE(initialize_tensor(tensor_B.host_view(), init_B, seed + 2018));
     EXPECT_TRUE(initialize_tensor(tensor_C.host_view(), init_C, seed + 2017));
 
     // It is possible to randomly initialize to all zeros, so override this with non-zeros
     // in the upper left corner of each operand.
-    cutlass::Coord<2> origin(0);
-    tensor_A.host_view().at(origin) = typename Gemm::ElementA(1);
-    tensor_B.host_view().at(origin) = typename Gemm::ElementB(1);
-    tensor_C.host_view().at(origin) = typename Gemm::ElementC(1);
+    tensor_A.host_view().at({0, 0}) = typename Gemm::ElementA(1);
+    tensor_B.host_view().at({0, 0}) = typename Gemm::ElementB(1);
+    tensor_C.host_view().at({0, 0}) = typename Gemm::ElementC(1);
 
     cutlass::reference::host::TensorCopy(reference_D.host_view(), tensor_C.host_view());
 
     tensor_A.sync_device();
     tensor_B.sync_device();
     tensor_C.sync_device();
     tensor_D.sync_device();
   }
 
   /// Compares computed reference with device reference and outputs to a file if incorrect
   bool compare_reference(
-    cutlass::gemm::GemmCoord problem_size,
-    ElementCompute alpha,
+    cutlass::gemm::GemmCoord problem_size, 
+    ElementCompute alpha, 
     ElementCompute beta) {
 
     tensor_D.sync_host();
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_A.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_B.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_C.host_view()), 0);
-
+    
     EXPECT_GT(cutlass::reference::host::TensorNorm(tensor_D.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(reference_D.host_view()), 0);
 
     bool passed = cutlass::reference::host::TensorEquals(reference_D.host_view(), tensor_D.host_view());
 
     EXPECT_TRUE(passed) << " mismatched reference";
 
@@ -198,80 +194,69 @@
       /*
       std::stringstream fname;
 
       fname << "error_Gemm_device_"
         << problem_size.m() << "x"
         << problem_size.n() << "x"
         << problem_size.k() << "_"
-        << Gemm::ThreadblockShape::kM << "x"
-        << Gemm::ThreadblockShape::kN << "x"
+        << Gemm::ThreadblockShape::kM << "x"  
+        << Gemm::ThreadblockShape::kN << "x"  
         << Gemm::ThreadblockShape::kK << "_"
-        << Gemm::WarpShape::kM << "x"
-        << Gemm::WarpShape::kN << "x"
+        << Gemm::WarpShape::kM << "x"  
+        << Gemm::WarpShape::kN << "x"  
         << Gemm::WarpShape::kK << ".txt";
 
       std::ofstream file(fname.str());
       */
 
       std::ofstream file("testbed_universal_errors.txt");
 
       file
-        << "problem: " << problem_size
+        << "problem: " << problem_size 
         << ", alpha: " << alpha << ", beta: " << beta << "\n\n";
 
-      file
+      file 
         << "A =\n" << tensor_A.host_view()
         << "\nB =\n" << tensor_B.host_view()
         << "\nC =\n" << tensor_C.host_view()
         << "\n\nReference =\n" << reference_D.host_view()
         << "\nComputed =\n" << tensor_D.host_view();
     }
 
     return passed;
   }
 
   /// Verifies the result is a GEMM
   bool verify(
-    cutlass::gemm::GemmCoord problem_size,
-    ElementCompute alpha,
+    cutlass::gemm::GemmCoord problem_size, 
+    ElementCompute alpha, 
     ElementCompute beta) {
 
     //
     // Verify
     //
 
     cutlass::reference::host::GemmComplex<
         typename Gemm::ElementA, typename Gemm::LayoutA,
         typename Gemm::ElementB, typename Gemm::LayoutB,
-        typename Gemm::ElementC, typename Gemm::LayoutC,
+        typename Gemm::ElementC, typename Gemm::LayoutC, 
         ElementCompute, ElementAccumulator
     >(
       problem_size,
-      alpha,
+      alpha, 
       tensor_A.host_ref(),
       Gemm::kTransformA,
       tensor_B.host_ref(),
       Gemm::kTransformB,
-      beta,
-      tensor_C.host_ref(),
-      reference_D.host_ref(),
+      beta, 
+      tensor_C.host_ref(), 
+      reference_D.host_ref(), 
       ElementAccumulator(0)
     );
 
-    if (Relu) {
-      for (int i = 0; i < problem_size.m(); ++i) {
-        for (int j = 0; j < problem_size.n(); ++j) {
-           reference_D.at(cutlass::MatrixCoord(i, j)) =
-                  ((ElementCompute)reference_D.at(cutlass::MatrixCoord(i, j)) < (ElementCompute)0)
-                  ? (typename Gemm::ElementC)0
-                  : reference_D.at(cutlass::MatrixCoord(i, j));
-        }
-      }
-    }
-
     return compare_reference(problem_size, alpha, beta);
   }
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
@@ -289,38 +274,28 @@
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
-    if (properties.sharedMemPerBlockOptin < smem_size) {
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
       return false;
     }
 
     return true;
   }
 
   /// Executes one test
   bool run(
     cutlass::gemm::GemmUniversalMode mode,
-    cutlass::gemm::GemmCoord problem_size,
+    cutlass::gemm::GemmCoord problem_size, 
     int batch_count = 1,
-    ElementCompute alpha = ElementCompute(1),
-    ElementCompute beta = ElementCompute(0))
-  {
-/*
-    std::cout << "\n-----------------------\n";
-    std::cout << "mode: " << (int) mode << "\n";
-    std::cout << "problem size: " << problem_size << "\n";
-    std::cout << "batch_count: " << batch_count << "\n";
-    std::cout << "alpha: " << alpha << "\n";
-    std::cout << "beta: " << beta << "\n";
-    std::cout << "-----------------------\n\n";
-*/
+    ElementCompute alpha = ElementCompute(1), 
+    ElementCompute beta = ElementCompute(0)) {
 
     // Waive test if insufficient CUDA device
     if (!sufficient()) {
       if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
         std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
       }
       return true;
@@ -380,40 +355,40 @@
     }
 
     return passed;
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
-template <typename Gemm, bool Relu = false>
+template <typename Gemm>
 bool TestGemmUniversal(
   cutlass::gemm::GemmCoord const & problem_size,
   cutlass::gemm::GemmUniversalMode mode,
   int batch_count,
-  double alpha = 1.0,
+  double alpha = 1.0, 
   double beta = 2.0) {
 
   bool passed = true;
 
-  TestbedUniversal<Gemm, Relu> testbed;
-
+  TestbedUniversal<Gemm> testbed;
+  
   using ElementCompute = typename Gemm::EpilogueOutputOp::ElementCompute;
 
   passed = testbed.run(
     mode,
-    problem_size,
+    problem_size, 
     batch_count,
-    cutlass::from_real<ElementCompute>(alpha),
+    cutlass::from_real<ElementCompute>(alpha), 
     cutlass::from_real<ElementCompute>(beta)
   );
 
   return passed;
 }
 
-template <typename Gemm, bool Relu = false>
+template <typename Gemm>
 bool TestAllGemmUniversal() {
   bool passed = true;
 
 
   int const kMinimumOperandElementSize = 
     std::min(
       int(cutlass::sizeof_bits<typename Gemm::ElementA>::value), 
@@ -433,32 +408,32 @@
                           cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::RowMajor>::value ? 4 : kAlignment;
 
   int const kAlignmentK = cutlass::platform::is_same<typename Gemm::OperatorClass, cutlass::arch::OpClassSimt>::value &&
                           cutlass::platform::is_same<typename Gemm::ElementA, int8_t>::value &&
                           cutlass::platform::is_same<typename Gemm::ElementB, int8_t>::value &&
                           (cutlass::platform::is_same<typename Gemm::LayoutA, cutlass::layout::RowMajor>::value ||
                           cutlass::platform::is_same<typename Gemm::LayoutB, cutlass::layout::ColumnMajor>::value) ? 4 : kAlignment;
-
-
-
+  
+  
+  
   cutlass::gemm::GemmUniversalMode modes[] = {
     cutlass::gemm::GemmUniversalMode::kGemm,
   };
 
   int problem_size_m[] = {
     kAlignmentM, 512 - 3*kAlignmentM
   };
 
   int problem_size_n[] = {
     kAlignmentN, 512 - 2*kAlignmentN
   };
 
   int problem_size_k[] = {
-    kAlignmentK,
-    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK,
+    kAlignmentK, 
+    Gemm::ThreadblockShape::kK * Gemm::kStages - kAlignmentK, 
     Gemm::ThreadblockShape::kK * Gemm::kStages * 3 - kAlignmentK
   };
 
   int batch_counts[] = {      // may be interpretted as batch count or split-K slices
     1, 2, 3, 5, 7
   };
 
@@ -489,21 +464,21 @@
                   if (k / batch_count < 2 * Gemm::ThreadblockShape::kK) {
                     continue;
                   }
                 }
 
                 cutlass::gemm::GemmCoord problem_size(m, n, k);
 
-                TestbedUniversal<Gemm, Relu> testbed;
+                TestbedUniversal<Gemm> testbed;
 
                 passed = testbed.run(
                   mode,
-                  problem_size,
+                  problem_size, 
                   batch_count,
-                  cutlass::from_real<ElementCompute>(alpha),
+                  cutlass::from_real<ElementCompute>(alpha), 
                   cutlass::from_real<ElementCompute>(beta)
                 );
 
                 if (!passed) {
                   return false;
                 }
               }
@@ -519,17 +494,17 @@
   for (int split_k_slices = 1; split_k_slices <= 3; ++split_k_slices) {
     TestbedUniversal<Gemm> testbed;
 
     cutlass::gemm::GemmCoord problem_size(72, 56, 8192);
 
     passed = testbed.run(
       cutlass::gemm::GemmUniversalMode::kGemm,
-      problem_size,
+      problem_size, 
       split_k_slices,
-      cutlass::from_real<ElementCompute>(1.0),
+      cutlass::from_real<ElementCompute>(1.0), 
       cutlass::from_real<ElementCompute>(2.0)
     );
 
     if (!passed) {
       break;
     }
   }
@@ -541,7 +516,8 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace gemm
 } // namespace test
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/testbed_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf32n_cf32n_cf32t_tensor_op_fast_f32_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64_cf64_cf64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_gaussian_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_cf64n_cf64n_cf64t_tensor_op_f64_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f32t_f32n_f32t_tensor_op_fast_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64_f64_f64_tensor_op_f64_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64n_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64n_f64t_f64t_tensor_op_f64_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_f64t_f64t_f64n_tensor_op_f64_rs_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32n_tf32t_f32t_tensor_op_f32_rs_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32n_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/device/trmm_tf32t_tf32n_f32t_tensor_op_f32_ls_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/batched_gemv.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/kernel/testbed_gemv.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -57,15 +57,15 @@
         typename ThreadShape_,
         typename ElementAB_,
         typename ElementAccumulator_,
         typename ElementCD_,
         typename LayoutA_,
         typename LayoutB_,
         typename LayoutCD_,
-        int THREAD_B = 1, // batch tile size
+        int LDG_B = 1, // batch tile size
         bool DEBUG=false>
 void batched_gemv_kernel_test(cutlass::gemm::BatchedGemmCoord problem_size,
                               ElementCD_ alpha = ElementCD_(1),
                               ElementCD_ beta = ElementCD_(0),
                               bool perf_test = false,
                               int perf_test_iter = 1)
 {
@@ -150,15 +150,15 @@
     matrix_C_computed.sync_device();
 
     ThreadBlockSwizzle swizzle;
 
     cutlass::gemm::BatchedGemmCoord tiled_size{ThreadBlockShape::kM,
                                                 ThreadBlockShape::kN,
                                                 problem_size.k(), // no split-k
-                                                DEBUG ? 1 : THREAD_B };
+                                                DEBUG ? 1 : LDG_B };
 
     cutlass::gemm::BatchedGemmCoord tiled_shape = swizzle.get_tiled_shape(problem_size, tiled_size);
 
     #if 0 
     printf("tiled_size = %d %d %d %d\n", tiled_size.m(), tiled_size.n(), tiled_size.k(), tiled_size.batch());
     printf("tiled_shape = %d %d %d %d\n", tiled_shape.m(), tiled_shape.n(), tiled_shape.k(), tiled_shape.batch());
     #endif
@@ -330,29 +330,29 @@
         typename ThreadShape_,
         typename ElementAB_,
         typename ElementAccumulator_,
         typename ElementCD_,
         typename LayoutA_,
         typename LayoutB_,
         typename LayoutCD_,
-        int THREAD_B = 1, // batch tile size
+        int LDG_B = 1, // batch tile size
         bool DEBUG=false>
 void batched_gemv_kernel_perf_test(cutlass::gemm::BatchedGemmCoord problem_size,
                                    ElementCD_ alpha = ElementCD_(1),
                                    ElementCD_ beta = ElementCD_(0),
                                    int iter = 50)
 {
     batched_gemv_kernel_test<ThreadBlockShape_,
                              ThreadShape_,
                              ElementAB_,
                              ElementAccumulator_,
                              ElementCD_,
                              LayoutA_,
                              LayoutB_,
                              LayoutCD_,
-                             THREAD_B,
+                             LDG_B,
                              DEBUG>(problem_size, alpha, beta, true, iter);
 }
     
 } // namespace threadblock
 } // namespace kernel
 } // namespace test
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/gemm_sm61.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/gemm_sm60_host.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/host/testbed_host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/thread/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/batched_gemv.cu`

 * *Files 16% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -103,28 +103,28 @@
 
 template<typename Shape_,
          typename ElementAB_,
          typename ElementC_,
          typename LayoutA_,
          typename LayoutB_,
          typename LayoutC_,
-         int THREAD_N,
-         int THREAD_K,
+         int LDG_N,
+         int LDG_K,
          int MAX_THREADS_PER_BLOCK=512,
          bool DEBUG=false>
 void batched_gemv_threadblock_test(cutlass::gemm::GemmCoord problem_size, int num_batch)
 {
   using Shape = Shape_;
   using ElementA = ElementAB_;
   using LayoutA = LayoutA_;
   using ElementB = ElementAB_;
   using LayoutB = LayoutB_;
   using ElementC = ElementC_;
   using LayoutC = LayoutC_;
-  using ThreadShape = cutlass::gemm::GemmShape<1, THREAD_N, THREAD_K>;
+  using ThreadShape = cutlass::gemm::GemmShape<1, LDG_N, LDG_K>;
 
   using Core = typename cutlass::gemm::threadblock::DefaultGemvCore<
     Shape,
     ThreadShape,
     ElementA,
     LayoutA,
     ElementB,
@@ -188,22 +188,22 @@
   }
 
   matrix_A.sync_device();
   matrix_B.sync_device();
   matrix_C_computed.sync_device();
 
   dim3 grid(1, 1);      // only 1 CTA is used
-  dim3 block(Shape::kN / THREAD_N, num_batch, 1);
+  dim3 block(Shape::kN / LDG_N, num_batch, 1);
 
   #if 0
   printf("block dim = %d x %d\n", block.x, block.y);
   #endif
 
   // Some sanity checks
-  EXPECT_TRUE( problem_size.n() % THREAD_N == 0 );
+  EXPECT_TRUE( problem_size.n() % LDG_N == 0 );
   EXPECT_TRUE( block.x*block.y <= MAX_THREADS_PER_BLOCK );
 
   test::gemm::threadblock::batched_gemv_threadblock_test_kernel<Mma><<< grid, block >>>(
     problem_size,
     matrix_A.capacity(),
     matrix_B.capacity(),
     matrix_C_computed.capacity(),
@@ -257,390 +257,390 @@
 // C: ColumnMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_crc_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int THREAD_N = 4;
-  const int THREAD_K = 4;
+  const int LDG_N = 4;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 8;
+  const int LDG_N = 2;
+  const int LDG_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_crc_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 4;
+  const int LDG_N = 2;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_crc_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 // A: RowMajor
 // B: ColumnMajor
 // C: RowMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_rcr_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int THREAD_N = 4;
-  const int THREAD_K = 4;
+  const int LDG_N = 4;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 8;
+  const int LDG_N = 2;
+  const int LDG_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcr_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 4;
+  const int LDG_N = 2;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcr_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::RowMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 // A: RowMajor
 // B: ColumnMajor
 // C: ColumnMajor
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp32_fp32_2N_2K) {
   
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 5x1x128x128_rcc_fp32_fp32_4N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 128, 128);
   const int num_batch = 5;
-  const int THREAD_N = 4;
-  const int THREAD_K = 4;
+  const int LDG_N = 4;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape, float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_fp32_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 float, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp16_fp32_2N_2K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 2;
+  const int LDG_N = 2;
+  const int LDG_K = 2;
   
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_fp16_fp32_2N_8K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 8;
+  const int LDG_N = 2;
+  const int LDG_K = 8;
  
-  using Shape = cutlass::gemm::GemmShape<1, 64, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 64, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_fp16_fp32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 cutlass::half_t, float, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 4x1x64x64_rcc_i8_i32_2N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 64, 64);
   const int num_batch = 4;
-  const int THREAD_N = 2;
-  const int THREAD_K = 4;
+  const int LDG_N = 2;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 128, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 128, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
 
 TEST(SM50_batched_gemv_threadblock, 16x1x17x64_rcc_i8_i32_1N_4K) {
   using namespace test::gemm::threadblock;
   cutlass::gemm::GemmCoord problem_size(1, 17, 64);
   const int num_batch = 16;
-  const int THREAD_N = 1;
-  const int THREAD_K = 4;
+  const int LDG_N = 1;
+  const int LDG_K = 4;
 
-  using Shape = cutlass::gemm::GemmShape<1, 32, THREAD_K>;
+  using Shape = cutlass::gemm::GemmShape<1, 32, LDG_K>;
   batched_gemv_threadblock_test<Shape,
                                 int8_t, int32_t, 
                                 cutlass::layout::RowMajor,
                                 cutlass::layout::ColumnMajor,
                                 cutlass::layout::ColumnMajor,
-                                THREAD_N, THREAD_K>(problem_size, num_batch);
+                                LDG_N, LDG_K>(problem_size, num_batch);
 }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/epilogue_workspace.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_slicedk.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_sparse_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -237,28 +237,34 @@
 
   /// Returns true if the CUDA device is sufficient to execute the kernel.
   bool sufficient() const {
     //
     // Determine SMEM requirements and waive if not satisfied
     //
 
+    int smem_size = int(sizeof(typename Mma::SharedStorage));
+
     cudaDeviceProp properties;
     int device_idx;
     cudaError_t result = cudaGetDevice(&device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDevice() API call failed.");
     }
 
     result = cudaGetDeviceProperties(&properties, device_idx);
 
     if (result != cudaSuccess) {
       throw std::runtime_error("cudaGetDeviceProperties() failed");
     }
 
+    if (properties.sharedMemPerMultiprocessor < smem_size) {
+      return false;
+    }
+
     return true;
   }
 
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
@@ -405,28 +411,22 @@
     reference_gemm(problem_size, ElementC(alpha),
                    matrix_A_uncompressed.host_view(), matrix_B.host_view(),
                    ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
-    EXPECT_TRUE(passed);
-
-    if (!passed && CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-
-      std::cout
-        << __FILE__ << ":" << __LINE__ << "  "
+    EXPECT_TRUE(passed)
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "E:\n" << matrix_E.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
         << matrix_C_computed.host_view() << "\n";
-    }
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
 
     return passed;
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -189,48 +189,19 @@
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
     matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    //
-    // Determine SMEM requirements and waive if not satisfied
-    //
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    return true;
-  }
-
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -343,26 +314,21 @@
     reference_gemm(
         problem_size, ElementC(alpha), matrix_A.host_view(),
         matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
-    EXPECT_TRUE(passed);
-
-    if (!passed && CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-      std::cout
-        << __FILE__ << ":" << __LINE__ << "  "
+    EXPECT_TRUE(passed) 
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
         << "Reference:\n"
         << matrix_C_reference.host_view() << "\n"
         << "Computed:\n"
         << matrix_C_computed.host_view() << "\n";
-    }
 
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_reference.host_view()), 0);
     EXPECT_GT(cutlass::reference::host::TensorNorm(matrix_C_computed.host_view()), 0);
 
     return passed;
   }
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_multistage_testbed_slicedk.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_simt.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_slicedk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -213,33 +213,19 @@
       : problem_size(m, n, k), alpha(alpha_), beta(beta_) {
     matrix_A.reset(cutlass::make_Coord(m, k));
     matrix_B.reset(cutlass::make_Coord(k, n));
     matrix_C_computed.reset(cutlass::make_Coord(m, n));
     matrix_C_reference.reset(cutlass::make_Coord(m, n), false);
   }
 
-  bool sufficient() {
-    return true;
-  }
-
   /// Runs the test
   bool run(
       dim3 grid, dim3 block,
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    // Waive test if insufficient CUDA device
-    if (!sufficient()) {
-      if (CUTLASS_TEST_UNIT_ENABLE_WARNINGS) {
-        std::cerr << "Test waived due to insufficient CUDA device." << std::endl;
-      }
-      return true;
-    }
-
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
 
       int scope_max = 8;
@@ -310,15 +296,15 @@
 
     //
     // Check error code
     //
 
     cudaError_t result = cudaDeviceSynchronize();
     EXPECT_EQ(result, cudaSuccess)
-        << " kernel error: " << cudaGetErrorString(result) << " on device " << GetCudaDevice();
+        << " kernel error: " << cudaGetErrorString(result);
 
     matrix_C_computed.sync_host();
 
     cutlass::reference::host::Gemm<ElementA, LayoutA, ElementB, LayoutB,
                                    ElementC, LayoutC, ElementC, ElementC,
                                    typename MmaCore::Operator>
         reference_gemm;
@@ -326,15 +312,15 @@
     reference_gemm(
         problem_size, ElementC(alpha), matrix_A.host_view(),
         matrix_B.host_view(), ElementC(beta), matrix_C_reference.host_view());
 
     bool passed = cutlass::reference::host::TensorEquals(
         matrix_C_computed.host_view(), matrix_C_reference.host_view());
 
-    EXPECT_TRUE(passed) << "Failed on device " << GetCudaDevice();
+    EXPECT_TRUE(passed);
 
     if (!passed) {
       std::ofstream output("mma_pipelined_testbed_errors.txt");
 
       output
         << "A:\n" << matrix_A.host_view() << "\n"
         << "B:\n" << matrix_B.host_view() << "\n"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_testbed_slicedk.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_pipelined_wmma_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_planar_complex_testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/threadblock/mma_singlestage_wmma_sm75.cu`

 * *Files 22% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -29,36 +29,37 @@
  *
  **************************************************************************************************/
 /*! \file
     \brief Unit tests for thread-level GEMM
 */
 #include "cutlass/arch/wmma.h"
 
-#ifdef CUTLASS_ARCH_WMMA_SM70_ENABLED
+#ifdef CUTLASS_ARCH_WMMA_SM75_ENABLED
 #include "mma_pipelined_testbed.h"
 #include "cutlass/gemm/threadblock/default_mma_core_wmma.h"
 
 /// All tests use single staged (kStages=1) mma pipeline for the gemm mainloop
-/// Test name format: SM[arch]_gemm_threadblock_singlestage_wmma_[alayout]_[blayout]_[clayout]_[dtype].[threadblock_shape]_[warp_shape]
+/// Test name format: SM[arch]_gemm_threadblock_singlestage_wmma_tensor_op_[alayout]_[blayout]_[clayout]_[atype].[threadblock_shape]_[warp_shape]_[instruction_shape]
 
-///////////////////////////////////////////////////////////////////////////////////////////////////////
-///       WMMA Floating point (f16 accumulation) - Single stage - Threadblock level tests          ////
-///////////////////////////////////////////////////////////////////////////////////////////////////////
-
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_16x16x16) {
-
-  using ElementA = cutlass::half_t;
+/////////////////////////////////////////////////////////////////////////
+///       Integer (s8 and u8) WMMA threadblock level tests          ////
+/////////////////////////////////////////////////////////////////////////
+
+#if defined(CUTLASS_ARCH_INTEGER_MATRIX_MULTIPLY_ENABLED)
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s8, 64x64x32_64x64x32_16x16x16) {
+ 
+  using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = int8_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::half_t;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
   static const int kStages = 1; 
 
-  cutlass::gemm::GemmCoord problem_size(64, 64, 32);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
   float alpha = 1.f;
   float beta = 0.0f;
@@ -75,104 +76,65 @@
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 128x128x32_64x64x32_16x16x16) {
-
-  using ElementA = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s8, 64x64x64_64x64x64_16x16x16) {
+ 
+  using ElementA = int8_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = int8_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::half_t;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
   static const int kStages = 1; 
 
-  cutlass::gemm::GemmCoord problem_size(128, 128, 64);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
   float alpha = 1.f;
   float beta = 0.0f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, 
       ElementA, LayoutA,
       ElementB, LayoutB, 
       ElementC, LayoutC,
       cutlass::arch::OpClassWmmaTensorOp, kStages>;
 
   dim3 grid(1, 1);
-  dim3 block(32, 4, 1);
-
-  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
-                                            problem_size.k(), alpha, beta)
-      .run(grid, block);
-}
-
-
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, multicta_256x256x96_128x128x32_64x64x32_16x16x16) {
-
-  using ElementA = cutlass::half_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::half_t;
-  using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 1; 
-
-  cutlass::gemm::GemmCoord problem_size(256, 256, 96);
-
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
-
-  float alpha = 1.f;
-  float beta = 0.0f;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, 
-      ElementA, LayoutA,
-      ElementB, LayoutB, 
-      ElementC, LayoutC,
-      cutlass::arch::OpClassWmmaTensorOp, kStages>;
-
-  dim3 grid(2, 2);
-  dim3 block(32, 4, 1);
+  dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-///////////////////////////////////////////////////////////////////////////////
-/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
-/// wmma.mma.sync.aligned.row.col.m32n8k16.f16.f16 (wmma native size 32x8x16)
-///////////////////////////////////////////////////////////////////////////////
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_32x8x16) {
 
-  using ElementA = cutlass::half_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_col_row_row_s8, 64x64x32_64x64x32_16x16x16) {
+ 
+  using ElementA = int8_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = int8_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
   static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
   using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
   using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<32, 8, 16>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
   float alpha = 1.f;
   float beta = 0.0f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, 
@@ -185,33 +147,29 @@
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-//////////////////////////////////////////////////////////////////////////////
-/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
-/// wmma.mma.sync.aligned.row.col.m8n32k16.f16.f16  (wmma native size 8x32x16)
-//////////////////////////////////////////////////////////////////////////////
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f16, 64x64x32_64x64x32_8x32x16) {
-
-  using ElementA = cutlass::half_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_col_row_row_s8, 64x64x64_64x64x64_16x16x16) {
+ 
+  using ElementA = int8_t;
+  using LayoutA = cutlass::layout::ColumnMajor;
+  using ElementB = int8_t;
+  using LayoutB = cutlass::layout::RowMajor;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 1;   
+  static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 32, 16>;
+  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
 
   float alpha = 1.f;
   float beta = 0.0f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
       ThreadblockShape, WarpShape, InstructionShape, 
@@ -223,195 +181,157 @@
   dim3 grid(1, 1);
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
+#endif //CUTLASS_ARCH_INTEGER_MATRIX_MULTIPLY_ENABLED
+
 
+////////////////////////////////////////////////////////////////////////
+///      SUBBYTE (s4 and b1) WMMA threadblock level tests          ////
+///////////////////////////////////////////////////////////////////////
 
-///////////////////////////////////////////////////////////////////////////////////////////////////////
-///       WMMA Floating point (f32 accumulation) - Single stage - Threadblock level tests          ////
-///////////////////////////////////////////////////////////////////////////////////////////////////////
-
-//////////////////////////////////////////////////////////////////////////////////
-/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
-/// wmma.mma.sync.aligned.row.col.m16n16k16.f32.f32   (wmma native size 16x16x16)
-//////////////////////////////////////////////////////////////////////////////////
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_16x16x16) {
+#if defined(CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED)
 
-  using ElementA = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_s4, 64x64x128_64x64x128_8x8x32) {
+  using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = cutlass::int4b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
   static const int kStages = 1; 
 
   cutlass::gemm::GemmCoord problem_size(64, 64, 128);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 128>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
 
   float alpha = 1.f;
-  float beta = 0.0f;
+  float beta = 0.f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, 
+      ThreadBlockShape, WarpShape, InstructionShape, 
       ElementA, LayoutA,
       ElementB, LayoutB, 
-      ElementC, LayoutC,
+      ElementC, LayoutC, 
       cutlass::arch::OpClassWmmaTensorOp, kStages>;
 
   dim3 grid(1, 1);
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 128x128x32_64x64x32_16x16x16) {
 
-  using ElementA = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_col_s4, 64x64x64_64x64x64_8x8x32) {
+  using ElementA = cutlass::int4b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = cutlass::int4b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 1;   
+  using ElementC = int32_t;
+  using LayoutC = cutlass::layout::ColumnMajor;
+  static const int kStages = 1; 
 
-  cutlass::gemm::GemmCoord problem_size(128, 128, 128);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 64);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 32>;
 
   float alpha = 1.f;
-  float beta = 0.0f;
+  float beta = 0.f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, 
+      ThreadBlockShape, WarpShape, InstructionShape, 
       ElementA, LayoutA,
       ElementB, LayoutB, 
-      ElementC, LayoutC,
+      ElementC, LayoutC, 
       cutlass::arch::OpClassWmmaTensorOp, kStages>;
 
   dim3 grid(1, 1);
-  dim3 block(32, 4, 1);
-
-  test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
-                                            problem_size.k(), alpha, beta)
-      .run(grid, block);
-}
-
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, multicta_256x256x96_128x128x32_64x64x32_16x16x16) {
-
-  using ElementA = cutlass::half_t;
-  using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
-  using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 1;   
-
-  cutlass::gemm::GemmCoord problem_size(256, 256, 96);
-
-  using ThreadblockShape = cutlass::gemm::GemmShape<128, 128, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<16, 16, 16>;
-
-  float alpha = 1.f;
-  float beta = 0.0f;
-
-  // Define the MmaCore components
-  using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementC, LayoutC,
-      cutlass::arch::OpClassWmmaTensorOp, kStages>;
-
-  dim3 grid(2, 2);
-  dim3 block(32, 4, 1);
+  dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-///////////////////////////////////////////////////////////
-/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
-/// wmma.mma.sync.aligned.row.col.m32n8k16.f32.f32   (wmma native size 32x8x16)
-////////////////////////////////////////////////////////////
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_32x8x16) {
-
-  using ElementA = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_b1, 64x64x512_64x64x512_8x8x128) {
+  using ElementA = cutlass::uint1b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = cutlass::uint1b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
+  using ElementC = int32_t;
   using LayoutC = cutlass::layout::RowMajor;
-  static const int kStages = 1;
+  static const int kStages = 1; 
 
-  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<32, 8, 16>;
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
 
   float alpha = 1.f;
-  float beta = 0.0f;
+  float beta = 0.f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementC, LayoutC,
-      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages,
+      cutlass::arch::OpXorPopc>;
 
   dim3 grid(1, 1);
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
 
-/////////////////////////////////////////////////////////////////////////////////
-/// wmma.mma.sync.aligned.alayout.blayout.shape.dtype.ctype
-/// wmma.mma.sync.aligned.row.col.m8n32k16.f32.f32   (wmma native size 8x32x16)
-/////////////////////////////////////////////////////////////////////////////////
-TEST(SM70_gemm_threadblock_singlestage_wmma_tensor_op_row_col_row_f32, 64x64x32_64x64x32_8x32x16) {
-
-  using ElementA = cutlass::half_t;
+TEST(SM75_gemm_threadblock_singlestage_wmma_tensor_op_row_col_col_b1, 64x64x512_64x64x512_8x8x128) {
+  using ElementA = cutlass::uint1b_t;
   using LayoutA = cutlass::layout::RowMajor;
-  using ElementB = cutlass::half_t;
+  using ElementB = cutlass::uint1b_t;
   using LayoutB = cutlass::layout::ColumnMajor;
-  using ElementC = float;
-  using LayoutC = cutlass::layout::RowMajor;
+  using ElementC = int32_t;
+  using LayoutC = cutlass::layout::ColumnMajor;
   static const int kStages = 1; 
 
-  cutlass::gemm::GemmCoord problem_size(64, 64, 128);
+  cutlass::gemm::GemmCoord problem_size(64, 64, 2048);
 
-  using ThreadblockShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
-  using InstructionShape = cutlass::gemm::GemmShape<8, 32, 16>;
+  using ThreadBlockShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
+  using InstructionShape = cutlass::gemm::GemmShape<8, 8, 128>;
 
   float alpha = 1.f;
-  float beta = 0.0f;
+  float beta = 0.f;
 
   // Define the MmaCore components
   using MmaCore = typename cutlass::gemm::threadblock::DefaultMmaCore<
-      ThreadblockShape, WarpShape, InstructionShape, ElementA, LayoutA,
-      ElementB, LayoutB, ElementC, LayoutC,
-      cutlass::arch::OpClassWmmaTensorOp, kStages>;
+      ThreadBlockShape, WarpShape, InstructionShape, 
+      ElementA, LayoutA,
+      ElementB, LayoutB, 
+      ElementC, LayoutC, 
+      cutlass::arch::OpClassWmmaTensorOp, kStages,
+      cutlass::arch::OpXorPopc>;
 
   dim3 grid(1, 1);
   dim3 block(32, 1, 1);
 
   test::gemm::threadblock::Testbed<MmaCore, kStages>(problem_size.m(), problem_size.n(),
                                             problem_size.k(), alpha, beta)
       .run(grid, block);
 }
+#endif //CUTLASS_SUBBYTE_INTEGER_MATRIX_MULTIPLY_ENABLED
 
-#endif //CUTLASS_ARCH_WMMA_SM70_ENABLED
+#endif //CUTLASS_ARCH_WMMA_SM75_ENABLED
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm80.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,15 +52,15 @@
 #include "testbed.h"
 
 #if defined(CUTLASS_ARCH_MMA_SM80_SUPPORTED)
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<double> * complex<double> => complex<double>
 // Input data type: complex<double>
-// Math instruction: mma.sync.aligned.m8n8k4.f64.f64.f64.f64
+// Math instruction: MMA.884.F64.F64
 // Output data type: complex<double>
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 TEST(SM80_warp_gemm_complex_tensor_op_f64, 8x8x4_8x8x4_nt) {
 
   using Shape = cutlass::gemm::GemmShape<8, 8, 4>;
   using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
   
@@ -289,15 +289,15 @@
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<float> * complex<float> => complex<float>
 // Input data type: complex<float>
-// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+// Math instruction: MMA.1688.F32.TF32
 // Output data type: complex<float>
 // Shared memory layout: Congrous
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_nt) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
@@ -491,15 +491,15 @@
       MmaTensorOp, cutlass::gemm::GemmShape<32, 32, 8> >()
       .run();
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 // complex<float> * complex<float> => complex<float>
 // Input data type: complex<float>
-// Math instruction: mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32
+// Math instruction: MMA.1688.F32.TF32
 // Output data type: complex<float>
 // Shared memory layout: Crosswise
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x8_16x8x8_tn) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 8>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
@@ -522,15 +522,15 @@
   >::Type;
 
   test::gemm::warp::TransformedTestbedComplex<
       MmaTensorOp, cutlass::gemm::GemmShape<16, 16, 8> >()
       .run();
 }
 
-// TEST FAILS crosswise complex<float> TN mma.sync.aligned.m16n8k8.f32.tf32.tf32.f32 test fails for k = 2*8 = 16
+// TEST FAILS crosswise complex<float> TN MMA.1688.F32.TF32 test fails for k = 2*8 = 16
 TEST(SM80_warp_gemm_complex_tensor_op_f32, 16x16x16_16x8x8_tn) {
 
   using Shape = cutlass::gemm::GemmShape<16, 16, 16>;
   using InstructionShape = cutlass::gemm::GemmShape<16, 8, 8>;
   
   using Element = cutlass::complex<float>;
   using ElementC = cutlass::complex<float>;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_complex_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_gaussian_complex_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm50.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm60.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm61.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sm90.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/gemm_sparse_sm80.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/testbed.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -187,55 +187,18 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.major == 9) {
-      // NVIDIA Hopper drops support for several data types
-      if (
-        cutlass::sizeof_bits<ElementA>::value < 8 ||
-        cutlass::sizeof_bits<ElementB>::value < 8 ||
-        cutlass::sizeof_bits<ElementC>::value < 8) {
-
-        return false;
-      }
-    }
-
-    return true;
-  }
-
-
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
@@ -434,54 +397,18 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.major == 9) {
-      // NVIDIA Hopper drops support for several data types
-      if (
-        cutlass::sizeof_bits<ElementA>::value < 8 ||
-        cutlass::sizeof_bits<ElementB>::value < 8 ||
-        cutlass::sizeof_bits<ElementC>::value < 8) {
-
-        return false;
-      }
-    }
-
-    return true;
-  }
-
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(tensor_A.host_view(),
@@ -745,54 +672,18 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.major == 9) {
-      // NVIDIA Hopper drops support for several data types
-      if (
-        cutlass::sizeof_bits<ElementA>::value < 8 ||
-        cutlass::sizeof_bits<ElementB>::value < 8 ||
-        cutlass::sizeof_bits<ElementC>::value < 8) {
-
-        return false;
-      }
-    }
-
-    return true;
-  }
-
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
@@ -983,54 +874,18 @@
     tensor_A.reset(cutlass::make_Coord(ThreadblockShape::kM, ThreadblockShape::kK));
     tensor_B.reset(cutlass::make_Coord(ThreadblockShape::kK, ThreadblockShape::kN));
     tensor_C.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_computed.reset(cutlass::make_Coord(Shape::kM, Shape::kN));
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.major == 9) {
-      // NVIDIA Hopper drops support for several data types
-      if (
-        cutlass::sizeof_bits<ElementA>::value < 8 ||
-        cutlass::sizeof_bits<ElementB>::value < 8 ||
-        cutlass::sizeof_bits<ElementC>::value < 8) {
-
-        return false;
-      }
-    }
-
-    return true;
-  }
-
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform) {
-
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       uint64_t seed = 7;
       cutlass::reference::host::TensorFillRandomUniform(tensor_A.host_view(),
@@ -1340,55 +1195,20 @@
     tensor_D_reference.reset(cutlass::make_Coord(Shape::kM, Shape::kN), false);
     tensor_E.reset(cutlass::make_Coord(
         Shape::kM, Shape::kK / Sparse / ElementsPerElementE));
     tensor_E_reordered.reset(cutlass::make_Coord(
         Shape::kM, Shape::kK / Sparse / ElementsPerElementE));
   }
 
-  /// Returns true if the CUDA device is sufficient to execute the kernel.
-  bool sufficient() const {
-
-    cudaDeviceProp properties;
-    int device_idx;
-    cudaError_t result = cudaGetDevice(&device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDevice() API call failed.");
-    }
-
-    result = cudaGetDeviceProperties(&properties, device_idx);
-
-    if (result != cudaSuccess) {
-      throw std::runtime_error("cudaGetDeviceProperties() failed");
-    }
-
-    if (properties.major == 9) {
-      // NVIDIA Hopper drops support for several data types
-      if (
-        cutlass::sizeof_bits<ElementA>::value < 8 ||
-        cutlass::sizeof_bits<ElementB>::value < 8 ||
-        cutlass::sizeof_bits<ElementC>::value < 8) {
-
-        return false;
-      }
-    }
-
-    return true;
-  }
-
   /// Runs the test
   bool run(
       cutlass::Distribution::Kind init_A = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_B = cutlass::Distribution::Uniform,
       cutlass::Distribution::Kind init_E = cutlass::Distribution::Uniform) {
 
-    if (!sufficient()) {
-      return true;
-    }
-
     //
     // initialize device memory
     //
 
     if (init_A == cutlass::Distribution::Uniform) {
       int scope_max = 8;
       int scope_min = -8;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm70.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm72.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/gemm/warp/wmma_sm75.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/matrix.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/layout/tensor_nhwc.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/cutlass/nvrtc/environment.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/kernel/thread/testbed_kernel.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/stdlib/stdint.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/gemm_nvrtc.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/nvrtc/thread/testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/pipeline_tma_async_warp_specialized_persistent.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/sequence_barrier.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/pipeline/testbed.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_contiguous.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/device/tensor_reduce_strided.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/kernel/reduce_splitk_testbed.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/reduction_thread.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/reduction/thread/testbed.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/test_unit.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/test_unit.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/predicated_tile_iterator.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/transform/threadblock/regular_tile_iterator_tensor_op.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/cutlass_test_levels.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/test/unit/util/tensor_reduce.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/arch_mappings.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -93,18 +93,13 @@
 };
 
 template <typename OperatorClass> struct ArchMap<arch::Sm86, OperatorClass> {
   static int const kMin = 86;
   static int const kMax = 1024;
 };
 
-template <typename OperatorClass> struct ArchMap<arch::Sm90, OperatorClass> {
-  static int const kMin = 90;
-  static int const kMax = 1024;
-};
-
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/handle.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -317,21 +317,21 @@
     int64_t ldb_imag,                         /// Leading dimension of imaginary part of B matrix
 
     void const * beta,                        /// Pointer to beta scalar
 
     NumericTypeID element_C,                  /// Data type of C and D matrix
 
     void const * const * ptr_C_real,          /// Pointer to array containing pointers to real part of C matrices
-    void const * const * ptr_C_imag,          /// Pointer to array containing pointers to imaginary part of C matrices
+    void const * const * ptr_C_imag,          /// Pointer to array containing poitners to imaginary part of C matrices
 
     int64_t ldc_real,                         /// Leading dimension of real part of C matrix
     int64_t ldc_imag,                         /// Leading dimension of imaginary part of C matrix
 
     void * const * ptr_D_real,                /// Pointer to array containing pointers to real part of D matrices
-    void * const * ptr_D_imag,                /// Pointer to array containing pointers to imaginary part of D matrices
+    void * const * ptr_D_imag,                /// Pointer to array containing poitners to imaginary part of D matrices
 
     int64_t ldd_real,                         /// Leading dimension of real part of D matrix
     int64_t ldd_imag                          /// Leading dimension of imaginary part of D matrix
   );
 
 };
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/library.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -350,37 +350,32 @@
 
   /// Minimum compute capability (e.g. 70, 75) of a device eligible to run the operation.
   int minimum_compute_capability;
 
   /// Minimum compute capability (e.g. 70, 75) of a device eligible to run the operation.
   int maximum_compute_capability;
 
-  /// Describes the shape of a cluster (in blocks)
-  cutlass::gemm::GemmCoord cluster_shape;
-
   //
   // Methods
   //
 
   TileDescription(
     cutlass::gemm::GemmCoord threadblock_shape = cutlass::gemm::GemmCoord(),
     int threadblock_stages = 0,
     cutlass::gemm::GemmCoord warp_count = cutlass::gemm::GemmCoord(),
     MathInstructionDescription math_instruction = MathInstructionDescription(),
     int minimum_compute_capability = 0,
-    int maximum_compute_capability = 0,
-    cutlass::gemm::GemmCoord cluster_shape = cutlass::gemm::GemmCoord(1,1,1)
+    int maximum_compute_capability = 0
   ):
     threadblock_shape(threadblock_shape), 
     threadblock_stages(threadblock_stages), 
     warp_count(warp_count),
     math_instruction(math_instruction),
     minimum_compute_capability(minimum_compute_capability),
-    maximum_compute_capability(maximum_compute_capability),
-    cluster_shape(cluster_shape) { }
+    maximum_compute_capability(maximum_compute_capability) { }
 
   // Equality operator
   inline
   bool operator==(TileDescription const& rhs) const{
     return (
       (threadblock_shape == rhs.threadblock_shape) &&
       (threadblock_stages == rhs.threadblock_stages) &&
@@ -514,15 +509,15 @@
     split_k_mode(split_k_mode),
     transform_A(transform_A),
     transform_B(transform_B) {} 
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Description for structured sparse GEMMs.
+/// Desciprion for structured sparse GEMMs.
 struct SparseGemmDescription : public GemmDescription {
 
   /// Description structure for structured sparse GEMM
   SparseGemmDescription(
     GemmKind gemm_kind = GemmKind::kGemm,
     TensorDescription const &A = TensorDescription(),
     TensorDescription const &B = TensorDescription(),
@@ -992,33 +987,24 @@
   int64_t lda;
   int64_t ldb;
   int64_t ldc;
   int64_t ldd;
 };
 
 struct GemmUniversalArguments {
-  // NOTE: these are replicated for 3.0 interfaces 
-  gemm::GemmCoord problem_size;
-  int batch_count;
 
   void const *A;
   void const *B;
   void const *C;
   void *D;
 
   void const *alpha;
   void const *beta;
   ScalarPointerMode pointer_mode;
 
-  // NOTE: these are replicated for 3.0 interfaces
-  int64_t lda;
-  int64_t ldb;
-  int64_t ldc;
-  int64_t ldd;
-
   int64_t batch_stride_A;
   int64_t batch_stride_B;
   int64_t batch_stride_C;
   int64_t batch_stride_D;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
@@ -1156,15 +1142,15 @@
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 //
 // OperationKind: kSparseGemm
 //
 
-/// Computes GEMM assuming one of the inputs has 2:4 structured sparsity.
+/// Computes GEMM assumine one of the inputs has 2:4 structured sparsity.
 struct SparseGemmConfiguration {
 
   GemmUniversalMode mode;
   gemm::GemmCoord problem_size;
   int batch_count;                /// number of sparse matrix products in batch
 
   int64_t lda;                    /// leading dimension of A operand
@@ -1183,15 +1169,15 @@
 /// Arguments for sparse GEMMs
 struct SparseGemmArguments {
 
   void const *A;                    /// pointer to A matrix
   void const *B;                    /// pointer to B matrix
   void const *C;                    /// pointer to C matrix
   void *D;                          /// pointer to D matrix
-  void const *E;                    /// pointer to E matrix (metadata)
+  void const *E;                    /// pointer to E matric (metadata)
 
   void const *alpha;                /// pointer to alpha scalar
   void const *beta;                 /// pointer to beta scalar
   ScalarPointerMode pointer_mode;   /// enumerant indicating whether alpha/beta pointers are host
                                     ///   or device pointers.
 };
 
@@ -1455,21 +1441,18 @@
   /////////////////////////////////////////////////////////
   /// pointer to implicit gemm matrix A
   void const *A;
 
   /// pointer to implicit gemm matrix B
   void const *B;
 
-  /// pointer to reordered matrix B
-  void const *reordered_B;
-  
   /// pointer to implicit gemm matrix C
   void const *C;
 
-  /// pointer to implicit gemm destination matrix D
+  /// pointer to implicit gemm desitination matrix D
   void *D;
 
   /// Host or device pointer to alpha scalar
   void const *alpha;
 
   /// Host or device pointer to beta scalar
   void const *beta;
@@ -1483,24 +1466,24 @@
 
 /// Configuration for Reduction operations
 //
 // OperationKind: Reduction
 //
 struct ReductionConfiguration {
 
-  /// Reduction problem size
+  /// Redcution problem size
   MatrixCoord problem_size;
 
   /// Number of partitions to reduce
   int partitions;
 
-  /// Number of elements between each partition
+  /// Number of lements between each partition
   int64_t partition_stride;
 
-  /// leading dimension of 'w'orkspace operand
+  /// leading dimension of 'w'orksace operand
   int64_t ldw; 
 
   /// leading dimension of 's'ource operand
   int64_t lds;
 
   /// leading dimension of 'd'estination operand
   int64_t ldd;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/manifest.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/operation_table.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/singleton.h`

 * *Files 7% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/include/cutlass/library/util.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/compiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cute.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/cutlass.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,17 +25,16 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief binding CUTLASS C++ APIs to Python
+   \brief binding cutlass C++ APIs to python
 */
-
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
 #include "builtin_types.h"
 #include "device_launch_parameters.h"
 #include "stddef.h"
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/arch.h`

 * *Files 3% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -47,13 +47,13 @@
 
 void bind_opcode(py::module &m) {
     py::enum_<cutlass::OpcodeClass>(m, "OpClass",
         R"pbdoc(classification of math operators)pbdoc")
         .value("Simt", cutlass::OpcodeClass::kSimt, 
             R"pbdoc(Tag classifying math operators as thread-level operations)pbdoc")
         .value("TensorOp", cutlass::OpcodeClass::kTensorOp, 
-            R"pbdoc(Tag classifying operators as Tensor Core operations)pbdoc")
+            R"pbdoc(Tag classifing operators as Tensor Core operations)pbdoc")
         .value("WmmaTensorOp", cutlass::OpcodeClass::kWmmaTensorOp, 
-            R"pbdoc(Tag classifying operators as WMMA Tensor Core operations)pbdoc")
+            R"pbdoc(Tag classifing operators as WMMA Tensor Core operations)pbdoc")
         .value("SparseTensorOp", cutlass::OpcodeClass::kSparseTensorOp, 
-            R"pbdoc(Tag classifying operators as sparseTensor Core operations)pbdoc");
+            R"pbdoc(Tag classifing operators as sparseTensor Core operations)pbdoc");
 }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/conv_problem_size.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,15 +38,15 @@
 #include "cutlass/conv/conv2d_problem_size.h"
 
 namespace py = pybind11;
 
 void bind_conv_problem_size(py::module &m) {
     //
     // Conv2d Problem Size: 
-    // include/cutlass/conv/conv2d_problem_size.h
+    // include/cutlass/conv/conv2d_problem_sizd.h
     //
     py::class_<cutlass::conv::Conv2dProblemSize>(m, "Conv2dProblemSize")
          // constructors
         .def(py::init<int, int, int, int, int, int, int, int, int, int, int, int, int, int, int, cutlass::conv::Mode, int, int>())
         .def(py::init<cutlass::Tensor4DCoord, cutlass::Tensor4DCoord, cutlass::Tensor4DCoord, cutlass::MatrixCoord, cutlass::MatrixCoord, cutlass::conv::Mode, int, int>())
         // attribute accessors
         .def_readwrite("N", &cutlass::conv::Conv2dProblemSize::N)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/convolution.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/conv/host.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_generic.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,19 +26,22 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
+  
+  \brief A file contains the epilogue visitor with CTA row-wise broadcast
 
-  \brief A generic wrapper around an epilogue visitor operation
+  The epilogue rearranges the result of a matrix product through shared memory to match canonical
+  tensor layouts in global memory. Epilogues support conversion and reduction operations.
+                          
 */
 
-
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/arch/memory.h"
 #include "cutlass/arch/memory_sm75.h"
 #include "cutlass/gemm/kernel/gemm_transpose_operands.h"
 #include "cutlass/gemm/kernel/default_gemm.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/binary_ops.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/unary_ops.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_accumulator.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_binary.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -80,18 +80,19 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
+    /// Combination Op TODO: generalize this
     using BinaryOp = BinaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
-    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess mismatches with Visitor B");
+    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename VisitorA::SharedStorage storage_a;
         typename VisitorB::SharedStorage storage_b;
 
         CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_broadcast.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_column_reduction.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -51,27 +51,28 @@
 ///
 template <
     typename ThreadblockShape_,             /// Threadblock shape
     typename ElementAccumulator_,           ///< Data type of the Accumulator
     typename ElementReduction_,             ///< Data type of the output reduction in device memory
     typename ElementReductionAccumulator_ , ///< Data type to accumulate reduction in smem and register
     typename OutputTileIterator_,           ///< Tile Iterator type
-    typename Visitor_                       ///< preceding visitor op
+    typename Visitor_                       ///< preceeding visitor op
 >
 class VisitorOpColumnReduction {
 public:
     using ElementAccumulator = ElementAccumulator_;
     using ElementReductionAccumulator = ElementReductionAccumulator_;
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
+    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     
 
     /// Fragment type returned from Visitor
@@ -79,15 +80,15 @@
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
 
     using VisitAccessType = VisitAccessTypeVisitor;
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Fragment type of reduction
+    /// Fragment type of redcution
     using ReductionAccumulatorAccessType = Array<ElementReductionAccumulator, kElementsPerAccess>;
 
     /// Thread map used by output tile iterators
     using ThreadMap = typename OutputTileIterator::ThreadMap;
     /// Used for the reduction
     struct ReductionDetail {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_linear_combination.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -78,19 +78,19 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op
+    /// Combination Op TODO: generalize this
     using CombinationOp = cutlass::plus<VisitAccessType>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeA::kElements, "kElementsPerAccess mismatches with Visitor A");
-    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess mismatches with Visitor B");
+    static_assert(kElementsPerAccess==VisitAccessTypeB::kElements, "kElementsPerAccess misnatches with Visitor B");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename VisitorA::SharedStorage storage_a;
         typename VisitorB::SharedStorage storage_b;
 
         CUTLASS_HOST_DEVICE
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_broadcast.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_row_reduction.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -52,41 +52,42 @@
 ///
 template <
     typename ThreadblockShape_,             /// Threadblock shape
     typename ElementAccumulator_,           ///< Data type of the Accumulator
     typename ElementReduction_,             ///< Data type of the output reduction in device memory
     typename ElementReductionAccumulator_ , ///< Data type to accumulate reduction in smem and register
     typename OutputTileIterator_,           ///< Tile Iterator type
-    typename Visitor_                       ///< preceding visitor op
+    typename Visitor_                       ///< preceeding visitor op
 >
 class VisitorOpRowReduction {
 public:
     using ElementAccumulator = ElementAccumulator_;
     using ElementReductionAccumulator = ElementReductionAccumulator_;
     using ElementReduction = ElementReduction_;
     using OutputTileIterator = OutputTileIterator_;
     using ThreadblockShape = ThreadblockShape_;
     using Visitor = Visitor_;
 
     static int const kElementsPerAccess = OutputTileIterator::kElementsPerAccess;
 
+    // TODO: generalize the reduction op
     using ReductionOp = cutlass::plus<Array<ElementReductionAccumulator, kElementsPerAccess>>;
     using ReductionOpScalar = cutlass::plus<ElementReductionAccumulator>;
     using ElementOutput = typename OutputTileIterator::Element;
 
     /// Fragment type returned from Visitor
     using VisitAccessTypeVisitor = typename Visitor::VisitAccessType;
     using ElementVisitor = typename VisitAccessTypeVisitor::Element;
 
     using VisitAccessType = VisitAccessTypeVisitor;
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Fragment type of reduction
+    /// Fragment type of redcution
     using ReductionAccumulatorAccessType = Array<ElementReductionAccumulator, kElementsPerAccess>;
 
     /// Thread map used by output tile iterators
     using ThreadMap = typename OutputTileIterator::ThreadMap;
     /// Used for the reduction
     struct ReductionDetail {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_input.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_tensor_output.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_op/visitor_op_unary.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -75,15 +75,15 @@
 
     /// Fragment type returned by this visitor
     using VisitAccessType = Array<ElementCompute, kElementsPerAccess>; 
 
     /// Fragment type of accumulator
     using AccumulatorAccessType = Array<ElementAccumulator, kElementsPerAccess>;
 
-    /// Combination Op
+    /// Combination Op TODO: generalize this
     using UnaryOp = UnaryOp_<ElementCompute, kElementsPerAccess>;
 
     static_assert(kElementsPerAccess==VisitAccessTypeVisitor::kElements, "kElementsPerAccess mismatches with Visitor");
 
     /// SMEM buffer class required in the epilogue visitor
     struct SharedStorage {
         typename Visitor::SharedStorage storage_visitor;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/epilogue/epilogue_visitor_with_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,19 +26,20 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief Epilogue visitor type used for partial computation of a layernorm operation
+    \brief A file contains all functioning classes needed by GemmLayernorm.
 
     GemmLayernorm example =  GEMM0 with partial reduction fused in epilogue (EpilogueVisitorLayerNorm)
                           +  lightweight full reduction kernel (ApplyFinalReduction)
-                          +  GEMM1 with elementwise operations fused in mainloop (GemmLayernormMainloopFusion)
+                          +  GEMM1 with elemenwise operations fused in mainloop (GemmLayernormMainloopFusion)
+                          
 */
 
 #pragma once
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 #include "cutlass/cutlass.h"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -48,15 +48,15 @@
     py::enum_<cutlass::gemm::GemmUniversalMode>(m, "Mode")
         .value("Gemm", cutlass::gemm::GemmUniversalMode::kGemm, "Ordinary GEMM & GEMM Split-K serial")
         .value("GemmSplitKParallel", cutlass::gemm::GemmUniversalMode::kGemmSplitKParallel, "GEMM Split-K parallel")
         .value("Batched", cutlass::gemm::GemmUniversalMode::kBatched, "Batched GEMM")
         .value("Array", cutlass::gemm::GemmUniversalMode::kArray)
         .value("Invalid", cutlass::gemm::GemmUniversalMode::kInvalid);
     
-    /// GemmCoord is a structure that specifies a location within the coordinate space of a GEMM problem
+    /// GemmCoord is a structure that specifies a location within the coordiate space of a GEMM problem
     py::class_<cutlass::gemm::GemmCoord>(m, "GemmCoord")
         .def(py::init<int, int, int>())
         .def("m", py::overload_cast<>(&cutlass::gemm::GemmCoord::m))
         .def("n", py::overload_cast<>(&cutlass::gemm::GemmCoord::n))
         .def("k", py::overload_cast<>(&cutlass::gemm::GemmCoord::k))
         // get tensor coords
         .def("mk",
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/gemm_universal_with_visitor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h`

 * *Files 26% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -26,599 +26,493 @@
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 
 /*! \file
-    \brief
+    \brief Grouped GEMM kernel with epilogue visitor customized for softmax
 */
 
 #pragma once
 
 #include "cutlass/cutlass.h"
 #include "cutlass/fast_math.h"
 #include "cutlass/gemm/gemm.h"
-#include "cutlass/gemm/kernel/params_universal_base.h"
 #include "cutlass/matrix_coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/semaphore.h"
-
+#include "cutlass/util/device_memory.h"
 #include "cutlass/layout/matrix.h"
-
 #include "cutlass/trace.h"
+#include "cutlass/gemm/kernel/gemm_transpose_operands.h"
+#include "cutlass/gemm/kernel/gemm_grouped_problem_visitor.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace gemm {
 namespace kernel {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <
-  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate
+  typename Mma_,                  ///! Threadblock-scoped matrix multiply-accumulate 
   typename Epilogue_,             ///! Epilogue
-  typename ThreadblockSwizzle_    ///! Threadblock swizzling function
+  typename ThreadblockSwizzle_,   ///! Threadblock swizzling function
+  GroupScheduleMode GroupScheduleMode_, ///! Type of scheduling to perform
+  bool Transposed_ = false,
+  bool UseMask_ = false
 >
-struct GemmUniversalwithEpilogueVisitor {
+struct GemmGroupedWithEpilogueVistor {
 public:
 
   using Mma = Mma_;
   using Epilogue = Epilogue_;
-  using EpilogueVisitor = typename Epilogue::Visitor;
   using ThreadblockSwizzle = ThreadblockSwizzle_;
 
-  using ElementA = typename Mma::IteratorA::Element;
-  using LayoutA = typename Mma::IteratorA::Layout;
-  using ElementB = typename Mma::IteratorB::Element;
-  using LayoutB = typename Mma::IteratorB::Layout;
+  static GroupScheduleMode const kGroupScheduleMode = GroupScheduleMode_;
+
+  using EpilogueVisitor = typename Epilogue::Visitor;
+  using EpilogueOutputOp = typename EpilogueVisitor::ElementwiseFunctor;
+  static bool const kTransposed = Transposed_;
+
+  // Optional transpose
+  using MapArguments = kernel::detail::MapArguments<
+    typename Mma::IteratorA::Element,
+    typename Mma::IteratorA::Layout,
+    Mma::kTransformA,
+    Mma::IteratorA::AccessType::kElements,
+    typename Mma::IteratorB::Element,
+    typename Mma::IteratorB::Layout,
+    Mma::kTransformB,
+    Mma::IteratorB::AccessType::kElements,
+    typename Mma::LayoutC,
+    kTransposed
+  >;
+
+  // Public-facing type definitions related to operand element type, layout, and complex conjugate
+  // operation. Must interact with the 'kTransposed' notion.
+  using ElementA = typename MapArguments::ElementA;
+  using LayoutA = typename MapArguments::LayoutA;
+  using ElementB = typename MapArguments::ElementB;
+  using LayoutB = typename MapArguments::LayoutB;
   using ElementC = typename EpilogueVisitor::ElementOutput;
-  using LayoutC = typename EpilogueVisitor::OutputTileIterator::Layout;
+  using LayoutC = typename MapArguments::LayoutC;
 
-  static ComplexTransform const kTransformA = Mma::kTransformA;
-  static ComplexTransform const kTransformB = Mma::kTransformB;
-  using Operator = typename Mma::Operator;
+  using ElementNorm = typename EpilogueVisitor::ElementNorm;
+  using ElementSum = typename EpilogueVisitor::ElementSum;
+
+  static ComplexTransform const kTransformA = MapArguments::kTransformA;
+  static ComplexTransform const kTransformB = MapArguments::kTransformB;
 
+  // Type definitions about the mainloop.
+  using Operator = typename Mma::Operator;
   using OperatorClass = typename Mma::Operator::OperatorClass;
   using ThreadblockShape = typename Mma::Shape;
   using WarpShape = typename Mma::Operator::Shape;
   using InstructionShape = typename Mma::Policy::Operator::InstructionShape;
   using ArchTag = typename Mma::ArchTag;
 
   static int const kStages = Mma::kStages;
-  static int const kAlignmentA = Mma::IteratorA::AccessType::kElements;
-  static int const kAlignmentB = Mma::IteratorB::AccessType::kElements;
+  static int const kAlignmentA = MapArguments::kAlignmentA;
+  static int const kAlignmentB = MapArguments::kAlignmentB;
   static int const kAlignmentC = EpilogueVisitor::kElementsPerAccess;
 
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
 
-  /// Split-K preserves splits that are 128b aligned
-  static int const kSplitKAlignment = const_max(
-    128 / sizeof_bits<ElementA>::value,
-    128 / sizeof_bits<ElementB>::value
-  );
+  using ProblemVisitor = GemmGroupedProblemVisitor<
+                            ThreadblockShape,
+                            kGroupScheduleMode,
+                            kThreadCount,
+                            kThreadCount,
+                            kTransposed>;
 
   //
   // Structures
   //
 
   /// Argument structure
-  struct Arguments : UniversalArgumentsBase {
+  struct Arguments {
 
     //
     // Data members
     //
 
+    GemmCoord *problem_sizes;
+    // when using mask, real problem sizes may not be aligned
+    // then we need to mask out unpadded elements in softmax
+    GemmCoord *problem_sizes_real;
+    int problem_count;
+    int threadblock_count;
+
+    ElementA ** ptr_A;
+    ElementB ** ptr_B;
+    ElementC ** ptr_C;
+    ElementC ** ptr_D;
+
+    ElementNorm **ptr_Max;
+    ElementSum **ptr_Sum;
+
+    typename LayoutA::Stride::LongIndex *lda;
+    typename LayoutB::Stride::LongIndex *ldb;
+    typename LayoutC::Stride::LongIndex *ldc;
+    typename LayoutC::Stride::LongIndex *ldd;
+
     typename EpilogueVisitor::Arguments epilogue_visitor;
 
-    void const * ptr_A;
-    void const * ptr_B;
-    void const * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-
-    typename LayoutA::Stride stride_a;
-    typename LayoutB::Stride stride_b;
-    typename LayoutC::Stride stride_c;
-    typename LayoutC::Stride stride_d;
-
-    typename LayoutA::Stride::LongIndex lda;
-    typename LayoutB::Stride::LongIndex ldb;
-    typename LayoutC::Stride::LongIndex ldc;
-    typename LayoutC::Stride::LongIndex ldd;
-
-    int const * ptr_gather_A_indices;
-    int const * ptr_gather_B_indices;
-    int const * ptr_scatter_D_indices;
+    // Only used by device-level operator
+    GemmCoord *host_problem_sizes;
 
     //
     // Methods
     //
 
-    Arguments():
-      ptr_A(nullptr), ptr_B(nullptr), ptr_C(nullptr), ptr_D(nullptr),
-      ptr_gather_A_indices(nullptr),
-      ptr_gather_B_indices(nullptr),
-      ptr_scatter_D_indices(nullptr) {}
-
-    /// constructs an arguments structure
-    Arguments(
-      GemmUniversalMode mode,
-      GemmCoord problem_size,
-      int batch_count,
-      typename EpilogueVisitor::Arguments epilogue_visitor,
-      void const * ptr_A,
-      void const * ptr_B,
-      void const * ptr_C,
-      void * ptr_D,
-      int64_t batch_stride_A,
-      int64_t batch_stride_B,
-      int64_t batch_stride_C,
-      int64_t batch_stride_D,
-      typename LayoutA::Stride stride_a,
-      typename LayoutB::Stride stride_b,
-      typename LayoutC::Stride stride_c,
-      typename LayoutC::Stride stride_d,
-      int const *ptr_gather_A_indices = nullptr,
-      int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr
-    ):
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue_visitor(epilogue_visitor),
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
-      stride_a(stride_a), stride_b(stride_b), stride_c(stride_c), stride_d(stride_d),
-      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
-      lda = 0;
-      ldb = 0;
-      ldc = 0;
-      ldd = 0;
-      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
-
-    /// constructs an arguments structure
-    Arguments(
-      GemmUniversalMode mode,
-      GemmCoord problem_size,
-      int batch_count,
-      typename EpilogueVisitor::Arguments epilogue_visitor,
-      void const * ptr_A,
-      void const * ptr_B,
-      void const * ptr_C,
-      void * ptr_D,
-      int64_t batch_stride_A,
-      int64_t batch_stride_B,
-      int64_t batch_stride_C,
-      int64_t batch_stride_D,
-      typename LayoutA::Stride::LongIndex lda,
-      typename LayoutB::Stride::LongIndex ldb,
-      typename LayoutC::Stride::LongIndex ldc,
-      typename LayoutC::Stride::LongIndex ldd,
-      int const *ptr_gather_A_indices = nullptr,
-      int const *ptr_gather_B_indices = nullptr,
-      int const *ptr_scatter_D_indices = nullptr
-    ):
-      UniversalArgumentsBase(mode, problem_size, batch_count, batch_stride_D),
-      epilogue_visitor(epilogue_visitor),
-      ptr_A(ptr_A), ptr_B(ptr_B), ptr_C(ptr_C), ptr_D(ptr_D),
-      batch_stride_A(batch_stride_A), batch_stride_B(batch_stride_B), batch_stride_C(batch_stride_C),
-      lda(lda), ldb(ldb), ldc(ldc), ldd(ldd),
-      ptr_gather_A_indices(ptr_gather_A_indices), ptr_gather_B_indices(ptr_gather_B_indices),
-      ptr_scatter_D_indices(ptr_scatter_D_indices) {
-      stride_a = make_Coord(lda);
-      stride_b = make_Coord(ldb);
-      stride_c = make_Coord(ldc);
-      stride_d = make_Coord(ldd);
-      CUTLASS_TRACE_HOST("GemmUniversal::Arguments::Arguments() - problem_size: " << problem_size);
-      }
-
-    /// Returns arguments for the transposed problem
-    Arguments transposed_problem() const {
-      Arguments args(*this);
-
-      std::swap(args.problem_size.m(), args.problem_size.n());
-      std::swap(args.ptr_A, args.ptr_B);
-      std::swap(args.lda, args.ldb);
-      std::swap(args.stride_a, args.stride_b);
-      std::swap(args.batch_stride_A, args.batch_stride_B);
-      std::swap(args.ptr_gather_A_indices, args.ptr_gather_B_indices);
+    /// Default ctor
+    CUTLASS_HOST_DEVICE
+    Arguments(): 
+      problem_count(0), 
+      threadblock_count(0), 
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_Max(nullptr),
+      ptr_Sum(nullptr),
+      lda(nullptr),
+      ldb(nullptr),
+      ldc(nullptr),
+      ldd(nullptr),
+      host_problem_sizes(nullptr)
+    {
+
+    }
+
+    /// Ctor
+    CUTLASS_HOST_DEVICE
+    Arguments(    
+      GemmCoord *problem_sizes,
+      int problem_count,
+      int threadblock_count,
+      ElementA ** ptr_A,
+      ElementB ** ptr_B,
+      ElementC ** ptr_C,
+      ElementC ** ptr_D,
+      ElementNorm **ptr_Max,
+      ElementSum **ptr_Sum,
+      typename LayoutA::Stride::LongIndex *lda,
+      typename LayoutB::Stride::LongIndex *ldb,
+      typename LayoutC::Stride::LongIndex *ldc,
+      typename LayoutC::Stride::LongIndex *ldd,
+      typename EpilogueVisitor::Arguments epilogue_visitor_,
+      GemmCoord *host_problem_sizes=nullptr,
+      GemmCoord *problem_sizes_real=nullptr
+    ): 
+      problem_sizes(problem_sizes),
+      problem_count(problem_count),
+      threadblock_count(threadblock_count),
+      ptr_A(ptr_A),
+      ptr_B(ptr_B),
+      ptr_C(ptr_C),
+      ptr_D(ptr_D),
+      ptr_Max(ptr_Max),
+      ptr_Sum(ptr_Sum),
+      lda(lda),
+      ldb(ldb),
+      ldc(ldc),
+      ldd(ldd),
+      epilogue_visitor(epilogue_visitor_),
+      host_problem_sizes(host_problem_sizes),
+      problem_sizes_real(problem_sizes_real)
+    {
 
-      return args;
     }
   };
 
   //
   // Structure for precomputing values in host memory and passing to kernels
   //
 
   /// Parameters structure
-  struct Params : UniversalParamsBase<
-    ThreadblockSwizzle,
-    ThreadblockShape,
-    ElementA,
-    ElementB,
-    ElementC> {
-
-    using ParamsBase = UniversalParamsBase<
-      ThreadblockSwizzle,
-      ThreadblockShape,
-      ElementA,
-      ElementB,
-      ElementC>;
-
-    typename Mma::IteratorA::Params params_A;
-    typename Mma::IteratorB::Params params_B;
-    typename EpilogueVisitor::OutputTileIterator::Params params_C;
-    typename EpilogueVisitor::OutputTileIterator::Params params_D;
-
-    typename EpilogueVisitor::Params epilogue_visitor;
+  struct Params {
 
-    void * ptr_A;
-    void * ptr_B;
-    void * ptr_C;
-    void * ptr_D;
-
-    int64_t batch_stride_A;
-    int64_t batch_stride_B;
-    int64_t batch_stride_C;
-
-    int * ptr_gather_A_indices;
-    int * ptr_gather_B_indices;
-    int * ptr_scatter_D_indices;
+    typename ProblemVisitor::Params problem_visitor;
+    GemmCoord *problem_sizes_real;
+    int threadblock_count;
+
+    ElementA ** ptr_A;
+    ElementB ** ptr_B;
+    ElementC ** ptr_C;
+    ElementC ** ptr_D;
+
+    ElementNorm **ptr_Max;
+    ElementSum **ptr_Sum;
+
+    typename LayoutA::Stride::LongIndex *lda;
+    typename LayoutB::Stride::LongIndex *ldb;
+    typename LayoutC::Stride::LongIndex *ldc;
+    typename LayoutC::Stride::LongIndex *ldd;
 
-    int *semaphore;
+    typename EpilogueVisitor::Params epilogue_visitor;
 
     //
     // Methods
     //
 
-    /// Default constructor
-    Params() = default;
+    CUTLASS_HOST_DEVICE
+    Params():
+      ptr_A(nullptr),
+      ptr_B(nullptr),
+      ptr_C(nullptr),
+      ptr_D(nullptr),
+      ptr_Max(nullptr),
+      ptr_Sum(nullptr),
+      lda(nullptr),
+      ldb(nullptr),
+      ldc(nullptr),
+      ldd(nullptr),
+      problem_sizes_real(problem_sizes_real)
+    { }
 
     CUTLASS_HOST_DEVICE
-    Params(
-      Arguments const &args,
-      int device_sms,
-      int sm_occupancy
-    ):
-      ParamsBase(args, device_sms, sm_occupancy),
-      params_A(args.lda ? make_Coord_with_padding<LayoutA::kStrideRank>(args.lda) : args.stride_a),
-      params_B(args.ldb ? make_Coord_with_padding<LayoutB::kStrideRank>(args.ldb) : args.stride_b),
-      params_C(args.ldc ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldc) : args.stride_c),
-      params_D(args.ldd ? make_Coord_with_padding<LayoutC::kStrideRank>(args.ldd) : args.stride_d),
-      epilogue_visitor(args.epilogue_visitor),
-      ptr_A(const_cast<void *>(args.ptr_A)),
-      ptr_B(const_cast<void *>(args.ptr_B)),
-      ptr_C(const_cast<void *>(args.ptr_C)),
+    Params(Arguments const &args, void *workspace = nullptr, int32_t tile_count = 0):
+      problem_visitor(args.problem_sizes, args.problem_count, workspace, tile_count),
+      threadblock_count(args.threadblock_count),
+      ptr_A(args.ptr_A),
+      ptr_B(args.ptr_B),
+      ptr_C(args.ptr_C),
       ptr_D(args.ptr_D),
-      batch_stride_A(args.batch_stride_A),
-      batch_stride_B(args.batch_stride_B),
-      batch_stride_C(args.batch_stride_C),
-      ptr_gather_A_indices(const_cast<int *>(args.ptr_gather_A_indices)),
-      ptr_gather_B_indices(const_cast<int *>(args.ptr_gather_B_indices)),
-      ptr_scatter_D_indices(const_cast<int *>(args.ptr_scatter_D_indices)) {
+      ptr_Max(args.ptr_Max),
+      ptr_Sum(args.ptr_Sum),
+      lda(args.lda),
+      ldb(args.ldb),
+      ldc(args.ldc),
+      ldd(args.ldd),
+      epilogue_visitor(args.epilogue_visitor),
+      problem_sizes_real(args.problem_sizes_real)
+    { 
 
     }
 
     CUTLASS_HOST_DEVICE
     void update(
       Arguments const &args,
-      void *workspace = nullptr) {
+      void *workspace = nullptr,
+      int32_t tile_count = -1) {
 
-      ptr_A = const_cast<void *>(args.ptr_A);
-      ptr_B = const_cast<void *>(args.ptr_B);
-      ptr_C = const_cast<void *>(args.ptr_C);
+      problem_visitor = typename ProblemVisitor::Params(args.problem_sizes, args.problem_count, workspace, tile_count);
+      threadblock_count = args.threadblock_count;
+      ptr_A = args.ptr_A;
+      ptr_B = args.ptr_B;
+      ptr_C = args.ptr_C;
       ptr_D = args.ptr_D;
-
-      ptr_gather_A_indices = const_cast<int *>(args.ptr_gather_A_indices);
-      ptr_gather_B_indices = const_cast<int *>(args.ptr_gather_B_indices);
-      ptr_scatter_D_indices = const_cast<int *>(args.ptr_scatter_D_indices);
-
-      batch_stride_A = args.batch_stride_A;
-      batch_stride_B = args.batch_stride_B;
-      batch_stride_C = args.batch_stride_C;
-
-      epilogue_visitor = args.epilogue_visitor;
-
-      semaphore = static_cast<int *>(workspace);
-      CUTLASS_TRACE_HOST("GemmUniversal::Params::update()");
+      ptr_Max = args.ptr_Max;
+      ptr_Sum = args.ptr_Sum;
+      lda = args.lda;
+      ldb = args.ldb;
+      ldc = args.ldc;
+      ldd = args.ldd;
+      problem_sizes_real = args.problem_sizes_real;
     }
   };
 
   /// Shared memory storage structure
-  union SharedStorage {
-    typename Mma::SharedStorage main_loop;
-    typename Epilogue::SharedStorage epilogue;
-    typename EpilogueVisitor::SharedStorage visitor;
+  struct SharedStorage {
+    union {
+      typename Mma::SharedStorage main_loop;
+      struct {
+        typename Epilogue::SharedStorage epilogue;
+        typename EpilogueVisitor::SharedStorage visitor;
+      } epilogue;
+    } kernel;
+
+    // ProblemVisitor shared storage can't be overlapped with others
+    typename ProblemVisitor::SharedStorage problem_visitor;
   };
 
+
 public:
 
   //
   // Methods
   //
 
   CUTLASS_DEVICE
-  GemmUniversalwithEpilogueVisitor() { }
+  GemmGroupedWithEpilogueVistor() { } 
 
   /// Determines whether kernel satisfies alignment
-  static Status can_implement(
-    cutlass::gemm::GemmCoord const & problem_size) {
-
-    CUTLASS_TRACE_HOST("GemmUniversalwithEpilogueVisitor::can_implement()");
-
-    static int const kAlignmentA = (platform::is_same<LayoutA,
-                                                      layout::ColumnMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<LayoutA,
-                                                        layout::ColumnMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Mma::IteratorA::AccessType::kElements;
-    static int const kAlignmentB = (platform::is_same<LayoutB,
-                                                      layout::RowMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<LayoutB,
-                                                        layout::RowMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Mma::IteratorB::AccessType::kElements;
-    static int const kAlignmentC = (platform::is_same<LayoutC,
-                                                      layout::ColumnMajorInterleaved<32>>::value)
-                                   ? 32
-                                   : (platform::is_same<LayoutC,
-                                                        layout::ColumnMajorInterleaved<64>>::value)
-                                     ? 64
-                                     : Epilogue::OutputTileIterator::kElementsPerAccess;
-
-    bool isAMisaligned = false;
-    bool isBMisaligned = false;
-    bool isCMisaligned = false;
-
-    if (platform::is_same<LayoutA, layout::RowMajor>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajor>::value) {
-      isAMisaligned = problem_size.m() % kAlignmentA;
-    } else if (platform::is_same<LayoutA, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutA, layout::ColumnMajorInterleaved<64>>::value) {
-      isAMisaligned = problem_size.k() % kAlignmentA;
-    }
-
-    if (platform::is_same<LayoutB, layout::RowMajor>::value) {
-      isBMisaligned = problem_size.n() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::ColumnMajor>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    } else if (platform::is_same<LayoutB, layout::RowMajorInterleaved<32>>::value
-            || platform::is_same<LayoutB, layout::RowMajorInterleaved<64>>::value) {
-      isBMisaligned = problem_size.k() % kAlignmentB;
-    }
-
-    if (platform::is_same<LayoutC, layout::RowMajor>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajor>::value) {
-      isCMisaligned = problem_size.m() % kAlignmentC;
-    } else if (platform::is_same<LayoutC, layout::ColumnMajorInterleaved<32>>::value
-            || platform::is_same<LayoutC, layout::ColumnMajorInterleaved<64>>::value) {
-      isCMisaligned = problem_size.n() % kAlignmentC;
-    }
-
-    if (isAMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for A operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    if (isBMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for B operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    if (isCMisaligned) {
-      CUTLASS_TRACE_HOST("  returning kErrorMisalignedOperand for C operand");
-      return Status::kErrorMisalignedOperand;
-    }
-
-    CUTLASS_TRACE_HOST("  returning kSuccess");
-
+  static Status can_implement(cutlass::gemm::GemmCoord const & problem_size) {
     return Status::kSuccess;
   }
 
   static Status can_implement(Arguments const &args) {
-    return can_implement(args.problem_size);
+    return Status::kSuccess;
   }
 
+  static size_t get_extra_workspace_size(
+    Arguments const &args,
+    cutlass::gemm::GemmCoord const &grid_tiled_shape) {
+
+    return 0;
+  }
+ 
   /// Executes one GEMM
   CUTLASS_DEVICE
   void operator()(Params const &params, SharedStorage &shared_storage) {
 
-    // Compute threadblock location
-    ThreadblockSwizzle threadblock_swizzle;
-
-    cutlass::gemm::GemmCoord threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
-
-    // Early exit if CTA is out of range
-    if (params.grid_tiled_shape.m() <= threadblock_tile_offset.m() ||
-      params.grid_tiled_shape.n() <= threadblock_tile_offset.n()) {
-
-      return;
-    }
-
-    int offset_k = 0;
-    int problem_size_k = params.problem_size.k();
-
-    ElementA *ptr_A = static_cast<ElementA *>(params.ptr_A);
-    ElementB *ptr_B = static_cast<ElementB *>(params.ptr_B);
-
     //
-    // Fetch pointers based on mode.
+    // These types shadow the type-level definitions and support the ability to implement
+    // a 'transposed' GEMM that computes the transposed problems.
     //
-    if (params.mode == GemmUniversalMode::kGemm ||
-      params.mode == GemmUniversalMode::kGemmSplitKParallel) {
-
-      if (threadblock_tile_offset.k() + 1 < params.grid_tiled_shape.k()) {
-
-        problem_size_k = (threadblock_tile_offset.k() + 1) * params.gemm_k_size;
-      }
-
-      offset_k = threadblock_tile_offset.k() * params.gemm_k_size;
-    }
-    else if (params.mode == GemmUniversalMode::kBatched) {
-      ptr_A += threadblock_tile_offset.k() * params.batch_stride_A;
-      ptr_B += threadblock_tile_offset.k() * params.batch_stride_B;
-    }
-    else if (params.mode == GemmUniversalMode::kArray) {
-      ptr_A = static_cast<ElementA * const *>(params.ptr_A)[threadblock_tile_offset.k()];
-      ptr_B = static_cast<ElementB * const *>(params.ptr_B)[threadblock_tile_offset.k()];
-    }
-
-    __syncthreads();
-
-    // Compute initial location in logical coordinates
-    cutlass::MatrixCoord tb_offset_A{
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      offset_k,
-    };
-
-    cutlass::MatrixCoord tb_offset_B{
-      offset_k,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    };
-
-    // Compute position within threadblock
-    int thread_idx = threadIdx.x;
-
-    // Construct iterators to A and B operands
-    typename Mma::IteratorA iterator_A(
-      params.params_A,
-      ptr_A,
-      {params.problem_size.m(), problem_size_k},
-      thread_idx,
-      tb_offset_A,
-      params.ptr_gather_A_indices);
-
-    typename Mma::IteratorB iterator_B(
-      params.params_B,
-      ptr_B,
-      {problem_size_k, params.problem_size.n()},
-      thread_idx,
-      tb_offset_B,
-      params.ptr_gather_B_indices);
-
-    // Broadcast the warp_id computed by lane 0 to ensure dependent code
-    // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-
-    int lane_idx = threadIdx.x % 32;
+    using ElementA = typename Mma::IteratorA::Element;
+    using LayoutA = typename Mma::IteratorA::Layout;
+    using ElementB = typename Mma::IteratorB::Element;
+    using LayoutB = typename Mma::IteratorB::Layout;
+    using ElementC = typename EpilogueVisitor::ElementOutput;
+    using LayoutC = typename Mma::LayoutC;
+
+    //
+    // Problem visitor.
+    //
+    ProblemVisitor problem_visitor(
+      params.problem_visitor,
+      shared_storage.problem_visitor,
+      blockIdx.x);
+
+    // Outer 'persistent' loop to iterate over tiles
+    while (problem_visitor.next_tile()) {
+
+      GemmCoord problem_size = problem_visitor.problem_size();
+      int32_t problem_idx    = problem_visitor.problem_index();
+      int32_t threadblock_idx        = int32_t(problem_visitor.threadblock_idx());
+
+      GemmCoord grid_shape = problem_visitor.grid_shape(problem_size);
+
+      cutlass::gemm::GemmCoord threadblock_offset(
+        int(threadblock_idx / grid_shape.n()) * Mma::Shape::kM,
+        int(threadblock_idx % grid_shape.n()) * Mma::Shape::kN,
+        0);
+
+      // Load element pointers. Exchange pointers and strides if working on the transpose
+      ElementA *ptr_A = reinterpret_cast<ElementA *>((kTransposed ? params.ptr_B[problem_idx] : params.ptr_A[problem_idx]));
+      typename LayoutA::LongIndex ldm_A = (kTransposed ? params.ldb[problem_idx] : params.lda[problem_idx]);
+
+      ElementB *ptr_B = reinterpret_cast<ElementB *>((kTransposed ? params.ptr_A[problem_idx] : params.ptr_B[problem_idx]));
+      typename LayoutB::LongIndex ldm_B = (kTransposed ? params.lda[problem_idx] : params.ldb[problem_idx]);
+
+      // Compute initial location in logical coordinates
+      cutlass::MatrixCoord tb_offset_A{
+        threadblock_offset.m(),
+        0,
+      };
+
+      cutlass::MatrixCoord tb_offset_B{
+        0,
+        threadblock_offset.n()
+      };
+
+      // Compute position within threadblock
+      int thread_idx = threadIdx.x;
+
+      // Construct iterators to A and B operands
+      typename Mma::IteratorA iterator_A(
+        LayoutA(ldm_A),
+        ptr_A,
+        {problem_size.m(), problem_size.k()},
+        thread_idx,
+        tb_offset_A);
 
-    //
-    // Main loop
-    //
+      typename Mma::IteratorB iterator_B(
+        LayoutB(ldm_B),
+        ptr_B,
+        {problem_size.k(), problem_size.n()},
+        thread_idx,
+        tb_offset_B);
 
-    // Construct thread-scoped matrix multiply
-    Mma mma(shared_storage.main_loop, thread_idx, warp_idx, lane_idx);
+      typename Mma::FragmentC accumulators;
 
-    typename Mma::FragmentC accumulators;
+      accumulators.clear();
+      
+      // Broadcast the warp_id computed by lane 0 to ensure dependent code
+      // is compiled as warp-uniform.
+      int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
 
-    accumulators.clear();
+      int lane_idx = threadIdx.x % 32;
 
-    // Compute threadblock-scoped matrix multiply-add
-    int gemm_k_iterations = (problem_size_k - offset_k + Mma::Shape::kK - 1) / Mma::Shape::kK;
-
-    // Compute threadblock-scoped matrix multiply-add
-    mma(
-      gemm_k_iterations,
-      accumulators,
-      iterator_A,
-      iterator_B,
-      accumulators);
+      //
+      // Matrix multiply phase
+      //
 
-    //
-    // Epilogue
-    //
+      // Construct thread-scoped matrix multiply
+      Mma mma(shared_storage.kernel.main_loop, thread_idx, warp_idx, lane_idx);
 
-    // EpilogueOutputOp output_op(params.output_op);
+      // Compute threadblock-scoped matrix multiply-add
+      int gemm_k_iterations = (problem_size.k() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
-    //
-    // Masked tile iterators constructed from members
-    //
+      // Wait for all threads to finish their epilogue phases from the previous tile.
+      __syncthreads();
 
-    threadblock_tile_offset = threadblock_swizzle.get_tile_offset(params.swizzle_log_tile);
+      // Compute threadblock-scoped matrix multiply-add
+      mma(
+        gemm_k_iterations, 
+        accumulators, 
+        iterator_A, 
+        iterator_B, 
+        accumulators);
 
-    //assume identity swizzle
-    MatrixCoord threadblock_offset(
-      threadblock_tile_offset.m() * Mma::Shape::kM,
-      threadblock_tile_offset.n() * Mma::Shape::kN
-    );
+      ElementC *ptr_C = params.ptr_C[problem_idx];
+      ElementC *ptr_D = params.ptr_D[problem_idx];
 
-    int block_idx = threadblock_tile_offset.m() + threadblock_tile_offset.n() * params.grid_tiled_shape.m();
+      ElementNorm *ptr_Max = params.ptr_Max[problem_idx];
+      ElementSum *ptr_Sum = params.ptr_Sum[problem_idx];
 
-    ElementC *ptr_C = static_cast<ElementC *>(params.ptr_C);
-    ElementC *ptr_D = static_cast<ElementC *>(params.ptr_D);
+      LayoutC layout_C(params.ldc[problem_idx]);
+      LayoutC layout_D(params.ldd[problem_idx]);
 
-    //
-    // Fetch pointers based on mode.
-    //
+      int column_offset = (threadblock_offset.n() / ThreadblockShape::kN) * problem_size.m();
 
-    // Construct the semaphore.
-    Semaphore semaphore(params.semaphore + block_idx, thread_idx);
+      typename EpilogueVisitor::OutputTileIterator::Params params_C(layout_C);
+      typename EpilogueVisitor::OutputTileIterator::Params params_D(layout_D);
 
-    // Tile iterator loading from source tensor.
+      //
+      // Construct the epilogue visitor
+      //
 
-    EpilogueVisitor epilogue_visitor(
+      EpilogueVisitor epilogue_visitor(
         params.epilogue_visitor,
-        shared_storage.visitor,
-        threadblock_offset,
-        threadblock_tile_offset,
+        shared_storage.kernel.epilogue.visitor,
+        problem_size.mn(),
         thread_idx,
-        params.problem_size.mn()
-    );
-
-    if (params.mode == GemmUniversalMode::kBatched || params.mode == GemmUniversalMode::kArray) {
-      epilogue_visitor.set_batch_index(threadblock_tile_offset.k());
-    }
-
-    Epilogue epilogue(
-      shared_storage.epilogue,
-      thread_idx,
-      warp_idx,
-      lane_idx);
-
-    // Wait on the semaphore - this latency may have been covered by iterator construction
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-
-      // For subsequent threadblocks, the source matrix is held in the 'D' tensor.
-      semaphore.wait(threadblock_tile_offset.k());
-    }
-
-
-    // Execute the epilogue operator to update the destination tensor.
-    epilogue(epilogue_visitor, accumulators);
-
-    //
-    // Release the semaphore
-    //
-
-    if (params.mode == GemmUniversalMode::kGemm && params.grid_tiled_shape.k() > 1) {
-
-      int lock = 0;
-      if (params.grid_tiled_shape.k() == threadblock_tile_offset.k() + 1) {
+        warp_idx,
+        lane_idx,
+        params_C,
+        params_D,
+        ptr_C,
+        ptr_D,
+        ptr_Max,
+        ptr_Sum,
+        threadblock_offset.mn(),
+        column_offset,
+        params.problem_sizes_real[problem_idx].mn()
+      );
+
+      // Construct the epilogue
+      Epilogue epilogue(
+        shared_storage.kernel.epilogue.epilogue, 
+        thread_idx, 
+        warp_idx, 
+        lane_idx);
 
-        // The final threadblock resets the semaphore for subsequent grids.
-        lock = 0;
-      }
-      else {
-        // Otherwise, the semaphore is incremented
-        lock = threadblock_tile_offset.k() + 1;
-      }
+      // Execute the epilogue operator to update the destination tensor
+      epilogue(epilogue_visitor, accumulators);
 
-      semaphore.release(lock);
+      // Next tile
+      problem_visitor.advance(gridDim.x);
     }
   }
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace kernel
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/gemm/host.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/layout.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/matrix.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/layout/tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/swizzling.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -34,27 +34,19 @@
 #pragma once
 #include <pybind11/pybind11.h>
 #include <pybind11/stl_bind.h>
 
 #include "cutlass/gemm/threadblock/threadblock_swizzle.h"
 #include "cutlass/conv/threadblock/threadblock_swizzle.h"
 
-#include <cxxabi.h>
+#include <boost/core/demangle.hpp>
 #include <cuda_runtime.h>
 
 namespace py = pybind11;
 
-std::string demangle(const char* mangled_name) {
-    std::size_t len = 0;
-    int status = 0;
-    std::unique_ptr<char> ptr(
-                __cxxabiv1::__cxa_demangle(mangled_name, nullptr, &len, &status));
-    return ptr.get();
-}
-
 template<typename T>
 void bind_identity_swizzle(py::module & m, std::string name) {
     py::class_<T>(m, name.c_str(),
         R"pbdoc(Threadblock swizzling function for GEMMs)pbdoc")
         .def(py::init<>())
         .def("get_tiled_shape",
             py::overload_cast<cutlass::gemm::GemmCoord, cutlass::gemm::GemmCoord, int>(
@@ -79,20 +71,21 @@
                 &T::get_tiled_shape, py::const_
             ), py::arg("conv_operator"), py::arg("problem_size"), py::arg("tile_size"), py::arg("split_k_slices"),
             R"pbdoc(Returns the shape of the problem in units of logical tiles
             
             :param problem_size: Implicit gemm problem size conv_operator(NZPQK, NDHWC, KTRSC)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
+        // TODO: the returned dim3 is not usable in python
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+            return boost::core::demangle(typeid(T).name());
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_swizzle(py::module & m, std::string name, std::string doc) {
     py::class_<T>(m, name.c_str(), doc.c_str())
         .def(py::init<>())
         .def("get_tiled_shape",
@@ -104,16 +97,16 @@
             :param problem_size: gemm(M, N, K)
             :type problem_size: :class:`cutlass.gemm.GemmCoord`
             )pbdoc")
         .def("get_grid_shape", &T::get_grid_shape,
             py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+            return boost::core::demangle(typeid(T).name());
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 template<typename T>
 void bind_dgrad_swizzle(py::module & m, std::string name) {
     py::class_<T>(m, name.c_str(),
         R"pbdoc(Threadblock swizzling function for strided dgrad convolution)pbdoc")
         .def(py::init<>())
@@ -127,16 +120,16 @@
             :type problem_size: :class:`cutlass.gemm.GemmCoord`)
             )pbdoc")
         .def("get_grid_shape", [](const T & swizzle, cutlass::gemm::GemmCoord tiled_shape) {
             return dim3(tiled_shape.m(), tiled_shape.n(), tiled_shape.k());
         }, py::arg("tiled_shape"), 
             R"pbdoc(Computes CUDA grid dimensions given a size in units of logical tiles)pbdoc")
         .def("tag", [](const T & swizzle){
-            return demangle(typeid(T).name());
-        }, R"pbdoc(Returns the c++ name of the swizzling for code emission)pbdoc");
+            return boost::core::demangle(typeid(T).name());
+        }, R"pbdoc(Returns the c++ name of the swizzling for code emittion)pbdoc");
 }
 
 void bind_threadblock_swizzle(py::module &m) {
 
     py::class_<dim3>(m, "dim3",
         R"pbdoc(A int3 type xyz contains three integers)pbdoc")
         .def(py::init<int, int, int>(),
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_coord.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/tensor_ref_view.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -16,15 +16,15 @@
  * contributors may be used to endorse or promote products derived from
  * this software without specific prior written permission.
  *
  * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
  * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
  * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
  * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
+ * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSE<cutlass::TensorRef<QUENTIAL
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/include/types.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/library.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/conv_problems.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/convolution.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/conv/host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/scripts/pycutlass/src/cpp/test/gemm/host.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv2d_operation.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -32,20 +32,17 @@
   \brief Defines operations for all CONV operation kinds in CUTLASS Library.
 */
 
 #pragma once
 #include <iostream>
 #include "cutlass/cutlass.h"
 #include "cutlass/conv/kernel/default_conv2d_fprop.h"
-#include "cutlass/conv/kernel/default_conv2d_group_fprop.h"
-#include "cutlass/conv/kernel/default_depthwise_fprop.h"
 #include "cutlass/conv/kernel/default_conv2d_dgrad.h"
 #include "cutlass/conv/kernel/default_conv2d_wgrad.h"
 #include "cutlass/conv/device/implicit_gemm_convolution.h"
-#include "cutlass/conv/device/direct_convolution.h"
 
 #include "cutlass/library/library.h"
 #include "library_internal.h"
 #include "cutlass/util/host_tensor.h"
 
 #include "cutlass/util/reference/host/convolution.h"
 #include "cutlass/util/reference/host/tensor_compare.h"
@@ -97,17 +94,17 @@
       Operator::ThreadblockShape::kM,
       Operator::ThreadblockShape::kN,
       Operator::ThreadblockShape::kK);
 
     description_.tile_description.threadblock_stages = Operator::kStages;
 
     description_.tile_description.warp_count = make_Coord(
-      Operator::UnderlyingKernel::WarpCount::kM,
-      Operator::UnderlyingKernel::WarpCount::kN,
-      Operator::UnderlyingKernel::WarpCount::kK);
+      Operator::ImplicitGemmKernel::WarpCount::kM,
+      Operator::ImplicitGemmKernel::WarpCount::kN,
+      Operator::ImplicitGemmKernel::WarpCount::kK);
     
     description_.tile_description.math_instruction.instruction_shape = make_Coord(
       Operator::InstructionShape::kM,
       Operator::InstructionShape::kN,
       Operator::InstructionShape::kK);
 
     description_.tile_description.math_instruction.element_accumulator = 
@@ -354,267 +351,15 @@
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
     std::cout << "Conv2dOperation::OperatorArguments" << std::endl
               << "  problem_size:" << std::endl 
               << operator_args.problem_size << std::endl
               << "  split_k_mode: "
               << (operator_args.split_k_mode == cutlass::conv::SplitKMode::kSerial ? "serial" : "parallel") << std::endl
-              << "  epilogue (alpha, beta): "
-              << operator_args.output_op.alpha << ", " 
-              << operator_args.output_op.beta << std::endl
-              << "  ref_A (ptr, {stride}): " 
-              << operator_args.ref_A.data() << ", {"
-              << operator_args.ref_A.stride(0) << ", " 
-              << operator_args.ref_A.stride(1) << ", " 
-              << operator_args.ref_A.stride(2) << "}" << std::endl
-              << "  ref_B (ptr, {stride}): " 
-              << operator_args.ref_B.data() << ", {"
-              << operator_args.ref_B.stride(0) << ", " 
-              << operator_args.ref_B.stride(1) << ", " 
-              << operator_args.ref_B.stride(2) << "}" << std::endl
-              << "  ref_C (ptr, {stride}): "
-              << operator_args.ref_C.data() << ", {"
-              << operator_args.ref_C.stride(0) << ", "
-              << operator_args.ref_C.stride(1) << ", " 
-              << operator_args.ref_C.stride(2) << "}" << std::endl
-              << "  ref_D (ptr, {stride}): "
-              << operator_args.ref_D.data() << ", {"
-              << operator_args.ref_D.stride(0) << ", "
-              << operator_args.ref_D.stride(1) << ", " 
-              << operator_args.ref_D.stride(2) << "}" << std::endl;
-  } 
-};
-
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
-//
-// DirectConv2d library operation class for cutlass profiler
-//
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-template <typename Operator_>
-class DirectConv2dOperation : public Conv2dOperation<Operator_> {
-public:
-
-  using Operator = Operator_;
-  using Base = Conv2dOperation<Operator_>;
-
-  using ElementA = typename Operator::ElementA;
-  using LayoutA = typename Operator::LayoutA;
-  using ElementB = typename Operator::ElementB;
-  using LayoutB = typename Operator::LayoutB;
-  using ElementC = typename Operator::ElementC;
-  using LayoutC = typename Operator::LayoutC;
-  using ElementAccumulator = typename Operator::ElementAccumulator;
-  using ElementCompute = typename Operator::EpilogueOutputOp::ElementCompute;
-  static cutlass::conv::Operator const kConvolutionalOperator = Operator::kConvolutionalOperator;
-
-  using OperatorArguments = typename Operator::Arguments;
-
-public:
-    /// Constructor
-  DirectConv2dOperation(char const *name = "unknown_direct)conv2d_fprop") : Conv2dOperation<Operator_>(name) {
-    this->description_.conv_kind = ConvKindMap<kConvolutionalOperator>::kId;
-  }
-
-protected:
-
-  /// Constructs the arguments structure given the configuration and arguments
-  static Status construct_arguments_(
-    OperatorArguments &operator_args,
-    Conv2dConfiguration const *configuration) {
-
-
-    operator_args.problem_size = configuration->problem_size;
-
-    operator_args.ref_A = 
-    {
-      nullptr, 
-      LayoutA::packed(implicit_gemm_tensor_a_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_B = 
-    {
-      nullptr, 
-      LayoutB::packed(implicit_gemm_tensor_b_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_reordered_B = 
-    {
-      nullptr, 
-      LayoutB::packed(implicit_gemm_tensor_b_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_C = 
-    {
-      nullptr, 
-      LayoutC::packed(implicit_gemm_tensor_c_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-    
-    operator_args.ref_D = 
-    {
-      nullptr, 
-      LayoutC::packed(implicit_gemm_tensor_c_extent(kConvolutionalOperator, configuration->problem_size))
-    };
-
-    operator_args.split_k_mode = configuration->split_k_mode;
-
-    return Status::kSuccess;
-  }
-
-  /// Constructs the arguments structure given the configuration and arguments
-  static Status update_arguments_(
-    OperatorArguments &operator_args,
-    ConvArguments const *arguments) {
-
-    if (arguments->pointer_mode == ScalarPointerMode::kHost) {
-      typename Operator::EpilogueOutputOp::Params params(
-        *static_cast<ElementCompute const *>(arguments->alpha),
-        *static_cast<ElementCompute const *>(arguments->beta)
-      );
-      operator_args.output_op = params;
-    }
-    else if (arguments->pointer_mode == ScalarPointerMode::kDevice){
-      typename Operator::EpilogueOutputOp::Params params(
-        static_cast<ElementCompute const *>(arguments->alpha),
-        static_cast<ElementCompute const *>(arguments->beta)
-      );
-      operator_args.output_op = params; 
-    }
-    else {
-      return Status::kErrorInvalidProblem;
-    }
-
-    operator_args.ref_A.reset(static_cast<ElementA *>(const_cast<void *>(arguments->A)));
-    operator_args.ref_B.reset(static_cast<ElementB *>(const_cast<void *>(arguments->B)));
-    operator_args.ref_C.reset(static_cast<ElementC *>(const_cast<void *>(arguments->C)));
-    operator_args.ref_D.reset(static_cast<ElementC *>(const_cast<void *>(arguments->D)));
-    operator_args.ref_reordered_B.reset(static_cast<ElementC *>(const_cast<void *>(arguments->reordered_B)));
-
-    return Status::kSuccess;
-  }
-
-public:
-
-  /// Returns success if the operation can proceed
-  virtual Status can_implement(
-    void const *configuration_ptr, 
-    void const *arguments_ptr) const {
-
-    Conv2dConfiguration const *configuration = 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr);
-
-    ConvArguments const *arguments = 
-      static_cast<ConvArguments const *>(arguments_ptr);
-
-    OperatorArguments args;
-
-    Status status = construct_arguments_(args, configuration);
-
-    if (status != Status::kSuccess) {
-      return status;
-    }
-
-    status = update_arguments_(args, arguments);
-
-    if (status != Status::kSuccess) {
-      return status;
-    }
-
-    return Operator::can_implement(args);
-
-  }
-  
-  /// Gets the host-side workspace
-  virtual uint64_t get_host_workspace_size(
-    void const *configuration) const {
-
-    return sizeof(Operator);
-  }
-  
-  /// Gets the device-side workspace
-  virtual uint64_t get_device_workspace_size(
-    void const *configuration_ptr,
-    void const *arguments_ptr = nullptr) const {
-
-    OperatorArguments args;
-
-    Status status = construct_arguments_(
-      args, 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr));
-
-    if (status != Status::kSuccess) {
-      return 0;
-    }
-
-    return Operator::get_workspace_size(args);
-  }
-  
-  /// Initializes the workspace
-  virtual Status initialize(
-    void const *configuration_ptr, 
-    void *host_workspace, 
-    void *device_workspace, 
-    cudaStream_t stream = nullptr) const {
-
-    OperatorArguments args;
-
-    Status status = construct_arguments_(
-      args, 
-      static_cast<Conv2dConfiguration const *>(configuration_ptr));
-
-    if (status != Status::kSuccess) {
-      return status;
-    }
-
-    Operator *op = new (host_workspace) Operator;
-    //std::cout << "initialize library::Conv2dOperation" << std::endl;
-    //print_operator_args(args);
-    return op->initialize(args, device_workspace, stream);
-
-  }
-
-  /// Runs the kernel
-  virtual Status run(
-    void const *arguments_ptr,
-    void *host_workspace, 
-    void *device_workspace = nullptr, 
-    cudaStream_t stream = nullptr) const {
-
-    OperatorArguments args;
-
-    Status status = update_arguments_(
-      args, 
-      static_cast<ConvArguments const *>(arguments_ptr));
-
-    if (status != Status::kSuccess) {
-      return status;
-    }
-
-    Operator *op = static_cast<Operator *>(host_workspace);
-
-    status = op->update(args, device_workspace);
-
-    if (status != Status::kSuccess) {
-      return status;
-    }
-    //std::cout << "run library::Conv2dOperation" << std::endl;
-    //print_operator_args(args);
-    return op->run(stream);
-  }
-
-  /// Call print_operator_args  from the Conv2dOperation::initialize()
-  // to dump arguments passed on to cutlass operator for debugging
-  void print_operator_args(OperatorArguments &operator_args) const {
-    std::cout << "Conv2dOperation::OperatorArguments" << std::endl
-              << "  problem_size:" << std::endl 
-              << operator_args.problem_size << std::endl
-              << "  split_k_mode: "
-              << (operator_args.split_k_mode == cutlass::conv::SplitKMode::kSerial ? "serial" : "parallel") << std::endl
-              << "  epilogue (alpha, beta): "
+              << "  epilouge (alpha, beta): "
               << operator_args.output_op.alpha << ", " 
               << operator_args.output_op.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
               << operator_args.ref_A.data() << ", {"
               << operator_args.ref_A.stride(0) << ", " 
               << operator_args.ref_A.stride(1) << ", " 
               << operator_args.ref_A.stride(2) << "}" << std::endl
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/conv3d_operation.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -94,17 +94,17 @@
       Operator::ThreadblockShape::kM,
       Operator::ThreadblockShape::kN,
       Operator::ThreadblockShape::kK);
 
     description_.tile_description.threadblock_stages = Operator::kStages;
 
     description_.tile_description.warp_count = make_Coord(
-      Operator::UnderlyingKernel::WarpCount::kM,
-      Operator::UnderlyingKernel::WarpCount::kN,
-      Operator::UnderlyingKernel::WarpCount::kK);
+      Operator::ImplicitGemmKernel::WarpCount::kM,
+      Operator::ImplicitGemmKernel::WarpCount::kN,
+      Operator::ImplicitGemmKernel::WarpCount::kK);
     
     description_.tile_description.math_instruction.instruction_shape = make_Coord(
       Operator::InstructionShape::kM,
       Operator::InstructionShape::kN,
       Operator::InstructionShape::kK);
 
     description_.tile_description.math_instruction.element_accumulator = 
@@ -345,15 +345,15 @@
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
     std::cout << "Conv3dOperation::OperatorArguments" << std::endl
               << "  problem_size: " 
               << operator_args.problem_size << std::endl
               << "  split_k_mode: "
               << (operator_args.split_k_mode == cutlass::conv::SplitKMode::kSerial ? "serial" : "parallel") << std::endl
-              << "  epilogue (alpha, beta): "
+              << "  epilouge (alpha, beta): " 
               << operator_args.output_op.alpha << ", " 
               << operator_args.output_op.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
               << operator_args.ref_A.data() << ", {"
               << operator_args.ref_A.stride(0) << ", " 
               << operator_args.ref_A.stride(1) << ", " 
               << operator_args.ref_A.stride(2) << ", "
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/gemm_operation.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -301,15 +301,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args);
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     return op->run(stream);
   }
@@ -503,15 +503,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args);
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     return op->run(stream);
   }
@@ -721,16 +721,16 @@
       static_cast<GemmUniversalArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     Operator *op = static_cast<Operator *>(host_workspace);
-
-    status = op->update(args);
+    
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     status = op->run(stream);
     
@@ -913,38 +913,38 @@
     
     return status;
   }
 
   /// Runs the kernel
   virtual Status run(
     void const *arguments_ptr,
-    void *host_workspace,
-    void *device_workspace = nullptr,
+    void *host_workspace, 
+    void *device_workspace = nullptr, 
     cudaStream_t stream = nullptr) const {
 
     OperatorArguments args;
-
+    
     Status status = update_arguments_(
-      args,
+      args, 
       static_cast<GemmPlanarComplexArguments const *>(arguments_ptr));
 
     if (status != Status::kSuccess) {
       return status;
     }
-
+    
     Operator *op = static_cast<Operator *>(host_workspace);
-
-    status = op->update(args);
+    
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
-
+    
     status = op->run(stream);
-
+    
     return status;
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 template <typename Operator_>
@@ -1130,15 +1130,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     Operator *op = static_cast<Operator *>(host_workspace);
     
-    status = op->update(args);
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
     
     status = op->run(stream);
     
@@ -1332,15 +1332,15 @@
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     Operator *op = static_cast<Operator *>(host_workspace);
 
-    status = op->update(args);
+    status = op->update(args, device_workspace);
 
     if (status != Status::kSuccess) {
       return status;
     }
 
     status = op->run(stream);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/handle.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/handle.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -618,27 +618,21 @@
   if (uint64_t(kHostWorkspaceSize) < host_workspace_size_needed) {
     return cutlass::Status::kErrorNotSupported;
   }
 
   char host_workspace[kHostWorkspaceSize];
 
   GemmUniversalArguments arguments{
-    {M, N, K},
-    batch_count,
     ptr_A,
     ptr_B,
     ptr_C,
     ptr_D,
     alpha,
     beta,
     scalar_pointer_mode_,
-    lda,
-    ldb,
-    ldc,
-    ldd,
     batch_stride_A,
     batch_stride_B,
     batch_stride_C,
     batch_stride_D
   };
 
   // Query device workspace size
@@ -904,21 +898,21 @@
   int64_t ldb_imag,                             /// Leading dimension of imaginary part of B matrix
 
   void const * beta,                        /// Pointer to beta scalar
 
   NumericTypeID element_C,                  /// Data type of C and D matrix
 
   void const * const * ptr_C_real,          /// Pointer to array containing pointers to real part of C matrices
-  void const * const * ptr_C_imag,          /// Pointer to array containing pointers to imaginary part of C matrices
+  void const * const * ptr_C_imag,          /// Pointer to array containing poitners to imaginary part of C matrices
 
   int64_t ldc_real,                             /// Leading dimension of real part of C matrix
   int64_t ldc_imag,                             /// Leading dimension of imaginary part of C matrix
 
   void * const * ptr_D_real,                /// Pointer to array containing pointers to real part of D matrices
-  void * const * ptr_D_imag,                /// Pointer to array containing pointers to imaginary part of D matrices
+  void * const * ptr_D_imag,                /// Pointer to array containing poitners to imaginary part of D matrices
 
   int64_t ldd_real,                             /// Leading dimension of real part of D matrix
   int64_t ldd_imag                              /// Leading dimension of imaginary part of D matrix
 ) {
   
   //
   // Find the operation
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/library_internal.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/library_internal.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/manifest.cpp`

 * *Files 5% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/operation_table.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_2k_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -343,15 +343,15 @@
 
   /// Call print_operator_args  from the Conv2dOperation::initialize()
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
     std::cout << "Rank2KOperation::OperatorArguments" << std::endl
               << "  problem_size:" << std::endl 
               << operator_args.problem_size << std::endl
-              << "  epilogue (alpha, beta): "
+              << "  epilouge (alpha, beta): "
               << operator_args.epilogue.alpha << ", " 
               << operator_args.epilogue.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
               << operator_args.ptr_A << ", {"
               << operator_args.lda << "}" << std::endl
               << "  ref_B (ptr, {stride}): " 
               << operator_args.ptr_B << ", {"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/rank_k_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/init_reduction_operations.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_device.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reduction/reduction_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -262,15 +262,15 @@
     std::cout << "Reduction::OperatorArguments" << std::endl
               << "  problem_size: " 
               << operator_args.problem_size << std::endl 
               << "  partitions: " 
               << operator_args.partitions << std::endl 
               << "  partition_stride: " 
               << operator_args.partition_stride << std::endl
-              << "  epilogue (alpha, beta): "
+              << "  epilouge (alpha, beta): " 
               << operator_args.output.alpha << ", " 
               << operator_args.output.beta << std::endl
               << "  workspace (ptr, stride): "
               << operator_args.workspace.data() << ", " 
               << operator_args.workspace.stride(0) << std::endl
               << "  source (ptr, stride): " 
               << operator_args.source.data() << ", "
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv2d.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv3d.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/conv_reference_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -70,15 +70,15 @@
   typename ElementCompute_,
   typename ElementAccumulator_ = ElementCompute_,
   typename ConvertOp_ = NumericConverter<ElementC_, ElementCompute_>,
   typename InnerProductOp_ = multiply_add<ElementAccumulator_>
 >
 struct ConvReferenceDispatcher;
 
-/// Dispatcher for Conv2d (partially specialized for kConvDim == 2)
+/// Dispatcher for Conv2d (partially specialied for kConvDim == 2)
 template <
   Provider kProvider,
   conv::Operator kConvolutionalOperator,
   typename ElementA,
   typename LayoutA,
   typename ElementB,
   typename LayoutB,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/gemm_reference_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/reference/initialize_reference_operations.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/singleton.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/singleton.cu`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,24 +38,30 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
 namespace library {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
+static std::unique_ptr<Singleton> instance;
+
+/////////////////////////////////////////////////////////////////////////////////////////////////
+
 Singleton::Singleton() {
 
   manifest.initialize();
 
   operation_table.append(manifest);
 }
 
 Singleton const & Singleton::get() {
-  static Singleton instance;
-  return instance;
+  if (!instance.get()) {
+    instance.reset(new Singleton);
+  }
+  return *instance.get();
 }
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace library
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/symm_operation.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -349,15 +349,15 @@
 
   /// Call print_operator_args  from the Conv2dOperation::initialize()
   // to dump arguments passed on to cutlass operator for debugging
   void print_operator_args(OperatorArguments &operator_args) const {
     std::cout << "SymmOperation::OperatorArguments" << std::endl
               << "  problem_size:" << std::endl 
               << operator_args.problem_size << std::endl
-              << "  epilogue (alpha, beta): "
+              << "  epilouge (alpha, beta): "
               << operator_args.epilogue.alpha << ", " 
               << operator_args.epilogue.beta << std::endl
               << "  ref_A (ptr, {stride}): " 
               << operator_args.ptr_A << ", {"
               << operator_args.lda << "}" << std::endl
               << "  ref_B (ptr, {stride}): " 
               << operator_args.ptr_B << ", {"
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/trmm_operation.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/library/src/util.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/library/src/util.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -424,15 +424,15 @@
 
 static struct {
   char const *text;
   char const *pretty;
   NumericTypeID enumerant;
 }
 NumericTypeID_enumerants[] = {
-  {"unknown", "<unknown>", NumericTypeID::kUnknown},
+  {"unknown", "<unkown>", NumericTypeID::kUnknown},
   {"void", "Void", NumericTypeID::kVoid},
   {"b1", "B1", NumericTypeID::kB1},
   {"u2", "U2", NumericTypeID::kU2},
   {"u4", "U4", NumericTypeID::kU4},
   {"u8", "U8", NumericTypeID::kU8},
   {"u16", "U16", NumericTypeID::kU16},
   {"u32", "U32", NumericTypeID::kU32},
@@ -461,15 +461,15 @@
   {"cu64", "CU64", NumericTypeID::kCU64},  
   {"cs2", "CS2", NumericTypeID::kCS2},
   {"cs4", "CS4", NumericTypeID::kCS4},
   {"cs8", "CS8", NumericTypeID::kCS8},
   {"cs16", "CS16", NumericTypeID::kCS16},
   {"cs32", "CS32", NumericTypeID::kCS32},
   {"cs64", "CS64", NumericTypeID::kCS64},
-  {"*", "<unknown/enumerate all>", NumericTypeID::kUnknown}
+  {"*", "<unkown/enumerate all>", NumericTypeID::kUnknown}
 };
 
 /// Converts a NumericTypeID enumerant to a string
 char const *to_string(NumericTypeID type, bool pretty) {
 
   for (auto const & possible : NumericTypeID_enumerants) {
     if (type == possible.enumerant) {
@@ -954,15 +954,15 @@
 
 static struct {
   char const *text;
   char const *pretty;
   ConvKind enumerant;
 }
 ConvKind_enumerants[] = {
-  {"unknown", "<unknown>", ConvKind::kUnknown},
+  {"unknown", "<unkown>", ConvKind::kUnknown},
   {"fprop", "<fprop>", ConvKind::kFprop},
   {"dgrad", "<dgrad>", ConvKind::kDgrad},
   {"wgrad", "<wgrad>", ConvKind::kWgrad},
 };
 
 /// Converts a ConvKind enumerant to a string
 char const *to_string(ConvKind type, bool pretty) {
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -63,15 +63,14 @@
       {ArgumentTypeID::kInteger, {"w", "input_w"}, "Input W dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"c", "input_c"}, "Input C dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"k", "filter_k"}, "Filter K dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"r", "filter_r"}, "Filter R dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"s", "filter_s"}, "Filter S dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"p", "output_p"}, "Output P dimension of the Conv2d problem space"},
       {ArgumentTypeID::kInteger, {"q", "output_q"}, "Output Q dimension of the Conv2d problem space"},
-      {ArgumentTypeID::kInteger, {"g", "groups"}, "Number of convolution groups"},
       {ArgumentTypeID::kInteger, {"pad_h"}, "Padding in H direction"},
       {ArgumentTypeID::kInteger, {"pad_w"}, "Padding in W direction"},
       {ArgumentTypeID::kInteger, {"stride_h"}, "Stride in H direction"},
       {ArgumentTypeID::kInteger, {"stride_w"}, "Stride in W direction"},
       {ArgumentTypeID::kInteger, {"dilation_h"}, "Dilation in H direction"},
       {ArgumentTypeID::kInteger, {"dilation_w"}, "Dilation in W direction"},
       {ArgumentTypeID::kTensor, {"Activation"}, "Tensor storing the Activation operand"},
@@ -230,19 +229,14 @@
   }
   
   if (!arg_as_int(problem_.s, "s", problem_space, problem)) {
     // default value
     problem_.s = 3;
   }
 
-  if (!arg_as_int(problem_.groups, "g", problem_space, problem)) {
-    // default value
-    problem_.groups = 1;
-  }
-
   if (!arg_as_int(problem_.pad_h, "pad_h", problem_space, problem)) {
     // default value
     problem_.pad_h = 1;
   }
 
   if (!arg_as_int(problem_.pad_w, "pad_w", problem_space, problem)) {
     // default value
@@ -266,15 +260,15 @@
 
   if (!arg_as_int(problem_.dilation_w, "dilation_w", problem_space, problem)) {
     // default value
     problem_.dilation_w = 1;
   }
 
   ////////////////////////  Convolution output dimensions p and q ////////////////////////
-  // Cutlass convolutions support arbitrary output sizes and not constrained by         //
+  // Cutlass convolutions support arbitrary output sizes and not constriant by          //
   // input, filter, padding, striding, dilation sizes.                                  //
   // cuDNN sets the output dimensions (p, q)  using following equations:                //
   //                                                                                    //
   // output = div_up(input + 2 * pad - ((filter - 1) * dilation + 1) + 1, stride)       //
   // where; div_up(a, b) : (a - 1)/b + 1                                                //
   //                                                                                    //
   // Thus, when output p and q dimensions are unspecified by the user                   //
@@ -384,15 +378,15 @@
                                                 int(problem_.pad_w),
                                                 int(problem_.stride_h),
                                                 int(problem_.stride_w),
                                                 int(problem_.dilation_h),
                                                 int(problem_.dilation_w),
                                                 static_cast<conv::Mode>(static_cast<int>(problem_.conv_mode)),
                                                 int(problem_.split_k_slices),
-                                                int(problem_.groups)
+                                                1 // groups
                                               );
   
   conv_workspace_.configuration.split_k_mode = static_cast<conv::SplitKMode>(static_cast<int>(problem_.split_k_mode));
 
   conv_workspace_.set_stride_vector(
       problem_, operation_desc.conv_kind, operation_desc.A.layout,
       operation_desc.B.layout, operation_desc.C.layout);
@@ -456,16 +450,14 @@
   set_argument(result, "k", problem_space, problem_.k);
   set_argument(result, "r", problem_space, problem_.r);
   set_argument(result, "s", problem_space, problem_.s);
   
   set_argument(result, "p", problem_space, problem_.p);
   set_argument(result, "q", problem_space, problem_.q);
 
-  set_argument(result, "g", problem_space, problem_.groups);
-
   set_argument(result, "pad_h", problem_space, problem_.pad_h);
   set_argument(result, "pad_w", problem_space, problem_.pad_w);
 
   set_argument(result, "stride_h", problem_space, problem_.stride_h);
   set_argument(result, "stride_w", problem_space, problem_.stride_w);
 
   set_argument(result, "dilation_h", problem_space, problem_.dilation_h);
@@ -498,23 +490,23 @@
 
   int64_t output_bytes = int64_t(library::sizeof_bits(operation_desc.output().element) / 8) * 
     conv_workspace_.configuration.problem_size.output_size();
 
   // Bytes of activation, filter, and output tensors
   result.bytes = problem_.bytes(operation_desc);
 
-  // Theoretical flops required for the computation
+  // Theoritical flops required for the computation
   result.flops = problem_.flops(operation_desc);
 
   // Measured runtime
   result.runtime = 0;
 
 }
 
-/// Initialize reduction problem dimensions and library::Operation
+/// Initialize reduction problem dimenstions and library::Operation
 bool Conv2dOperationProfiler::initialize_reduction_configuration_(
   Options const &options,  
   PerformanceReport &report,
   DeviceContext &device_context,
   library::Operation const *operation,
   ProblemSpace const &problem_space,
   ProblemSpace::Problem const &problem) {
@@ -531,15 +523,15 @@
   if (!cast_from_double(problem_.beta_zero, conv_desc.element_epilogue, 0)) {
    return false;
   }
 
   /// This chooses the appropriate stride element of the row-major C tensor.
   int const & tensor_c_stride_idx = (conv_kind == library::ConvKind::kWgrad ? 2 : 0);
 
-  /// initialize library::ReductionConfiguration
+  /// intialize library::ReductionConfiguration
   conv_workspace_.reduction_configuration.problem_size     = problem_.eq_gemm_size(conv_kind).mn();
   conv_workspace_.reduction_configuration.partitions       = int(problem_.split_k_slices);
   conv_workspace_.reduction_configuration.partition_stride = problem_.eq_gemm_size(conv_kind).mn().product();
   conv_workspace_.reduction_configuration.ldw =
       conv_workspace_.configuration.stride_c[tensor_c_stride_idx];
   conv_workspace_.reduction_configuration.lds =
       conv_workspace_.configuration.stride_c[tensor_c_stride_idx];
@@ -628,27 +620,14 @@
       operation_desc.B.element,
       operation_desc.B.layout,
       problem_.extent_b(operation_desc.conv_kind),
       conv_workspace_.configuration.stride_b,
       conv_workspace_.problem_count
     );
 
-    if(problem_.groups == problem_.c && problem_.groups == problem_.k){
-      // Depthwise direct conv kernel needs reorder the filter.
-      conv_workspace_.reordered_B = device_context.allocate_tensor(
-        options,
-        "B",
-        operation_desc.B.element,
-        operation_desc.B.layout,
-        problem_.extent_b(operation_desc.conv_kind),
-        conv_workspace_.configuration.stride_b,
-        conv_workspace_.problem_count
-      );
-    }
-
     conv_workspace_.C = device_context.allocate_tensor(
       options,
       "C",
       operation_desc.C.element,
       operation_desc.C.layout,
       problem_.extent_c(operation_desc.conv_kind),
       conv_workspace_.configuration.stride_c,
@@ -755,29 +734,23 @@
   conv_workspace_.arguments.B = conv_workspace_.B->data();
   conv_workspace_.arguments.C = conv_workspace_.C->data();
   conv_workspace_.arguments.D = conv_workspace_.Computed->data();
   conv_workspace_.arguments.alpha = problem_.alpha.data();
   conv_workspace_.arguments.beta = problem_.beta.data();
   conv_workspace_.arguments.pointer_mode = library::ScalarPointerMode::kHost;
 
-  if (conv_workspace_.reordered_B != nullptr){
-    conv_workspace_.arguments.reordered_B = conv_workspace_.reordered_B->data();
-  }else{
-    conv_workspace_.arguments.reordered_B = nullptr;
-  }
-
   conv_workspace_.Computed->copy_from_device(conv_workspace_.C->data());
   
   if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
     // update library::ConvArguments for parallel split-k reduction
     conv_workspace_.arguments.D = conv_workspace_.device_workspace.data();
     conv_workspace_.arguments.alpha = problem_.alpha_one.data();
     conv_workspace_.arguments.beta = problem_.beta_zero.data();
 
-    /// initialize library::ReductionArguments
+    /// intialize library::ReductionArguments
     conv_workspace_.reduction_arguments.workspace           = conv_workspace_.device_workspace.data();
     conv_workspace_.reduction_arguments.source              = conv_workspace_.C->data();
     conv_workspace_.reduction_arguments.destination         = conv_workspace_.Computed->data();
     conv_workspace_.reduction_arguments.alpha               = problem_.alpha.data();
     conv_workspace_.reduction_arguments.beta                = problem_.beta.data();
     conv_workspace_.reduction_arguments.pointer_mode        = library::ScalarPointerMode::kHost;
   }
@@ -957,15 +930,15 @@
       conv_desc.B.element,
       conv_desc.B.layout,
       conv_desc.C.element,
       conv_desc.C.layout,
       conv_desc.tile_description.math_instruction.element_accumulator, 
       conv_desc.element_epilogue);
 
-#if 0 // debug print to check which host reference instance is selected
+#if 0 // debug print to check which host refererence instance is selected
     std::cout << conv2d_key << "\n";
 #endif
 
     auto operators_it = Singleton::get().operation_table.conv2d_operations.find(conv2d_key);
 
     if(operators_it == Singleton::get().operation_table.conv2d_operations.end()) {
 
@@ -978,15 +951,15 @@
     auto cc_it = operators_it->second.find(preference_key);
     
     if(cc_it == operators_it->second.end()) {
       results_.back().verification_map[library::Provider::kReferenceHost] = Disposition::kNotRun;
       return true;
     }
 
-    // host reference has only one instances in Conv2dOperationVectorMap
+    // host refernce has only one instances in Conv2dOperationVectorMap
     library::Operation const *reference_op = cc_it->second[0];
 
     //
     // Copy input tensors A, B, and C from device to host buffers
     //
     conv_workspace_.host_tensor_a.resize(conv_workspace_.A->bytes());
     conv_workspace_.host_tensor_b.resize(conv_workspace_.B->bytes());
@@ -1005,15 +978,15 @@
     conv_workspace_.arguments.D = conv_workspace_.host_tensor_c.data();
 
     conv_workspace_.arguments.alpha = problem_.alpha.data();
     conv_workspace_.arguments.beta = problem_.beta.data();
     conv_workspace_.arguments.pointer_mode = library::ScalarPointerMode::kHost;
 
     //
-    // Initialize host reference operation
+    // Intialize host reference operation
     //
     std::vector<uint8_t> host_workspace_reference_op;
 
     uint64_t workspace_size = reference_op->get_host_workspace_size(&conv_workspace_.configuration);
     host_workspace_reference_op.resize(workspace_size, 0);
 
     reference_op->initialize(
@@ -1110,19 +1083,19 @@
     
     if(cc_it == operators_it->second.end()) {
       results_.back().verification_map[library::Provider::kReferenceDevice] = Disposition::kNotRun;
 
       return true;
     }
 
-    // device reference has only one instances in Conv2dOperationVectorMap
+    // device refernce has only one instances in Conv2dOperationVectorMap
     library::Operation const *reference_op = cc_it->second[0];
   
     //
-    // Initialize device reference operation
+    // Intialize device reference operation
     //
     std::vector<uint8_t> host_workspace_reference_op;
 
     uint64_t workspace_size = reference_op->get_host_workspace_size(&conv_workspace_.configuration);
     host_workspace_reference_op.resize(workspace_size, 0);
 
     reference_op->initialize(
@@ -1201,15 +1174,15 @@
 
     if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
       // update library::ConvArguments for parallel split-k reduction
       conv_workspace_.arguments.D = conv_workspace_.device_workspace.data();
       conv_workspace_.arguments.alpha = problem_.alpha_one.data();
       conv_workspace_.arguments.beta = problem_.beta_zero.data();
 
-      /// initialize library::ReductionArguments
+      /// intialize library::ReductionArguments
       conv_workspace_.reduction_arguments.workspace           = conv_workspace_.device_workspace.data();
       conv_workspace_.reduction_arguments.source              = conv_workspace_.C->data();
       conv_workspace_.reduction_arguments.destination         = conv_workspace_.Computed->data();
       conv_workspace_.reduction_arguments.alpha               = problem_.alpha.data();
       conv_workspace_.reduction_arguments.beta                = problem_.beta.data();
       conv_workspace_.reduction_arguments.pointer_mode        = library::ScalarPointerMode::kHost;
     }
@@ -1272,15 +1245,15 @@
     conv_arguments->C = conv_workspace_.C->batch_data(problem_idx);
     conv_arguments->D = conv_workspace_.Computed->batch_data(problem_idx);
     
     if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
       // update library::ConvArguments for parallel split-k reduction
       conv_arguments->D = conv_workspace_.device_workspace.data();
 
-      /// initialize library::ReductionArguments
+      /// intialize library::ReductionArguments
       conv_workspace_.reduction_arguments.workspace           = conv_workspace_.device_workspace.data();
       conv_workspace_.reduction_arguments.source              = conv_workspace_.C->batch_data(problem_idx);
       conv_workspace_.reduction_arguments.destination         = conv_workspace_.Computed->batch_data(problem_idx);
     }
 
     // Run underlying conv2d operation
     status = underlying_operation->run(
@@ -1325,15 +1298,15 @@
     conv_arguments->C = conv_workspace_.C->batch_data(problem_idx);
     conv_arguments->D = conv_workspace_.Computed->batch_data(problem_idx);
 
     if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
       // update library::ConvArguments for parallel split-k reduction
       conv_arguments->D = conv_workspace_.device_workspace.data();
 
-      /// initialize library::ReductionArguments
+      /// intialize library::ReductionArguments
       conv_workspace_.reduction_arguments.workspace           = conv_workspace_.device_workspace.data();
       conv_workspace_.reduction_arguments.source              = conv_workspace_.C->batch_data(problem_idx);
       conv_workspace_.reduction_arguments.destination         = conv_workspace_.Computed->batch_data(problem_idx);
     }
 
     // Run underlying conv2d operation
     status = underlying_operation->run(
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv2d_operation_profiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -71,15 +71,14 @@
 class Conv2dOperationProfiler : public OperationProfiler {
 public:
 
   /// Problem structure obtained from problem space
   struct Conv2dProblem {
 
     int64_t n, h, w, c, p, q, k, r, s;
-    int64_t groups;
     int64_t pad_h, pad_w;
     int64_t stride_h, stride_w;
     int64_t dilation_h, dilation_w;
 
     std::vector<uint8_t> alpha;
     std::vector<uint8_t> beta;
 
@@ -111,15 +110,15 @@
       q = ((w + pad_w - s * dilation_w) / stride_w) + 1;
     }
 
     // Returns equivalent gemm problem size for convolution
     cutlass::gemm::GemmCoord eq_gemm_size(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
-        case library::ConvKind::kFprop: return cutlass::gemm::GemmCoord(int(n * p * q), int(k), int(r * s * c / groups));
+        case library::ConvKind::kFprop: return cutlass::gemm::GemmCoord(int(n * p * q), int(k), int(r * s * c));
         case library::ConvKind::kDgrad: return cutlass::gemm::GemmCoord(int(n * h * w), int(c), int(k * r * s));
         case library::ConvKind::kWgrad: return cutlass::gemm::GemmCoord(int(k), int(r * s * c), int(n * p * q));
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
     // Returns extent for tensor A
@@ -133,15 +132,15 @@
       }
     }
 
     // Returns extent for tensor B
     std::vector<int> extent_b(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
-        case library::ConvKind::kFprop: return {int(k), int(r), int(s), int(c / groups)};
+        case library::ConvKind::kFprop: return {int(k), int(r), int(s), int(c)};
         case library::ConvKind::kDgrad: return {int(k), int(r), int(s), int(c)};
         case library::ConvKind::kWgrad: return {int(n), int(h), int(w), int(c)};
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
     // Returns extent for tensor C
@@ -185,37 +184,37 @@
         case library::ConvKind::kFprop:
         case library::ConvKind::kDgrad: 
         case library::ConvKind::kWgrad: return library::LayoutTypeID::kColumnMajor;
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix A
+    // Returns leading dimenstion for equivalent gemm matrix A
     int64_t eq_gemm_lda(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kDgrad: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).m();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix B
+    // Returns leading dimenstion for equivalent gemm matrix B
     int64_t eq_gemm_ldb(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kDgrad: return eq_gemm_size(conv_kind).n();
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).n();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix C
+    // Returns leading dimenstion for equivalent gemm matrix C
     int64_t eq_gemm_ldc(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: 
         case library::ConvKind::kDgrad: 
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).m();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
@@ -225,15 +224,14 @@
 
   /// Workspace used 
   struct Conv2dWorkspace {
 
     /// Conv device allocations
     DeviceAllocation *A;
     DeviceAllocation *B;
-    DeviceAllocation *reordered_B;
     DeviceAllocation *C;
     DeviceAllocation *Computed;
     DeviceAllocation *Reference;
     
     /// Library configuration and arguments for convolution operator
     library::Conv2dConfiguration configuration;
     library::ConvArguments arguments;
@@ -268,15 +266,14 @@
     //
     // Methods
     //
 
     Conv2dWorkspace()
         : A(nullptr),
           B(nullptr),
-          reordered_B(nullptr),
           C(nullptr),
           Computed(nullptr),
           Reference(nullptr) {}
 
     // Set stride vector for tensor activations, filters, output
     void set_stride_vector(Conv2dProblem const &problem,
                            library::ConvKind const &conv_kind,
@@ -316,18 +313,18 @@
       } else {
         // Strides for the rest cases
         stride_activations.push_back(int(problem.c));
         stride_activations.push_back(int(problem.w) * int(problem.c));
         stride_activations.push_back(int(problem.h) * int(problem.w) *
                                      int(problem.c));
 
-        stride_filters.push_back(int(problem.c / problem.groups));
-        stride_filters.push_back(int(problem.s) * int(problem.c / problem.groups));
+        stride_filters.push_back(int(problem.c));
+        stride_filters.push_back(int(problem.s) * int(problem.c));
         stride_filters.push_back(int(problem.r) * int(problem.s) *
-                                 int(problem.c / problem.groups));
+                                 int(problem.c));
 
         stride_output.push_back(int(problem.k));
         stride_output.push_back(int(problem.q) * int(problem.k));
         stride_output.push_back(int(problem.q) * int(problem.p) *
                                 int(problem.k));
       }
 
@@ -432,15 +429,15 @@
     Options const &options,
     library::Operation const *operation,
     void *arguments,
     void *host_workspace,
     void *device_workspace);
  
  
-  /// Initialize reduction problem dimensions and library::Operation
+  /// Initialize reduction problem dimenstions and library::Operation
   bool initialize_reduction_configuration_(
     Options const &options,  
     PerformanceReport &report,
     DeviceContext &device_context,
     library::Operation const *operation,
     ProblemSpace const &problem_space,
     ProblemSpace::Problem const &problem);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.cu`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -280,15 +280,15 @@
 
   if (!arg_as_int(problem_.dilation_w, "dilation_w", problem_space, problem)) {
     // default value
     problem_.dilation_w = 1;
   }
 
   ////////////////////////  Convolution output dimensions p and q ////////////////////////
-  // Cutlass convolutions support arbitrary output sizes and not constrained by         //
+  // Cutlass convolutions support arbitrary output sizes and not constriant by          //
   // input, filter, padding, striding, dilation sizes.                                  //
   // cuDNN sets the output dimensions (p, q)  using following equations:                //
   //                                                                                    //
   // output = div_up(input + 2 * pad - ((filter - 1) * dilation + 1) + 1, stride)       //
   // where; div_up(a, b) : (a - 1)/b + 1                                                //
   //                                                                                    //
   // Thus, when output p and q dimensions are unspecified by the user                   //
@@ -541,23 +541,23 @@
     std::string(library::to_string(problem_.eq_gemm_provider)));
 
   OperationProfiler::initialize_result_(result, operation_desc, problem_space);
 
   // Bytes of activation, filter, and output tensors
   result.bytes = problem_.bytes(operation_desc);
 
-  // Theoretical flops required for the computation
+  // Theoritical flops required for the computation
   result.flops = problem_.flops(operation_desc);
 
   // Measured runtime
   result.runtime = 0;
 
 }
 
-/// Initialize reduction problem dimensions and library::Operation
+/// Initialize reduction problem dimenstions and library::Operation
 bool Conv3dOperationProfiler::initialize_reduction_configuration_(
   Options const &options,  
   PerformanceReport &report,
   DeviceContext &device_context,
   library::Operation const *operation,
   ProblemSpace const &problem_space,
   ProblemSpace::Problem const &problem) {
@@ -574,15 +574,15 @@
   if (!cast_from_double(problem_.beta_zero, conv_desc.element_epilogue, 0)) {
    return false;
   }
 
   /// This chooses the appropriate stride element of the row-major C tensor.
   int const & tensor_c_stride_idx = (conv_kind == library::ConvKind::kWgrad ? 3 : 0);
 
-  /// initialize library::ReductionConfiguration
+  /// intialize library::ReductionConfiguration
   conv_workspace_.reduction_configuration.problem_size     = problem_.eq_gemm_size(conv_kind).mn();
   conv_workspace_.reduction_configuration.partitions       = int(problem_.split_k_slices);
   conv_workspace_.reduction_configuration.partition_stride = problem_.eq_gemm_size(conv_kind).mn().product();
   conv_workspace_.reduction_configuration.ldw              = conv_workspace_.configuration.layout_c(conv_kind).stride()[tensor_c_stride_idx];
   conv_workspace_.reduction_configuration.lds              = conv_workspace_.configuration.layout_c(conv_kind).stride()[tensor_c_stride_idx];
   conv_workspace_.reduction_configuration.ldd              = conv_workspace_.configuration.layout_c(conv_kind).stride()[tensor_c_stride_idx];
 
@@ -943,15 +943,15 @@
     conv_desc.B.element,
     conv_desc.B.layout,
     conv_desc.C.element,
     conv_desc.C.layout,
     conv_desc.tile_description.math_instruction.element_accumulator, 
     conv_desc.element_epilogue);
 
-#if 0 // debug print to check which host reference instance is selected
+#if 0 // debug print to check which host refererence instance is selected
     std::cout << conv_key << "\n";
 #endif
 
   auto operators_it = Singleton::get().operation_table.conv3d_operations.find(conv_key);
 
   if(operators_it == Singleton::get().operation_table.conv3d_operations.end()) {
 
@@ -964,15 +964,15 @@
   auto cc_it = operators_it->second.find(preference_key);
   
   if(cc_it == operators_it->second.end()) {
     results_.back().verification_map[library::Provider::kReferenceHost] = Disposition::kNotRun;
     return true;
   }
 
-  // host reference has only one instances in ConvOperationVectorMap
+  // host refernce has only one instances in ConvOperationVectorMap
   library::Operation const *reference_op = cc_it->second[0];
 
   //
   // Copy input tensors A, B, and C from device to host buffers
   //
   conv_workspace_.host_tensor_a.resize(conv_workspace_.A->bytes());
   conv_workspace_.host_tensor_b.resize(conv_workspace_.B->bytes());
@@ -989,15 +989,15 @@
   conv_workspace_.arguments.C = conv_workspace_.host_tensor_c.data();
   conv_workspace_.arguments.D = conv_workspace_.host_tensor_c.data();
   conv_workspace_.arguments.alpha = problem_.alpha.data();
   conv_workspace_.arguments.beta = problem_.beta.data();
   conv_workspace_.arguments.pointer_mode = library::ScalarPointerMode::kHost;
 
   //
-  // Initialize host reference operation
+  // Intialize host reference operation
   //
   std::vector<uint8_t> host_workspace_reference_op;
 
   uint64_t workspace_size = reference_op->get_host_workspace_size(&conv_workspace_.configuration);
   host_workspace_reference_op.resize(workspace_size, 0);
 
   reference_op->initialize(
@@ -1105,15 +1105,15 @@
 
   if (conv_workspace_.configuration.split_k_mode == conv::SplitKMode::kParallel) {
     // update library::ConvArguments for parallel split-k reduction
     conv_workspace_.arguments.D = conv_workspace_.device_workspace.data();
     conv_workspace_.arguments.alpha = problem_.alpha_one.data();
     conv_workspace_.arguments.beta = problem_.beta_zero.data();
 
-    /// initialize library::ReductionArguments
+    /// intialize library::ReductionArguments
     conv_workspace_.reduction_arguments.workspace           = conv_workspace_.device_workspace.data();
     conv_workspace_.reduction_arguments.source              = conv_workspace_.C->batch_data(problem_idx);
     conv_workspace_.reduction_arguments.destination         = conv_workspace_.Computed->batch_data(problem_idx);
     conv_workspace_.reduction_arguments.alpha               = problem_.alpha.data();
     conv_workspace_.reduction_arguments.beta                = problem_.beta.data();
     conv_workspace_.reduction_arguments.pointer_mode        = library::ScalarPointerMode::kHost;
   }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/conv3d_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -101,15 +101,15 @@
 
     /// Total number of bytes loaded
     int64_t bytes(library::ConvDescription const &operation_desc) const;
 
     /// Total number of flops computed
     int64_t flops(library::ConvDescription const &operation_desc) const;
 
-    /// Infers output size from the input size, padding, stride, and dilation
+    /// Infers output size from theinput size, padding, stride, and dilation
     void set_default_output_size() {
       z = ((d + pad_d - t * dilation_d) / stride_d) + 1;
       p = ((h + pad_h - r * dilation_h) / stride_h) + 1;
       q = ((w + pad_w - s * dilation_w) / stride_w) + 1;
     }
 
     // Returns equivalent gemm problem size for convolution
@@ -186,37 +186,37 @@
         case library::ConvKind::kFprop:
         case library::ConvKind::kDgrad: 
         case library::ConvKind::kWgrad: return library::LayoutTypeID::kColumnMajor;
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix A
+    // Returns leading dimenstion for equivalent gemm matrix A
     int64_t eq_gemm_lda(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kDgrad: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).m();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix B
+    // Returns leading dimenstion for equivalent gemm matrix B
     int64_t eq_gemm_ldb(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: return eq_gemm_size(conv_kind).k();
         case library::ConvKind::kDgrad: return eq_gemm_size(conv_kind).n();
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).n();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
       }
     }
 
-    // Returns leading dimension for equivalent gemm matrix C
+    // Returns leading dimenstion for equivalent gemm matrix C
     int64_t eq_gemm_ldc(library::ConvKind const &conv_kind) const {
 
       switch (conv_kind) {
         case library::ConvKind::kFprop: 
         case library::ConvKind::kDgrad: 
         case library::ConvKind::kWgrad: return eq_gemm_size(conv_kind).m();
         default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
@@ -385,15 +385,15 @@
     double &runtime,
     Options const &options,
     library::Operation const *operation,
     void *arguments,
     void *host_workspace,
     void *device_workspace);
   
-  /// Initialize reduction problem dimensions and library::Operation
+  /// Initialize reduction problem dimenstions and library::Operation
   bool initialize_reduction_configuration_(
     Options const &options,  
     PerformanceReport &report,
     DeviceContext &device_context,
     library::Operation const *operation,
     ProblemSpace const &problem_space,
     ProblemSpace::Problem const &problem);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -53,15 +53,15 @@
     case CUBLAS_STATUS_NOT_SUPPORTED:
       return Status::kErrorNotSupported;
     default: break;
   }
   return Status::kErrorInternal;
 }
 
-/// Converts a cuBLAS status to cutlass::profiler::Disposition
+/// Converts a cuBLASS status to cutlass::profiler::Disposition
 Disposition get_cutlass_disposition(cublasStatus_t cublas_status) {
 
   if (cublas_status == CUBLAS_STATUS_INVALID_VALUE) {
     return Disposition::kInvalidProblem;
   }
   else if (cublas_status == CUBLAS_STATUS_NOT_SUPPORTED) {
     return Disposition::kNotSupported;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cublas_helpers.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -50,15 +50,15 @@
 namespace profiler {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Converts a cuBLAS status to cutlass::Status
 Status get_cutlass_status(cublasStatus_t cublas);
 
-/// Converts a cuBLAS status to cutlass::profiler::Disposition
+/// Converts a cuBLASS status to cutlass::profiler::Disposition
 Disposition get_cutlass_disposition(cublasStatus_t cublas_status);
 
 /// Maps a CUTLASS tensor layout to a cuBLAS transpose operation
 bool get_cublas_transpose_operation(
   cublasOperation_t &operation,
   library::LayoutTypeID layout,
   library::ComplexTransform transform = library::ComplexTransform::kNone);
@@ -83,15 +83,15 @@
 Status cublas_satisfies(library::TrmmDescription const &desc);
 
 /// Returns a status if cuBLAS can satisfy a particular SYMM/HEMM description
 Status cublas_satisfies(library::SymmDescription const &desc);
 
 /// This is a helper class to create cublasHandle_t automatically on CublasCreate object creation and 
 /// to destroy cublasHandle_t on CublasCreate object destruction. 
-/// Additionally, it provides implicit cast from CublasCreate's object to cublasHandle_t's object
+/// Additionaly, it provides implicit cast from CublasCreate's object to cublasHandle_t's object
 class CublasCreate {
 private:
 	cublasHandle_t handle;
 	cublasStatus_t status;
 
 public:
 	CublasCreate() {
@@ -192,15 +192,15 @@
 
   //
   // Data members
   //
   library::GemmUniversalConfiguration configuration;
   library::GemmUniversalArguments arguments;
 
-  // cublas-specific data structures to fill cublas API call arguments
+  // cublass-specific data structures to fill cublas API call arguments
   cublasOperation_t trans_A;
   cublasOperation_t trans_B;
   cudaDataType_t data_type_A;
   cudaDataType_t data_type_B;
   cudaDataType_t data_type_C;
   cudaDataType_t compute_data_type;
 
@@ -233,15 +233,15 @@
 
   //
   // Data members
   //
   library::RankKConfiguration configuration;
   library::RankKArguments arguments;
 
-  // cublas-specific data structures to fill cublas API call arguments
+  // cublass-specific data structures to fill cublas API call arguments
   cublasOperation_t trans_A;
   cublasFillMode_t uplo;
   cudaDataType_t data_type_A;
   cudaDataType_t data_type_C;
   cudaDataType_t compute_data_type;
 
 #if (__CUDACC_VER_MAJOR__ >= 11)
@@ -273,15 +273,15 @@
 
   //
   // Data members
   //
   library::TrmmConfiguration configuration;
   library::TrmmArguments arguments;
 
-  // cublas-specific data structures to fill cublas API call arguments
+  // cublass-specific data structures to fill cublas API call arguments
   cublasOperation_t trans_A;
   cublasSideMode_t side;
   cublasFillMode_t uplo;
   cublasDiagType_t diag;
   cudaDataType_t data_type_A;
   cudaDataType_t data_type_B;
   cudaDataType_t data_type_D;
@@ -314,15 +314,15 @@
 
   //
   // Data members
   //
   library::SymmConfiguration configuration;
   library::SymmArguments arguments;
 
-  // cublas-specific data structures to fill cublas API call arguments
+  // cublass-specific data structures to fill cublas API call arguments
   cublasSideMode_t side;
   cublasFillMode_t uplo;
   cudaDataType_t data_type_A;
   cudaDataType_t data_type_B;
   cudaDataType_t data_type_C;
   cudaDataType_t compute_data_type;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.cpp`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -64,15 +64,15 @@
   }
   else if (cudnn_status == CUDNN_STATUS_NOT_SUPPORTED) {
     return Disposition::kNotSupported;
   }
   return Disposition::kFailed;
 }
 
-/// Checks cudnnStatus_t converts to cutlass status and returns if Status::kSuccess o.w. throws exception
+/// Checks cudnnStatus_t converts to cutlas status and returns if Status::kSuccess o.w. throws exception
 Status checkCudnnErr(cudnnStatus_t cudnn_status) {
   Status cutlass_status = get_cutlass_status(cudnn_status);
   if(cutlass_status != Status::kSuccess) {
     throw std::runtime_error("checkCudnnErr failed");
   }
   return cutlass_status;
 }
@@ -191,20 +191,15 @@
       {
         cudnn_math_type = CUDNN_TENSOR_OP_MATH_ALLOW_CONVERSION;
       }
 
       return true;
     }
     case library::OpcodeClassID::kSimt:
-      #if (defined(CUDNN_VERSION) && CUDNN_VERSION <= 8000)
-        cudnn_math_type = CUDNN_DEFAULT_MATH;
-      #else
-        cudnn_math_type = CUDNN_FMA_MATH;
-      #endif
-      return true;
+      return false;
   }
 
   return false;
 }
 
 /// Cudnn compute type seems to be hardcoded to float (To handle a possible cudnn issue)
 float cast_cudnn_compute_type_to_float(library::NumericTypeID type, void const * src) {
@@ -239,15 +234,15 @@
   auto const &math_instruction = desc.tile_description.math_instruction;
 
   if(a_tensor.element != b_tensor.element) {
     return Status::kErrorInvalidDataType;
   }
 
   ////////////////////////  Convolution output dimensions p and q ///////////////////////
-  // Cutlass convolutions support arbitrary output dimensions and not constrained by   //
+  // Cutlass convolutions support arbitrary output dimensions and not constriant by    //
   // input, filter, padding, striding, dilation sizes.                                 //
   // cuDNN sets the output dimensions (p, q) using following equations:                //
   //                                                                                   //
   // output = div_up(input + 2 * pad - ((filter - 1) * dilation + 1) + 1, stride)      //
   // where; div_up(a, b) : (a - 1)/b + 1                                               //
   //                                                                                   //
   // Before launching cudnn verification or profiling check that output p and q        //
@@ -369,15 +364,15 @@
   auto const &math_instruction = desc.tile_description.math_instruction;
 
   if(a_tensor.element != b_tensor.element) {
     return Status::kErrorInvalidDataType;
   }
 
   ////////////////////////  Convolution output dimensions p and q ///////////////////////
-  // Cutlass convolutions support arbitrary output dimensions and not constrained by   //
+  // Cutlass convolutions support arbitrary output dimensions and not constriant by    //
   // input, filter, padding, striding, dilation sizes.                                 //
   // cuDNN sets the output dimensions (p, q) using following equations:                //
   //                                                                                   //
   // output = div_up(input + 2 * pad - ((filter - 1) * dilation + 1) + 1, stride)      //
   // where; div_up(a, b) : (a - 1)/b + 1                                               //
   //                                                                                   //
   // Before launching cudnn verification or profiling check that output p and q        //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cudnn_helpers.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -51,15 +51,15 @@
 /////////////////////////////////////////////////////////////////////////////////////////////////
 /// Converts a cuDNN status to cutlass::Status
 Status get_cutlass_status(cudnnStatus_t cudnn_status);
 
 /// Converts a cuDNN status to cutlass::profiler::Disposition
 Disposition get_cutlass_disposition(cudnnStatus_t cudnn_status);
 
-/// Checks cudnnStatus_t converts to cutlass status and returns if Status::kSuccess o.w. throws exception
+/// Checks cudnnStatus_t converts to cutlas status and returns if Status::kSuccess o.w. throws exception
 Status checkCudnnErr(cudnnStatus_t cudnn_status);
 
 /// Maps a CUTLASS conv mode to a cuDNN conv mode enumeration
 bool get_cudnn_conv_mode(cudnnConvolutionMode_t &cudnn_conv_mode, conv::Mode conv_mode);
 
 /// Maps a CUTLASS layout type to a cuDNN data type enumeration
 bool get_cudnn_layout(cudnnTensorFormat_t &cudnn_layout, library::LayoutTypeID layout);
@@ -78,15 +78,15 @@
 
 /// Cudnn compute type seems to be hardcoded to float (To handle a possible cudnn issue)
 float cast_cudnn_compute_type_to_float(library::NumericTypeID type, void const * src);
 
 
 /// This is a helper class to create cudnnHandle_t automatically on CudnnCreate object creation and 
 /// to destroy cudnnHandle_t on CudnnCreate object destruction. 
-/// Additionally, it provides implicit cast from CudnnCreate's object to cudnnHandle_t's object
+/// Additionaly, it provides implicit cast from CudnnCreate's object to cudnnHandle_t's object
 class CudnnCreate {
 private:
 	cudnnHandle_t handle;
 	cudnnStatus_t status;
 
 public:
 	CudnnCreate() {
@@ -158,15 +158,15 @@
 
   Status status;
   
   //
   // Methods
   //
 
-  // TODO: unify ctor cudnnConvDispatcher for conv2d and conv3d by unifying Conv2dConfiguration
+  // TODO: unify ctor cudnnConvDispatcher for conv2d and conv3d by unifying Conv2dConfigration
   
   // ctor for conv2d 
   cudnnConvDispatcher( 
     library::ConvDescription const &op_desc,
     library::Conv2dConfiguration configuration,
     library::ConvArguments arguments_,
     cudnnHandle_t handle
@@ -241,15 +241,15 @@
 
     status = get_cutlass_status(
       cudnnSetFilter4dDescriptor(
         filter_desc,
         data_type_filter,
         layout_filter,
         configuration.problem_size.K,
-        configuration.problem_size.C / configuration.problem_size.groups,
+        configuration.problem_size.C,
         configuration.problem_size.R,
         configuration.problem_size.S
     ));
 
     status = get_cutlass_status(
       cudnnSetTensor4dDescriptor(
         output_desc,
@@ -492,15 +492,15 @@
         )); break;
 
     }
 
     workspace = cutlass::device_memory::allocation<char>(workspace_size_in_bytes);
   }
 
-  /// Executes Conv2d operator from cudnn library
+  /// Executes Conv2d operater from cudnn library
   cudnnStatus_t operator()(cudnnHandle_t handle) {
 
     switch (conv_kind) {
       case library::ConvKind::kFprop:
         return cudnnConvolutionForward(
           handle,
           &alpha,
@@ -548,15 +548,15 @@
           filter_desc,
           arguments.D
         );
       default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
     }
   }
 
-  // Returns Activation Tensor
+  // Returns Actviation Tensor
   void const * activation() const {
     switch(conv_kind) {
       case library::ConvKind::kFprop : return arguments.A;
       case library::ConvKind::kDgrad : return arguments.C;
       case library::ConvKind::kWgrad : return arguments.B;
       default : throw std::runtime_error("Invalid Conv Operator (fprop, dgrad, wgrad)");
     }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/cutlass_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu`

 * *Files 12% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -25,72 +25,37 @@
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
 /* \file
-   \brief Execution environment
+   \brief
 */
 
 #pragma once
-// CUTLASS Library includes
-#include "cutlass/library/library.h"
-#include "cutlass/library/manifest.h"
-#include "cutlass/library/singleton.h"
 
-#include "options.h"
-#include "operation_profiler.h"
+#include <vector>
 
-/////////////////////////////////////////////////////////////////////////////////////////////////
+#include "cutlass/cutlass.h"
+
+// CUTLASS Profiler includes
+#include "enumerated_types.h"
+#include "performance_result.h"
+
+// CUTLASS Library includes
+#include "cutlass/library/library.h"
+#include "cutlass/library/util.h"
 
 namespace cutlass {
 namespace profiler {
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// CUTLASS Profiler application 
-class CutlassProfiler {
-private:
-
-  //
-  // Data members
-  //
-
-  /// Performance testbench options
-  Options options_;
-
-  /// Entry points for each operation
-  OperationProfilerVector operation_profilers_;
-
-private:
-
-  /// Prints usage
-  void print_usage_(std::ostream &);
-  
-  /// Prints usage
-  void print_options_(std::ostream &);
-
-  /// Initializes the device
-  void initialize_device_();
-
-  /// Enumerates all operations
-  void enumerate_();
-
-  /// Profiles all operations
-  int profile_();
-
-public:
-
-  CutlassProfiler(Options const &options);
-  ~CutlassProfiler();
-
-  /// Invokes profiling operations
-  int operator()();
-};
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace profiler
 } // namespace cutlass
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/debug.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/debug.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -35,15 +35,15 @@
 #pragma once
 
 #include <iostream>
 
 //#define report(x) { std::cout << "\033[31m" << __FILE__ << ":" << __LINE__ << "  " << x << "\033[0m" << std::endl; }
 //#define report(x) {}
 
-// Enable/Disable Profiler debug prints
+// Enable/Disble Profiler debug prints
 //#define DEBUG_PROFILER 
 
 //RED    31m   // profiler prints debug messages in red
 //YELLOW 33m   // ir prints debug messages in yellow
 
 #ifndef DEBUG_PROFILER
 #define debugprof(...)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -438,20 +438,20 @@
 }
 
 /// Gets the number of adjacent tensors in memory
 int DeviceAllocation::batch_count() const {
   return batch_count_;
 }
 
-/// Gets the stride (in units of elements) between items
+/// Gets the stride (in units of elements) beteween items
 int64_t DeviceAllocation::batch_stride() const {
   return batch_stride_;
 }
 
-/// Gets the stride (in units of bytes) between items
+/// Gets the stride (in units of bytes) beteween items
 int64_t DeviceAllocation::batch_stride_bytes() const {
   return bytes(type_, batch_stride_);
 }
 
 size_t DeviceAllocation::capacity() const {
   return capacity_;
 }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_allocation.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -172,18 +172,18 @@
 
   /// Gets the extent vector
   std::vector<int> const & extent() const;
 
   /// Gets the number of adjacent tensors in memory
   int batch_count() const;
 
-  /// Gets the stride (in units of elements) between items
+  /// Gets the stride (in units of elements) beteween items
   int64_t batch_stride() const;
 
-  /// Gets the stride (in units of bytes) between items
+  /// Gets the stride (in units of bytes) beteween items
   int64_t batch_stride_bytes() const;
 
   /// Capacity of allocation in number of elements
   size_t capacity() const;
   
   /// Capacity of allocation in bytes
   size_t bytes() const;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/device_context.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/enumerated_types.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -104,15 +104,15 @@
 
     << "Schmoo over accumulator types:\n"
     << "  $ cutlass_profiler --operation=Gemm --accumulator-type=f16,f32\n\n"
 
     << "Run when A is f16 with column-major and B is any datatype with row-major (For column major, use column, col, or n. For row major use, row or t):\n"
     << "  $ cutlass_profiler --operation=Gemm --A=f16:column --B=*:row\n\n"
 
-    << "Profile a particular problem size with split K and parallel reduction:\n"
+    << "Profile a particular problem size with split K and paralell reduction:\n"
     << "  $ cutlass_profiler --operation=Gemm --split_k_mode=parallel --split_k_slices=2 --m=1024 --n=1024 --k=128\n\n"
 
     << "Using various input value distribution:\n"
     << "  $ cutlass_profiler --operation=Gemm --dist=uniform,min:0,max:3\n"
     << "  $ cutlass_profiler --operation=Gemm --dist=gaussian,mean:0,stddev:3\n"
     << "  $ cutlass_profiler --operation=Gemm --dist=sequential,start:0,delta:1\n\n"
 
@@ -164,15 +164,15 @@
   
   if (!arg_as_int(this->k, "k", problem_space, problem)) {
     // default value
     this->k = 1024;
   }
 
   if (!arg_as_SplitKModeID(this->split_k_mode, "split_k_mode", problem_space, problem)) {
-    // default value
+    // defualt value
     this->split_k_mode = library::SplitKMode::kSerial;
   }
   
   this->mode = library::GemmUniversalMode::kGemm;
   if(this->split_k_mode == library::SplitKMode::kParallel) {
     this->mode = library::GemmUniversalMode::kGemmSplitKParallel;
   }
@@ -401,15 +401,15 @@
 
   result.bytes = problem_.bytes(operation_desc);
   result.flops = problem_.flops(operation_desc);
   result.runtime = 0;
 
 }
 
-/// Initialize reduction problem dimensions and library::Operation
+/// Initialize redution problem dimentions and library::Operation
 bool GemmOperationProfiler::initialize_reduction_configuration_(
   library::Operation const *operation,
   ProblemSpace::Problem const &problem) {
   library::GemmDescription const &gemm_desc =
     static_cast<library::GemmDescription const&>(operation->description());
 
   if (!cast_from_double(problem_.alpha_one, gemm_desc.element_epilogue, 1)) {
@@ -430,15 +430,15 @@
 
   // find reduction operation
   library::ReductionFunctionalKey reduction_key(
     library::Provider::kCUTLASS,
     gemm_desc.tile_description.math_instruction.element_accumulator,    // element workspace
     gemm_desc.tile_description.math_instruction.element_accumulator,    // element accumulator
     gemm_desc.C.element,                                                // element output
-    gemm_desc.element_epilogue                                          // element compute
+    gemm_desc.element_epilogue                                          // element coumpute
   );
 
   auto reduction_it = library::Singleton::get().operation_table.reduction_operations.find(reduction_key);
  
   if (reduction_it == library::Singleton::get().operation_table.reduction_operations.end()) {
     return false;
   }
@@ -533,21 +533,14 @@
       {int(problem_.m), int(problem_.n)},
       {int(problem_.ldc)},
       problem_.batch_count * gemm_workspace_.problem_count
     );
 
     gemm_workspace_.Reference->copy_from_device(gemm_workspace_.C->data());
 
-    // NOTE: the leading non-batch strides are duplicated here for 3.0 API kernels
-    gemm_workspace_.arguments.problem_size = {int(problem_.m), int(problem_.n), int(problem_.k)};
-    gemm_workspace_.arguments.batch_count = problem_.batch_count;
-    gemm_workspace_.arguments.lda = problem_.lda;
-    gemm_workspace_.arguments.ldb = problem_.ldb;
-    gemm_workspace_.arguments.ldc = problem_.ldc;
-    gemm_workspace_.arguments.ldd = problem_.ldc;
     gemm_workspace_.arguments.batch_stride_A = gemm_workspace_.A->batch_stride();
     gemm_workspace_.arguments.batch_stride_B = gemm_workspace_.B->batch_stride();
     gemm_workspace_.arguments.batch_stride_C = gemm_workspace_.C->batch_stride();
     gemm_workspace_.arguments.batch_stride_D = gemm_workspace_.Computed->batch_stride();
   }
 
   //
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gemm_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -90,15 +90,15 @@
     result = cudaDeviceSynchronize();
     if (result != cudaSuccess) {
       throw std::runtime_error("Failed to synchronize with CUDA device.");
     }
   }
 }
 
-/// Returns the duration in milliseconds
+/// Returns the duration in miliseconds
 double GpuTimer::duration(int iterations) const {
 
   float avg_ms;
 
   cudaError_t result = cudaEventElapsedTime(&avg_ms, events[0], events[1]);
   if (result != cudaSuccess) {
     throw std::runtime_error("Failed to query elapsed time from CUDA events.");
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/gpu_timer.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -58,15 +58,15 @@
 
   /// Records a stop event in the stream
   void stop(cudaStream_t stream = nullptr);
 
   /// Records a stop event in the stream and synchronizes on the stream
   void stop_and_wait(cudaStream_t stream = nullptr);
 
-  /// Returns the duration in milliseconds
+  /// Returns the duration in miliseconds
   double duration(int iterations = 1) const;
 };
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace profiler
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/main.cpp`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.cu`

 * *Files 15% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -71,17 +71,14 @@
 
   ArgumentDescriptionVector tile_description_arguments{
     {ArgumentTypeID::kEnumerated, {"op_class", "opcode-class"}, "Class of math instruction (simt, tensorop, wmmatensorop, wmma)"},
     {ArgumentTypeID::kEnumerated, {"accum", "accumulator-type"}, "Math instruction accumulator data type"},
     {ArgumentTypeID::kInteger, {"cta_m", "threadblock-shape::m"}, "Threadblock shape in the M dimension"},
     {ArgumentTypeID::kInteger, {"cta_n", "threadblock-shape::n"}, "Threadblock shape in the N dimension"},
     {ArgumentTypeID::kInteger, {"cta_k", "threadblock-shape::k"}, "Threadblock shape in the K dimension"},
-    {ArgumentTypeID::kInteger, {"cluster_m", "cluster-shape::m"}, "Cluster shape in the M dimension"},
-    {ArgumentTypeID::kInteger, {"cluster_n", "cluster-shape::n"}, "Cluster shape in the N dimension"},
-    {ArgumentTypeID::kInteger, {"cluster_k", "cluster-shape::k"}, "Cluster shape in the K dimension"},
     {ArgumentTypeID::kInteger, {"stages", "threadblock-stages"}, "Number of stages of threadblock-scoped matrix multiply"},
     {ArgumentTypeID::kInteger, {"warps_m", "warp-count::m"}, "Number of warps within threadblock along the M dimension"},
     {ArgumentTypeID::kInteger, {"warps_n", "warp-count::n"}, "Number of warps within threadblock along the N dimension"},
     {ArgumentTypeID::kInteger, {"warps_k", "warp-count::k"}, "Number of warps within threadblock along the K dimension"},
     {ArgumentTypeID::kInteger, {"inst_m", "instruction-shape::m"}, "Math instruction shape in the M dimension"},
     {ArgumentTypeID::kInteger, {"inst_n", "instruction-shape::n"}, "Math instruction shape in the N dimension"},
     {ArgumentTypeID::kInteger, {"inst_k", "instruction-shape::k"}, "Math instruction shape in the K dimension"},
@@ -197,32 +194,14 @@
 
   if (arg_as_int(int_value, "cta_k", problem_space, problem)) {
     if (int64_t(op_desc.tile_description.threadblock_shape.k()) != int_value) {
       return false;
     }
   }
 
-  if (arg_as_int(int_value, "cluster_m", problem_space, problem)) {
-    if (int64_t(op_desc.tile_description.cluster_shape.m()) != int_value) {
-      return false;
-    }
-  }
-
-  if (arg_as_int(int_value, "cluster_n", problem_space, problem)) {
-    if (int64_t(op_desc.tile_description.cluster_shape.n()) != int_value) {
-      return false;
-    }
-  }
-
-  if (arg_as_int(int_value, "cluster_k", problem_space, problem)) {
-    if (int64_t(op_desc.tile_description.cluster_shape.k()) != int_value) {
-      return false;
-    }
-  }
-
   if (arg_as_int(int_value, "stages", problem_space, problem)) {
     if (int64_t(op_desc.tile_description.threadblock_stages) != int_value) {
       return false;
     }
   }
 
   if (arg_as_int(int_value, "warps_m", problem_space, problem)) {
@@ -613,17 +592,14 @@
 
   set_argument(result, "accum", problem_space,
     library::to_string(operation_desc.tile_description.math_instruction.element_accumulator));
 
   set_argument(result, "cta_m", problem_space, operation_desc.tile_description.threadblock_shape.m());
   set_argument(result, "cta_n", problem_space, operation_desc.tile_description.threadblock_shape.n());
   set_argument(result, "cta_k", problem_space, operation_desc.tile_description.threadblock_shape.k());
-  set_argument(result, "cluster_m", problem_space, operation_desc.tile_description.cluster_shape.m());
-  set_argument(result, "cluster_n", problem_space, operation_desc.tile_description.cluster_shape.n());
-  set_argument(result, "cluster_k", problem_space, operation_desc.tile_description.cluster_shape.k());
   set_argument(result, "stages", problem_space, operation_desc.tile_description.threadblock_stages);
   set_argument(result, "warps_m", problem_space, operation_desc.tile_description.warp_count.m());
   set_argument(result, "warps_n", problem_space, operation_desc.tile_description.warp_count.n());
   set_argument(result, "warps_k", problem_space, operation_desc.tile_description.warp_count.k());
   set_argument(result, "inst_m", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.m());
   set_argument(result, "inst_n", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.n());
   set_argument(result, "inst_k", problem_space, operation_desc.tile_description.math_instruction.instruction_shape.k());
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -77,15 +77,15 @@
 
   /// Arguments parsed from command line
   ArgumentDescriptionVector arguments_;
 
   /// List of providers used to verify and compare each result
   ProviderVector verification_providers_;
 
-  /// Model performance result initialized by the operation profiler with workload statistics
+  /// Model performance result initailized by the operation profiler with workload statistics
   /// and reasonable default state.
   PerformanceResult model_result_;
 
   /// Performance result vector constructed by profiling the operation
   PerformanceResultVector results_;
 
 public:
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/options.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -185,15 +185,15 @@
     std::string str;
     cmdline.get_cmd_line_argument("initialization-provider", str);
     provider = library::from_string<library::Provider>(str);
     if (provider == library::Provider::kInvalid) {
       enabled = false;
     }
     else if (provider != library::Provider::kReferenceHost && provider != library::Provider::kReferenceDevice) {
-      throw std::runtime_error("Unsupported initialization provider specified.");
+      throw std::runtime_error("Unsupported intialization provider specified."); 
     }
   }
   else {
     provider = library::Provider::kReferenceDevice;
   }
 
   cmdline.get_cmd_line_argument("seed", seed, 2019);
@@ -201,15 +201,15 @@
   if (cmdline.check_cmd_line_flag("dist")) {
     // user has set the data distribution (fix data distribution once set)
     fix_data_distribution = true;
     // set user provided data distribution
     get_distribution(cmdline, "dist", data_distribution);
   }
   else {
-    // profiler chosen data distribution (allowed to change based on numeric types)
+    // profiler choosen data distribution (allowed to change based on numeric types)
     fix_data_distribution = false;
     // set uniform data distribution with range [-4, 4] 
     data_distribution.set_uniform(-4, 4, 0);
   }
   
 
 }
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/options.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/options.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -227,15 +227,15 @@
     /// Path to a file containing junit xml results
     std::string junit_output_path;
 
     /// Sequence of tags to attach to each result
     std::vector<std::pair<std::string, std::string>> pivot_tags;
 
     /// If true, reports status of all kernels including those that were
-    /// not run for the given arguments
+    /// not run for the given argumetns
     bool report_not_run;
 
     /// Prints human-readable text to stdout. If false, nothing is written to stdout
     bool verbose;
 
     /// Sort results by (currently by flops-per-byte)
     bool sort_results;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_report.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h`

 * *Files 10% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,38 +24,15 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-/* \file
-   \brief
-*/
 
 #pragma once
 
-#include <vector>
-
 #include "cutlass/cutlass.h"
+#include "cutlass/numeric_types.h"
 
-// CUTLASS Profiler includes
-#include "enumerated_types.h"
-#include "performance_result.h"
-
-// CUTLASS Library includes
-#include "cutlass/library/library.h"
-#include "cutlass/library/util.h"
-
-namespace cutlass {
-namespace profiler {
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
-
-} // namespace profiler
-} // namespace cutlass
-
-/////////////////////////////////////////////////////////////////////////////////////////////////
+// integer_sequence moved to cutlass/numeric_types.h
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/performance_result.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.cpp`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/problem_space.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -280,15 +280,15 @@
     virtual std::unique_ptr<KernelArgument::Value> at() const;
   };
 
   //
   // Data members
   //
 
-  /// Set of possible values
+  /// Set of posible values
   ValueCollection values;
 
   //
   // Methods
   //
 
   /// Default ctor
@@ -536,15 +536,15 @@
     virtual std::unique_ptr<KernelArgument::Value> at() const;
   };
 
   //
   // Data members
   //
 
-  /// Set of possible values
+  /// Set of posible values
   RangeCollection ranges;
 
   //
   // Methods
   //
 
   /// Default ctor
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_2k_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/rank_k_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/reduction_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.cu`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/sparse_gemm_operation_profiler.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/symm_operation_profiler.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.cu`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/profiler/src/trmm_operation_profiler.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/command_line.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -184,15 +184,15 @@
       // Clear any default values
       vals.clear();
 
       // Recover from multi-value string
       for (int i = 0; i < keys.size(); ++i) {
         if (keys[i] == string(arg_name)) {
           string val_string(values[i]);
-          separate_string(val_string, vals, sep);
+          seperate_string(val_string, vals, sep);
         }
       }
     }
   }
 
   /**
    * Returns the values specified for a given commandline parameter
@@ -221,15 +221,15 @@
     std::vector<std::string> ranges;
     get_cmd_line_arguments(arg_name, ranges, delim);
 
     for (std::vector<std::string>::const_iterator range = ranges.begin();
       range != ranges.end(); ++range) {
 
       std::vector<std::string> range_vals;
-      separate_string(*range, range_vals, sep);
+      seperate_string(*range, range_vals, sep);
       vals.push_back(range_vals);
     }
   }
 
   /**
    * The number of pairs parsed
    */
@@ -279,15 +279,15 @@
     tokenize(token_pairs, str, delim, sep);
     for (token_iterator tok = token_pairs.begin(); tok != token_pairs.end(); ++tok) {
       tokens.push_back(tok->first);
     }
   }
 
   template <typename value_t>
-  static void separate_string(std::string const& str,
+  static void seperate_string(std::string const& str,
                               std::vector<value_t>& vals,
                               char sep = ',') {
     std::istringstream str_stream(str);
     std::string::size_type old_pos = 0;
     std::string::size_type new_pos = 0;
 
     // Iterate <sep>-delimited values
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/debug.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_dump.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_groupnorm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_layernorm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -452,15 +452,15 @@
   const T* beta = ref_beta.data();
   dim3 grid(m);
   dim3 block((n + 31)/32*32);
   if (block.x > 1024){
     block.x = 1024;
   }
   // TODO : There should be better configs for different cases, we only use several samples to show how to use here
-  // TODO : using registers to store values locally can reduce the loads from global memory and speedup the kernels.
+  // TODO : using registers to store values locally can reduce the ldgs from global memory and speedup the kernels.
   if ((n % 4 == 0) && (n >= 128) && (n <= 4096)) {
     block.x = (n/4 + 31)/32*32;
     if (std::is_same<T, float>::value) {
       layernorm_twoPassAlgo_stored_locally_e4<float4, float, 1><<<grid, block, 0, stream>>>(
         (float4*)output,
         (const float4*)input,
         (const float4*)gamma,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_memory.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nchw_to_nhwc.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_padding.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -105,17 +105,17 @@
   __shared__ Tio shm[192];
   const int tidx = blockIdx.x * 192 + threadIdx.x;  
   const int threadidx = threadIdx.x; 
 
   shm[threadIdx.x] = tidx >= max_input_element ? zero_io : input[tidx];  
   __syncthreads();
   
-  const int output_offset = blockIdx.x * 256;
-  const int lower_bound = max_output_element < output_offset + 256 ? max_output_element : output_offset + 256;
-  for (int i = output_offset + threadidx, j = threadidx ; i < lower_bound ; i+=192, j+=192)
+  const int ouput_offset = blockIdx.x * 256;
+  const int lower_bound = max_output_element < ouput_offset + 256 ? max_output_element : ouput_offset + 256;
+  for (int i = ouput_offset + threadidx, j = threadidx ; i < lower_bound ; i+=192, j+=192)
   {
     const Telement* shm_element = (const Telement*)shm + j*3*element_in_Tio/4;
     Telement array[element_in_Tio];
     CUTLASS_PRAGMA_UNROLL
     for (int k = 0 ; k < element_in_Tio ; k++)
       array[k] = ((k+1)%4 == 0) ? zero_element : shm_element[(k > 3) ? (k - 1) : k];
     output[i] = *((const Tio *)array);
@@ -136,17 +136,17 @@
   __shared__ Tio shm[192];
   const int tidx = blockIdx.x * 192 + threadIdx.x;  
   const int threadidx = threadIdx.x; 
 
   shm[threadIdx.x] = tidx >= max_input_element ? zero_io : input[tidx];  
   __syncthreads();
   
-  const int output_offset = blockIdx.x * 512;
-  const int lower_bound = max_output_element < output_offset + 512 ? max_output_element : output_offset + 512;
-  for (int i = output_offset + threadidx, j = threadidx ; i < lower_bound ; i+=192, j+=192)
+  const int ouput_offset = blockIdx.x * 512;
+  const int lower_bound = max_output_element < ouput_offset + 512 ? max_output_element : ouput_offset + 512;
+  for (int i = ouput_offset + threadidx, j = threadidx ; i < lower_bound ; i+=192, j+=192)
   {
     const Telement* shm_element = (const Telement*)shm + (element_in_Tio == 4 ? j/2 : j)*3;
     Telement array[element_in_Tio];
     //float
     if (element_in_Tio == 4){
       CUTLASS_PRAGMA_UNROLL
       for (int k = 0 ; k < element_in_Tio ; k++)
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_pooling.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_nhwc_to_nchw.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/device_utils.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/distribution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/exceptions.h`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /******************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_tensor_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/host_uncompress.h`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -38,15 +38,14 @@
 #include "cutlass/util/host_tensor.h"
 #include "cutlass/tensor_view.h"
 #include "cutlass/util/tensor_view_io.h"
 #include "cutlass/util/reference/host/gemm.h"
 
 namespace cutlass {
 
-// uncompress sparse tensor core A matrix
 template <typename ElementA, typename LayoutA, typename ElementE,
           typename LayoutE>
 void uncompress(TensorRef<ElementA, LayoutA> uncompressed_tensor_a,
                 TensorRef<ElementA, LayoutA> tensor_a,
                 TensorRef<ElementE, LayoutE> tensor_e, int row, int col) {
   // How many uncompressed data we can get with ElementE meta data
   int DecompressedElementsPerElementE =
@@ -116,42 +115,9 @@
                   ElementA(0);
           }
         }
       }
     }
   }
 }
-
-// uncompress ELL block sparse matrix
-template <typename ElementA, typename LayoutA,
-          typename ElementE, typename LayoutE>
-void uncompress_ell_block_sparse(
-                TensorRef<ElementA, LayoutA> uncompressed_tensor_a,
-                TensorRef<ElementA, LayoutA> tensor_a,
-                TensorRef<ElementE, LayoutE> ell_idx,
-                int rows, int cols,
-                int ell_num_cols, int ell_blocksize) {
-
-  for (int r = 0; r < rows / ell_blocksize; ++r) {
-    for (int c = 0; c < ell_num_cols / ell_blocksize; ++c) {
-
-      ElementE idx = ell_idx.at(MatrixCoord(r, c));
-
-      if (idx != -1) {
-        int row_begin = r * ell_blocksize;
-        int col_begin_real = idx * ell_blocksize;
-        int col_begin = c * ell_blocksize;
-  
-        for (int i = 0; i < ell_blocksize; ++i) {
-          for (int j = 0; j < ell_blocksize; ++j) {
-            uncompressed_tensor_a.at(MatrixCoord(row_begin + i, col_begin_real + j)) =
-                tensor_a.at(
-                    MatrixCoord(row_begin + i, col_begin +j));
-          }
-        }
-      }
-    }
-  }
-}
-
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/index_sequence.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h`

 * *Files 9% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -24,15 +24,19 @@
  * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
  * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
  * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
  * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
  * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
  **************************************************************************************************/
-
 #pragma once
 
+
 #include "cutlass/cutlass.h"
-#include "cutlass/numeric_types.h"
 
-// integer_sequence moved to cutlass/numeric_types.h
+// The contents of this file have been moved  to 'tensor_reduce' to cover other types of reductions.
+
+#include "cutlass/util/reference/host/tensor_reduce.h"
+
+///////////////////////////////////////////////////////////////////////////////////////////////////
+
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/inner_product.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/detail/linear_to_coordinate.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -244,15 +244,15 @@
                  NumericConverterClamp<ElementC, ScalarType>>(
         problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for XOR-popc
+/// Parital specialization for XOR-popc
 template <typename ElementA, typename LayoutA, typename ElementB,
           typename LayoutB, typename ElementC, typename LayoutC,
           typename ScalarType, typename AccumulatorType>
 struct Gemm<ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC, ScalarType,
             AccumulatorType, arch::OpXorPopc> {
 
   void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/gemm_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/gemm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/kernel/tensor_foreach.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -68,15 +68,15 @@
   }
 };
 
 /// Helper to perform for-each operation
 template <typename Func, int Rank>
 struct TensorForEachHelper<Func, Rank, 0> {
 
-  /// Constructor for fastest changing rank
+  /// Constructor for fastest chaning rank
   __inline__ __device__
   TensorForEachHelper(Func &func, Coord<Rank> const &size, Coord<Rank> &coord, int64_t index) {
 
     coord[Rank - 1] = index;
 
     if (coord < size) {
       func(coord);
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/rank_2k_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_compare.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_fill.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -1304,15 +1304,15 @@
   Element val = Element(0)) {               ///< value to uniformly fill it with
 
   TensorFillDiagonal(view, val, val);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Fills a tensor's diagonal with 1 and 0 everywhere else.
+/// Fills a tensor's digonal with 1 and 0 everywhere else.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillIdentity(
   TensorView<Element, Layout> view) {               ///< destination tensor
 
   TensorFillDiagonal(view, Element(1), Element(0));
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_foreach.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -129,8 +129,8 @@
   }
 };
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace device
 } // namespace reference
-} // namespace cutlass
+} // namesace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_reduce.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/tensor_relu.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/device/thread/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/convolution.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/error_metrics.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -331,15 +331,15 @@
                  NumericConverterClamp<ElementC, ScalarType>>(
         problem_size, alpha, tensor_a, tensor_b, beta, tensor_c, tensor_d, initial_accum);
   }
 };
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Partial specialization for XOR-popc
+/// Parital specialization for XOR-popc
 template <typename ElementA, typename LayoutA, typename ElementB,
           typename LayoutB, typename ElementC, typename LayoutC,
           typename ScalarType, typename ComputeType>
 struct Gemm<ElementA, LayoutA, ElementB, LayoutB, ElementC, LayoutC, ScalarType,
             ComputeType, arch::OpXorPopc> {
 
   void operator()(gemm::GemmCoord problem_size, ScalarType alpha,
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -35,18 +35,16 @@
 #pragma once
 
 #include "cutlass/coord.h"
 #include "cutlass/complex.h"
 #include "cutlass/numeric_types.h"
 #include "cutlass/functional.h"
 #include "cutlass/numeric_conversion.h"
-#include "cutlass/matrix_coord.h"
 
 #include "cutlass/tensor_view.h"
-
 #include "cutlass/gemm/gemm.h"
 
 namespace cutlass {
 namespace reference {
 namespace host {
 
 ////////////////////////////////////////////////////////////////////////////////////////////////////
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/gemm_planar_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_2k_complex.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/rank_k_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/symm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_compare.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_copy.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_elementwise.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -64,30 +64,30 @@
 
   //
   // Data members
   //
 
   /// View of left-hand-side tensor
   TensorView<ElementD, LayoutD> view_d;
-  TensorRef<ElementA, LayoutA> view_a;
-  TensorRef<ElementB, LayoutB> view_b;
+  TensorRef<ElementA, LayoutA> ref_a;
+  TensorRef<ElementB, LayoutB> ref_b;
   BinaryFunc func;
 
   //
   // Methods
   //
 
   /// Constructor
   TensorFuncBinaryOp() { }
 
   /// Constructor
   TensorFuncBinaryOp(
     TensorView<ElementD, LayoutD> const & view_d_,
-    TensorRef<ElementA, LayoutA> const & view_a_,
-    TensorRef<ElementB, LayoutB> const & view_b_,
+    TensorRef<ElementA, LayoutA> const & ref_a_,
+    TensorRef<ElementB, LayoutB> const & ref_b_,
     BinaryFunc func = BinaryFunc()
   ):
     view_d(view_d_), view_a(view_a_), view_b(view_b_), func(func) { }
 
   /// Equality check
   void operator()(Coord<LayoutD::kRank> const &coord) const {
     view_d.at(coord) = func(
@@ -280,15 +280,15 @@
   typename ElementA,
   typename LayoutA
 >
 void TensorDiv(
   TensorView<ElementD, LayoutD> d,      ///< destination tensor view
   TensorRef<ElementA, LayoutA> a        ///< A tensor reference
 ) {
-  TensorDiv(d, d, a);
+  TensorMul(d, d, a);
 }
 
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Divides two tensors and stores in the destination tensor: d = a ./ b
 template <
@@ -308,15 +308,15 @@
   detail::TensorFuncBinaryOp<
     ElementD, 
     LayoutD,
     ElementA,
     LayoutA,
     ElementB,
     LayoutB,
-    cutlass::divides<ElementD>
+    cutlass::modulus<ElementD>
   > func(d, a, b);
 
   TensorForEach(
     d.extent(),
     func);
 }
 
@@ -327,15 +327,15 @@
   typename ElementA,
   typename LayoutA
 >
 void TensorModulus(
   TensorView<ElementD, LayoutD> d,      ///< destination tensor view
   TensorRef<ElementA, LayoutA> a        ///< A tensor reference
 ) {
-  TensorDiv(d, d, a);
+  TensorMul(d, d, a);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 } // namespace host
 } // namespace reference
 } // namespace cutlass
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_fill.h`

 * *Files 2% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -88,33 +88,14 @@
   ): view(view_), value(value_) { }
 
   void operator()(Coord<Layout::kRank> const & coord) const {
     view.at(coord) = value;
   }
 };
 
-/// Returns a pair of values of the Gaussian distribution generated by the Box Muller method 
-struct BoxMullerFunc {
-
-  BoxMullerFunc() {}
-
-  void operator()(
-    double* rnd,                     ///< Size-2 vector to be filled with random values
-    double  mean = 0,                ///< Mean of the Gaussian distribution
-    double  stddev = 1,              ///< Standard deviation of the Gaussian distribution
-    double  pi = std::acos(-1)) const {
-
-    double u1 = double(std::rand()) / double(RAND_MAX);
-    double u2 = double(std::rand()) / double(RAND_MAX);
-    rnd[0] = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
-    rnd[1] = std::sqrt(-2 * std::log(u1)) * std::sin(2 * pi * u2);
-    rnd[0] = mean + stddev * rnd[0];
-    rnd[1] = mean + stddev * rnd[1];
-  }
-};
 } // namespace detail
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Fills a tensor with a uniform value
 template <
   typename Element,               ///< Element type
@@ -220,26 +201,30 @@
   }
 
   /// Compute random value and update RNG state
   complex<Element> operator()() const {
 
     Element reals[2];
 
-    double rnd[2];
-    detail::BoxMullerFunc func;
-    func(rnd, mean, stddev, pi);
+    for (int i = 0; i < 2; ++i) {
+      // Box-Muller transform to generate random numbers with Normal distribution
+      double u1 = double(std::rand()) / double(RAND_MAX);
+      double u2 = double(std::rand()) / double(RAND_MAX);
+
+      // Compute Gaussian random value
+      double rnd = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
+      rnd = mean + stddev * rnd;
 
-    if (int_scale >= 0) {
-      rnd[0] = double(int(rnd[0] * double(1 << int_scale)));
-      rnd[1] = double(int(rnd[1] * double(1 << int_scale)));
-      reals[0] = from_real<Element>(rnd[0] / double(1 << int_scale));
-      reals[1] = from_real<Element>(rnd[1] / double(1 << int_scale));
-    } else {
-      reals[0] = from_real<Element>(rnd[0]);
-      reals[1] = from_real<Element>(rnd[1]);
+      if (int_scale >= 0) {
+        rnd = double(int(rnd * double(1 << int_scale)));
+        reals[i] = from_real<Element>(rnd / double(1 << int_scale));
+      }
+      else {
+        reals[i] = from_real<Element>(rnd);
+      }
     }
 
     return complex<Element>(reals[0], reals[1]);
   }
 };
 
 /// Partial specialization for initializing a complex value.
@@ -266,35 +251,30 @@
   }
 
   /// Compute random value and update RNG state
   Quaternion<Element> operator()() const {
 
     Element reals[4];
 
-    double rnd1[2];
-    double rnd2[2];
-    detail::BoxMullerFunc func;
-    func(rnd1, mean, stddev, pi);
-    func(rnd2, mean, stddev, pi);
+    for (int i = 0; i < 4; ++i) {
+      // Box-Muller transform to generate random numbers with Normal distribution
+      double u1 = double(std::rand()) / double(RAND_MAX);
+      double u2 = double(std::rand()) / double(RAND_MAX);
+
+      // Compute Gaussian random value
+      double rnd = std::sqrt(-2 * std::log(u1)) * std::cos(2 * pi * u2);
+      rnd = mean + stddev * rnd;
 
-    if (int_scale >= 0) {
-      rnd1[0] = double(int(rnd1[0] * double(1 << int_scale)));
-      rnd1[1] = double(int(rnd1[1] * double(1 << int_scale)));
-      rnd2[0] = double(int(rnd2[0] * double(1 << int_scale)));
-      rnd2[1] = double(int(rnd2[1] * double(1 << int_scale)));
-
-      reals[0] = from_real<Element>(rnd1[0] / double(1 << int_scale));
-      reals[1] = from_real<Element>(rnd1[1] / double(1 << int_scale));
-      reals[2] = from_real<Element>(rnd2[0] / double(1 << int_scale));
-      reals[3] = from_real<Element>(rnd2[1] / double(1 << int_scale));
-    } else {
-      reals[0] = from_real<Element>(rnd1[0]);
-      reals[1] = from_real<Element>(rnd1[1]);
-      reals[2] = from_real<Element>(rnd2[0]);
-      reals[3] = from_real<Element>(rnd2[1]);
+      if (int_scale >= 0) {
+        rnd = double(int(rnd * double(1 << int_scale)));
+        reals[i] = from_real<Element>(rnd / double(1 << int_scale));
+      }
+      else {
+        reals[i] = from_real<Element>(rnd);
+      }
     }
 
     return Quaternion<Element>(reals[0], reals[1], reals[2], reals[3]);
   }
 };
 
 /// Computes a random Gaussian distribution
@@ -327,15 +307,15 @@
 
   /// Compute random value and update RNG state
   void operator()(Coord<Layout::kRank> const &coord) const {
     view.at(coord) = func();
   }
 };
 
-/// Computes a random Gaussian distribution for a rank-2 tensor
+/// Computes a random Gaussian distribution
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillSymmetricGaussianFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -420,15 +400,15 @@
                                                        ///  data.
   
   TensorFillRandomGaussian(dst.view_real(), seed, mean, stddev, bits);
   TensorFillRandomGaussian(dst.view_imag(), ~seed, mean, stddev, bits);
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
-/// Fills the upper or lower part of a symmetric rank-2 tensor with random values of a Gaussian distribution.
+/// Fills a tensor with random values with a Gaussian distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillSymmetricRandomGaussian(
   TensorView<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                          ///< seed for RNG
   cutlass::FillMode fill_mode,            ///< FillMode for symmetric matrices
@@ -450,15 +430,15 @@
     dst.extent(),
     func
   );
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Fills a tensor with random values of a Gaussian distribution.
+/// Fills a tensor with random values with a Gaussian distribution.
 template <
   typename Element                        ///< Element type
 >
 void BlockFillRandomGaussian(
   Element *ptr,                           ///< destination buffer
   size_t capacity,                        ///< number of elements
   uint64_t seed,                          ///< seed for RNG
@@ -630,15 +610,15 @@
       }
     }
 
     return make_Quaternion(reals[0], reals[1], reals[2], reals[3]);
   }
 };
 
-/// Computes a random uniform distribution
+/// Computes a random Gaussian distribution
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillRandomUniformFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -649,15 +629,15 @@
   TensorView view;
   RandomUniformFunc<Element> func;
 
   //
   // Methods
   //
 
-  /// Construction of uniform RNG functor.
+  /// Construction of Gaussian RNG functor.
   TensorFillRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>()
   ):
     view(view_), func(func_) {
 
   }
@@ -665,15 +645,15 @@
   /// Compute random value and update RNG state
   void operator()(Coord<Layout::kRank> const &coord) const {
 
     view.at(coord) = func();
   }
 };
 
-/// Fills the upper or lower part of a symmetric rank-2 tensor with random values of a uniform distribution.
+/// Computes a random Gaussian distribution
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 struct TensorFillSymmetricRandomUniformFunc {
 
   using TensorView = TensorView<Element, Layout>;
 
@@ -685,15 +665,15 @@
   RandomUniformFunc<Element> func;
   cutlass::FillMode fill_mode;
 
   //
   // Methods
   //
 
-  /// Construction of uniform RNG functor.
+  /// Construction of Gaussian RNG functor.
   TensorFillSymmetricRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>(),
     cutlass::FillMode fill_mode_ = cutlass::FillMode::kInvalid
   ):
     view(view_), func(func_), fill_mode(fill_mode_) {
 
@@ -731,15 +711,15 @@
   cutlass::FillMode fill_mode;
   int alignment;
 
   //
   // Methods
   //
 
-  /// Construction of uniform RNG functor.
+  /// Construction of Gaussian RNG functor.
   TensorFillPadDiagonalRandomUniformFunc(
     TensorView view_ = TensorView(),
     RandomUniformFunc<Element> func_ = RandomUniformFunc<Element>(),
     cutlass::FillMode fill_mode_ = cutlass::FillMode::kInvalid,
     int alignment_ = 1
   ):
     view(view_), func(func_), fill_mode(fill_mode_), alignment(alignment_) {
@@ -763,15 +743,15 @@
   }
 };
 
 } // namespace detail
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Fills a tensor with random values of a uniform random distribution.
+/// Fills a tensor with random values with a uniform random distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillRandomUniform(
   TensorView<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                          ///< seed for RNG
   double max = 1,                         ///< upper bound of distribution
@@ -788,15 +768,15 @@
 
   TensorForEach(
     dst.extent(),
     func
   );
 }
 
-/// Fills a tensor with random values of a uniform random distribution.
+/// Fills a tensor with random values with a uniform random distribution.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillRandomUniform(
   TensorViewPlanarComplex<Element, Layout> dst,        ///< destination tensor
   uint64_t seed,                                       ///< seed for RNG
   double max = 1,                                      ///< upper bound of distribution
@@ -988,15 +968,15 @@
     func
   );
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 ///////////////////////////////////////////////////////////////////////////////////////////////////
 
-/// Helper to fill a tensor's diagonal with 1 and 0 everywhere else.
+/// Helper to fill a tensor's digonal with 1 and 0 everywhere else.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorFillIdentity(
   TensorView<Element, Layout> dst) {               ///< destination tensor
 
   TensorFillDiagonal(dst, Element(1), Element(0));
@@ -1268,15 +1248,15 @@
 
 namespace detail {
 
 template <typename Element>
 struct RandomSparseMetaFunc {
   
   uint64_t seed;
-  int range;
+  double range;
   int MetaSizeInBits;
 
   //
   // Methods
   //
 
   RandomSparseMetaFunc(
@@ -1298,16 +1278,17 @@
     Element TwoToOneMeta[2] = {0x4, 0xe};
 
     Element * MetaArray = (MetaSizeInBits == 2) ? FourToTwoMeta : TwoToOneMeta;
 
     Element result = 0x0;
 
     for (int i = 0; i < cutlass::sizeof_bits<Element>::value / 4; ++i) {
-      int rnd = std::rand() % range;
-      Element meta = MetaArray[rnd];
+      double rnd = double(std::rand()) / double(RAND_MAX);
+      rnd = range * rnd;
+      Element meta = MetaArray[(int)rnd];
 
       result = (Element)(result | ((Element)(meta << (i * 4))));
     }
 
     return result;
   }
 };
@@ -1389,45 +1370,14 @@
 
   for (size_t i = 0; i < capacity; ++i) {
     ptr[i] = random_func();
   }
 }
 
 ///////////////////////////////////////////////////////////////////////////////////////////////////
-///////////////////////////////////////////////////////////////////////////////////////////////////
-
-/// Fills a ell block index matrix with random values with a uniform random distribution.
-template <
-  typename Element,                                ///< Element type
-  typename Layout>                                 ///< Layout function
-void TensorFillRandomEllIdx(
-  TensorView<Element, Layout> dst,                 ///< destination tensor
-  uint64_t seed,                                   ///< seed for RNG
-  int rows, int ell_cols, int cols) {              ///< dimension of the matrix 
-
-  std::srand((unsigned)seed);
-
-  for (int i = 0; i < rows; ++i) {
-    int col_idx = std::rand() % cols;
-   
-    for (int j = 0; j < ell_cols; ++j) {
-      dst.at({i, j}) = col_idx;
-
-      if (col_idx != -1) {
-        if (col_idx == (cols - 1)) {
-          col_idx = -1;
-        } else {
-          col_idx = std::rand() % (cols - col_idx - 1) + col_idx + 1;
-        }
-      }
-    }
-  }
-}
-
-///////////////////////////////////////////////////////////////////////////////////////////////////
 
 /// Copies a diagonal in from host memory without modifying off-diagonal elements.
 template <
   typename Element,               ///< Element type
   typename Layout>                ///< Layout function
 void TensorCopyDiagonalIn(
   TensorView<Element, Layout> dst,          ///< destination tensor
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_foreach.h`

 * *Files 1% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
@@ -65,15 +65,15 @@
 /// Helper to perform for-each operation
 template <typename Func, int Rank>
 struct TensorForEachHelper<Func, Rank, 0> {
 
   /// Index of the active rank
   static int const kActiveRank = Rank - 1;
 
-  /// Constructor for fastest changing rank
+  /// Constructor for fastest chaning rank
   TensorForEachHelper(
     Func &func,
     Coord<Rank> const &extent,
     Coord<Rank> &coord) {
 
     for (int i = 0; i < extent.at(kActiveRank); ++i) {
       coord[kActiveRank] = i;
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_norm.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_blockmask.h`

 * *Files 24% similar despite different names*

```diff
@@ -1,42 +1,57 @@
-/***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
- * SPDX-License-Identifier: BSD-3-Clause
- *
+/******************************************************************************
+ * Copyright (c) 2011-2021, NVIDIA CORPORATION.  All rights reserved.
+ * 
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
+ *     * Redistributions of source code must retain the above copyright
+ *       notice, this list of conditions and the following disclaimer.
+ *     * Redistributions in binary form must reproduce the above copyright
+ *       notice, this list of conditions and the following disclaimer in the
+ *       documentation and/or other materials provided with the distribution.
+ *     * Neither the name of the NVIDIA CORPORATION nor the
+ *       names of its contributors may be used to endorse or promote products
+ *       derived from this software without specific prior written permission.
+ * 
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
+ * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+ * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+ * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
+ * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+ * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
+ * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
+ * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
+ * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
  *
- * 1. Redistributions of source code must retain the above copyright notice, this
- * list of conditions and the following disclaimer.
- *
- * 2. Redistributions in binary form must reproduce the above copyright notice,
- * this list of conditions and the following disclaimer in the documentation
- * and/or other materials provided with the distribution.
- *
- * 3. Neither the name of the copyright holder nor the names of its
- * contributors may be used to endorse or promote products derived from
- * this software without specific prior written permission.
- *
- * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
- * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
- * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
- * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
- * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
- * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
- * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
- * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
- * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
- * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
- *
- **************************************************************************************************/
+ ******************************************************************************/
+
 #pragma once
 
+#include <fmha.h>
+#include <fmha/utils.h>
+#include <fmha/smem_tile.h>
+#include <fmha/gmem_tile.h>
+#include <fmha/mask.h>
+#include <fmha/softmax.h>
+
+namespace fmha {
+
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
-#include "cutlass/cutlass.h"
+struct Blockmask {
 
-// The contents of this file have been moved  to 'tensor_reduce' to cover other types of reductions.
+    template<typename Params>
+    __device__ Blockmask(const Params &params, int loop_step_idx) :
+        blockmask_ptr(params.blockmask + loop_step_idx * params.seqlen_q / 16) {
+    }
 
-#include "cutlass/util/reference/host/tensor_reduce.h"
+    __device__ int mask_val(int block_row_idx) const {
+        return blockmask_ptr[block_row_idx];
+    }
 
-///////////////////////////////////////////////////////////////////////////////////////////////////
+    const int *blockmask_ptr;
+};
 
+////////////////////////////////////////////////////////////////////////////////////////////////////
 
+}  // namespace fmha
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/tensor_reduce.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/reference/host/trmm_complex.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/tensor_view_io.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h` & `flash_attn-1.0.1/csrc/flash_attn/cutlass/tools/util/include/cutlass/util/type_traits.h`

 * *Files 0% similar despite different names*

```diff
@@ -1,9 +1,9 @@
 /***************************************************************************************************
- * Copyright (c) 2017 - 2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
+ * Copyright (c) 2017 - 2022 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
  * SPDX-License-Identifier: BSD-3-Clause
  *
  * Redistribution and use in source and binary forms, with or without
  * modification, are permitted provided that the following conditions are met:
  *
  * 1. Redistributions of source code must retain the above copyright notice, this
  * list of conditions and the following disclaimer.
```

### Comparing `flash_attn-1.0.0/csrc/flash_attn/flash_api.cpp` & `flash_attn-1.0.1/csrc/flash_attn/flash_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/fmha_api.cpp` & `flash_attn-1.0.1/csrc/flash_attn/fmha_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/block_info.h` & `flash_attn-1.0.1/csrc/flash_attn/src/block_info.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_bf16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim128_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim64_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_hdim96_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_bak.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_bak.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_new.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_new.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_kernel_reverse.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_kernel_reverse.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_bwd_launch_template.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_bwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_bf16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim128_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim160_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim192_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim32_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_bf16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim64_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_fp16_sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_hdim96_sm80_fp16.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_kernel.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_kernel_old.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_kernel_old.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/flash_fwd_launch_template.h` & `flash_attn-1.0.1/csrc/flash_attn/src/flash_fwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/gemm.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/gemm.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/gmem_tile.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/gmem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/kernel_traits.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/mask.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/mask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/smem_tile.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/smem_tile.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/softmax.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha/utils.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_fp16_kernel_loop.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_dgrad_kernel_1xN_loop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_fp16_kernel.sm80.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_block_fprop_kernel_1xN.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_hdim32.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_hdim64.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_bwd_launch_template.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_bwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_dgrad_kernel_1xN_loop.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fprop_kernel_1xN.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_hdim32.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim32.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_hdim64.cu` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_hdim64.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_fwd_launch_template.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_fwd_launch_template.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_kernel.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_kernel.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/fmha_utils.h` & `flash_attn-1.0.1/csrc/flash_attn/src/fmha_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/kernel_traits.h` & `flash_attn-1.0.1/csrc/flash_attn/src/kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/mask.h` & `flash_attn-1.0.1/csrc/flash_attn/src/mask.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/philox.cuh` & `flash_attn-1.0.1/csrc/flash_attn/src/philox.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/softmax.h` & `flash_attn-1.0.1/csrc/flash_attn/src/softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/static_switch.h` & `flash_attn-1.0.1/csrc/flash_attn/src/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_attn/src/utils.h` & `flash_attn-1.0.1/csrc/flash_attn/src/utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/flash_gen/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.1/csrc/flash_gen/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/cuda_bf16_fallbacks.cuh` & `flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_fallbacks.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/cuda_bf16_wrapper.h` & `flash_attn-1.0.1/csrc/ft_attention/cuda_bf16_wrapper.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention.cu` & `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention.h` & `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/decoder_masked_multihead_attention_utils.h` & `flash_attn-1.0.1/csrc/ft_attention/decoder_masked_multihead_attention_utils.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/ft_attention/ft_attention.cpp` & `flash_attn-1.0.1/csrc/ft_attention/ft_attention.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_dense_lib/fused_dense.cpp` & `flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_dense_lib/fused_dense_cuda.cu` & `flash_attn-1.0.1/csrc/fused_dense_lib/fused_dense_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/fused_softmax.cpp` & `flash_attn-1.0.1/csrc/fused_softmax/fused_softmax.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/scaled_masked_softmax.h` & `flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/scaled_masked_softmax_cuda.cu` & `flash_attn-1.0.1/csrc/fused_softmax/scaled_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h` & `flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu` & `flash_attn-1.0.1/csrc/fused_softmax/scaled_upper_triang_masked_softmax_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/fused_softmax/type_shim.h` & `flash_attn-1.0.1/csrc/fused_softmax/type_shim.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln.h` & `flash_attn-1.0.1/csrc/layer_norm/ln.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_api.cpp` & `flash_attn-1.0.1/csrc/layer_norm/ln_api.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1024.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1280.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_1536.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_2048.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_256.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_2560.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_3072.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_4096.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_512.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_5120.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_6144.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_7168.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_768.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_8192.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_kernels.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_bwd_semi_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1024.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_10240.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_10240.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_12288.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_12288.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_128.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1280.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_1536.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_2048.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_256.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_2560.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_3072.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_384.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_384.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_4096.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_512.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_5120.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_6144.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_7168.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_768.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_8192.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_9216.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_9216.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_cuda_kernel_old.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_fwd_kernels.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_kernel_traits.h` & `flash_attn-1.0.1/csrc/layer_norm/ln_kernel_traits.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1024.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1280.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_1536.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_2048.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_256.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_2560.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_3072.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_4096.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_512.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_5120.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_6144.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_7168.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_768.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_bwd_8192.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_bwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1024.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1024.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_128.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_128.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1280.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1280.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_1536.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_1536.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_2048.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2048.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_256.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_256.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_2560.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_2560.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_3072.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_3072.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_4096.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_4096.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_512.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_5120.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_5120.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_6144.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_6144.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_7168.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_7168.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_768.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_768.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_fwd_8192.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_fwd_8192.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_res_fwd_kernel.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_bwd_512.cu` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_512.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_bwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernel.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_parallel_residual_fwd_kernels.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/ln_utils.cuh` & `flash_attn-1.0.1/csrc/layer_norm/ln_utils.cuh`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/layer_norm/static_switch.h` & `flash_attn-1.0.1/csrc/layer_norm/static_switch.h`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/rotary/rotary.cpp` & `flash_attn-1.0.1/csrc/rotary/rotary.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/rotary/rotary_cuda.cu` & `flash_attn-1.0.1/csrc/rotary/rotary_cuda.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/xentropy/interface.cpp` & `flash_attn-1.0.1/csrc/xentropy/interface.cpp`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/csrc/xentropy/xentropy_kernel.cu` & `flash_attn-1.0.1/csrc/xentropy/xentropy_kernel.cu`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/attention_kernl.py` & `flash_attn-1.0.1/flash_attn/attention_kernl.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/bert_padding.py` & `flash_attn-1.0.1/flash_attn/bert_padding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attention.py` & `flash_attn-1.0.1/flash_attn/flash_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_interface.py` & `flash_attn-1.0.1/flash_attn/flash_attn_interface.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton_og.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton_single_query.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton_single_query.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton_tmp.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton_tmp_og.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton_tmp_og.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_attn_triton_varlen.py` & `flash_attn-1.0.1/flash_attn/flash_attn_triton_varlen.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_blocksparse_attention.py` & `flash_attn-1.0.1/flash_attn/flash_blocksparse_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/flash_blocksparse_attn_interface.py` & `flash_attn-1.0.1/flash_attn/flash_blocksparse_attn_interface.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/fused_softmax.py` & `flash_attn-1.0.1/flash_attn/fused_softmax.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/layers/patch_embed.py` & `flash_attn-1.0.1/flash_attn/layers/patch_embed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/layers/rotary.py` & `flash_attn-1.0.1/flash_attn/layers/rotary.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/losses/cross_entropy.py` & `flash_attn-1.0.1/flash_attn/losses/cross_entropy.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/losses/cross_entropy_apex.py` & `flash_attn-1.0.1/flash_attn/losses/cross_entropy_apex.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/losses/cross_entropy_parallel.py` & `flash_attn-1.0.1/flash_attn/losses/cross_entropy_parallel.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/bert.py` & `flash_attn-1.0.1/flash_attn/models/bert.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/gpt.py` & `flash_attn-1.0.1/flash_attn/models/gpt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/gpt_j.py` & `flash_attn-1.0.1/flash_attn/models/gpt_j.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/gpt_neox.py` & `flash_attn-1.0.1/flash_attn/models/gpt_neox.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/gptj.py` & `flash_attn-1.0.1/flash_attn/models/gptj.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/opt.py` & `flash_attn-1.0.1/flash_attn/models/opt.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/models/vit.py` & `flash_attn-1.0.1/flash_attn/models/vit.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/modules/block.py` & `flash_attn-1.0.1/flash_attn/modules/block.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/modules/embedding.py` & `flash_attn-1.0.1/flash_attn/modules/embedding.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/modules/mha.py` & `flash_attn-1.0.1/flash_attn/modules/mha.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/modules/mlp.py` & `flash_attn-1.0.1/flash_attn/modules/mlp.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/ops/fused_dense.py` & `flash_attn-1.0.1/flash_attn/ops/fused_dense.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/ops/gelu_activation.py` & `flash_attn-1.0.1/flash_attn/ops/gelu_activation.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/ops/layer_norm.py` & `flash_attn-1.0.1/flash_attn/ops/layer_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/ops/rms_norm.py` & `flash_attn-1.0.1/flash_attn/ops/rms_norm.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/rotary.py` & `flash_attn-1.0.1/flash_attn/rotary.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/triton/fused_attention.py` & `flash_attn-1.0.1/flash_attn/triton/fused_attention.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/utils/benchmark.py` & `flash_attn-1.0.1/flash_attn/utils/benchmark.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/utils/distributed.py` & `flash_attn-1.0.1/flash_attn/utils/distributed.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/utils/generation.py` & `flash_attn-1.0.1/flash_attn/utils/generation.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn/utils/pretrained.py` & `flash_attn-1.0.1/flash_attn/utils/pretrained.py`

 * *Files identical despite different names*

### Comparing `flash_attn-1.0.0/flash_attn.egg-info/PKG-INFO` & `flash_attn-1.0.1/flash_attn.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: flash-attn
-Version: 1.0.0
+Version: 1.0.1
 Summary: Flash Attention: Fast and Memory-Efficient Exact Attention
 Home-page: https://github.com/HazyResearch/flash-attention
 Author: Tri Dao
 Author-email: trid@stanford.edu
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
```

### Comparing `flash_attn-1.0.0/flash_attn.egg-info/SOURCES.txt` & `flash_attn-1.0.1/flash_attn.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -126,14 +126,17 @@
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/epilogue_predicated_tile_iterator.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/make_residual_last.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_access_iterator_residual_last.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/predicated_tile_iterator_residual_last.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/transpose_warp_iterator.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/iterators/warp_iterator_from_smem.h
 csrc/flash_attn/cutlass/examples/41_fused_multi_head_attention/transform/tile_smem_loader.h
+csrc/flash_attn/cutlass/examples/41_multi_head_attention/fused_multihead_attention.cu
+csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_attention.h
+csrc/flash_attn/cutlass/examples/41_multi_head_attention/gemm_grouped_with_softmax_visitor.h
 csrc/flash_attn/cutlass/examples/42_ampere_tensorop_group_conv/ampere_tensorop_group_conv.cu
 csrc/flash_attn/cutlass/examples/43_ell_block_sparse_gemm/ell_block_sparse_gemm.cu
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/leaky_bias.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/utils.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_bias_act_epilogue_tensor_op.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/default_thread_map_tensor_op_for_fused_bias.h
 csrc/flash_attn/cutlass/examples/44_multi_gemm_ir_and_codegen/fixed_impl/epilogue/threadblock/fused_bias_act_epilogue.h
@@ -651,14 +654,15 @@
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm75.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/conv3d_wgrad_implicit_gemm_tf32ndhwc_tf32ndhwc_f32ndhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_direct_conv_testbed.h
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
+csrc/flash_attn/cutlass/test/unit/conv/device/depthwise_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu
 csrc/flash_attn/cutlass/test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu
 csrc/flash_attn/cutlass/test/unit/core/array.cu
 csrc/flash_attn/cutlass/test/unit/core/bfloat16.cu
 csrc/flash_attn/cutlass/test/unit/core/complex.cu
 csrc/flash_attn/cutlass/test/unit/core/float8.cu
 csrc/flash_attn/cutlass/test/unit/core/functional.cu
 csrc/flash_attn/cutlass/test/unit/core/half.cu
```

### Comparing `flash_attn-1.0.0/setup.py` & `flash_attn-1.0.1/setup.py`

 * *Files 1% similar despite different names*

```diff
@@ -158,15 +158,15 @@
             Path(this_dir) / 'csrc' / 'flash_attn' / 'cutlass' / 'include',
         ],
     )
 )
 
 setup(
     name="flash_attn",
-    version="1.0.0",
+    version="1.0.1",
     packages=find_packages(
         exclude=("build", "csrc", "include", "tests", "dist", "docs", "benchmarks", "flash_attn.egg-info",)
     ),
     author="Tri Dao",
     author_email="trid@stanford.edu",
     description="Flash Attention: Fast and Memory-Efficient Exact Attention",
     long_description=long_description,
```

